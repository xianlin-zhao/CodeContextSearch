{
    "features": [
        {
            "cluster_id": 23,
            "feature_id": 1,
            "feature_desc": "gamma=0.0100; k=1; a=0.25; combined=0.492; stability(ARI)=1.000; sep=0.164",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1032,
                    "func_name": "parse_setup_cmd",
                    "func_desc": "parse_setup_cmd",
                    "func_file": "setup",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def parse_setup_cmd(cmd):\\n    \"\"\"Parse a setup/bootstrap command, finding and pulling out Hadoop\\n    Distributed Cache-style paths (\"hash paths\").\\n\\n    :param string cmd: shell command to parse\\n    :return: a list containing dictionaries (parsed hash paths) and strings\\n             (parts of the original command, left unparsed)\\n\\n    Hash paths look like ``path#name``, where *path* is either a local path\\n    or a URI pointing to something we want to upload to Hadoop/EMR, and *name*\\n    is the name we want it to have when we upload it; *name* is optional\\n    (no name means to pick a unique one).\\n\\n    If *name* is followed by a trailing slash, that indicates *path* is an\\n    archive (e.g. a tarball), and should be unarchived into a directory on the\\n    remote system. The trailing slash will *also* be kept as part of the\\n    original command.\\n\\n    If *path* is followed by a trailing slash, that indicates *path* is a\\n    directory and should be tarballed and later unarchived into a directory\\n    on the remote system. The trailing slash will also be kept as part of\\n    the original command. You may optionally include a slash after *name* as\\n    well (this will only result in a single slash in the final command).\\n\\n    Parsed hash paths are dicitionaries with the keys ``path``, ``name``, and\\n    ``type`` (either ``'file'``, ``'archive'``, or ``'dir'``).\\n\\n    Most of the time, this function will just do what you expect. Rules for\\n    finding hash paths:\\n\\n    * we only look for hash paths outside of quoted strings\\n    * *path* may not contain quotes or whitespace\\n    * *path* may not contain `:` or `=` unless it is a URI (starts with\\n      ``<scheme>://``); this allows you to do stuff like\\n      ``export PYTHONPATH=$PYTHONPATH:foo.egg#``.\\n    * *name* may not contain whitespace or any of the following characters:\\n      ``'\":;><|=/#``, so you can do stuff like\\n      ``sudo dpkg -i fooify.deb#; fooify bar``\\n\\n    If you really want to include forbidden characters, you may use backslash\\n    escape sequences in *path* and *name*. (We can't guarantee Hadoop/EMR\\n    will accept them though!). Also, remember that shell syntax allows you\\n    to concatenate strings ``like\"\"this``.\\n\\n    Environment variables and ``~`` (home dir) in *path* will be resolved\\n    (use backslash escapes to stop this). We don't resolve *name* because it\\n    doesn't make sense. Environment variables and ``~`` elsewhere in the\\n    command are considered to be part of the script and will be resolved\\n    on the remote system.\\n    \"\"\"\\n    tokens = []\\n\\n    for m in _SETUP_CMD_RE.finditer(cmd):\\n        keep_as_is = (m.group('single_quoted') or\\n                      m.group('double_quoted') or\\n                      m.group('unquoted') or\\n                      m.group('whitespace') or\\n                      m.group('colon_or_equals'))\\n\\n        if keep_as_is:\\n            if tokens and isinstance(tokens[-1], string_types):\\n                tokens[-1] += keep_as_is\\n            else:\\n                tokens.append(keep_as_is)\\n        elif m.group('hash_path'):\\n            if m.group('path_slash'):\\n                token_type = 'dir'\\n            elif m.group('name_slash'):\\n                token_type = 'archive'\\n            else:\\n                token_type = 'file'\\n\\n            tokens.append({\\n                'path': _resolve_path(m.group('path')),\\n                'name': m.group('name') or None,\\n                'type': token_type\\n            })\\n\\n            if m.group('path_slash') or m.group('name_slash'):\\n                tokens.append('/')\\n        elif m.group('error'):\\n            # these match the error messages from shlex.split()\\n            if m.group('error').startswith('\\\\'):\\n                raise ValueError('No escaped character')\\n            else:\\n                raise ValueError('No closing quotation')\\n\\n    return tokens",
                    "func_fullName": "mrjob.setup.parse_setup_cmd( cmd )"
                },
                {
                    "func_id": 1033,
                    "func_name": "_resolve_path",
                    "func_desc": "_resolve_path",
                    "func_file": "setup",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _resolve_path(path):\\n    \"\"\"Helper for :py:func:`parse_setup_cmd`.\\n\\n    Resolve ``~`` (home dir) and environment variables in the\\n    given path, and unescape backslashes.\"\"\"\\n    result = ''\\n\\n    for m in _ESCAPE_RE.finditer(path):\\n        if m.group('escaped'):\\n            result += m.group('escaped')\\n        elif m.group('unescaped'):\\n            result += expand_path(m.group('unescaped'))\\n        else:\\n            raise ValueError('No escaped character')\\n\\n    return result",
                    "func_fullName": "mrjob.setup._resolve_path( path )"
                },
                {
                    "func_id": 1034,
                    "func_name": "parse_legacy_hash_path",
                    "func_desc": "parse_legacy_hash_path",
                    "func_file": "setup",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def parse_legacy_hash_path(type, path, must_name=None):\\n    \"\"\"Parse hash paths from old setup/bootstrap options.\\n\\n    This is similar to parsing hash paths out of shell commands (see\\n    :py:func:`parse_setup_cmd`) except that we pass in\\n    path type explicitly, and we don't always require the ``#`` character.\\n\\n    :param type: Type of the path (``'archive'`` or ``'file'``)\\n    :param path: Path to parse, possibly with a ``#``\\n    :param must_name: If set, use *path*'s filename as its name if there\\n                      is no ``'#'`` in *path*, and raise an exception\\n                      if there is just a ``'#'`` with no name. Set *must_name*\\n                      to the name of the relevant option so we can print\\n                      a useful error message. This is intended for options\\n                      like ``upload_files`` that merely upload a file\\n                      without tracking it.\\n    \"\"\"\\n    if type not in _SUPPORTED_TYPES:\\n        raise ValueError('bad path type %r, must be one of %s' % (\\n            type, ', '.join(sorted(_SUPPORTED_TYPES))))\\n\\n    if '#' in path:\\n        path, name = path.split('#', 1)\\n\\n        # allow a slash after the name of an archive or dir because that's\\n        # the new-way of specifying archive paths\\n        if type in ('archive', 'dir'):\\n            name = name.rstrip('/' + os.sep)\\n\\n        if '/' in name or '#' in name:\\n            raise ValueError(\\n                'Bad path %r; name must not contain # or /' % (path,))\\n    else:\\n        if must_name:\\n            if type == 'dir':\\n                # handle trailing slash on directory names\\n                name = os.path.basename(path.rstrip('/' + os.sep))\\n            else:\\n                name = os.path.basename(path)\\n        else:\\n            name = None\\n\\n    if not path:\\n        raise ValueError('Path may not be empty!')\\n\\n    if not name:\\n        if must_name:\\n            raise ValueError(\\n                'Empty name makes no sense for %s: %r' % (must_name, path))\\n        else:\\n            name = None\\n\\n    return {'path': path, 'name': name, 'type': type}",
                    "func_fullName": "mrjob.setup.parse_legacy_hash_path( type, path, must_name )"
                },
                {
                    "func_id": 1035,
                    "func_name": "name_uniquely",
                    "func_desc": "name_uniquely",
                    "func_file": "setup",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def name_uniquely(path, names_taken=(), proposed_name=None, unhide=False,\\n                  strip_ext=False, suffix=''):\\n    \"\"\"Come up with a unique name for *path*.\\n\\n    :param names_taken: a dictionary or set of names not to use.\\n    :param proposed_name: name to use if it is not taken. If this is not set,\\n                          we propose a name based on the filename.\\n    :param unhide: make sure final name doesn't start with periods or\\n                   underscores\\n    :param strip_ext: if we propose a name, it shouldn't have a file extension\\n    :param suffix: if set to a string, add this to the end of any filename\\n                    we propose. Should include the ``.``.\\n\\n    If the proposed name is taken, we add a number to the end of the\\n    filename, keeping the extension the same. For example:\\n\\n    >>> name_uniquely('foo.txt', {'foo.txt'})\\n    'foo-1.txt'\\n    >>> name_uniquely('bar.tar.gz', {'bar'}, strip_ext=True)\\n    'bar-1'\\n    \"\"\"\\n    filename = proposed_name or os.path.basename(path.rstrip('/' + os.sep))\\n    ext = file_ext(filename)\\n    prefix = filename[:-len(ext) or None]\\n\\n    if strip_ext and not proposed_name:\\n        ext = ''\\n\\n    if suffix and not proposed_name:\\n        ext += suffix\\n\\n    if unhide:\\n        prefix = prefix.lstrip('.').lstrip('_')\\n\\n    # is our proposed name taken?\\n    name = prefix + ext\\n    if prefix and name not in names_taken:\\n        return name\\n\\n    # add 1, 2, etc. to the name until it's not taken\\n    for i in itertools.count(1):\\n        if prefix:\\n            name = '%s-%d%s' % (prefix, i, ext)\\n        else:\\n            # if no prefix is left (due to empty filename or unhiding)\\n            # just use numbers; don't start filenames with '-'\\n            name = '%d%s' % (i, ext)\\n        if name not in names_taken:\\n            return name",
                    "func_fullName": "mrjob.setup.name_uniquely( path, names_taken, proposed_name, unhide, strip_ext, suffix )"
                },
                {
                    "func_id": 1047,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "setup",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, prefix):\\n        \"\"\"Make an :py:class`UploadDirManager`.\\n\\n        :param string prefix: The URI for the directory (e.g.\\n                              `s3://bucket/dir/`). It doesn't matter if\\n                              *prefix* has a trailing slash; :py:meth:`uri`\\n                              will do the right thing.\\n        \"\"\"\\n        self.prefix = prefix\\n\\n        self._path_to_name = {}\\n        self._names_taken = set()",
                    "func_fullName": "mrjob.setup.__init__( self, prefix )"
                },
                {
                    "func_id": 1048,
                    "func_name": "add",
                    "func_desc": "add",
                    "func_file": "setup",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def add(self, path):\\n        \"\"\"Add a path. If *path* hasn't been added before, assign it a name.\\n                       If *path* is a URI don't add it; just return the URI.\\n\\n        :return: the URI assigned to the path\"\"\"\\n        if is_uri(path):\\n            return path\\n\\n        if path not in self._path_to_name:\\n            # use unhide so that input files won't be hidden from Hadoop,\\n            # see #1200\\n            name = name_uniquely(\\n                path, names_taken=self._names_taken, unhide=True)\\n            self._names_taken.add(name)\\n            self._path_to_name[path] = name\\n\\n        return self.uri(path)",
                    "func_fullName": "mrjob.setup.add( self, path )"
                },
                {
                    "func_id": 1049,
                    "func_name": "uri",
                    "func_desc": "uri",
                    "func_file": "setup",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def uri(self, path):\\n        \"\"\"Get the URI for the given path. If *path* is a URI, just return it.\\n        \"\"\"\\n        if is_uri(path):\\n            return path\\n\\n        if path in self._path_to_name:\\n            return posixpath.join(self.prefix, self._path_to_name[path])\\n        else:\\n            raise ValueError('%r is not a URI or a known local file' % (path,))",
                    "func_fullName": "mrjob.setup.uri( self, path )"
                },
                {
                    "func_id": 1050,
                    "func_name": "path_to_uri",
                    "func_desc": "path_to_uri",
                    "func_file": "setup",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def path_to_uri(self):\\n        \"\"\"Get a map from path to URI for all paths that were added,\\n        so we can figure out which files we need to upload.\"\"\"\\n        return dict((path, self.uri(path))\\n                    for path in self._path_to_name)",
                    "func_fullName": "mrjob.setup.path_to_uri( self )"
                },
                {
                    "func_id": 1051,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "setup",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, archive_file_suffix=''):\\n        # map from paths added without a name to None or lazily chosen name\\n        self._typed_path_to_auto_name = {}\\n        self._name_to_typed_path = {}\\n        self._archive_file_suffix = archive_file_suffix",
                    "func_fullName": "mrjob.setup.__init__( self, archive_file_suffix )"
                },
                {
                    "func_id": 1052,
                    "func_name": "add",
                    "func_desc": "add",
                    "func_file": "setup",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def add(self, type, path, name=None):\\n        \"\"\"Add a path as either a file or an archive, optionally\\n        assigning it a name.\\n\\n        :param type: either ``'archive'`` or ``'file'``\\n        :param path: path/URI to add\\n        :param name: optional name that this path *must* be assigned, or\\n                     None to assign this file a name later.\\n\\n        if *type* is ``archive``, we'll also add *path* as an\\n        auto-named ``archive_file``. This reserves space in the working dir\\n        in case we need to copy the archive into the working dir and\\n        un-archive it ourselves.\\n        \"\"\"\\n        self._check_name(name)\\n        self._check_type(type)\\n\\n        # stop name collisions\\n        if name in self._name_to_typed_path:\\n            current_type, current_path = self._name_to_typed_path[name]\\n\\n            if (type, path) == (current_type, current_path):\\n                return  # already added\\n            else:\\n                raise ValueError(\\n                    \"%s %s#%s won't work because we already have %s %s#%s\" % (\\n                        type, path, name, current_type, current_path, name))\\n\\n        # if a name was specified, reserve it\\n        if name:\\n            self._name_to_typed_path[name] = (type, path)\\n        # otherwise, get ready to auto-name the file\\n        else:\\n            self._typed_path_to_auto_name.setdefault((type, path), None)\\n\\n        # reserve a name for the archive to be copied to, in case we need\\n        # to un-archive it ourselves\\n        if type == 'archive':\\n            self.add('archive_file', path)",
                    "func_fullName": "mrjob.setup.add( self, type, path, name )"
                },
                {
                    "func_id": 1053,
                    "func_name": "name",
                    "func_desc": "name",
                    "func_file": "setup",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def name(self, type, path, name=None):\\n        \"\"\"Get the name for a path previously added to this\\n        :py:class:`WorkingDirManager`, assigning one as needed.\\n\\n        This is primarily for getting the name of auto-named files. If\\n        the file was added with an assigned name, you must include it\\n        (and we'll just return *name*).\\n\\n        We won't ever give an auto-name that's the same an assigned name\\n        (even for the same path and type).\\n\\n        :param type: either ``'archive'`` or ``'file'``\\n        :param path: path/URI\\n        :param name: known name of the file\\n        \"\"\"\\n        self._check_name(name)\\n        self._check_type(type)\\n\\n        if name:\\n            if name not in self._name_to_typed_path:\\n                raise ValueError('unknown name: %r' % name)\\n\\n            return name\\n\\n        if (type, path) not in self._typed_path_to_auto_name:\\n            # print useful error message\\n            if (type, path) in self._name_to_typed_path.values():\\n                raise ValueError('%s %r was never added without a name!' %\\n                                 (type, path))\\n            else:\\n                raise ValueError('%s %r was never added!' % (type, path))\\n\\n        if not self._typed_path_to_auto_name[(type, path)]:\\n            strip_ext = False\\n            suffix = ''\\n\\n            if type == 'archive':\\n                # weird to have .tar.gz etc. in a directory name\\n                strip_ext = True\\n            elif type == 'archive_file':\\n                # keep Spark from auto-untarring by adding .file\\n                suffix = self._archive_file_suffix\\n\\n            name = name_uniquely(\\n                path, names_taken=self._name_to_typed_path,\\n                strip_ext=strip_ext, suffix=suffix)\\n\\n            self._name_to_typed_path[name] = (type, path)\\n            self._typed_path_to_auto_name[(type, path)] = name\\n\\n        return self._typed_path_to_auto_name[(type, path)]",
                    "func_fullName": "mrjob.setup.name( self, type, path, name )"
                },
                {
                    "func_id": 1054,
                    "func_name": "name_to_path",
                    "func_desc": "name_to_path",
                    "func_file": "setup",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def name_to_path(self, type=None):\\n        \"\"\"Get a map from name (in the setup directory) to path for\\n        all known files/archives, so we can build :option:`-file` and\\n        :option:`-archive` options to Hadoop (or fake them in a bootstrap\\n        script).\\n\\n        :param type: either ``'archive'`` or ``'file'``\\n        \"\"\"\\n        if type is not None:\\n            self._check_type(type)\\n\\n        for path_type, path in self._typed_path_to_auto_name:\\n            if type is None or path_type == type:\\n                self.name(path_type, path)\\n\\n        return dict((name, typed_path[1])\\n                    for name, typed_path\\n                    in self._name_to_typed_path.items()\\n                    if (type is None or typed_path[0] == type))",
                    "func_fullName": "mrjob.setup.name_to_path( self, type )"
                },
                {
                    "func_id": 1055,
                    "func_name": "paths",
                    "func_desc": "paths",
                    "func_file": "setup",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def paths(self, type=None):\\n        \"\"\"Get a set of all paths tracked by this WorkingDirManager.\"\"\"\\n        paths = set()\\n\\n        for path_type, path in self._typed_path_to_auto_name:\\n            if type is None or path_type == type:\\n                paths.add(path)\\n\\n        for path_type, path in self._name_to_typed_path.values():\\n            if type is None or path_type == type:\\n                paths.add(path)\\n\\n        return paths",
                    "func_fullName": "mrjob.setup.paths( self, type )"
                },
                {
                    "func_id": 1056,
                    "func_name": "_check_name",
                    "func_desc": "_check_name",
                    "func_file": "setup",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _check_name(self, name):\\n        if name is None:\\n            return\\n\\n        if not isinstance(name, string_types):\\n            raise TypeError('name must be a string or None: %r' % (name,))\\n\\n        if '/' in name:\\n            raise ValueError('names may not contain slashes: %r' % (name,))",
                    "func_fullName": "mrjob.setup._check_name( self, name )"
                },
                {
                    "func_id": 1057,
                    "func_name": "_check_type",
                    "func_desc": "_check_type",
                    "func_file": "setup",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _check_type(self, type):\\n        if type not in self._SUPPORTED_TYPES:\\n            raise ValueError('bad path type %r, must be one of: %s' % (\\n                type, ', '.join(sorted(self._SUPPORTED_TYPES))))",
                    "func_fullName": "mrjob.setup._check_type( self, type )"
                },
                {
                    "func_id": 1725,
                    "func_name": "_check_for_nonzero_return_code",
                    "func_desc": "_check_for_nonzero_return_code",
                    "func_file": "bootstrap",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _check_for_nonzero_return_code(reason):\\n    \"\"\"Given a reason for cluster termination, check if it's because\\n    a bootstrap action terminated with an error.\\n\\n    If it is, return a dictionary with the keys action_num (0-indexed\\n    bootstrap action number) and node_id (a string). Otherwise return None.\\n    \"\"\"\\n    m = _BOOTSTRAP_NONZERO_RETURN_CODE_RE.match(reason)\\n\\n    if m:\\n        return _extract_action_num_and_node_id(m)\\n    else:\\n        return None",
                    "func_fullName": "mrjob.logs.bootstrap._check_for_nonzero_return_code( reason )"
                },
                {
                    "func_id": 1726,
                    "func_name": "_ls_emr_bootstrap_stderr_logs",
                    "func_desc": "_ls_emr_bootstrap_stderr_logs",
                    "func_file": "bootstrap",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _ls_emr_bootstrap_stderr_logs(\\n        fs, log_dir_stream, action_num=None, node_id=None):\\n    \"\"\"Find all stderr from bootstrap actions in the given dir. Sort\\n    so the most recent one comes first, using node ID as a tiebreaker.\\n\\n    (In practice, we look at a single a action on a single node anyway.)\\n    \"\"\"\\n    matches = _ls_logs(fs, log_dir_stream, _match_emr_bootstrap_stderr_path,\\n                       action_num=None, node_id=None)\\n\\n    return sorted(matches, key=lambda m: (-m['action_num'], m['node_id']))",
                    "func_fullName": "mrjob.logs.bootstrap._ls_emr_bootstrap_stderr_logs( fs, log_dir_stream, action_num, node_id )"
                },
                {
                    "func_id": 1727,
                    "func_name": "_match_emr_bootstrap_stderr_path",
                    "func_desc": "_match_emr_bootstrap_stderr_path",
                    "func_file": "bootstrap",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _match_emr_bootstrap_stderr_path(path, node_id=None, action_num=None):\\n    \"\"\"If *path* corresponds to a bootstrap stderr file, return a dict\\n    with the keys *action_num* (an 0-indexed int) and *node_id*. Otherwise\\n    return None.\\n\\n    Optionally, filter by *action_num* and *node_id*.\\n    \"\"\"\\n    m = _EMR_BOOTSTRAP_STDERR_PATH_RE.match(path)\\n    if not m:\\n        return\\n\\n    result = _extract_action_num_and_node_id(m)\\n\\n    if action_num is not None and action_num != result['action_num']:\\n        return None\\n\\n    if node_id is not None and node_id != result['node_id']:\\n        return None\\n\\n    return result",
                    "func_fullName": "mrjob.logs.bootstrap._match_emr_bootstrap_stderr_path( path, node_id, action_num )"
                },
                {
                    "func_id": 1728,
                    "func_name": "_interpret_emr_bootstrap_stderr",
                    "func_desc": "_interpret_emr_bootstrap_stderr",
                    "func_file": "bootstrap",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _interpret_emr_bootstrap_stderr(fs, matches, partial=True):\\n    \"\"\"Extract errors from bootstrap stderr.\\n\\n    If *partial* is true, stop when we find the first match.\\n\\n    (In practice, we usually target a single file anyway.)\\n    \"\"\"\\n    result = {}\\n\\n    for match in matches:\\n        stderr_path = match['path']\\n\\n        task_error = _parse_task_stderr(_cat_log_lines(fs, stderr_path))\\n        if task_error:\\n            task_error = dict(task_error)  # make a copy\\n            task_error['path'] = stderr_path\\n            error = dict(\\n                action_num=match['action_num'],\\n                node_id=match['node_id'],\\n                task_error=task_error)\\n            result.setdefault('errors', [])\\n            result['errors'].append(error)\\n\\n            if partial:\\n                result['partial'] = True\\n                break\\n\\n    return result",
                    "func_fullName": "mrjob.logs.bootstrap._interpret_emr_bootstrap_stderr( fs, matches, partial )"
                },
                {
                    "func_id": 1729,
                    "func_name": "_extract_action_num_and_node_id",
                    "func_desc": "_extract_action_num_and_node_id",
                    "func_file": "bootstrap",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _extract_action_num_and_node_id(m):\\n    \"\"\"Helper method: Extract *action_num* and *node_id* from the given regex\\n    match. Convert *action_num* to a 0-indexed integer.\"\"\"\\n    return dict(\\n        action_num=(int(m.group('action_num')) - 1),\\n        node_id=m.group('node_id'),\\n    )",
                    "func_fullName": "mrjob.logs.bootstrap._extract_action_num_and_node_id( m )"
                }
            ]
        },
        {
            "cluster_id": 41,
            "feature_id": 2,
            "feature_desc": "gamma=0.0273; k=2; a=0.25; combined=0.435; stability(ARI)=1.000; sep=0.058",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 0,
                    "func_name": "_cluster_state_name",
                    "func_desc": "_cluster_state_name",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _cluster_state_name(state_value):\\n    return google.cloud.dataproc_v1beta2.types.ClusterStatus.State.Name(\\n        state_value)",
                    "func_fullName": "mrjob.dataproc._cluster_state_name( state_value )"
                },
                {
                    "func_id": 2,
                    "func_name": "_gcp_zone_uri",
                    "func_desc": "_gcp_zone_uri",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _gcp_zone_uri(project, zone):\\n    return (\\n        'https://www.googleapis.com/compute/%(gce_api_version)s/projects/'\\n        '%(project)s/zones/%(zone)s' % dict(\\n            gce_api_version=_GCE_API_VERSION, project=project, zone=zone))",
                    "func_fullName": "mrjob.dataproc._gcp_zone_uri( project, zone )"
                },
                {
                    "func_id": 3,
                    "func_name": "_gcp_instance_group_config",
                    "func_desc": "_gcp_instance_group_config",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _gcp_instance_group_config(\\n        project, zone, count, instance_type, is_preemptible=False):\\n    if zone:\\n        zone_uri = _gcp_zone_uri(project, zone)\\n        machine_type = \"%(zone_uri)s/machineTypes/%(machine_type)s\" % dict(\\n            zone_uri=zone_uri, machine_type=instance_type)\\n    else:\\n        machine_type = instance_type\\n\\n    return dict(\\n        num_instances=count,\\n        machine_type_uri=machine_type,\\n        is_preemptible=is_preemptible\\n    )",
                    "func_fullName": "mrjob.dataproc._gcp_instance_group_config( project, zone, count, instance_type, is_preemptible )"
                },
                {
                    "func_id": 7,
                    "func_name": "_zone_to_region",
                    "func_desc": "_zone_to_region",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _zone_to_region(zone):\\n    \"\"\"Convert a zone (like us-west1-b) to the corresponding region\\n    (like us-west1).\"\"\"\\n    # See https://cloud.google.com/compute/docs/regions-zones/#identifying_a_region_or_zone  # noqa\\n    return '-'.join(zone.split('-')[:-1])",
                    "func_fullName": "mrjob.dataproc._zone_to_region( zone )"
                },
                {
                    "func_id": 84,
                    "func_name": "_fix_zone_and_region_opts",
                    "func_desc": "_fix_zone_and_region_opts",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _fix_zone_and_region_opts(self):\\n        \"\"\"Ensure that exactly one of region and zone is set.\"\"\"\\n        if self._opts['region'] and self._opts['zone']:\\n            log.warning('you do not need to set region if you set zone')\\n            self._opts['region'] = None\\n            return\\n\\n        if not (self._opts['region'] or self._opts['zone']):\\n            if environ.get('CLOUDSDK_COMPUTE_ZONE'):\\n                self._opts['zone'] = environ['CLOUDSDK_COMPUTE_ZONE']\\n            elif environ.get('CLOUDSDK_COMPUTE_REGION'):\\n                self._opts['region'] = environ['CLOUDSDK_COMPUTE_REGION']\\n            else:\\n                self._opts['region'] = _DEFAULT_GCE_REGION",
                    "func_fullName": "mrjob.dataproc._fix_zone_and_region_opts( self )"
                },
                {
                    "func_id": 87,
                    "func_name": "cluster_client",
                    "func_desc": "cluster_client",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def cluster_client(self):\\n        return google.cloud.dataproc_v1beta2.ClusterControllerClient(\\n            **self._client_create_kwargs())",
                    "func_fullName": "mrjob.dataproc.cluster_client( self )"
                },
                {
                    "func_id": 94,
                    "func_name": "_region",
                    "func_desc": "_region",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _region(self):\\n        # region of cluster, which is either the region set by the user,\\n        # or the region derived from the zone they set.\\n        # used to pick bucket location and name cluster\\n        return self._opts['region'] or _zone_to_region(self._opts['zone'])",
                    "func_fullName": "mrjob.dataproc._region( self )"
                },
                {
                    "func_id": 102,
                    "func_name": "_cleanup_cloud_tmp",
                    "func_desc": "_cleanup_cloud_tmp",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cleanup_cloud_tmp(self):\\n        # delete all the files we created\\n        if not self._job_tmpdir:\\n            return\\n\\n        try:\\n            log.info('Removing all files in %s' % self._job_tmpdir)\\n            self.fs.rm(self._job_tmpdir)\\n            self._job_tmpdir = None\\n        except Exception as e:\\n            log.exception(e)",
                    "func_fullName": "mrjob.dataproc._cleanup_cloud_tmp( self )"
                },
                {
                    "func_id": 105,
                    "func_name": "_cleanup_cluster",
                    "func_desc": "_cleanup_cluster",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cleanup_cluster(self):\\n        if not self._cluster_id:\\n            # If we don't have a cluster, then we can't terminate it.\\n            return\\n\\n        try:\\n            log.info(\"Attempting to terminate cluster\")\\n            self._delete_cluster(self._cluster_id)\\n        except Exception as e:\\n            log.exception(e)\\n            return\\n        log.info('cluster %s successfully terminated' % self._cluster_id)",
                    "func_fullName": "mrjob.dataproc._cleanup_cluster( self )"
                },
                {
                    "func_id": 110,
                    "func_name": "_hadoop_streaming_jar_uri",
                    "func_desc": "_hadoop_streaming_jar_uri",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _hadoop_streaming_jar_uri(self):\\n        if self._opts['hadoop_streaming_jar']:\\n            return self._upload_mgr.uri(self._opts['hadoop_streaming_jar'])\\n        else:\\n            return _HADOOP_STREAMING_JAR_URI",
                    "func_fullName": "mrjob.dataproc._hadoop_streaming_jar_uri( self )"
                },
                {
                    "func_id": 111,
                    "func_name": "_launch_cluster",
                    "func_desc": "_launch_cluster",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _launch_cluster(self):\\n        \"\"\"Create an empty cluster on Dataproc, and set self._cluster_id to\\n        its ID.\"\"\"\\n        self.fs.mkdir(self._job_tmpdir)\\n\\n        # clusterName must be a match of\\n        # regex '(?:[a-z](?:[-a-z0-9]{0,53}[a-z0-9])?).'\\n        # as documented in an API error message\\n        # (not currently documented in the Dataproc docs)\\n        if not self._cluster_id:\\n            self._cluster_id = '-'.join(\\n                ['mrjob', self._region(), random_identifier()])\\n\\n        # Create the cluster if it's missing, otherwise join an existing one\\n        try:\\n            self._get_cluster(self._cluster_id)\\n            log.info('Adding job to existing cluster - %s' % self._cluster_id)\\n        except google.api_core.exceptions.NotFound:\\n            log.info(\\n                'Creating Dataproc Hadoop cluster - %s' % self._cluster_id)\\n\\n            cluster_data = self._cluster_create_kwargs()\\n            self._create_cluster(cluster_data)\\n\\n            self._wait_for_cluster_ready(self._cluster_id)\\n\\n        self._set_up_ssh_tunnel()\\n\\n        # keep track of when we launched our job\\n        self._dataproc_job_start = time.time()\\n        return self._cluster_id",
                    "func_fullName": "mrjob.dataproc._launch_cluster( self )"
                },
                {
                    "func_id": 112,
                    "func_name": "_wait_for_cluster_ready",
                    "func_desc": "_wait_for_cluster_ready",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _wait_for_cluster_ready(self, cluster_id):\\n        # See https://cloud.google.com/dataproc/reference/rest/v1/projects.regions.clusters#State  # noqa\\n        cluster_state = None\\n\\n        # Poll until cluster is ready\\n        while cluster_state not in ('RUNNING', 'UPDATING'):\\n            cluster = self._get_cluster(cluster_id)\\n            cluster_state = cluster.status.State.Name(cluster.status.state)\\n\\n            if cluster_state in ('ERROR', 'DELETING'):\\n                raise DataprocException(cluster)\\n\\n            self._wait_for_api('cluster to accept jobs')\\n\\n        return cluster_id",
                    "func_fullName": "mrjob.dataproc._wait_for_cluster_ready( self, cluster_id )"
                },
                {
                    "func_id": 129,
                    "func_name": "get_hadoop_version",
                    "func_desc": "get_hadoop_version",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_hadoop_version(self):\\n        if self._hadoop_version is None:\\n            self._store_cluster_info()\\n        return self._hadoop_version",
                    "func_fullName": "mrjob.dataproc.get_hadoop_version( self )"
                },
                {
                    "func_id": 131,
                    "func_name": "_store_cluster_info",
                    "func_desc": "_store_cluster_info",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _store_cluster_info(self):\\n        \"\"\"Set self._image_version and self._hadoop_version.\"\"\"\\n        if not self._cluster_id:\\n            raise ValueError('cluster has not yet been created')\\n\\n        cluster = self._get_cluster(self._cluster_id)\\n        self._image_version = (\\n            cluster.config.software_config.image_version)\\n        # protect against new versions, including patch versions\\n        # we didn't explicitly request. See #1428\\n        self._hadoop_version = map_version(\\n            self._image_version, _DATAPROC_IMAGE_TO_HADOOP_VERSION)",
                    "func_fullName": "mrjob.dataproc._store_cluster_info( self )"
                },
                {
                    "func_id": 134,
                    "func_name": "get_cluster_id",
                    "func_desc": "get_cluster_id",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_cluster_id(self):\\n        return self._cluster_id",
                    "func_fullName": "mrjob.dataproc.get_cluster_id( self )"
                },
                {
                    "func_id": 135,
                    "func_name": "_cluster_create_kwargs",
                    "func_desc": "_cluster_create_kwargs",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cluster_create_kwargs(self):\\n        gcs_init_script_uris = []\\n        if self._master_bootstrap_script_path:\\n            gcs_init_script_uris.append(\\n                self._upload_mgr.uri(self._master_bootstrap_script_path))\\n\\n        cluster_metadata = dict()\\n        cluster_metadata['mrjob-version'] = mrjob.__version__\\n\\n        # TODO: remove mrjob-max-secs-idle once lifecycle_config is visible\\n        # through the gcloud utility and the Google Cloud Console\\n        cluster_metadata['mrjob-max-secs-idle'] = str(int(\\n            self._opts['max_mins_idle'] * 60))\\n\\n        gce_cluster_config = dict(\\n            metadata=cluster_metadata,\\n            service_account_scopes=self._opts['service_account_scopes'],\\n        )\\n\\n        if self._opts['network']:\\n            gce_cluster_config['network_uri'] = self._opts['network']\\n\\n        if self._opts['subnet']:\\n            gce_cluster_config['subnetwork_uri'] = self._opts['subnet']\\n\\n        if self._opts['service_account']:\\n            gce_cluster_config['service_account'] = (\\n                self._opts['service_account'])\\n\\n        if self._opts['service_account_scopes']:\\n            gce_cluster_config['service_account_scopes'] = (\\n                self._opts['service_account_scopes'])\\n\\n        if self._opts['zone']:\\n            gce_cluster_config['zone_uri'] = _gcp_zone_uri(\\n                project=self._project_id, zone=self._opts['zone'])\\n\\n        cluster_config = dict(\\n            gce_cluster_config=gce_cluster_config,\\n            initialization_actions=[\\n                dict(executable_file=init_script_uri)\\n                for init_script_uri in gcs_init_script_uris\\n            ]\\n        )\\n\\n        # Task tracker\\n        master_conf = _gcp_instance_group_config(\\n            project=self._project_id, zone=self._opts['zone'],\\n            count=1, instance_type=self._opts['master_instance_type'],\\n        )\\n        if self._opts['master_instance_config']:\\n            master_conf.update(self._opts['master_instance_config'])\\n\\n        # Compute + storage\\n        worker_conf = _gcp_instance_group_config(\\n            project=self._project_id, zone=self._opts['zone'],\\n            count=self._opts['num_core_instances'],\\n            instance_type=self._opts['core_instance_type']\\n        )\\n        if self._opts['core_instance_config']:\\n            worker_conf.update(self._opts['core_instance_config'])\\n\\n        # Compute ONLY\\n        secondary_worker_conf = _gcp_instance_group_config(\\n            project=self._project_id, zone=self._opts['zone'],\\n            count=self._opts['num_task_instances'],\\n            instance_type=self._opts['task_instance_type'],\\n            is_preemptible=True\\n        )\\n        if self._opts['task_instance_config']:\\n            secondary_worker_conf.update(self._opts['task_instance_config'])\\n\\n        cluster_config['master_config'] = master_conf\\n        cluster_config['worker_config'] = worker_conf\\n        if secondary_worker_conf.get('num_instances'):\\n            cluster_config['secondary_worker_config'] = secondary_worker_conf\\n\\n        cluster_config['lifecycle_config'] = dict(\\n            idle_delete_ttl=dict(\\n                seconds=int(self._opts['max_mins_idle'] * 60)))\\n\\n        software_config = {}\\n\\n        if self._opts['cluster_properties']:\\n            software_config['properties'] = _values_to_text(\\n                self._opts['cluster_properties'])\\n\\n        # See - https://cloud.google.com/dataproc/dataproc-versions\\n        if self._opts['image_version']:\\n            software_config['image_version'] = self._opts['image_version']\\n\\n        if software_config:\\n            cluster_config['software_config'] = software_config\\n\\n        # in Python 2, dict keys loaded from JSON will be unicode, which\\n        # the Google protobuf objects don't like\\n        if PY2:\\n            cluster_config = _clean_json_dict_keys(cluster_config)\\n\\n        kwargs = dict(project_id=self._project_id,\\n                      cluster_name=self._cluster_id,\\n                      config=cluster_config)\\n\\n        return self._add_extra_cluster_params(kwargs)",
                    "func_fullName": "mrjob.dataproc._cluster_create_kwargs( self )"
                },
                {
                    "func_id": 136,
                    "func_name": "_get_cluster",
                    "func_desc": "_get_cluster",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_cluster(self, cluster_id):\\n        return self.cluster_client.get_cluster(\\n            cluster_name=cluster_id,\\n            **self._project_id_and_region()\\n        )",
                    "func_fullName": "mrjob.dataproc._get_cluster( self, cluster_id )"
                },
                {
                    "func_id": 137,
                    "func_name": "_create_cluster",
                    "func_desc": "_create_cluster",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _create_cluster(self, cluster_data):\\n        # https://cloud.google.com/dataproc/reference/rest/v1/projects.regions.clusters/create  # noqa\\n        # https://cloud.google.com/dataproc/reference/rest/v1/projects.regions.clusters/get  # noqa\\n\\n        self.cluster_client.create_cluster(\\n            cluster=cluster_data,\\n            **self._project_id_and_region()\\n        )",
                    "func_fullName": "mrjob.dataproc._create_cluster( self, cluster_data )"
                },
                {
                    "func_id": 138,
                    "func_name": "_delete_cluster",
                    "func_desc": "_delete_cluster",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _delete_cluster(self, cluster_id):\\n        return self.cluster_client.delete_cluster(\\n            cluster_name=cluster_id,\\n            **self._project_id_and_region()\\n        )",
                    "func_fullName": "mrjob.dataproc._delete_cluster( self, cluster_id )"
                },
                {
                    "func_id": 143,
                    "func_name": "_project_id_and_region",
                    "func_desc": "_project_id_and_region",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _project_id_and_region(self):\\n        return dict(\\n            project_id=self._project_id,\\n            region=(self._opts['region'] or 'global'),\\n        )",
                    "func_fullName": "mrjob.dataproc._project_id_and_region( self )"
                },
                {
                    "func_id": 146,
                    "func_name": "_ssh_tunnel_config",
                    "func_desc": "_ssh_tunnel_config",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ssh_tunnel_config(self):\\n        return _SSH_TUNNEL_CONFIG",
                    "func_fullName": "mrjob.dataproc._ssh_tunnel_config( self )"
                }
            ]
        },
        {
            "cluster_id": 41,
            "feature_id": 3,
            "feature_desc": "gamma=0.0273; k=2; a=0.25; combined=0.435; stability(ARI)=1.000; sep=0.058",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1,
                    "func_name": "_job_state_name",
                    "func_desc": "_job_state_name",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _job_state_name(state_value):\\n    return google.cloud.dataproc_v1beta2.types.JobStatus.State.Name(\\n        state_value)",
                    "func_fullName": "mrjob.dataproc._job_state_name( state_value )"
                },
                {
                    "func_id": 4,
                    "func_name": "_wait_for",
                    "func_desc": "_wait_for",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _wait_for(msg, sleep_secs):\\n    log.info(\"Waiting for %s - sleeping %.1f second(s)\", msg, sleep_secs)\\n    time.sleep(sleep_secs)",
                    "func_fullName": "mrjob.dataproc._wait_for( msg, sleep_secs )"
                },
                {
                    "func_id": 5,
                    "func_name": "_cleanse_gcp_job_id",
                    "func_desc": "_cleanse_gcp_job_id",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _cleanse_gcp_job_id(job_id):\\n    return re.sub(r'[^a-zA-Z0-9_\\-]', '-', job_id)",
                    "func_fullName": "mrjob.dataproc._cleanse_gcp_job_id( job_id )"
                },
                {
                    "func_id": 6,
                    "func_name": "_check_and_fix_fs_dir",
                    "func_desc": "_check_and_fix_fs_dir",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _check_and_fix_fs_dir(gcs_uri):\\n    \"\"\"Helper for __init__\"\"\"\\n    # TODO - mtai @ davidmarin - push this to fs/*.py\\n    if not is_gcs_uri(gcs_uri):\\n        raise ValueError('Invalid GCS URI: %r' % gcs_uri)\\n    if not gcs_uri.endswith('/'):\\n        gcs_uri += '/'\\n\\n    return gcs_uri",
                    "func_fullName": "mrjob.dataproc._check_and_fix_fs_dir( gcs_uri )"
                },
                {
                    "func_id": 75,
                    "func_name": "_log_filter_str",
                    "func_desc": "_log_filter_str",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _log_filter_str(name_to_value):\\n    \"\"\"return a map from name to value into a log filter query that requires\\n    each name to equal the given value.\"\"\"\\n    return ' AND '.join(\\n        '%s = %s' % (name, _quote_filter_value(value))\\n        for name, value in sorted(name_to_value.items()))",
                    "func_fullName": "mrjob.dataproc._log_filter_str( name_to_value )"
                },
                {
                    "func_id": 76,
                    "func_name": "_quote_filter_value",
                    "func_desc": "_quote_filter_value",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _quote_filter_value(s):\\n    \"\"\"Put a string in double quotes, escaping double quote characters\"\"\"\\n    return '\"%s\"' % s.replace('\"', r'\\\"')",
                    "func_fullName": "mrjob.dataproc._quote_filter_value( s )"
                },
                {
                    "func_id": 77,
                    "func_name": "_log_entries_to_log4j",
                    "func_desc": "_log_entries_to_log4j",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _log_entries_to_log4j(entries):\\n    \"\"\"Convert log entries from a single log file to log4j format, tracking\\n    line number.\\n\\n    See :py:meth:`mrjob.logs.log4j._parse_hadoop_log4j_records`\\n    for format.\\n    \"\"\"\\n    line_num = 0\\n\\n    for entry in entries:\\n        message = entry.payload.get('message') or ''\\n\\n        # NOTE: currently, google.cloud.logging seems strip newlines :(\\n        num_lines = len(message.split('\\n'))\\n\\n        yield dict(\\n            caller_location='',\\n            level=(entry.severity or ''),\\n            logger=(entry.payload.get('class') or ''),\\n            message=message,\\n            num_lines=num_lines,\\n            start_line=line_num,\\n            thread='',\\n            timestamp=(entry.timestamp or ''),\\n        )\\n\\n        line_num += num_lines",
                    "func_fullName": "mrjob.dataproc._log_entries_to_log4j( entries )"
                },
                {
                    "func_id": 78,
                    "func_name": "_fix_java_stack_trace",
                    "func_desc": "_fix_java_stack_trace",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _fix_java_stack_trace(s):\\n    # this is what we get from `gcloud logging`\\n    if '\\n' in s:\\n        return s\\n    else:\\n        return s.replace('\\t', '\\n\\t')",
                    "func_fullName": "mrjob.dataproc._fix_java_stack_trace( s )"
                },
                {
                    "func_id": 79,
                    "func_name": "_fix_traceback",
                    "func_desc": "_fix_traceback",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _fix_traceback(s):\\n    lines = s.split('\\n')\\n\\n    # strip log4j warnings (which do have proper linebreaks)\\n    lines = [\\n        line for line in lines\\n        if line and not _STDERR_LOG4J_WARNING.match(line)\\n    ]\\n\\n    s = '\\n'.join(lines)\\n\\n    if '\\n' in s:\\n        return s  # traceback does have newlines\\n\\n    s = s.replace('  File', '\\n  File')\\n    s = s.replace('    ', '\\n    ')\\n    s = _TRACEBACK_EXCEPTION_RE.sub(lambda m: '\\n' + m.group(0), s)\\n\\n    return s",
                    "func_fullName": "mrjob.dataproc._fix_traceback( s )"
                },
                {
                    "func_id": 80,
                    "func_name": "_clean_json_dict_keys",
                    "func_desc": "_clean_json_dict_keys",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _clean_json_dict_keys(x):\\n    \"\"\"Cast any dictionary keys in the given JSON object to str.\\n    We can assume that x isn't a recursive data structure, and that\\n    this is only called in Python 2.\"\"\"\\n    if isinstance(x, dict):\\n        return {str(k): _clean_json_dict_keys(v) for k, v in x.items()}\\n    elif isinstance(x, list):\\n        return [_clean_json_dict_keys(item) for item in x]\\n    else:\\n        return x",
                    "func_fullName": "mrjob.dataproc._clean_json_dict_keys( x )"
                },
                {
                    "func_id": 81,
                    "func_name": "_values_to_text",
                    "func_desc": "_values_to_text",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _values_to_text(d):\\n    \"\"\"Return a dictionary with the same keys as *d*, but where the\\n    non-string, non-bytes values have been JSON-encoded.\\n\\n    Used to encode cluster properties.\\n    \"\"\"\\n    result = {}\\n\\n    for k, v in d.items():\\n        if not isinstance(v, (string_types, bytes)):\\n            v = json.dumps(v)\\n\\n        result[k] = v\\n\\n    return result",
                    "func_fullName": "mrjob.dataproc._values_to_text( d )"
                },
                {
                    "func_id": 82,
                    "func_name": "_fully_qualify_scope_uri",
                    "func_desc": "_fully_qualify_scope_uri",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _fully_qualify_scope_uri(uri):\\n    if is_uri(uri):\\n        return uri\\n    else:\\n        return 'https://www.googleapis.com/auth/%s' % uri",
                    "func_fullName": "mrjob.dataproc._fully_qualify_scope_uri( uri )"
                },
                {
                    "func_id": 83,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, **kwargs):\\n        \"\"\":py:class:`~mrjob.dataproc.DataprocJobRunner` takes the same\\n        arguments as\\n        :py:class:`~mrjob.runner.MRJobRunner`, plus some additional options\\n        which can be defaulted in :ref:`mrjob.conf <mrjob.conf>`.\\n        \"\"\"\\n        super(DataprocJobRunner, self).__init__(**kwargs)\\n\\n        # check for library support\\n        if google is None:\\n            raise ImportError(\\n                'You must install google-cloud-logging and '\\n                'google-cloud-storage to connect to Dataproc')\\n\\n        # Dataproc requires a master and >= 2 core instances\\n        # num_core_instances refers ONLY to number of CORE instances and does\\n        # NOT include the required 1 instance for master\\n        # In other words, minimum cluster size is 3 machines, 1 master and 2\\n        # \"num_core_instances\" workers\\n        if self._opts['num_core_instances'] < _DATAPROC_MIN_WORKERS:\\n            raise DataprocException(\\n                'Dataproc expects at LEAST %d workers' % _DATAPROC_MIN_WORKERS)\\n\\n        if (self._opts['core_instance_type'] !=\\n                self._opts['task_instance_type']):\\n            raise DataprocException(\\n                'Dataproc v1 expects core/task instance types to be identical')\\n\\n        # see #1820\\n        if self._opts['image_id']:\\n            log.warning('mrjob does not yet support custom machine images'\\n                        ' on Dataproc')\\n\\n        # load credentials and project ID\\n        self._credentials, auth_project_id = google.auth.default(\\n            scopes=[_FULL_SCOPE])  # needed for $GOOGLE_APPLICATION_CREDENTIALS\\n\\n        self._project_id = self._opts['project_id'] or auth_project_id\\n\\n        if not self._project_id:\\n            raise DataprocException(\\n                'project_id must be set. Use --project_id or'\\n                ' set $GOOGLE_CLOUD_PROJECT')\\n\\n        self._fix_zone_and_region_opts()\\n\\n        if self._opts['service_account_scopes']:\\n            self._opts['service_account_scopes'] = [\\n                _fully_qualify_scope_uri(s)\\n                for s in self._opts['service_account_scopes']\\n            ]\\n\\n        # cluster_id can be None here\\n        self._cluster_id = self._opts['cluster_id']\\n\\n        self._api_client = None\\n        self._gcs_fs = None\\n        self._fs = None\\n\\n        # BEGIN - setup directories\\n        base_tmpdir = self._get_tmpdir(self._opts['cloud_tmp_dir'])\\n\\n        self._cloud_tmp_dir = _check_and_fix_fs_dir(base_tmpdir)\\n\\n        # use job key to make a unique tmp dir\\n        self._job_tmpdir = self._cloud_tmp_dir + self._job_key + '/'\\n\\n        # pick/validate output dir\\n        if self._output_dir:\\n            self._output_dir = _check_and_fix_fs_dir(self._output_dir)\\n        else:\\n            self._output_dir = self._job_tmpdir + 'output/'\\n        # END - setup directories\\n\\n        # manage local files that we want to upload to GCS. We'll add them\\n        # to this manager just before we need them.\\n        fs_files_dir = self._job_tmpdir + 'files/'\\n        self._upload_mgr = UploadDirManager(fs_files_dir)\\n\\n        # when did our particular task start?\\n        self._dataproc_job_start = None\\n\\n        # init hadoop, ami version caches\\n        self._image_version = None\\n        self._hadoop_version = None\\n\\n        # map driver_output_uri to a dict with the keys:\\n        # log_uri: uri of file we're reading from\\n        # pos: position in file\\n        # buffer: bytes read from file already\\n        self._driver_output_state = {}\\n\\n        # This will be filled by _run_steps()\\n        # NOTE - log_interpretations will be empty except job_id until we\\n        # parse task logs\\n        self._log_interpretations = []",
                    "func_fullName": "mrjob.dataproc.__init__( self, **kwargs )"
                },
                {
                    "func_id": 85,
                    "func_name": "_default_opts",
                    "func_desc": "_default_opts",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _default_opts(cls):\\n        return combine_dicts(\\n            super(DataprocJobRunner, cls)._default_opts(),\\n            dict(\\n                bootstrap_python=True,\\n                check_cluster_every=_DEFAULT_CHECK_CLUSTER_EVERY,\\n                cleanup=['CLUSTER', 'JOB', 'LOCAL_TMP'],\\n                cloud_fs_sync_secs=_DEFAULT_CLOUD_FS_SYNC_SECS,\\n                image_version=_DEFAULT_IMAGE_VERSION,\\n                instance_type=_DEFAULT_INSTANCE_TYPE,\\n                master_instance_type=_DEFAULT_INSTANCE_TYPE,\\n                num_core_instances=_DATAPROC_MIN_WORKERS,\\n                num_task_instances=0,\\n            )\\n        )",
                    "func_fullName": "mrjob.dataproc._default_opts( cls )"
                },
                {
                    "func_id": 86,
                    "func_name": "_combine_opts",
                    "func_desc": "_combine_opts",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _combine_opts(self, opt_list):\\n        \"\"\"Blank out conflicts between *network*/*subnet* and\\n        *region*/*zone*.\"\"\"\\n        opt_list = _blank_out_conflicting_opts(opt_list, ['region', 'zone'])\\n        opt_list = _blank_out_conflicting_opts(opt_list, ['network', 'subnet'])\\n\\n        # now combine opts, with region/zone blanked out\\n        return super(DataprocJobRunner, self)._combine_opts(opt_list)",
                    "func_fullName": "mrjob.dataproc._combine_opts( self, opt_list )"
                },
                {
                    "func_id": 88,
                    "func_name": "job_client",
                    "func_desc": "job_client",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def job_client(self):\\n        return google.cloud.dataproc_v1beta2.JobControllerClient(\\n            **self._client_create_kwargs())",
                    "func_fullName": "mrjob.dataproc.job_client( self )"
                },
                {
                    "func_id": 89,
                    "func_name": "logging_client",
                    "func_desc": "logging_client",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def logging_client(self):\\n        return google.cloud.logging.Client(credentials=self._credentials,\\n                                           project=self._project_id)",
                    "func_fullName": "mrjob.dataproc.logging_client( self )"
                },
                {
                    "func_id": 90,
                    "func_name": "_client_create_kwargs",
                    "func_desc": "_client_create_kwargs",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _client_create_kwargs(self):\\n        if self._opts['region']:\\n            endpoint = '%s-%s' % (self._opts['region'], _DEFAULT_ENDPOINT)\\n            return dict(\\n                channel=google.api_core.grpc_helpers.create_channel(\\n                    endpoint, credentials=self._credentials))\\n        else:\\n            return dict(credentials=self._credentials)",
                    "func_fullName": "mrjob.dataproc._client_create_kwargs( self )"
                },
                {
                    "func_id": 91,
                    "func_name": "api_client",
                    "func_desc": "api_client",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def api_client(self):\\n        raise NotImplementedError(\\n            '\"api_client\" was disabled in v0.6.2. Use \"cluster_client\"'\\n            ' or \"job_client\" instead.')",
                    "func_fullName": "mrjob.dataproc.api_client( self )"
                },
                {
                    "func_id": 92,
                    "func_name": "fs",
                    "func_desc": "fs",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def fs(self):\\n        \"\"\":py:class:`~mrjob.fs.base.Filesystem` object for SSH, S3, GCS, and\\n        the local filesystem.\\n        \"\"\"\\n        if self._fs is None:\\n            self._fs = CompositeFilesystem()\\n\\n            location = self._opts['region'] or _zone_to_region(\\n                self._opts['zone'])\\n\\n            self._fs.add_fs('gcs', GCSFilesystem(\\n                credentials=self._credentials,\\n                project_id=self._project_id,\\n                part_size=self._upload_part_size(),\\n                location=location,\\n                object_ttl_days=_DEFAULT_CLOUD_TMP_DIR_OBJECT_TTL_DAYS,\\n            ))\\n\\n            self._fs.add_fs('local', LocalFilesystem())\\n\\n        return self._fs",
                    "func_fullName": "mrjob.dataproc.fs( self )"
                },
                {
                    "func_id": 93,
                    "func_name": "_get_tmpdir",
                    "func_desc": "_get_tmpdir",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_tmpdir(self, given_tmpdir):\\n        \"\"\"Helper for _fix_tmpdir\"\"\"\\n        if given_tmpdir:\\n            return given_tmpdir\\n\\n        # Loop over buckets until we find one that matches region\\n        # NOTE - because this is a tmpdir, we look for a GCS bucket in the\\n        # same GCE region\\n        chosen_bucket_name = None\\n\\n        # determine region for bucket\\n        region = self._region()\\n\\n        for tmp_bucket_name in self.fs.gcs.get_all_bucket_names(\\n                prefix='mrjob-'):\\n            tmp_bucket = self.fs.gcs.get_bucket(tmp_bucket_name)\\n\\n            # NOTE - GCP ambiguous Behavior - Bucket location is being\\n            # returned as UPPERCASE, ticket filed as of Apr 23, 2016 as docs\\n            # suggest lowercase. (As of Feb. 12, 2018, this is still true,\\n            # observed on google-cloud-sdk)\\n            if tmp_bucket.location.lower() == region:\\n                # Regions are both specified and match\\n                log.info(\"using existing temp bucket %s\" % tmp_bucket_name)\\n                chosen_bucket_name = tmp_bucket_name\\n                break\\n\\n        # Example default - \"mrjob-us-central1-RANDOMHEX\"\\n        if not chosen_bucket_name:\\n            chosen_bucket_name = '-'.join(\\n                ['mrjob', region, random_identifier()])\\n\\n        return 'gs://%s/tmp/' % chosen_bucket_name",
                    "func_fullName": "mrjob.dataproc._get_tmpdir( self, given_tmpdir )"
                },
                {
                    "func_id": 95,
                    "func_name": "_run",
                    "func_desc": "_run",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run(self):\\n        self._launch()\\n        self._run_steps()",
                    "func_fullName": "mrjob.dataproc._run( self )"
                },
                {
                    "func_id": 96,
                    "func_name": "_launch",
                    "func_desc": "_launch",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _launch(self):\\n        self._prepare_for_launch()\\n        self._launch_cluster()",
                    "func_fullName": "mrjob.dataproc._launch( self )"
                },
                {
                    "func_id": 97,
                    "func_name": "_prepare_for_launch",
                    "func_desc": "_prepare_for_launch",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _prepare_for_launch(self):\\n        self._check_output_not_exists()\\n        self._create_setup_wrapper_scripts()\\n        self._add_bootstrap_files_for_upload()\\n        self._add_job_files_for_upload()\\n        self._upload_local_files()\\n        self._wait_for_fs_sync()",
                    "func_fullName": "mrjob.dataproc._prepare_for_launch( self )"
                },
                {
                    "func_id": 98,
                    "func_name": "_check_output_not_exists",
                    "func_desc": "_check_output_not_exists",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _check_output_not_exists(self):\\n        \"\"\"Verify the output path does not already exist. This avoids\\n        provisioning a cluster only to have Hadoop refuse to launch.\\n        \"\"\"\\n        if self.fs.exists(self._output_dir):\\n            raise IOError(\\n                'Output path %s already exists!' % (self._output_dir,))",
                    "func_fullName": "mrjob.dataproc._check_output_not_exists( self )"
                },
                {
                    "func_id": 99,
                    "func_name": "_add_bootstrap_files_for_upload",
                    "func_desc": "_add_bootstrap_files_for_upload",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _add_bootstrap_files_for_upload(self):\\n        \"\"\"Add files needed by the bootstrap script to self._upload_mgr.\\n\\n        Create the master bootstrap script if necessary.\\n\\n        \"\"\"\\n        # all other files needed by the script are already in\\n        # _bootstrap_dir_mgr\\n        for path in self._bootstrap_dir_mgr.paths():\\n            self._upload_mgr.add(path)\\n\\n        # now that we know where the above files live, we can create\\n        # the master bootstrap script\\n        self._create_master_bootstrap_script_if_needed()\\n        if self._master_bootstrap_script_path:\\n            self._upload_mgr.add(self._master_bootstrap_script_path)",
                    "func_fullName": "mrjob.dataproc._add_bootstrap_files_for_upload( self )"
                },
                {
                    "func_id": 100,
                    "func_name": "_add_job_files_for_upload",
                    "func_desc": "_add_job_files_for_upload",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _add_job_files_for_upload(self):\\n        \"\"\"Add files needed for running the job (setup and input)\\n        to self._upload_mgr.\"\"\"\\n        if self._opts['hadoop_streaming_jar']:\\n            self._upload_mgr.add(self._opts['hadoop_streaming_jar'])\\n\\n        for step in self._get_steps():\\n            if step.get('jar'):\\n                self._upload_mgr.add(step['jar'])",
                    "func_fullName": "mrjob.dataproc._add_job_files_for_upload( self )"
                },
                {
                    "func_id": 101,
                    "func_name": "cleanup",
                    "func_desc": "cleanup",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def cleanup(self, mode=None):\\n        super(DataprocJobRunner, self).cleanup(mode=mode)\\n\\n        # close our SSH tunnel, if any\\n        self._kill_ssh_tunnel()\\n\\n        # stop the cluster if it belongs to us (it may have stopped on its\\n        # own already, but that's fine)\\n        if self._cluster_id and not self._opts['cluster_id']:\\n            self._cleanup_cluster()",
                    "func_fullName": "mrjob.dataproc.cleanup( self, mode )"
                },
                {
                    "func_id": 103,
                    "func_name": "_cleanup_logs",
                    "func_desc": "_cleanup_logs",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cleanup_logs(self):\\n        super(DataprocJobRunner, self)._cleanup_logs()",
                    "func_fullName": "mrjob.dataproc._cleanup_logs( self )"
                },
                {
                    "func_id": 104,
                    "func_name": "_cleanup_job",
                    "func_desc": "_cleanup_job",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cleanup_job(self):\\n        job_prefix = self._dataproc_job_prefix()\\n        for job in self._list_jobs(\\n                cluster_name=self._cluster_id,\\n                state_matcher=_STATE_MATCHER_ACTIVE):\\n            # Kill all active jobs with the same job_prefix as this job\\n            job_id = job.reference.job_id\\n\\n            if not job_id.startswith(job_prefix):\\n                continue\\n\\n            self._cancel_job(job_id)\\n            self._wait_for_api('job cancellation')",
                    "func_fullName": "mrjob.dataproc._cleanup_job( self )"
                },
                {
                    "func_id": 106,
                    "func_name": "_wait_for_api",
                    "func_desc": "_wait_for_api",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _wait_for_api(self, msg):\\n        _wait_for(msg, self._opts['check_cluster_every'])",
                    "func_fullName": "mrjob.dataproc._wait_for_api( self, msg )"
                },
                {
                    "func_id": 107,
                    "func_name": "_wait_for_fs_sync",
                    "func_desc": "_wait_for_fs_sync",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _wait_for_fs_sync(self):\\n        \"\"\"Sleep for a little while, to give FS a chance to sync up.\\n        \"\"\"\\n        _wait_for('GCS sync (eventual consistency)',\\n                  self._opts['cloud_fs_sync_secs'])",
                    "func_fullName": "mrjob.dataproc._wait_for_fs_sync( self )"
                },
                {
                    "func_id": 108,
                    "func_name": "_streaming_step_job_kwarg",
                    "func_desc": "_streaming_step_job_kwarg",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _streaming_step_job_kwarg(self, step_num):\\n        \"\"\"Returns a map from ``'hadoop_job'`` to a dict representing\\n        a hadoop streaming job.\\n        \"\"\"\\n        return dict(\\n            hadoop_job=dict(\\n                args=self._hadoop_streaming_jar_args(step_num),\\n                main_jar_file_uri=self._hadoop_streaming_jar_uri(),\\n            )\\n        )",
                    "func_fullName": "mrjob.dataproc._streaming_step_job_kwarg( self, step_num )"
                },
                {
                    "func_id": 109,
                    "func_name": "_jar_step_job_kwarg",
                    "func_desc": "_jar_step_job_kwarg",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _jar_step_job_kwarg(self, step_num):\\n        \"\"\"Returns a map from ``'hadoop_job'`` to a dict representing\\n        a Hadoop job that runs a JAR\"\"\"\\n        step = self._get_step(step_num)\\n\\n        hadoop_job = {}\\n\\n        hadoop_job['args'] = (\\n            self._interpolate_jar_step_args(step['args'], step_num))\\n\\n        jar_uri = self._upload_mgr.uri(step['jar'])\\n\\n        # can't specify main_class and main_jar_file_uri; see\\n        # https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs#HadoopJob  # noqa\\n        if step.get('main_class'):\\n            hadoop_job['jar_file_uris'] = [jar_uri]\\n            hadoop_job['main_class'] = step['main_class']\\n        else:\\n            hadoop_job['main_jar_file_uri'] = jar_uri\\n\\n        return dict(hadoop_job=hadoop_job)",
                    "func_fullName": "mrjob.dataproc._jar_step_job_kwarg( self, step_num )"
                },
                {
                    "func_id": 113,
                    "func_name": "_dataproc_job_prefix",
                    "func_desc": "_dataproc_job_prefix",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _dataproc_job_prefix(self):\\n        return _cleanse_gcp_job_id(self._job_key)",
                    "func_fullName": "mrjob.dataproc._dataproc_job_prefix( self )"
                },
                {
                    "func_id": 114,
                    "func_name": "_run_steps",
                    "func_desc": "_run_steps",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run_steps(self):\\n        \"\"\"Wait for every step of the job to complete, one by one.\"\"\"\\n        total_steps = self._num_steps()\\n        # define out steps\\n        for step_num in range(total_steps):\\n            job_id = self._launch_step(step_num)\\n\\n            self._wait_for_step_to_complete(\\n                job_id, step_num=step_num, num_steps=total_steps)\\n\\n            log.info('Completed Dataproc Hadoop Job - %s', job_id)\\n\\n        # After all steps completed, wait for the last output (which is\\n        # usually written to GCS) to sync\\n        self._wait_for_fs_sync()",
                    "func_fullName": "mrjob.dataproc._run_steps( self )"
                },
                {
                    "func_id": 115,
                    "func_name": "_launch_step",
                    "func_desc": "_launch_step",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _launch_step(self, step_num):\\n        step = self._get_step(step_num)\\n\\n        # Clean-up step name\\n        step_name = '%s---step-%05d-of-%05d' % (\\n            self._dataproc_job_prefix(), step_num + 1, self._num_steps())\\n\\n        # Build step\\n\\n        # job_kwarg is a single-item dict, where the key is 'hadoop_job',\\n        # 'spark_job', etc.\\n        if step['type'] == 'streaming':\\n            job_kwarg = self._streaming_step_job_kwarg(step_num)\\n        elif step['type'] == 'jar':\\n            job_kwarg = self._jar_step_job_kwarg(step_num)\\n        else:\\n            raise NotImplementedError(\\n                'Unsupported step type: %r' % step['type'])\\n\\n        # Submit it\\n        log.info('Submitting Dataproc Hadoop Job - %s', step_name)\\n        result = self._submit_job(step_name, job_kwarg)\\n        log.info('Submitted Dataproc Hadoop Job - %s', step_name)\\n\\n        job_id = result.reference.job_id\\n        assert job_id == step_name\\n\\n        return job_id",
                    "func_fullName": "mrjob.dataproc._launch_step( self, step_num )"
                },
                {
                    "func_id": 116,
                    "func_name": "_wait_for_step_to_complete",
                    "func_desc": "_wait_for_step_to_complete",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _wait_for_step_to_complete(self, job_id, step_num, num_steps):\\n        \"\"\"Helper for _wait_for_step_to_complete(). Wait for\\n        step with the given ID to complete, and fetch counters.\\n        If it fails, attempt to diagnose the error, and raise an\\n        exception.\\n\\n        This also adds an item to self._log_interpretations\\n        \"\"\"\\n        log_interpretation = dict(job_id=job_id)\\n        self._log_interpretations.append(log_interpretation)\\n\\n        log_interpretation['step'] = {}\\n        step_type = self._get_step(step_num)['type']\\n\\n        while True:\\n            # https://cloud.google.com/dataproc/reference/rest/v1/projects.regions.jobs#JobStatus  # noqa\\n            job = self._get_job(job_id)\\n\\n            job_state = job.status.State.Name(job.status.state)\\n\\n            log.info('%s => %s' % (job_id, job_state))\\n\\n            log_interpretation['step']['driver_output_uri'] = (\\n                job.driver_output_resource_uri)\\n\\n            self._interpret_step_logs(log_interpretation, step_type)\\n\\n            progress = log_interpretation['step'].get('progress')\\n            if progress:\\n                log.info(' ' + progress['message'])\\n\\n            # https://cloud.google.com/dataproc/reference/rest/v1/projects.regions.jobs#State  # noqa\\n            # these are the states covered by the ACTIVE job state matcher,\\n            # plus SETUP_DONE\\n            if job_state in ('PENDING', 'RUNNING',\\n                             'CANCEL_PENDING', 'SETUP_DONE'):\\n                self._wait_for_api('job completion')\\n                continue\\n\\n            # print counters if job wasn't CANCELLED\\n            if job_state != 'CANCELLED':\\n                self._log_counters(log_interpretation, step_num)\\n\\n            if job_state == 'ERROR':\\n                error = self._pick_error(log_interpretation, step_type)\\n                if error:\\n                    _log_probable_cause_of_failure(log, error)\\n\\n            # we're done, will return at the end of this\\n            if job_state == 'DONE':\\n                break\\n            else:\\n                raise StepFailedException(\\n                    step_num=step_num, num_steps=num_steps)",
                    "func_fullName": "mrjob.dataproc._wait_for_step_to_complete( self, job_id, step_num, num_steps )"
                },
                {
                    "func_id": 117,
                    "func_name": "_default_step_output_dir",
                    "func_desc": "_default_step_output_dir",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _default_step_output_dir(self):\\n        # put intermediate data in HDFS\\n        return 'hdfs:///tmp/mrjob/%s/step-output' % self._job_key",
                    "func_fullName": "mrjob.dataproc._default_step_output_dir( self )"
                },
                {
                    "func_id": 118,
                    "func_name": "_interpret_step_logs",
                    "func_desc": "_interpret_step_logs",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _interpret_step_logs(self, log_interpretation, step_type):\\n        \"\"\"Hook for interpreting step logs.\\n\\n        Unlike with most runners, you may call this multiple times and it\\n        will continue to parse the step log incrementally, which is useful\\n        for getting job progress.\"\"\"\\n        # don't turn this off even if read_logs opt is false; it's\\n        # the only way this runner can track job progress\\n\\n        driver_output_uri = log_interpretation.get(\\n            'step', {}).get('driver_output_uri')\\n\\n        if driver_output_uri:\\n            self._update_step_interpretation(\\n                log_interpretation['step'], driver_output_uri)",
                    "func_fullName": "mrjob.dataproc._interpret_step_logs( self, log_interpretation, step_type )"
                },
                {
                    "func_id": 119,
                    "func_name": "_update_step_interpretation",
                    "func_desc": "_update_step_interpretation",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _update_step_interpretation(\\n            self, step_interpretation, driver_output_uri):\\n        new_lines = self._get_new_driver_output_lines(driver_output_uri)\\n        _interpret_new_dataproc_step_stderr(step_interpretation, new_lines)",
                    "func_fullName": "mrjob.dataproc._update_step_interpretation( self, step_interpretation, driver_output_uri )"
                },
                {
                    "func_id": 120,
                    "func_name": "_get_new_driver_output_lines",
                    "func_desc": "_get_new_driver_output_lines",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_new_driver_output_lines(self, driver_output_uri):\\n        \"\"\"Get a list of complete job driver output lines that are\\n        new since the last time we checked.\\n        \"\"\"\\n        state = self._driver_output_state.setdefault(\\n            driver_output_uri,\\n            dict(log_uri=None, pos=0, buffer=b''))\\n\\n        # driver output is in logs with names like driveroutput.000000000\\n        log_uris = sorted(self.fs.ls(driver_output_uri + '*'))\\n\\n        for log_uri in log_uris:\\n            # initialize log_uri with first URI we see\\n            if state['log_uri'] is None:\\n                # log the location of job driver output just once\\n                log.info(\\n                    '  Parsing job driver output from %s*' % driver_output_uri)\\n                state['log_uri'] = log_uri\\n\\n            # skip log files already parsed\\n            if log_uri < state['log_uri']:\\n                continue\\n\\n            # when parsing the next file, reset *pos*\\n            elif log_uri > state['log_uri']:\\n                state['pos'] = 0\\n                state['log_uri'] = log_uri\\n\\n            log_blob = self.fs.gcs._get_blob(log_uri)\\n\\n            try:\\n                new_data = log_blob.download_as_string(start=state['pos'])\\n            except (google.api_core.exceptions.NotFound,\\n                    google.api_core.exceptions.RequestRangeNotSatisfiable):\\n                # blob was just created, or no more data is available\\n                break\\n\\n            state['buffer'] += new_data\\n            state['pos'] += len(new_data)\\n\\n        # convert buffer into lines, saving leftovers for next time\\n        stream = BytesIO(state['buffer'])\\n        state['buffer'] = b''\\n\\n        lines = []\\n\\n        for line_bytes in stream:\\n            if line_bytes.endswith(b'\\n'):\\n                lines.append(to_unicode(line_bytes))\\n            else:\\n                # leave final partial line (if any) in buffer\\n                state['buffer'] = line_bytes\\n\\n        return lines",
                    "func_fullName": "mrjob.dataproc._get_new_driver_output_lines( self, driver_output_uri )"
                },
                {
                    "func_id": 121,
                    "func_name": "_interpret_history_log",
                    "func_desc": "_interpret_history_log",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _interpret_history_log(self, log_interpretation):\\n        \"\"\"Does nothing. We can't get the history logs, and we don't need\\n        them.\"\"\"\\n        if not self._read_logs():\\n            return\\n\\n        log_interpretation.setdefault('history', {})",
                    "func_fullName": "mrjob.dataproc._interpret_history_log( self, log_interpretation )"
                },
                {
                    "func_id": 122,
                    "func_name": "_interpret_task_logs",
                    "func_desc": "_interpret_task_logs",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _interpret_task_logs(self, log_interpretation, step_type,\\n                             error_attempt_ids=(), partial=True):\\n        \"\"\"Scan node manager log to find failed container IDs of failed\\n        tasks, and then scan the corresponding stderr and syslogs.\"\"\"\\n        if 'task' in log_interpretation and (\\n                partial or not log_interpretation['task'].get('partial')):\\n            return   # already interpreted\\n\\n        if not self._read_logs():\\n            return\\n\\n        step_interpretation = log_interpretation.get('step') or {}\\n\\n        application_id = step_interpretation.get('application_id')\\n        if not application_id:\\n            log.warning(\\n                \"Can't parse node manager logs; missing application ID\")\\n            return\\n\\n        log_interpretation['task'] = self._task_log_interpretation(\\n            application_id, step_type, partial)",
                    "func_fullName": "mrjob.dataproc._interpret_task_logs( self, log_interpretation, step_type, error_attempt_ids, partial )"
                },
                {
                    "func_id": 123,
                    "func_name": "_task_log_interpretation",
                    "func_desc": "_task_log_interpretation",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _task_log_interpretation(\\n            self, application_id, step_type, partial=True):\\n        \"\"\"Helper for :py:meth:`_interpret_task_logs`\"\"\"\\n        # not bothering with _read_logs() since this is a helper method\\n        result = {}\\n\\n        for container_id in self._failed_task_container_ids(application_id):\\n            error = _parse_task_syslog_records(\\n                self._task_syslog_records(\\n                    application_id, container_id, step_type))\\n\\n            if not error.get('hadoop_error'):\\n                # not sure if this ever happens, since we already know\\n                # which containers failed\\n                continue\\n\\n            error['container_id'] = container_id\\n\\n            # fix weird munging of java stacktrace\\n            error['hadoop_error']['message'] = _fix_java_stack_trace(\\n                error['hadoop_error']['message'])\\n\\n            task_error = _parse_task_stderr(\\n                self._task_stderr_lines(\\n                    application_id, container_id, step_type))\\n\\n            if task_error:\\n                task_error['message'] = _fix_traceback(task_error['message'])\\n                error['task_error'] = task_error\\n\\n            result.setdefault('errors', []).append(error)\\n\\n            # if partial is true, bail out when we find the first task error\\n            if task_error and partial:\\n                result['partial'] = True\\n                return result\\n\\n        return result",
                    "func_fullName": "mrjob.dataproc._task_log_interpretation( self, application_id, step_type, partial )"
                },
                {
                    "func_id": 124,
                    "func_name": "_failed_task_container_ids",
                    "func_desc": "_failed_task_container_ids",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _failed_task_container_ids(self, application_id):\\n        \"\"\"Stream container IDs of failed tasks, in reverse order.\"\"\"\\n        container_id_prefix = 'container' + application_id[11:]\\n\\n        log_filter = self._make_log_filter(\\n            'yarn-yarn-nodemanager',\\n            {'jsonPayload.class': _CONTAINER_EXECUTOR_CLASS_NAME})\\n\\n        log.info('Scanning node manager logs for IDs of failed tasks...')\\n\\n        # it doesn't seem to work to do self.logging_client.logger();\\n        # there's some RPC dispute about whether the log name should\\n        # be qualified by project name or not\\n        entries = self.logging_client.list_entries(\\n            filter_=log_filter, order_by=google.cloud.logging.DESCENDING)\\n\\n        for entry in entries:\\n            message = entry.payload.get('message')\\n            if not message:\\n                continue\\n\\n            m = _CONTAINER_EXIT_RE.match(message)\\n            if not m:\\n                continue\\n\\n            returncode = int(m.group('returncode'))\\n            if not returncode:\\n                continue\\n\\n            container_id = m.group('container_id')\\n            # matches some other step\\n            if not container_id.startswith(container_id_prefix):\\n                continue\\n\\n            log.debug('  %s' % container_id)\\n            yield container_id",
                    "func_fullName": "mrjob.dataproc._failed_task_container_ids( self, application_id )"
                },
                {
                    "func_id": 125,
                    "func_name": "_task_stderr_lines",
                    "func_desc": "_task_stderr_lines",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _task_stderr_lines(self, application_id, container_id, step_type):\\n        \"\"\"Yield lines from a specific stderr log.\"\"\"\\n        log_filter = self._make_log_filter(\\n            'yarn-userlogs', {\\n                'jsonPayload.application': application_id,\\n                'jsonPayload.container': container_id,\\n                # TODO: pick based on step_type\\n                'jsonPayload.container_logname': 'stderr',\\n            })\\n\\n        log.info('    reading stderr log...')\\n        entries = self.logging_client.list_entries(filter_=log_filter)\\n\\n        # use log4j parsing to handle tab -> newline conversion\\n        for record in _log_entries_to_log4j(entries):\\n            for line in record['message'].split('\\n'):\\n                yield line",
                    "func_fullName": "mrjob.dataproc._task_stderr_lines( self, application_id, container_id, step_type )"
                },
                {
                    "func_id": 126,
                    "func_name": "_task_syslog_records",
                    "func_desc": "_task_syslog_records",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _task_syslog_records(self, application_id, container_id, step_type):\\n        \"\"\"Yield log4j records from a specific syslog.\\n        \"\"\"\\n        log_filter = self._make_log_filter(\\n            'yarn-userlogs', {\\n                'jsonPayload.application': application_id,\\n                'jsonPayload.container': container_id,\\n                # TODO: pick based on step_type\\n                'jsonPayload.container_logname': 'syslog',\\n            })\\n\\n        log.info('    reading syslog...')\\n        entries = self.logging_client.list_entries(filter_=log_filter)\\n\\n        return _log_entries_to_log4j(entries)",
                    "func_fullName": "mrjob.dataproc._task_syslog_records( self, application_id, container_id, step_type )"
                },
                {
                    "func_id": 127,
                    "func_name": "_make_log_filter",
                    "func_desc": "_make_log_filter",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _make_log_filter(self, log_name=None, extra_values=None):\\n        # we only want logs from this project, cluster, and region\\n        d = {}\\n\\n        d['resource.labels.cluster_name'] = self._cluster_id\\n        d['resource.labels.project_id'] = self._project_id\\n        d['resource.labels.region'] = self._region()\\n        d['resource.type'] = 'cloud_dataproc_cluster'\\n\\n        if log_name:\\n            d['logName'] = 'projects/%s/logs/%s' % (\\n                self._project_id, log_name)\\n\\n        if extra_values:\\n            d.update(extra_values)\\n\\n        return _log_filter_str(d)",
                    "func_fullName": "mrjob.dataproc._make_log_filter( self, log_name, extra_values )"
                },
                {
                    "func_id": 128,
                    "func_name": "counters",
                    "func_desc": "counters",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def counters(self):\\n        return [_pick_counters(log_interpretation)\\n                for log_interpretation in self._log_interpretations]",
                    "func_fullName": "mrjob.dataproc.counters( self )"
                },
                {
                    "func_id": 130,
                    "func_name": "get_image_version",
                    "func_desc": "get_image_version",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_image_version(self):\\n        \"\"\"Get the version that our cluster is running.\\n        \"\"\"\\n        if self._image_version is None:\\n            self._store_cluster_info()\\n        return self._image_version",
                    "func_fullName": "mrjob.dataproc.get_image_version( self )"
                },
                {
                    "func_id": 132,
                    "func_name": "_bootstrap_pre_commands",
                    "func_desc": "_bootstrap_pre_commands",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _bootstrap_pre_commands(self):\\n        # don't run the bootstrap script in / (see #1601)\\n        return [\\n            'mkdir /tmp/mrjob',\\n            'cd /tmp/mrjob',\\n        ]",
                    "func_fullName": "mrjob.dataproc._bootstrap_pre_commands( self )"
                },
                {
                    "func_id": 133,
                    "func_name": "_bootstrap_python",
                    "func_desc": "_bootstrap_python",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _bootstrap_python(self):\\n        \"\"\"Return a (possibly empty) list of parsed commands (in the same\\n        format as returned by parse_setup_cmd())'\"\"\"\\n        if not self._opts['bootstrap_python']:\\n            return []\\n\\n        if PY2:\\n            # Python 2 is already installed; install pip and dev packages\\n            return [\\n                ['sudo apt-get install -y python-pip python-dev'],\\n            ]\\n        else:\\n            return [\\n                ['sudo apt-get install -y python3 python3-pip python3-dev'],\\n            ]",
                    "func_fullName": "mrjob.dataproc._bootstrap_python( self )"
                },
                {
                    "func_id": 139,
                    "func_name": "_list_jobs",
                    "func_desc": "_list_jobs",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _list_jobs(self, cluster_name=None, state_matcher=None):\\n        # https://cloud.google.com/dataproc/reference/rest/v1/projects.regions.jobs/list#JobStateMatcher  # noqa\\n        list_kwargs = self._project_id_and_region()\\n\\n        if cluster_name:\\n            list_kwargs['cluster_name'] = cluster_name\\n\\n        if state_matcher:\\n            list_kwargs['job_state_matcher'] = state_matcher\\n\\n        return self.job_client.list_jobs(**list_kwargs)",
                    "func_fullName": "mrjob.dataproc._list_jobs( self, cluster_name, state_matcher )"
                },
                {
                    "func_id": 140,
                    "func_name": "_get_job",
                    "func_desc": "_get_job",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_job(self, job_id):\\n        return self.job_client.get_job(\\n            job_id=job_id,\\n            **self._project_id_and_region()\\n        )",
                    "func_fullName": "mrjob.dataproc._get_job( self, job_id )"
                },
                {
                    "func_id": 141,
                    "func_name": "_cancel_job",
                    "func_desc": "_cancel_job",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cancel_job(self, job_id):\\n        return self.job_client.cancel_job(\\n            job_id=job_id,\\n            **self._project_id_and_region()\\n        )",
                    "func_fullName": "mrjob.dataproc._cancel_job( self, job_id )"
                },
                {
                    "func_id": 142,
                    "func_name": "_submit_job",
                    "func_desc": "_submit_job",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _submit_job(self, step_name, job_kwarg):\\n        # https://cloud.google.com/dataproc/reference/rest/v1/projects.regions.jobs/submit  # noqa\\n        # https://cloud.google.com/dataproc/reference/rest/v1/projects.regions.jobs#HadoopJob  # noqa\\n        # https://cloud.google.com/dataproc/reference/rest/v1/projects.regions.jobs#JobReference  # noqa\\n\\n        submit_job_kwargs = dict(\\n            job=dict(\\n                reference=dict(project_id=self._project_id, job_id=step_name),\\n                placement=dict(cluster_name=self._cluster_id),\\n                **job_kwarg\\n            ),\\n            **self._project_id_and_region()\\n        )\\n\\n        log.debug('  submit_job(%s)' % ', '.join(\\n            '%s=%r' % (k, v) for k, v in sorted(submit_job_kwargs.items())))\\n\\n        return self.job_client.submit_job(**submit_job_kwargs)",
                    "func_fullName": "mrjob.dataproc._submit_job( self, step_name, job_kwarg )"
                },
                {
                    "func_id": 144,
                    "func_name": "_manifest_download_commands",
                    "func_desc": "_manifest_download_commands",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _manifest_download_commands(self):\\n        return [\\n            # TODO: SSH in and figure out how to use gsutil or similar\\n            # ('gs://*', 'gsutil cp'),\\n            ('*://*', 'hadoop fs -copyToLocal'),\\n        ]",
                    "func_fullName": "mrjob.dataproc._manifest_download_commands( self )"
                },
                {
                    "func_id": 145,
                    "func_name": "_job_tracker_host",
                    "func_desc": "_job_tracker_host",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _job_tracker_host(self):\\n        return '%s-m' % self._cluster_id",
                    "func_fullName": "mrjob.dataproc._job_tracker_host( self )"
                },
                {
                    "func_id": 147,
                    "func_name": "_launch_ssh_proc",
                    "func_desc": "_launch_ssh_proc",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _launch_ssh_proc(self, args):\\n        ssh_proc = super(DataprocJobRunner, self)._launch_ssh_proc(args)\\n\\n        # enter an empty passphrase if creating a key for the first time\\n        ssh_proc.stdin.write(b'\\n\\n')\\n\\n        return ssh_proc",
                    "func_fullName": "mrjob.dataproc._launch_ssh_proc( self, args )"
                },
                {
                    "func_id": 148,
                    "func_name": "_ssh_launch_wait_secs",
                    "func_desc": "_ssh_launch_wait_secs",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ssh_launch_wait_secs(self):\\n        \"\"\"Wait 20 seconds because gcloud has to update project metadata\\n        (unless we were going to check the cluster sooner anyway).\"\"\"\\n        return min(20.0, self._opts['check_cluster_every'])",
                    "func_fullName": "mrjob.dataproc._ssh_launch_wait_secs( self )"
                },
                {
                    "func_id": 149,
                    "func_name": "_ssh_tunnel_args",
                    "func_desc": "_ssh_tunnel_args",
                    "func_file": "dataproc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ssh_tunnel_args(self, bind_port):\\n        if not self._cluster_id:\\n            return\\n\\n        gcloud_bin = self._opts['gcloud_bin'] or ['gcloud']\\n\\n        cluster = self._get_cluster(self._cluster_id)\\n        zone = cluster.config.gce_cluster_config.zone_uri.split('/')[-1]\\n\\n        return gcloud_bin + [\\n            'compute', 'ssh',\\n            '--zone', zone,\\n            self._job_tracker_host(),\\n            '--',\\n        ] + self._ssh_tunnel_opts(bind_port)",
                    "func_fullName": "mrjob.dataproc._ssh_tunnel_args( self, bind_port )"
                }
            ]
        },
        {
            "cluster_id": 55,
            "feature_id": 4,
            "feature_desc": "gamma=0.2027; k=4; a=0.25; combined=0.426; stability(ARI)=1.000; sep=0.198",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 172,
                    "func_name": "_loads",
                    "func_desc": "_loads",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _loads(self, value):\\n        \"\"\"Decode a single key/value, and return it.\"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.protocol._loads( self, value )"
                },
                {
                    "func_id": 176,
                    "func_name": "_loads",
                    "func_desc": "_loads",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _loads(self, value):\\n        return rapidjson.loads(value)",
                    "func_fullName": "mrjob.protocol._loads( self, value )"
                },
                {
                    "func_id": 180,
                    "func_name": "_loads",
                    "func_desc": "_loads",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _loads(self, value):\\n        # simplejson can handle bytes even in Python 3\\n        return simplejson.loads(value)",
                    "func_fullName": "mrjob.protocol._loads( self, value )"
                },
                {
                    "func_id": 182,
                    "func_name": "_loads",
                    "func_desc": "_loads",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _loads(self, value):\\n        # ujson can handle bytes even in Python 3\\n        return ujson.loads(value)",
                    "func_fullName": "mrjob.protocol._loads( self, value )"
                },
                {
                    "func_id": 192,
                    "func_name": "_loads",
                    "func_desc": "_loads",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _loads(self, value):\\n        return safeeval(value)",
                    "func_fullName": "mrjob.protocol._loads( self, value )"
                },
                {
                    "func_id": 194,
                    "func_name": "_loads",
                    "func_desc": "_loads",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def _loads(self, value):\\n            return json.loads(value)",
                    "func_fullName": "mrjob.protocol._loads( self, value )"
                },
                {
                    "func_id": 196,
                    "func_name": "_loads",
                    "func_desc": "_loads",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def _loads(self, value):\\n            # Python 3's json module does not accept bytes\\n            return json.loads(value.decode('utf_8'))",
                    "func_fullName": "mrjob.protocol._loads( self, value )"
                },
                {
                    "func_id": 210,
                    "func_name": "_loads",
                    "func_desc": "_loads",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def _loads(self, value):\\n            return pickle.loads(value.decode('string_escape'))",
                    "func_fullName": "mrjob.protocol._loads( self, value )"
                },
                {
                    "func_id": 212,
                    "func_name": "_loads",
                    "func_desc": "_loads",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def _loads(self, value):\\n            return pickle.loads(\\n                value.decode('unicode_escape').encode('latin_1'))",
                    "func_fullName": "mrjob.protocol._loads( self, value )"
                }
            ]
        },
        {
            "cluster_id": 55,
            "feature_id": 5,
            "feature_desc": "gamma=0.2027; k=4; a=0.25; combined=0.426; stability(ARI)=1.000; sep=0.198",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 173,
                    "func_name": "_dumps",
                    "func_desc": "_dumps",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _dumps(self, value):\\n        \"\"\"Encode a single key/value, and return it.\"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.protocol._dumps( self, value )"
                },
                {
                    "func_id": 177,
                    "func_name": "_dumps",
                    "func_desc": "_dumps",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _dumps(self, value):\\n        return rapidjson.dumps(value).encode('utf_8')",
                    "func_fullName": "mrjob.protocol._dumps( self, value )"
                },
                {
                    "func_id": 195,
                    "func_name": "_dumps",
                    "func_desc": "_dumps",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def _dumps(self, value):\\n            return json.dumps(value)",
                    "func_fullName": "mrjob.protocol._dumps( self, value )"
                },
                {
                    "func_id": 197,
                    "func_name": "_dumps",
                    "func_desc": "_dumps",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def _dumps(self, value):\\n            return json.dumps(value).encode('utf_8')",
                    "func_fullName": "mrjob.protocol._dumps( self, value )"
                },
                {
                    "func_id": 202,
                    "func_name": "_dumps",
                    "func_desc": "_dumps",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def _dumps(self, value):\\n            return simplejson.dumps(value)",
                    "func_fullName": "mrjob.protocol._dumps( self, value )"
                },
                {
                    "func_id": 203,
                    "func_name": "_dumps",
                    "func_desc": "_dumps",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def _dumps(self, value):\\n            return simplejson.dumps(value).encode('utf_8')",
                    "func_fullName": "mrjob.protocol._dumps( self, value )"
                },
                {
                    "func_id": 206,
                    "func_name": "_dumps",
                    "func_desc": "_dumps",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def _dumps(self, value):\\n            return ujson.dumps(value)",
                    "func_fullName": "mrjob.protocol._dumps( self, value )"
                },
                {
                    "func_id": 207,
                    "func_name": "_dumps",
                    "func_desc": "_dumps",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def _dumps(self, value):\\n            return ujson.dumps(value).encode('utf_8')",
                    "func_fullName": "mrjob.protocol._dumps( self, value )"
                },
                {
                    "func_id": 211,
                    "func_name": "_dumps",
                    "func_desc": "_dumps",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def _dumps(self, value):\\n            return pickle.dumps(value).encode('string_escape')",
                    "func_fullName": "mrjob.protocol._dumps( self, value )"
                },
                {
                    "func_id": 213,
                    "func_name": "_dumps",
                    "func_desc": "_dumps",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def _dumps(self, value):\\n            return pickle.dumps(value).decode(\\n                'latin_1').encode('unicode_escape')",
                    "func_fullName": "mrjob.protocol._dumps( self, value )"
                },
                {
                    "func_id": 218,
                    "func_name": "_dumps",
                    "func_desc": "_dumps",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def _dumps(self, value):\\n            return repr(value)",
                    "func_fullName": "mrjob.protocol._dumps( self, value )"
                },
                {
                    "func_id": 219,
                    "func_name": "_dumps",
                    "func_desc": "_dumps",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def _dumps(self, value):\\n            return repr(value).encode('utf_8')",
                    "func_fullName": "mrjob.protocol._dumps( self, value )"
                }
            ]
        },
        {
            "cluster_id": 55,
            "feature_id": 6,
            "feature_desc": "gamma=0.2027; k=4; a=0.25; combined=0.426; stability(ARI)=1.000; sep=0.198",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 174,
                    "func_name": "read",
                    "func_desc": "read",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def read(self, line):\\n        \"\"\"Decode a line of input.\\n\\n        :type line: str\\n        :param line: A line of raw input to the job, without trailing newline.\\n\\n        :return: A tuple of ``(key, value)``.\"\"\"\\n\\n        raw_key, raw_value = line.split(b'\\t', 1)\\n\\n        if raw_key != self._last_key_encoded:\\n            self._last_key_encoded = raw_key\\n            self._last_key_decoded = self._loads(raw_key)\\n        return (self._last_key_decoded, self._loads(raw_value))",
                    "func_fullName": "mrjob.protocol.read( self, line )"
                },
                {
                    "func_id": 178,
                    "func_name": "read",
                    "func_desc": "read",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def read(self, line):\\n        return (None, rapidjson.loads(line))",
                    "func_fullName": "mrjob.protocol.read( self, line )"
                },
                {
                    "func_id": 181,
                    "func_name": "read",
                    "func_desc": "read",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def read(self, line):\\n        # simplejson can handle bytes even in Python 3\\n        return (None, simplejson.loads(line))",
                    "func_fullName": "mrjob.protocol.read( self, line )"
                },
                {
                    "func_id": 183,
                    "func_name": "read",
                    "func_desc": "read",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def read(self, line):\\n        # ujson can handle bytes even in Python 3\\n        return (None, ujson.loads(line))",
                    "func_fullName": "mrjob.protocol.read( self, line )"
                },
                {
                    "func_id": 184,
                    "func_name": "read",
                    "func_desc": "read",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def read(self, line):\\n        key_value = line.split(b'\\t', 1)\\n        if len(key_value) == 1:\\n            key_value.append(None)\\n\\n        return tuple(key_value)",
                    "func_fullName": "mrjob.protocol.read( self, line )"
                },
                {
                    "func_id": 186,
                    "func_name": "read",
                    "func_desc": "read",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def read(self, line):\\n        return (None, line)",
                    "func_fullName": "mrjob.protocol.read( self, line )"
                },
                {
                    "func_id": 188,
                    "func_name": "read",
                    "func_desc": "read",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def read(self, line):\\n        try:\\n            line = line.decode('utf_8')\\n        except UnicodeDecodeError:\\n            line = line.decode('latin_1')\\n\\n        key_value = line.split(u'\\t', 1)\\n        if len(key_value) == 1:\\n            key_value.append(None)\\n\\n        return tuple(key_value)",
                    "func_fullName": "mrjob.protocol.read( self, line )"
                },
                {
                    "func_id": 190,
                    "func_name": "read",
                    "func_desc": "read",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def read(self, line):\\n        try:\\n            return (None, line.decode('utf_8'))\\n        except UnicodeDecodeError:\\n            return (None, line.decode('latin_1'))",
                    "func_fullName": "mrjob.protocol.read( self, line )"
                },
                {
                    "func_id": 193,
                    "func_name": "read",
                    "func_desc": "read",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def read(self, line):\\n        return (None, safeeval(line))",
                    "func_fullName": "mrjob.protocol.read( self, line )"
                },
                {
                    "func_id": 198,
                    "func_name": "read",
                    "func_desc": "read",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def read(self, line):\\n            return (None, json.loads(line))",
                    "func_fullName": "mrjob.protocol.read( self, line )"
                },
                {
                    "func_id": 200,
                    "func_name": "read",
                    "func_desc": "read",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def read(self, line):\\n            # Python 3's json module does not accept bytes\\n            return (None, json.loads(line.decode('utf_8')))",
                    "func_fullName": "mrjob.protocol.read( self, line )"
                },
                {
                    "func_id": 214,
                    "func_name": "read",
                    "func_desc": "read",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def read(self, line):\\n            return (None, pickle.loads(line.decode('string_escape')))",
                    "func_fullName": "mrjob.protocol.read( self, line )"
                },
                {
                    "func_id": 216,
                    "func_name": "read",
                    "func_desc": "read",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def read(self, line):\\n            return (None, pickle.loads(\\n                line.decode('unicode_escape').encode('latin_1')))",
                    "func_fullName": "mrjob.protocol.read( self, line )"
                }
            ]
        },
        {
            "cluster_id": 55,
            "feature_id": 7,
            "feature_desc": "gamma=0.2027; k=4; a=0.25; combined=0.426; stability(ARI)=1.000; sep=0.198",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 175,
                    "func_name": "write",
                    "func_desc": "write",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def write(self, key, value):\\n        \"\"\"Encode a key and value.\\n\\n        :param key: A key (of any type) yielded by a mapper/reducer\\n        :param value: A value (of any type) yielded by a mapper/reducer\\n\\n        :rtype: str\\n        :return: A line, without trailing newline.\"\"\"\\n        return self._dumps(key) + b'\\t' + self._dumps(value)",
                    "func_fullName": "mrjob.protocol.write( self, key, value )"
                },
                {
                    "func_id": 179,
                    "func_name": "write",
                    "func_desc": "write",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def write(self, key, value):\\n        return rapidjson.dumps(value).encode('utf_8')",
                    "func_fullName": "mrjob.protocol.write( self, key, value )"
                },
                {
                    "func_id": 185,
                    "func_name": "write",
                    "func_desc": "write",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def write(self, key, value):\\n        return b'\\t'.join(x for x in (key, value) if x is not None)",
                    "func_fullName": "mrjob.protocol.write( self, key, value )"
                },
                {
                    "func_id": 187,
                    "func_name": "write",
                    "func_desc": "write",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def write(self, key, value):\\n        return value",
                    "func_fullName": "mrjob.protocol.write( self, key, value )"
                },
                {
                    "func_id": 189,
                    "func_name": "write",
                    "func_desc": "write",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def write(self, key, value):\\n        return b'\\t'.join(\\n            x.encode('utf_8') for x in (key, value) if x is not None)",
                    "func_fullName": "mrjob.protocol.write( self, key, value )"
                },
                {
                    "func_id": 191,
                    "func_name": "write",
                    "func_desc": "write",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def write(self, key, value):\\n        return value.encode('utf_8')",
                    "func_fullName": "mrjob.protocol.write( self, key, value )"
                },
                {
                    "func_id": 199,
                    "func_name": "write",
                    "func_desc": "write",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def write(self, key, value):\\n            return json.dumps(value)",
                    "func_fullName": "mrjob.protocol.write( self, key, value )"
                },
                {
                    "func_id": 201,
                    "func_name": "write",
                    "func_desc": "write",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def write(self, key, value):\\n            return json.dumps(value).encode('utf_8')",
                    "func_fullName": "mrjob.protocol.write( self, key, value )"
                },
                {
                    "func_id": 204,
                    "func_name": "write",
                    "func_desc": "write",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def write(self, key, value):\\n            return simplejson.dumps(value)",
                    "func_fullName": "mrjob.protocol.write( self, key, value )"
                },
                {
                    "func_id": 205,
                    "func_name": "write",
                    "func_desc": "write",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def write(self, key, value):\\n            return simplejson.dumps(value).encode('utf_8')",
                    "func_fullName": "mrjob.protocol.write( self, key, value )"
                },
                {
                    "func_id": 208,
                    "func_name": "write",
                    "func_desc": "write",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def write(self, key, value):\\n            return ujson.dumps(value)",
                    "func_fullName": "mrjob.protocol.write( self, key, value )"
                },
                {
                    "func_id": 209,
                    "func_name": "write",
                    "func_desc": "write",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def write(self, key, value):\\n            return ujson.dumps(value).encode('utf_8')",
                    "func_fullName": "mrjob.protocol.write( self, key, value )"
                },
                {
                    "func_id": 215,
                    "func_name": "write",
                    "func_desc": "write",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def write(self, key, value):\\n            return pickle.dumps(value).encode('string_escape')",
                    "func_fullName": "mrjob.protocol.write( self, key, value )"
                },
                {
                    "func_id": 217,
                    "func_name": "write",
                    "func_desc": "write",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def write(self, key, value):\\n            return pickle.dumps(value).decode(\\n                'latin_1').encode('unicode_escape')",
                    "func_fullName": "mrjob.protocol.write( self, key, value )"
                },
                {
                    "func_id": 220,
                    "func_name": "write",
                    "func_desc": "write",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def write(self, key, value):\\n            return repr(value)",
                    "func_fullName": "mrjob.protocol.write( self, key, value )"
                },
                {
                    "func_id": 221,
                    "func_name": "write",
                    "func_desc": "write",
                    "func_file": "protocol",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def write(self, key, value):\\n            return repr(value).encode('utf_8')",
                    "func_fullName": "mrjob.protocol.write( self, key, value )"
                }
            ]
        },
        {
            "cluster_id": 16,
            "feature_id": 8,
            "feature_desc": "gamma=0.0100; k=1; a=0.25; combined=0.440; stability(ARI)=1.000; sep=0.116",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 222,
                    "func_name": "bunzip2_stream",
                    "func_desc": "bunzip2_stream",
                    "func_file": "cat",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def bunzip2_stream(fileobj, bufsize=1024):\\n    \"\"\"Decompress gzipped data on the fly.\\n\\n    :param fileobj: object supporting ``read()``\\n    :param bufsize: number of bytes to read from *fileobj* at a time.\\n\\n    .. warning::\\n\\n        This yields decompressed chunks; it does *not* split on lines. To get\\n        lines, wrap this in :py:func:`to_lines`.\\n    \"\"\"\\n    if bz2 is None:\\n        raise Exception(\\n            'bz2 module was not successfully imported (likely not installed).')\\n\\n    d = bz2.BZ2Decompressor()\\n\\n    for chunk in to_chunks(fileobj):\\n        part = d.decompress(chunk)\\n        if part:\\n            yield part",
                    "func_fullName": "mrjob.cat.bunzip2_stream( fileobj, bufsize )"
                },
                {
                    "func_id": 223,
                    "func_name": "gunzip_stream",
                    "func_desc": "gunzip_stream",
                    "func_file": "cat",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def gunzip_stream(fileobj, bufsize=1024):\\n    \"\"\"Decompress gzipped data on the fly.\\n\\n    :param fileobj: object supporting ``read()``\\n    :param bufsize: number of bytes to read from *fileobj* at a time. The\\n                    default is the same as in :py:mod:`gzip`.\\n\\n    .. warning::\\n\\n        This yields decompressed chunks; it does *not* split on lines. To get\\n        lines, wrap this in :py:func:`to_lines`.\\n    \"\"\"\\n    # see Issue #601 for why we need this.\\n\\n    # we need this flag to read gzip rather than raw zlib, but it's not\\n    # actually defined in zlib, so we define it here.\\n    READ_GZIP_DATA = 16\\n    d = zlib.decompressobj(READ_GZIP_DATA | zlib.MAX_WBITS)\\n    for chunk in to_chunks(fileobj, bufsize):\\n        data = d.decompress(chunk)\\n        if data:\\n            yield data",
                    "func_fullName": "mrjob.cat.gunzip_stream( fileobj, bufsize )"
                },
                {
                    "func_id": 224,
                    "func_name": "decompress",
                    "func_desc": "decompress",
                    "func_file": "cat",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def decompress(readable, path, bufsize=1024):\\n    \"\"\"Take a *readable* which supports the ``.read()`` method correponding to\\n    the given path and returns an iterator that yields chunks of bytes,\\n    possibly decompressing based on *path*.\\n\\n    if *readable* appears to be a fileobj, pass it through as-is.\\n\\n    if *readable* does not have a ``read()`` method, assume that it's\\n    a generator that yields chunks of bytes\\n    \"\"\"\\n    if path.endswith('.gz'):\\n        return gunzip_stream(readable)\\n    elif path.endswith('.bz2'):\\n        if bz2 is None:\\n            raise Exception('bz2 module was not successfully imported'\\n                            ' (likely not installed).')\\n\\n        return bunzip2_stream(readable)\\n    elif hasattr(readable, '__iter__'):\\n        return readable\\n    else:\\n        # not a real readable (e.g. boto3 StreamingBody)\\n        return to_chunks(readable, bufsize=bufsize)",
                    "func_fullName": "mrjob.cat.decompress( readable, path, bufsize )"
                },
                {
                    "func_id": 225,
                    "func_name": "is_compressed",
                    "func_desc": "is_compressed",
                    "func_file": "cat",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def is_compressed(path):\\n    return path.endswith('.bz2') or path.endswith('.gz')",
                    "func_fullName": "mrjob.cat.is_compressed( path )"
                },
                {
                    "func_id": 226,
                    "func_name": "to_chunks",
                    "func_desc": "to_chunks",
                    "func_file": "cat",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def to_chunks(readable, bufsize=1024):\\n    \"\"\"Convert *readable*, which is any object supporting ``read()``\\n    (e.g. fileobjs) to a stream of non-empty ``bytes``.\\n\\n    If *readable* has an ``__iter__`` method but not a ``read`` method,\\n    pass through as-is.\\n    \"\"\"\\n    if hasattr(readable, '__iter__') and not hasattr(readable, 'read'):\\n        for chunk in readable:\\n            yield chunk\\n        return\\n\\n    while True:\\n        chunk = readable.read(bufsize)\\n        if chunk:\\n            yield chunk\\n        else:\\n            return",
                    "func_fullName": "mrjob.cat.to_chunks( readable, bufsize )"
                },
                {
                    "func_id": 1506,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "composite",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self):\\n        # names of sub-filesystems, in the order to call them. (The filesystems\\n        # themselves are stored in the attribute with that name.)\\n        self._fs_names = []\\n\\n        # map from fs name to *disable_if* method (see :py:meth:`add`).\\n        self._disable_if = {}\\n\\n        # set of names of filesystems that have been disabled\\n        self._disabled = set()",
                    "func_fullName": "mrjob.fs.composite.__init__( self )"
                },
                {
                    "func_id": 1507,
                    "func_name": "__getattr__",
                    "func_desc": "__getattr__",
                    "func_file": "composite",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __getattr__(self, name):\\n        # don't confuse pickling (e.g. __getstate__())\\n        if name.startswith('__'):\\n            raise AttributeError(name)\\n\\n        # go through non-disabled filesystems and pick the first\\n        # attribute with a matching name\\n        for fs_name in self._fs_names:\\n            if fs_name in self._disabled:\\n                continue\\n\\n            fs = getattr(self, fs_name)\\n            if hasattr(fs, name):\\n                log.warning(\\n                    'passing %s() through to the top-level filesystem is'\\n                    ' deprecated and going away in v0.7.0. Try'\\n                    ' fs.%s.%s(...) instead' % (name, fs_name, name))\\n                return getattr(fs, name)\\n\\n        raise AttributeError(name)",
                    "func_fullName": "mrjob.fs.composite.__getattr__( self, name )"
                },
                {
                    "func_id": 1508,
                    "func_name": "add_fs",
                    "func_desc": "add_fs",
                    "func_file": "composite",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def add_fs(self, name, fs, disable_if=None):\\n        \"\"\"Add a filesystem.\\n\\n        :param fs: a :py:class:~mrjob.fs.base.Filesystem to forward calls to.\\n        :param name string: Name of this filesystem. It will be directly\\n                            accessible through that the attribute with that\\n                            name. Recommended usage is the same name as\\n                            the module that contains the fs class.\\n        :param disable_if: A function called with a single argument, an\\n                           exception raised by ``fs``. If it returns true,\\n                           futher calls will not be forwarded to ``fs``.\\n        \"\"\"\\n        if name in self._fs_names:\\n            raise ValueError('name %r is already taken' % name)\\n\\n        setattr(self, name, fs)\\n        self._fs_names.append(name)\\n\\n        if disable_if:\\n            self._disable_if[name] = disable_if",
                    "func_fullName": "mrjob.fs.composite.add_fs( self, name, fs, disable_if )"
                },
                {
                    "func_id": 1509,
                    "func_name": "can_handle_path",
                    "func_desc": "can_handle_path",
                    "func_file": "composite",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def can_handle_path(self, path):\\n        \"\"\"We can handle any path handled by any (non-disabled) filesystem.\"\"\"\\n        for fs_name in self._fs_names:\\n            if fs_name in self._disabled:\\n                continue\\n\\n            fs = getattr(self, fs_name)\\n            if fs.can_handle_path(path):\\n                return True\\n\\n        return False",
                    "func_fullName": "mrjob.fs.composite.can_handle_path( self, path )"
                },
                {
                    "func_id": 1510,
                    "func_name": "_handle",
                    "func_desc": "_handle",
                    "func_file": "composite",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _handle(self, name, path_to_handle, *args, **kwargs):\\n        \"\"\"Call method named *name* on the first (non-disabled) filesystem\\n        that says it can handle every path in *paths_to_handle*. If it raises\\n        an exception, either disable the filesystem and continue, or\\n        re-raise the exception.\"\"\"\\n        for fs_name in self._fs_names:\\n            if fs_name in self._disabled:\\n                continue\\n\\n            fs = getattr(self, fs_name)\\n            if not fs.can_handle_path(path_to_handle):\\n                continue\\n\\n            try:\\n                return getattr(fs, name)(*args, **kwargs)\\n            except Exception as ex:\\n                if (fs_name in self._disable_if and\\n                        self._disable_if[fs_name](ex)):\\n                    log.debug('disabling %s fs: %r' % (fs_name, ex))\\n\\n                    self._disabled.add(fs_name)\\n                else:\\n                    raise\\n\\n        raise IOError(\"Can't handle path: %s\" % path_to_handle)",
                    "func_fullName": "mrjob.fs.composite._handle( self, name, path_to_handle, *args, **kwargs )"
                },
                {
                    "func_id": 1511,
                    "func_name": "_do",
                    "func_desc": "_do",
                    "func_file": "composite",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _do(self, name, path):\\n        \"\"\"Handle the common case, where a method operates on a single path.\"\"\"\\n        return self._handle(name, path, path)",
                    "func_fullName": "mrjob.fs.composite._do( self, name, path )"
                },
                {
                    "func_id": 1512,
                    "func_name": "cat",
                    "func_desc": "cat",
                    "func_file": "composite",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def cat(self, path_glob):\\n        return self._do('cat', path_glob)",
                    "func_fullName": "mrjob.fs.composite.cat( self, path_glob )"
                },
                {
                    "func_id": 1513,
                    "func_name": "_cat_file",
                    "func_desc": "_cat_file",
                    "func_file": "composite",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cat_file(self, path):\\n        # mrjob/runner.py accesses this directly for efficiency\\n        return self._do('_cat_file', path)",
                    "func_fullName": "mrjob.fs.composite._cat_file( self, path )"
                },
                {
                    "func_id": 1514,
                    "func_name": "du",
                    "func_desc": "du",
                    "func_file": "composite",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def du(self, path_glob):\\n        return self._do('du', path_glob)",
                    "func_fullName": "mrjob.fs.composite.du( self, path_glob )"
                },
                {
                    "func_id": 1515,
                    "func_name": "ls",
                    "func_desc": "ls",
                    "func_file": "composite",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def ls(self, path_glob):\\n        return self._do('ls', path_glob)",
                    "func_fullName": "mrjob.fs.composite.ls( self, path_glob )"
                },
                {
                    "func_id": 1516,
                    "func_name": "exists",
                    "func_desc": "exists",
                    "func_file": "composite",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def exists(self, path_glob):\\n        return self._do('exists', path_glob)",
                    "func_fullName": "mrjob.fs.composite.exists( self, path_glob )"
                },
                {
                    "func_id": 1517,
                    "func_name": "mkdir",
                    "func_desc": "mkdir",
                    "func_file": "composite",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mkdir(self, path):\\n        return self._do('mkdir', path)",
                    "func_fullName": "mrjob.fs.composite.mkdir( self, path )"
                },
                {
                    "func_id": 1518,
                    "func_name": "join",
                    "func_desc": "join",
                    "func_file": "composite",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def join(self, path, *paths):\\n        return self._handle('join', path, path, *paths)",
                    "func_fullName": "mrjob.fs.composite.join( self, path, *paths )"
                },
                {
                    "func_id": 1519,
                    "func_name": "put",
                    "func_desc": "put",
                    "func_file": "composite",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def put(self, src, path):\\n        return self._handle('put', path, src, path)",
                    "func_fullName": "mrjob.fs.composite.put( self, src, path )"
                },
                {
                    "func_id": 1520,
                    "func_name": "rm",
                    "func_desc": "rm",
                    "func_file": "composite",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def rm(self, path_glob):\\n        return self._do('rm', path_glob)",
                    "func_fullName": "mrjob.fs.composite.rm( self, path_glob )"
                },
                {
                    "func_id": 1521,
                    "func_name": "touchz",
                    "func_desc": "touchz",
                    "func_file": "composite",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def touchz(self, path):\\n        return self._do('touchz', path)",
                    "func_fullName": "mrjob.fs.composite.touchz( self, path )"
                },
                {
                    "func_id": 1522,
                    "func_name": "md5sum",
                    "func_desc": "md5sum",
                    "func_file": "composite",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def md5sum(self, path):\\n        return self._do('md5sum', path)",
                    "func_fullName": "mrjob.fs.composite.md5sum( self, path )"
                },
                {
                    "func_id": 1646,
                    "func_name": "can_handle_path",
                    "func_desc": "can_handle_path",
                    "func_file": "base",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def can_handle_path(self, path):\\n        \"\"\"Can we handle this path at all?\"\"\"\\n        False",
                    "func_fullName": "mrjob.fs.base.can_handle_path( self, path )"
                },
                {
                    "func_id": 1647,
                    "func_name": "cat",
                    "func_desc": "cat",
                    "func_file": "base",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def cat(self, path_glob):\\n        \"\"\"cat all files matching **path_glob**, decompressing if necessary\\n\\n        This yields bytes, which don't necessarily correspond to lines\\n        (see #1544). If multiple files are catted, yields ``b''`` between\\n        each file.\\n        \"\"\"\\n        for i, filename in enumerate(self.ls(path_glob)):\\n            if i > 0:\\n                yield b''  # mark end of previous file\\n\\n            for line in self._cat_file(filename):\\n                yield line",
                    "func_fullName": "mrjob.fs.base.cat( self, path_glob )"
                },
                {
                    "func_id": 1648,
                    "func_name": "du",
                    "func_desc": "du",
                    "func_file": "base",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def du(self, path_glob):\\n        \"\"\"Get the total size of files matching ``path_glob``\\n\\n        Corresponds roughly to: ``hadoop fs -du path_glob``\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.fs.base.du( self, path_glob )"
                },
                {
                    "func_id": 1649,
                    "func_name": "ls",
                    "func_desc": "ls",
                    "func_file": "base",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def ls(self, path_glob):\\n        \"\"\"Recursively list all files in the given path.\\n\\n        We don't return directories for compatibility with S3 (which\\n        has no concept of them)\\n\\n        Corresponds roughly to: ``hadoop fs -ls -R path_glob``\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.fs.base.ls( self, path_glob )"
                },
                {
                    "func_id": 1650,
                    "func_name": "_cat_file",
                    "func_desc": "_cat_file",
                    "func_file": "base",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cat_file(self, path):\\n        \"\"\"Yield the contents of the file at *path* as a series of ``bytes``,\\n        not necessarily respecting line boundaries.\"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.fs.base._cat_file( self, path )"
                },
                {
                    "func_id": 1651,
                    "func_name": "exists",
                    "func_desc": "exists",
                    "func_file": "base",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def exists(self, path_glob):\\n        \"\"\"Does the given path/URI exist?\\n\\n        Corresponds roughly to: ``hadoop fs -test -e path_glob``\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.fs.base.exists( self, path_glob )"
                },
                {
                    "func_id": 1652,
                    "func_name": "join",
                    "func_desc": "join",
                    "func_file": "base",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def join(self, path, *paths):\\n        \"\"\"Join *paths* onto *path* (which may be a URI)\"\"\"\\n        from mrjob.parse import urlparse\\n        from mrjob.parse import is_uri\\n        all_paths = (path,) + paths\\n\\n        # if there's a URI, we only care about it and what follows\\n        for i in range(len(all_paths), 0, -1):\\n            if is_uri(all_paths[i - 1]):\\n                scheme, netloc, uri_path = urlparse(all_paths[i - 1])[:3]\\n                return '%s://%s%s' % (\\n                    scheme, netloc, posixpath.join(\\n                        uri_path or '/', *all_paths[i:]))\\n        else:\\n            return os.path.join(*all_paths)",
                    "func_fullName": "mrjob.fs.base.join( self, path, *paths )"
                },
                {
                    "func_id": 1653,
                    "func_name": "mkdir",
                    "func_desc": "mkdir",
                    "func_file": "base",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mkdir(self, path):\\n        \"\"\"Create the given dir and its subdirs (if they don't already\\n        exist). On cloud filesystems (e.g. S3), also create the corresponding\\n        bucket as needed\\n\\n        Corresponds roughly to: ``hadoop fs -mkdir -p path``\\n\\n        .. versionadded:: 0.6.8 creates buckets on cloud filesystems\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.fs.base.mkdir( self, path )"
                },
                {
                    "func_id": 1654,
                    "func_name": "put",
                    "func_desc": "put",
                    "func_file": "base",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def put(self, src, path):\\n        \"\"\"Upload a file on the local filesystem (*src*) to *path*.\\n        Like with :py:func:`shutil.copyfile`, *path* should be the full path\\n        of the new file, not a directory which should contain it.\\n\\n        Corresponds roughly to ``hadoop fs -put src path``.\\n\\n        .. versionadded:: 0.6.8\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.fs.base.put( self, src, path )"
                },
                {
                    "func_id": 1655,
                    "func_name": "rm",
                    "func_desc": "rm",
                    "func_file": "base",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def rm(self, path_glob):\\n        \"\"\"Recursively delete the given file/directory, if it exists\\n\\n        Corresponds roughly to: ``hadoop fs -rm -R path_glob``\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.fs.base.rm( self, path_glob )"
                },
                {
                    "func_id": 1656,
                    "func_name": "touchz",
                    "func_desc": "touchz",
                    "func_file": "base",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def touchz(self, path):\\n        \"\"\"Make an empty file in the given location. Raises an error if\\n        a non-zero length file already exists in that location.\\n\\n        Correponds to: ``hadoop fs -touchz path``\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.fs.base.touchz( self, path )"
                },
                {
                    "func_id": 1657,
                    "func_name": "md5sum",
                    "func_desc": "md5sum",
                    "func_file": "base",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def md5sum(self, path):\\n        \"\"\"Generate the md5 sum of the file at *path*\"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.fs.base.md5sum( self, path )"
                }
            ]
        },
        {
            "cluster_id": 7,
            "feature_id": 9,
            "feature_desc": "gamma=0.0550; k=15; a=0.25; combined=0.401; stability(ARI)=0.936; sep=0.153",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 227,
                    "func_name": "_default_to",
                    "func_desc": "_default_to",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _default_to(namespace, dest, value):\\n    \"\"\"Helper function; set the given attribute to *value* if it's None.\"\"\"\\n    if getattr(namespace, dest) is None:\\n        setattr(namespace, dest, value)",
                    "func_fullName": "mrjob.options._default_to( namespace, dest, value )"
                },
                {
                    "func_id": 577,
                    "func_name": "fs",
                    "func_desc": "fs",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def fs(self):\\n        \"\"\":py:class:`~mrjob.fs.base.Filesystem` object for the local\\n        filesystem.\\n        \"\"\"\\n        if self._fs is None:\\n            # wrap LocalFilesystem in LocalFilesystem to get IOError\\n            # on URIs (see #1185)\\n            self._fs = CompositeFilesystem()\\n            self._fs.add_fs('local', LocalFilesystem())\\n        return self._fs",
                    "func_fullName": "mrjob.runner.fs( self )"
                },
                {
                    "func_id": 599,
                    "func_name": "_owner",
                    "func_desc": "_owner",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _owner(self):\\n        \"\"\"Return *owner* opt (which defaults to :py:func:`getpass.getuser`),\\n        or ``'no_user'`` if not set.\"\"\"\\n        if self._opts['owner']:\\n            # owner opt defaults to getpass.getuser()\\n            return self._opts['owner']\\n        else:\\n            return 'no_user'",
                    "func_fullName": "mrjob.runner._owner( self )"
                },
                {
                    "func_id": 640,
                    "func_name": "_cmdenv",
                    "func_desc": "_cmdenv",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cmdenv(self):\\n        \"\"\"Return a copy of ``self._opts['cmdenv']``. This exists so we\\n        can instrument cmdenv in runner subclasses.\"\"\"\\n        return dict(self._opts['cmdenv'])",
                    "func_fullName": "mrjob.runner._cmdenv( self )"
                },
                {
                    "func_id": 1449,
                    "func_name": "fs",
                    "func_desc": "fs",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def fs(self):\\n        # Spark supports basically every filesystem there is\\n\\n        if not self._fs:\\n            self._fs = CompositeFilesystem()\\n\\n            if boto3_installed:\\n                self._fs.add_fs('s3', S3Filesystem(\\n                    aws_access_key_id=self._opts['aws_access_key_id'],\\n                    aws_secret_access_key=self._opts['aws_secret_access_key'],\\n                    aws_session_token=self._opts['aws_session_token'],\\n                    s3_endpoint=self._opts['s3_endpoint'],\\n                    s3_region=self._opts['s3_region'],\\n                ), disable_if=_is_permanent_boto3_error)\\n\\n            if google_libs_installed:\\n                self._fs.add_fs('gcs', GCSFilesystem(\\n                    project_id=self._opts['project_id'],\\n                    location=self._opts['gcs_region'],\\n                    object_ttl_days=_DEFAULT_CLOUD_TMP_DIR_OBJECT_TTL_DAYS,\\n                ), disable_if=_is_permanent_google_error)\\n\\n            # Hadoop FS is responsible for all URIs that fall through to it\\n            self._fs.add_fs('hadoop', HadoopFilesystem(\\n                self._opts['hadoop_bin']))\\n\\n            self._fs.add_fs('local', LocalFilesystem())\\n\\n        return self._fs",
                    "func_fullName": "mrjob.spark.runner.fs( self )"
                }
            ]
        },
        {
            "cluster_id": 7,
            "feature_id": 10,
            "feature_desc": "gamma=0.0550; k=15; a=0.25; combined=0.401; stability(ARI)=0.936; sep=0.153",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 238,
                    "func_name": "_combiners",
                    "func_desc": "_combiners",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _combiners(opt_names, runner_alias=None):\\n    return {\\n        name: config['combiner']\\n        for name, config in _RUNNER_OPTS.items()\\n        if name in opt_names and 'combiner' in config\\n    }",
                    "func_fullName": "mrjob.options._combiners( opt_names, runner_alias )"
                },
                {
                    "func_id": 249,
                    "func_name": "_alphabetize_actions",
                    "func_desc": "_alphabetize_actions",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _alphabetize_actions(arg_parser):\\n    \"\"\"Alphabetize arg parser actions for the sake of nicer help printouts.\"\"\"\\n    # based on https://stackoverflow.com/questions/12268602/sort-argparse-help-alphabetically  # noqa\\n    for g in arg_parser._action_groups:\\n        g._group_actions.sort(key=lambda opt: opt.dest)",
                    "func_fullName": "mrjob.options._alphabetize_actions( arg_parser )"
                },
                {
                    "func_id": 643,
                    "func_name": "_sort_values_partitioner",
                    "func_desc": "_sort_values_partitioner",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _sort_values_partitioner(self):\\n        \"\"\"Partitioner to use with *sort_values* keyword to the constructor.\"\"\"\\n        if self._sort_values:\\n            return _SORT_VALUES_PARTITIONER\\n        else:\\n            return None",
                    "func_fullName": "mrjob.runner._sort_values_partitioner( self )"
                },
                {
                    "func_id": 1470,
                    "func_name": "_run_combiner",
                    "func_desc": "_run_combiner",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _run_combiner(combiner_job, rdd, sort_values=False, num_reducers=None):\\n    \"\"\"Run our job's combiner, and group lines with the same key together.\\n\\n    :param combiner_job: an instance of our job, instantiated to be the mapper\\n                         for the step we wish to run\\n    :param rdd: an RDD containing lines representing encoded key-value pairs\\n    :param sort_values: if true, ensure all lines corresponding to a given key\\n                        are sorted (by their encoded value)\\n    :param num_reducers: limit the number of paratitions of output rdd, which\\n                         is similar to mrjob's limit on number of reducers.\\n    :return: an RDD containing \"reducer ready\" lines representing encoded\\n             key-value pairs, that is, where all lines with the same key are\\n             adjacent and in the same partition\\n    \"\"\"\\n    step_num = combiner_job.options.step_num\\n\\n    c_read, c_write = combiner_job.pick_protocols(step_num, 'combiner')\\n\\n    # decode lines into key-value pairs\\n    #\\n    # line -> (k, v)\\n    if c_read:\\n        rdd = rdd.map(c_read)\\n\\n    # The common case for MRJob combiners is to yield a single key-value pair\\n    # (for example ``(key, sum(values))``. If the combiner does something\\n    # else, just build a list of values so we don't end up running multiple\\n    # values through the MRJob's combiner multiple times.\\n    def combiner_helper(pairs1, pairs2):\\n        if len(pairs1) == len(pairs2) == 1:\\n            return list(\\n                combiner_job.combine_pairs(pairs1 + pairs2, step_num),\\n            )\\n        else:\\n            pairs1.extend(pairs2)\\n            return pairs1\\n\\n    # include key in \"value\", so MRJob combiner can see it\\n    #\\n    # (k, v) -> (k, (k, v))\\n    rdd = rdd.map(lambda k_v: (k_v[0], k_v))\\n\\n    # :py:meth:`pyspark.RDD.combineByKey()`, where the magic happens.\\n    #\\n    # (k, (k, v)), ... -> (k, ([(k, v1), (k, v2), ...]))\\n    #\\n    # Our \"values\" are key-value pairs, and our \"combined values\" are lists of\\n    # key-value pairs (single-item lists in the common case).\\n    #\\n    # note that unlike Hadoop combiners, combineByKey() sees *all* the\\n    # key-value pairs, essentially doing a shuffle-and-sort for free.\\n    rdd = rdd.combineByKey(\\n        createCombiner=lambda k_v: [k_v],\\n        mergeValue=lambda k_v_list, k_v: combiner_helper(k_v_list, [k_v]),\\n        mergeCombiners=combiner_helper,\\n        numPartitions=num_reducers\\n    )\\n\\n    # encode lists of key-value pairs into lists of lines\\n    #\\n    # (k, [(k, v1), (k, v2), ...]) -> (k, [line1, line2, ...])\\n    if c_write:\\n        rdd = rdd.mapValues(\\n            lambda pairs: [c_write(*pair) for pair in pairs])\\n\\n    # free the lines!\\n    #\\n    # (k, [line1, line2, ...]) -> line1, line2, ...\\n    rdd = _discard_key_and_flatten_values(rdd, sort_values=sort_values)\\n\\n    return rdd",
                    "func_fullName": "mrjob.spark.harness._run_combiner( combiner_job, rdd, sort_values, num_reducers )"
                },
                {
                    "func_id": 1471,
                    "func_name": "_shuffle_and_sort",
                    "func_desc": "_shuffle_and_sort",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _shuffle_and_sort(\\n        rdd, sort_values=False, num_reducers=None,\\n        skip_internal_protocol=False):\\n    \"\"\"Simulate Hadoop's shuffle-and-sort step, so that data will be in the\\n    format the reducer expects.\\n\\n    :param rdd: an RDD containing lines representing encoded key-value pairs,\\n                where the encoded key comes first and is followed by a TAB\\n                character (the encoded key may not contain TAB).\\n    :param sort_values: if true, ensure all lines corresponding to a given key\\n                        are sorted (by their encoded value)\\n    :param num_reducers: limit the number of paratitions of output rdd, which\\n                         is similar to mrjob's limit on number of reducers.\\n    :param skip_internal_protocol: if true, assume *rdd* contains key/value\\n                                   pairs, not lines\\n\\n    :return: an RDD containing \"reducer ready\" lines representing encoded\\n             key-value pairs, that is, where all lines with the same key are\\n             adjacent and in the same partition\\n    \"\"\"\\n    if skip_internal_protocol:\\n        def key_func(k_v):\\n            return k_v[0]\\n    else:\\n        def key_func(line):\\n            return line.split(b'\\t')[0]\\n\\n    rdd = rdd.groupBy(key_func, numPartitions=num_reducers)\\n    rdd = _discard_key_and_flatten_values(rdd, sort_values=sort_values)\\n\\n    return rdd",
                    "func_fullName": "mrjob.spark.harness._shuffle_and_sort( rdd, sort_values, num_reducers, skip_internal_protocol )"
                },
                {
                    "func_id": 1473,
                    "func_name": "_discard_key_and_flatten_values",
                    "func_desc": "_discard_key_and_flatten_values",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _discard_key_and_flatten_values(rdd, sort_values=False):\\n    \"\"\"Helper function for :py:func:`_run_combiner` and\\n    :py:func:`_shuffle_and_sort`.\\n\\n    Given an RDD containing (key, [line1, line2, ...]), discard *key*\\n    and return an RDD containing line1, line2, ...\\n\\n    Guarantees that lines in the same list will end up in the same partition.\\n\\n    If *sort_values* is true, sort each list of lines before flattening it.\\n    \"\"\"\\n    if sort_values:\\n        def map_f(key_and_lines):\\n            return sorted(key_and_lines[1])\\n    else:\\n        def map_f(key_and_lines):\\n            return key_and_lines[1]\\n\\n    return rdd.flatMap(map_f, preservesPartitioning=True)",
                    "func_fullName": "mrjob.spark.harness._discard_key_and_flatten_values( rdd, sort_values )"
                },
                {
                    "func_id": 1482,
                    "func_name": "combiner_helper",
                    "func_desc": "combiner_helper",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def combiner_helper(pairs1, pairs2):\\n        if len(pairs1) == len(pairs2) == 1:\\n            return list(\\n                combiner_job.combine_pairs(pairs1 + pairs2, step_num),\\n            )\\n        else:\\n            pairs1.extend(pairs2)\\n            return pairs1",
                    "func_fullName": "mrjob.spark.harness.combiner_helper( pairs1, pairs2 )"
                },
                {
                    "func_id": 1483,
                    "func_name": "reduce_lines",
                    "func_desc": "reduce_lines",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def reduce_lines(lines):\\n        job = make_mrc_job('reducer', step_num)\\n\\n        read, write = job.pick_protocols(step_num, 'reducer')\\n\\n        # decode lines into key-value pairs (as a generator, not a list)\\n        #\\n        # line -> (k, v)\\n        if read:\\n            pairs = (read(line) for line in lines)\\n        else:\\n            pairs = lines  # pairs were never encoded\\n\\n        # reduce_pairs() runs key-value pairs through reducer\\n        #\\n        # (k, v), ... -> (k, v), ...\\n        for k, v in job.reduce_pairs(pairs, step_num):\\n            # encode key-value pairs back into lines\\n            #\\n            # (k, v) -> line\\n            if write:\\n                yield write(k, v)\\n            else:\\n                yield k, v",
                    "func_fullName": "mrjob.spark.harness.reduce_lines( lines )"
                }
            ]
        },
        {
            "cluster_id": 7,
            "feature_id": 11,
            "feature_desc": "gamma=0.0550; k=15; a=0.25; combined=0.401; stability(ARI)=0.936; sep=0.153",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 239,
                    "func_name": "_deprecated_aliases",
                    "func_desc": "_deprecated_aliases",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _deprecated_aliases(opt_names):\\n    results = {}\\n\\n    for name, config in _RUNNER_OPTS.items():\\n        if name not in opt_names:\\n            continue\\n\\n        if config.get('deprecated_aliases'):\\n            for alias in config['deprecated_aliases']:\\n                results[alias] = name\\n\\n    return results",
                    "func_fullName": "mrjob.options._deprecated_aliases( opt_names )"
                },
                {
                    "func_id": 241,
                    "func_name": "_add_runner_args",
                    "func_desc": "_add_runner_args",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _add_runner_args(parser, opt_names=None, include_deprecated=True,\\n                     customize_switches=None, suppress_switches=None):\\n    \"\"\"add switches for the given runner opts to the given\\n    ArgumentParser, alphabetically by destination. If *opt_names* is\\n    None, include all runner opts.\"\"\"\\n    if opt_names is None:\\n        opt_names = set(_RUNNER_OPTS)\\n\\n    for opt_name in sorted(opt_names):\\n        _add_runner_args_for_opt(\\n            parser, opt_name,\\n            include_deprecated=include_deprecated,\\n            customize_switches=customize_switches,\\n            suppress_switches=suppress_switches\\n        )",
                    "func_fullName": "mrjob.options._add_runner_args( parser, opt_names, include_deprecated, customize_switches, suppress_switches )"
                },
                {
                    "func_id": 243,
                    "func_name": "_add_basic_args",
                    "func_desc": "_add_basic_args",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _add_basic_args(parser):\\n    \"\"\"Switches for all command line tools\"\"\"\\n\\n    parser.add_argument(\\n        '-c', '--conf-path', dest='conf_paths',\\n        action='append',\\n        help='Path to alternate mrjob.conf file to read from')\\n\\n    parser.add_argument(\\n        '--no-conf', dest='conf_paths', action='store_const', const=[],\\n        help=\"Don't load mrjob.conf even if it's available\")\\n\\n    parser.add_argument(\\n        '-q', '--quiet', dest='quiet', default=None,\\n        action='store_true',\\n        help=\"Don't print anything to stderr\")\\n\\n    parser.add_argument(\\n        '-v', '--verbose', dest='verbose', default=None,\\n        action='store_true', help='print more messages to stderr')",
                    "func_fullName": "mrjob.options._add_basic_args( parser )"
                },
                {
                    "func_id": 244,
                    "func_name": "_add_job_args",
                    "func_desc": "_add_job_args",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _add_job_args(parser, include_deprecated=True, include_steps=True):\\n\\n    parser.add_argument(\\n        '--cat-output', dest='cat_output',\\n        default=None, action='store_true',\\n        help=\"Stream job output to stdout\")\\n\\n    parser.add_argument(\\n        '--no-cat-output', dest='cat_output',\\n        default=None, action='store_false',\\n        help=\"Don't stream job output to stdout\")\\n\\n    if include_deprecated:\\n        parser.add_argument(\\n            '--no-output', dest='cat_output',\\n            default=None, action='store_false',\\n            help='Deprecated alias for --no-cat-output')\\n\\n    parser.add_argument(\\n        '-o', '--output-dir', dest='output_dir', default=None,\\n        help='Where to put final job output. This must be an s3:// URL ' +\\n        'for EMR, an HDFS path for Hadoop, and a system path for local,' +\\n        'and must be empty')\\n\\n    parser.add_argument(\\n        '-r', '--runner', dest='runner',\\n        choices=sorted(_RUNNER_ALIASES),\\n        help=('Where to run the job; one of: %s' % ', '.join(\\n            sorted(_RUNNER_ALIASES))))\\n\\n    parser.add_argument(\\n        '--step-output-dir', dest='step_output_dir', default=None,\\n        help=('A directory to store output from job steps other than'\\n              ' the last one. Useful for debugging. Currently'\\n              ' ignored by local runners.'))\\n\\n    if include_deprecated:\\n        parser.add_argument(\\n            '--deprecated', dest='deprecated', action='store_true',\\n            help='include help for deprecated options')\\n\\n    parser.add_argument(\\n        '-h', '--help', dest='help', action='store_true',\\n        help='show this message and exit')",
                    "func_fullName": "mrjob.options._add_job_args( parser, include_deprecated, include_steps )"
                },
                {
                    "func_id": 248,
                    "func_name": "_parse_raw_args",
                    "func_desc": "_parse_raw_args",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_raw_args(parser, args):\\n    \"\"\"Simulate parsing by *parser*, return a list of tuples of\\n    (dest, option_string, args).\\n\\n    If *args* contains unknown args or is otherwise malformed, we don't\\n    raise an error (we leave this to the actual argument parser).\\n    \"\"\"\\n    results = []\\n\\n    class RawArgAction(Action):\\n        def __call__(self, parser, namespace, values, option_string=None):\\n            # ignore *namespace*, append to *results*\\n            results.append((self.dest, option_string, values))\\n\\n    def error(msg):\\n        raise ValueError(msg)\\n\\n    raw_parser = ArgumentParser(add_help=False)\\n    raw_parser.error = error\\n\\n    for action in parser._actions:\\n        # single args become single item lists\\n        nargs = 1 if action.nargs is None else action.nargs\\n\\n        raw_parser.add_argument(*action.option_strings,\\n                                action=RawArgAction,\\n                                dest=action.dest,\\n                                nargs=nargs)\\n\\n    # leave errors to the real parser\\n    raw_parser.parse_known_args(args)\\n\\n    return results",
                    "func_fullName": "mrjob.options._parse_raw_args( parser, args )"
                },
                {
                    "func_id": 616,
                    "func_name": "_args_for_task",
                    "func_desc": "_args_for_task",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _args_for_task(self, step_num, mrc):\\n        return [\\n            '--step-num=%d' % step_num,\\n            '--%s' % mrc,\\n        ] + self._mr_job_extra_args()",
                    "func_fullName": "mrjob.runner._args_for_task( self, step_num, mrc )"
                },
                {
                    "func_id": 617,
                    "func_name": "_mr_job_extra_args",
                    "func_desc": "_mr_job_extra_args",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _mr_job_extra_args(self, local=False):\\n        \"\"\"Return arguments to add to every invocation of MRJob.\\n\\n        :type local: boolean\\n        :param local: if this is True, use files' local paths rather than\\n            the path they'll have inside Hadoop streaming\\n        \"\"\"\\n        result = []\\n\\n        for extra_arg in self._extra_args:\\n            if isinstance(extra_arg, dict):\\n                if local:\\n                    result.append(extra_arg['path'])\\n                else:\\n                    result.append(self._working_dir_mgr.name(**extra_arg))\\n            else:\\n                result.append(extra_arg)\\n\\n        return result",
                    "func_fullName": "mrjob.runner._mr_job_extra_args( self, local )"
                },
                {
                    "func_id": 644,
                    "func_name": "_upload_args",
                    "func_desc": "_upload_args",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _upload_args(self):\\n        # just upload every file and archive in the working dir manager\\n        return self._upload_args_helper('-files', None, '-archives', None)",
                    "func_fullName": "mrjob.runner._upload_args( self )"
                },
                {
                    "func_id": 645,
                    "func_name": "_upload_args_helper",
                    "func_desc": "_upload_args_helper",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _upload_args_helper(\\n            self, files_opt_str, files, archives_opt_str, archives,\\n            always_use_hash=True, emulate_archives=False):\\n        args = []\\n\\n        file_hash_paths = list(\\n            self._file_arg_hash_paths(files,\\n                                      always_use_hash=always_use_hash))\\n\\n        # if emulating --archives, upload archives with files (we'll unpack\\n        # them later with a setup script)\\n        if emulate_archives:\\n            file_hash_paths.extend(\\n                self._file_archive_hash_paths(archives))\\n\\n        # --files ...\\n        if file_hash_paths:\\n            args.append(files_opt_str)\\n            args.append(','.join(file_hash_paths))\\n\\n        if not emulate_archives:\\n            archive_hash_paths = list(self._archive_arg_hash_paths(archives))\\n\\n            # --archives ...\\n            if archive_hash_paths:\\n                args.append(archives_opt_str)\\n                args.append(','.join(archive_hash_paths))\\n\\n        return args",
                    "func_fullName": "mrjob.runner._upload_args_helper( self, files_opt_str, files, archives_opt_str, archives, always_use_hash, emulate_archives )"
                },
                {
                    "func_id": 1476,
                    "func_name": "_make_arg_parser",
                    "func_desc": "_make_arg_parser",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _make_arg_parser():\\n    parser = ArgumentParser()\\n\\n    parser.add_argument(\\n        dest='job_class',\\n        help=('dot-separated module and name of MRJob class. For example:'\\n              ' mrjob.examples.mr_wc.MRWordCountUtility'))\\n\\n    parser.add_argument(\\n        dest='input_path',\\n        help=('Where to read input from. Can be a path or a URI, or several of'\\n              ' these joined by commas'))\\n\\n    parser.add_argument(\\n        dest='output_path',\\n        help=('An empty directory to write output to. Can be a path or URI.'))\\n\\n    # can't put this in _PASSTHRU_OPTIONS because it's also a runner opt\\n    parser.add_argument(\\n        '--max-output-files',\\n        dest='max_output_files',\\n        type=int,\\n        help='Directly limit number of output files, using coalesce()',\\n    )\\n    parser.add_argument(\\n        '--emulate-map-input-file',\\n        dest='emulate_map_input_file',\\n        action='store_true',\\n        help=('Set mapreduce_map_input_file to the input file path'\\n              ' in the first mapper function, so we can read it'\\n              ' with mrjob.compat.jobconf_from_env(). Ignored if'\\n              ' job has a Hadoop input format'),\\n    )\\n    parser.add_argument(\\n        '--skip-internal-protocol',\\n        dest='skip_internal_protocol',\\n        action='store_true',\\n        help=(\"Don't use the job's internal protocol to communicate\"\\n              \" between tasks internal to the job, instead relying\"\\n              \" on Spark to encode and decode raw data structures.\")\\n    )\\n\\n    for args, kwargs in _PASSTHRU_OPTIONS:\\n        parser.add_argument(*args, **kwargs)\\n\\n    return parser",
                    "func_fullName": "mrjob.spark.harness._make_arg_parser(  )"
                },
                {
                    "func_id": 1844,
                    "func_name": "_make_arg_parser",
                    "func_desc": "_make_arg_parser",
                    "func_file": "spark_submit",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _make_arg_parser():\\n    # this parser is never used for help messages, but\\n    # will show usage on error\\n    parser = ArgumentParser(usage=_USAGE, add_help=False)\\n\\n    # add positional arguments\\n    parser.add_argument(dest='script_or_jar', nargs='?')\\n    parser.add_argument(dest='args', nargs=REMAINDER)\\n\\n    _add_basic_args(parser)\\n    _add_runner_alias_arg(parser)\\n    _add_help_arg(parser)\\n    _add_deprecated_arg(parser)\\n\\n    # add runner opts\\n    runner_opt_names = set(_RUNNER_OPTS) - set(_HARD_CODED_OPTS)\\n    _add_runner_args(parser, opt_names=runner_opt_names)\\n\\n    # add spark-specific opts (without colliding with runner opts)\\n    for opt_name, switch in _SPARK_SUBMIT_SWITCHES.items():\\n        if opt_name in _RUNNER_OPTS and switch not in _SWITCH_ALIASES:\\n            continue\\n        _add_spark_submit_arg(parser, opt_name)\\n\\n    return parser",
                    "func_fullName": "mrjob.tools.spark_submit._make_arg_parser(  )"
                },
                {
                    "func_id": 1845,
                    "func_name": "_add_runner_alias_arg",
                    "func_desc": "_add_runner_alias_arg",
                    "func_file": "spark_submit",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _add_runner_alias_arg(parser):\\n    # we can't set default here because -r also affects help\\n    parser.add_argument(\\n        '-r', '--runner', dest='runner',\\n        choices=_SPARK_RUNNERS,\\n        help=('Where to run the job (default: \"%s\")'\\n              % _DEFAULT_RUNNER))",
                    "func_fullName": "mrjob.tools.spark_submit._add_runner_alias_arg( parser )"
                },
                {
                    "func_id": 1846,
                    "func_name": "_add_help_arg",
                    "func_desc": "_add_help_arg",
                    "func_file": "spark_submit",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _add_help_arg(parser):\\n    parser.add_argument(\\n        '-h', '--help', dest='help', action='store_true',\\n        help='show this message and exit')",
                    "func_fullName": "mrjob.tools.spark_submit._add_help_arg( parser )"
                },
                {
                    "func_id": 1847,
                    "func_name": "_add_deprecated_arg",
                    "func_desc": "_add_deprecated_arg",
                    "func_file": "spark_submit",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _add_deprecated_arg(parser):\\n    parser.add_argument(\\n        '--deprecated', dest='deprecated', action='store_true',\\n        help='include help for deprecated options')",
                    "func_fullName": "mrjob.tools.spark_submit._add_deprecated_arg( parser )"
                },
                {
                    "func_id": 1851,
                    "func_name": "_make_basic_help_parser",
                    "func_desc": "_make_basic_help_parser",
                    "func_file": "spark_submit",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _make_basic_help_parser(include_deprecated=False):\\n    \"\"\"Make an arg parser that's used only for printing basic help.\\n\\n    This prints help very similar to spark-submit itself. Runner args\\n    are not included unless they are also spark-submit args (e.g. --py-files)\\n    \"\"\"\\n    help_parser = ArgumentParser(usage=_USAGE, description=_DESCRIPTION,\\n                                 epilog=_BASIC_HELP_EPILOG, add_help=False)\\n\\n    _add_runner_alias_arg(help_parser)\\n\\n    for group_desc, opt_names in _SPARK_SUBMIT_ARG_GROUPS:\\n        if group_desc is None:\\n            parser_or_group = help_parser\\n        else:\\n            parser_or_group = help_parser.add_argument_group(group_desc)\\n\\n        for opt_name in opt_names:\\n            _add_spark_submit_arg(parser_or_group, opt_name)\\n\\n        if group_desc is None:\\n            _add_basic_args(help_parser)\\n            _add_help_arg(help_parser)\\n            if include_deprecated:\\n                _add_deprecated_arg(help_parser)\\n\\n    return help_parser",
                    "func_fullName": "mrjob.tools.spark_submit._make_basic_help_parser( include_deprecated )"
                }
            ]
        },
        {
            "cluster_id": 7,
            "feature_id": 12,
            "feature_desc": "gamma=0.0550; k=15; a=0.25; combined=0.401; stability(ARI)=0.936; sep=0.153",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 240,
                    "func_name": "_filter_by_role",
                    "func_desc": "_filter_by_role",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _filter_by_role(opt_names, *cloud_roles):\\n    return {\\n        opt_name\\n        for opt_name, conf in _RUNNER_OPTS.items()\\n        if opt_name in opt_names and conf.get('cloud_role') in cloud_roles\\n    }",
                    "func_fullName": "mrjob.options._filter_by_role( opt_names, *cloud_roles )"
                },
                {
                    "func_id": 592,
                    "func_name": "get_job_key",
                    "func_desc": "get_job_key",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_job_key(self):\\n        \"\"\"Get the unique key for the job run by this runner.\\n        This has the format ``label.owner.date.time.microseconds``\\n        \"\"\"\\n        return self._job_key",
                    "func_fullName": "mrjob.runner.get_job_key( self )"
                },
                {
                    "func_id": 597,
                    "func_name": "_make_unique_job_key",
                    "func_desc": "_make_unique_job_key",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _make_unique_job_key(self, label=None, owner=None):\\n        \"\"\"Come up with a useful unique ID for this job. Optionally,\\n        you can specify a custom label or owner (otherwise we use\\n        :py:meth:`_label` and :py:meth:`_owner`.\\n\\n        We use this to choose the output directory, etc. for the job.\\n        \"\"\"\\n        if label is None:\\n            label = self._label()\\n\\n        if owner is None:\\n            owner = self._owner()\\n\\n        now = datetime.datetime.utcnow()\\n        return '%s.%s.%s.%06d' % (\\n            label, owner,\\n            now.strftime('%Y%m%d.%H%M%S'), now.microsecond)",
                    "func_fullName": "mrjob.runner._make_unique_job_key( self, label, owner )"
                },
                {
                    "func_id": 623,
                    "func_name": "_bootstrap_mrjob",
                    "func_desc": "_bootstrap_mrjob",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _bootstrap_mrjob(self):\\n        \"\"\"Should we bootstrap mrjob?\"\"\"\\n        if self._opts['bootstrap_mrjob'] is None:\\n            return True\\n        else:\\n            return bool(self._opts['bootstrap_mrjob'])",
                    "func_fullName": "mrjob.runner._bootstrap_mrjob( self )"
                },
                {
                    "func_id": 642,
                    "func_name": "_sort_values_jobconf",
                    "func_desc": "_sort_values_jobconf",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _sort_values_jobconf(self):\\n        \"\"\"Jobconf dictionary to enable sorting by value.\\n        \"\"\"\\n        if not self._sort_values:\\n            return {}\\n\\n        # translate _SORT_VALUES_JOBCONF to the correct Hadoop version,\\n        # without logging a warning\\n        hadoop_version = self.get_hadoop_version()\\n\\n        jobconf = {}\\n        for k, v in _SORT_VALUES_JOBCONF.items():\\n            if hadoop_version:\\n                jobconf[translate_jobconf(k, hadoop_version)] = v\\n            else:\\n                for j in translate_jobconf_for_all_versions(k):\\n                    jobconf[j] = v\\n\\n        return jobconf",
                    "func_fullName": "mrjob.runner._sort_values_jobconf( self )"
                },
                {
                    "func_id": 1450,
                    "func_name": "_job_script_module_name",
                    "func_desc": "_job_script_module_name",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _job_script_module_name(self):\\n        \"\"\"A unique module name to use with the MRJob script.\"\"\"\\n        return re.sub(r'[^\\w\\d]', '_', self._job_key)",
                    "func_fullName": "mrjob.spark.runner._job_script_module_name( self )"
                },
                {
                    "func_id": 1451,
                    "func_name": "_create_job_script_zip",
                    "func_desc": "_create_job_script_zip",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _create_job_script_zip(self):\\n        if not self._job_script_zip_path:\\n            zip_path = os.path.join(self._get_local_tmp_dir(), 'script.zip')\\n            name_in_zip = self._job_script_module_name() + '.py'\\n\\n            log.debug('archiving %s -> %s as %s' % (\\n                self._script_path, zip_path, name_in_zip))\\n            with _create_zip_file(zip_path) as zip_file:\\n                zip_file.write(self._script_path, arcname=name_in_zip)\\n\\n            self._job_script_zip_path = zip_path\\n\\n        return self._job_script_zip_path",
                    "func_fullName": "mrjob.spark.runner._create_job_script_zip( self )"
                },
                {
                    "func_id": 1469,
                    "func_name": "_run_mapper",
                    "func_desc": "_run_mapper",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _run_mapper(make_mrc_job, step_num, rdd, rdd_includes_input_path):\\n    \"\"\"Run our job's mapper.\\n\\n    :param make_mrc_job: an instance of our job, instantiated to be the mapper\\n                         for the step we wish to run\\n    :param rdd: an RDD containing lines representing encoded key-value pairs\\n    :param rdd_includes_input_path: if true, rdd contains pairs of\\n                                    (input_file_path, line). set\\n                                    $mapreduce_map_input_file to\\n                                    *input_file_path*.\\n    :return: an RDD containing lines representing encoded key-value pairs\\n    \"\"\"\\n    # initialize job class inside mapPartitions(). this deals with jobs that\\n    # can't be initialized in the Spark driver (see #2044)\\n\\n    def map_lines(lines):\\n        job = make_mrc_job('mapper', step_num)\\n\\n        read, write = job.pick_protocols(step_num, 'mapper')\\n\\n        if rdd_includes_input_path:\\n            # rdd actually contains pairs of (input_path, line), convert\\n            path_line_pairs = lines\\n\\n            # emulate the mapreduce.map.input.file config property\\n            # set in Hadoop\\n            #\\n            # do this first so that mapper_init() etc. will work.\\n            # we can assume *rdd* contains at least one record.\\n            input_path, first_line = next(path_line_pairs)\\n            os.environ['mapreduce_map_input_file'] = input_path\\n\\n            # reconstruct *lines* (without dumping to memory)\\n            lines = chain([first_line], (line for _, line in path_line_pairs))\\n\\n        # decode lines into key-value pairs (as a generator, not a list)\\n        #\\n        # line -> (k, v)\\n        if read:\\n            pairs = (read(line) for line in lines)\\n        else:\\n            pairs = lines  # was never encoded\\n\\n        # reduce_pairs() runs key-value pairs through mapper\\n        #\\n        # (k, v), ... -> (k, v), ...\\n        for k, v in job.map_pairs(pairs, step_num):\\n            # encode key-value pairs back into lines\\n            #\\n            # (k, v) -> line\\n            if write:\\n                yield write(k, v)\\n            else:\\n                yield k, v\\n\\n    return rdd.mapPartitions(map_lines)",
                    "func_fullName": "mrjob.spark.harness._run_mapper( make_mrc_job, step_num, rdd, rdd_includes_input_path )"
                },
                {
                    "func_id": 1472,
                    "func_name": "_run_reducer",
                    "func_desc": "_run_reducer",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _run_reducer(make_mrc_job, step_num, rdd, num_reducers=None):\\n    \"\"\"Run our job's combiner, and group lines with the same key together.\\n\\n    :param reducer_job: an instance of our job, instantiated to be the mapper\\n                        for the step we wish to run\\n    :param rdd: an RDD containing \"reducer ready\" lines representing encoded\\n                key-value pairs, that is, where all lines with the same key are\\n                adjacent and in the same partition\\n    :param num_reducers: limit the number of paratitions of output rdd, which\\n                         is similar to mrjob's limit on number of reducers.\\n    :return: an RDD containing encoded key-value pairs\\n    \"\"\"\\n    # initialize job class inside mapPartitions(). this deals with jobs that\\n    # can't be initialized in the Spark driver (see #2044)\\n    def reduce_lines(lines):\\n        job = make_mrc_job('reducer', step_num)\\n\\n        read, write = job.pick_protocols(step_num, 'reducer')\\n\\n        # decode lines into key-value pairs (as a generator, not a list)\\n        #\\n        # line -> (k, v)\\n        if read:\\n            pairs = (read(line) for line in lines)\\n        else:\\n            pairs = lines  # pairs were never encoded\\n\\n        # reduce_pairs() runs key-value pairs through reducer\\n        #\\n        # (k, v), ... -> (k, v), ...\\n        for k, v in job.reduce_pairs(pairs, step_num):\\n            # encode key-value pairs back into lines\\n            #\\n            # (k, v) -> line\\n            if write:\\n                yield write(k, v)\\n            else:\\n                yield k, v\\n\\n    # if *num_reducers* is set, don't re-partition. otherwise, doesn't matter\\n    return rdd.mapPartitions(\\n        reduce_lines,\\n        preservesPartitioning=bool(num_reducers))",
                    "func_fullName": "mrjob.spark.harness._run_reducer( make_mrc_job, step_num, rdd, num_reducers )"
                },
                {
                    "func_id": 1480,
                    "func_name": "make_mrc_job",
                    "func_desc": "make_mrc_job",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def make_mrc_job(mrc, step_num):\\n        j = job_class(job_args + [\\n            '--%s' % mrc, '--step-num=%d' % step_num\\n        ])\\n\\n        # patch increment_counter() to update the accumulator for this step\\n        j.increment_counter = make_increment_counter(step_num)\\n\\n        # if skip_internal_protocol is true, patch internal_protocol() to\\n        # return an object whose *read* and *write* attributes are ``None``\\n        if args.skip_internal_protocol:\\n            j.internal_protocol = lambda: _NO_INTERNAL_PROTOCOL\\n\\n        return j",
                    "func_fullName": "mrjob.spark.harness.make_mrc_job( mrc, step_num )"
                },
                {
                    "func_id": 1783,
                    "func_name": "yield_records",
                    "func_desc": "yield_records",
                    "func_file": "spark",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def yield_records():\\n        for record in _parse_hadoop_log4j_records(lines):\\n            if record_callback:\\n                record_callback(record)\\n            yield record",
                    "func_fullName": "mrjob.logs.spark.yield_records(  )"
                }
            ]
        },
        {
            "cluster_id": 7,
            "feature_id": 13,
            "feature_desc": "gamma=0.0550; k=15; a=0.25; combined=0.401; stability(ARI)=0.936; sep=0.153",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 242,
                    "func_name": "_add_runner_args_for_opt",
                    "func_desc": "_add_runner_args_for_opt",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _add_runner_args_for_opt(parser, opt_name, include_deprecated=True,\\n                             customize_switches=None, suppress_switches=None):\\n    \"\"\"Add switches for a single option (*opt_name*) to the given parser.\"\"\"\\n    if customize_switches is None:\\n        customize_switches = {}\\n\\n    if suppress_switches is None:\\n        suppress_switches = set()\\n\\n    conf = _RUNNER_OPTS[opt_name]\\n\\n    if conf.get('deprecated') and not include_deprecated:\\n        return\\n\\n    switches = conf.get('switches') or []\\n\\n    def suppressed(switches):\\n        return any(sw in suppress_switches for sw in switches)\\n\\n    for args, kwargs in switches:\\n        kwargs = dict(kwargs)\\n\\n        # allow customization\\n        for switch in args:\\n            if switch in customize_switches:\\n                kwargs.update(customize_switches[switch])\\n\\n        deprecated_aliases = kwargs.pop('deprecated_aliases', None)\\n        deprecated = kwargs.pop('deprecated', False)\\n\\n        # add this switch\\n        if (include_deprecated or not deprecated) and not suppressed(args):\\n            kwargs['dest'] = opt_name\\n\\n            if kwargs.get('action') == 'append':\\n                kwargs['default'] = []\\n            else:\\n                kwargs['default'] = None\\n\\n            parser.add_argument(*args, **kwargs)\\n\\n        # add a switch for deprecated aliases\\n        if (deprecated_aliases and include_deprecated and\\n                not suppressed(deprecated_aliases)):\\n            help = 'Deprecated alias%s for %s' % (\\n                ('es' if len(deprecated_aliases) > 1 else ''),\\n                args[-1])\\n            parser.add_argument(\\n                *deprecated_aliases,\\n                **combine_dicts(kwargs, dict(help=help)))",
                    "func_fullName": "mrjob.options._add_runner_args_for_opt( parser, opt_name, include_deprecated, customize_switches, suppress_switches )"
                },
                {
                    "func_id": 564,
                    "func_name": "_fix_env",
                    "func_desc": "_fix_env",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _fix_env(env):\\n    \"\"\"Convert environment dictionary to strings (Python 2.7 on Windows\\n    doesn't allow unicode).\"\"\"\\n    def _to_str(s):\\n        if isinstance(s, string_types) and not isinstance(s, str):\\n            return s.encode('utf_8')\\n        else:\\n            return s\\n\\n    return dict((_to_str(k), _to_str(v)) for k, v in env.items())",
                    "func_fullName": "mrjob.runner._fix_env( env )"
                },
                {
                    "func_id": 565,
                    "func_name": "_blank_out_conflicting_opts",
                    "func_desc": "_blank_out_conflicting_opts",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _blank_out_conflicting_opts(opt_list, opt_names, conflicting_opts=None):\\n    \"\"\"Utility for :py:meth:`MRJobRunner._combine_opts()`: if multiple\\n    configs specify conflicting opts, blank them out in all but the\\n    last config (so, for example, the command line beats the config file).\\n\\n    This returns a copy of *opt_list*\\n    \"\"\"\\n    conflicting_opts = set(conflicting_opts or ()) | set(opt_names)\\n\\n    # copy opt_list so we can modify it\\n    opt_list = [dict(opts) for opts in opt_list]\\n\\n    # blank out region/zone before the last config where they are set\\n    blank_out = False\\n    for opts in reversed(opt_list):\\n        if blank_out:\\n            for opt_name in opt_names:\\n                opts[opt_name] = None\\n        elif any(opts.get(opt_name) is not None\\n                 for opt_name in conflicting_opts):\\n            blank_out = True\\n\\n    return opt_list",
                    "func_fullName": "mrjob.runner._blank_out_conflicting_opts( opt_list, opt_names, conflicting_opts )"
                },
                {
                    "func_id": 569,
                    "func_name": "_default_opts",
                    "func_desc": "_default_opts",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _default_opts(cls):\\n        try:\\n            owner = getpass.getuser()\\n        except:\\n            owner = None\\n\\n        return dict(\\n            check_input_paths=True,\\n            cleanup=['ALL'],\\n            cleanup_on_failure=['NONE'],\\n            owner=owner,\\n        )",
                    "func_fullName": "mrjob.runner._default_opts( cls )"
                },
                {
                    "func_id": 570,
                    "func_name": "_combine_confs",
                    "func_desc": "_combine_confs",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _combine_confs(self, source_and_opt_list):\\n        \"\"\"Combine several opt dictionaries into one.\\n\\n        *source_and_opt_list* is a list of tuples of *source*,\\n        *opts* where *opts* is a dictionary and *source* is either\\n        None or a description of where the opts came from (usually a path).\\n\\n        Only override this if you need truly fine-grained control,\\n        including knowledge of the options' source.\\n        \"\"\"\\n        opt_list = [\\n            self._fix_opts(opts, source)\\n            for source, opts in source_and_opt_list\\n        ]\\n\\n        return self._combine_opts(opt_list)",
                    "func_fullName": "mrjob.runner._combine_confs( self, source_and_opt_list )"
                },
                {
                    "func_id": 571,
                    "func_name": "_combine_opts",
                    "func_desc": "_combine_opts",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _combine_opts(self, opt_list):\\n        \"\"\"Combine several opt dictionaries into one. *opt_list*\\n        is a list of dictionaries containing validated options\\n\\n        Override this if you need to base options off the values of\\n        other options, but don't need to issue warnings etc.\\n        about the options' source.\\n        \"\"\"\\n        return combine_opts(self._opt_combiners(), *opt_list)",
                    "func_fullName": "mrjob.runner._combine_opts( self, opt_list )"
                },
                {
                    "func_id": 572,
                    "func_name": "_opt_combiners",
                    "func_desc": "_opt_combiners",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _opt_combiners(self):\\n        \"\"\"A dictionary mapping opt name to combiner funciton. This\\n        won't necessarily include every opt name (we default to\\n        :py:func:`~mrjob.conf.combine_value`).\\n        \"\"\"\\n        return _combiners(self.OPT_NAMES)",
                    "func_fullName": "mrjob.runner._opt_combiners( self )"
                },
                {
                    "func_id": 573,
                    "func_name": "_fix_opts",
                    "func_desc": "_fix_opts",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _fix_opts(self, opts, source=None):\\n        \"\"\"Take an options dictionary, and either return a sanitized\\n        version of it, or raise an exception.\\n\\n        *source* is either a string describing where the opts came from\\n        or None.\\n\\n        This ensures that opt dictionaries are really dictionaries\\n        and handles deprecated options.\\n        \"\"\"\\n        if source is None:\\n            source = 'defaults'  # defaults shouldn't trigger warnings\\n\\n        if not isinstance(opts, dict):\\n            raise TypeError(\\n                'options for %s (from %s) must be a dict' %\\n                (self.alias, source))\\n\\n        deprecated_aliases = _deprecated_aliases(self.OPT_NAMES)\\n\\n        results = {}\\n\\n        for k, v in sorted(opts.items()):\\n            # rewrite deprecated aliases\\n            if k in deprecated_aliases:\\n                if v is None:  # don't care\\n                    continue\\n\\n                aliased_opt = deprecated_aliases\\n\\n                log.warning('Deprecated option %s (from %s) has been renamed'\\n                            ' to %s and will be removed in v0.7.0' % (\\n                                k, source, aliased_opt))\\n\\n                if opts.get(aliased_opt) is not None:\\n                    return  # don't overwrite non-aliased opt\\n\\n                k = aliased_opt\\n\\n            if k in self.OPT_NAMES:\\n                if v is None:\\n                    fixed_v = None\\n                elif isinstance(v, ClearedValue):\\n                    # _fix_opt() doesn't need to know about !clear (see #2102)\\n                    fixed_v = ClearedValue(self._fix_opt(k, v.value, source))\\n                else:\\n                    fixed_v = self._fix_opt(k, v, source)\\n\\n                results[k] = fixed_v\\n            elif v:\\n                log.warning('Unexpected option %s (from %s)' % (k, source))\\n\\n        return results",
                    "func_fullName": "mrjob.runner._fix_opts( self, opts, source )"
                },
                {
                    "func_id": 574,
                    "func_name": "_fix_opt",
                    "func_desc": "_fix_opt",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _fix_opt(self, opt_key, opt_value, source):\\n        \"\"\"Fix a single option, returning its correct value or raising\\n        an exception. This is not called for options that are ``None``.\\n\\n        This currently handles cleanup opts.\\n\\n        Override this if you require additional opt validation or cleanup.\\n        \"\"\"\\n        if opt_key in ('cleanup', 'cleanup_on_failure'):\\n            return self._fix_cleanup_opt(opt_key, opt_value, source)\\n        else:\\n            return opt_value",
                    "func_fullName": "mrjob.runner._fix_opt( self, opt_key, opt_value, source )"
                },
                {
                    "func_id": 576,
                    "func_name": "_obfuscate_opt",
                    "func_desc": "_obfuscate_opt",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _obfuscate_opt(self, opt_key, opt_value):\\n        \"\"\"Return value of opt to show in debug printout. Used to obfuscate\\n        credentials, etc.\"\"\"\\n        return opt_value",
                    "func_fullName": "mrjob.runner._obfuscate_opt( self, opt_key, opt_value )"
                },
                {
                    "func_id": 591,
                    "func_name": "get_opts",
                    "func_desc": "get_opts",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_opts(self):\\n        \"\"\"Get options set for this runner, as a dict.\"\"\"\\n        log.warning('get_opts() is deprecated and will be removed in v0.7.0')\\n        return copy.deepcopy(self._opts)",
                    "func_fullName": "mrjob.runner.get_opts( self )"
                },
                {
                    "func_id": 654,
                    "func_name": "mode_has",
                    "func_desc": "mode_has",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def mode_has(*args):\\n            return any((choice in mode) for choice in args)",
                    "func_fullName": "mrjob.runner.mode_has( *args )"
                },
                {
                    "func_id": 1443,
                    "func_name": "_default_opts",
                    "func_desc": "_default_opts",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _default_opts(cls):\\n        return combine_dicts(\\n            super(SparkMRJobRunner, cls)._default_opts(),\\n            dict(\\n                cloud_part_size_mb=_DEFAULT_CLOUD_PART_SIZE_MB,\\n            ),\\n        )",
                    "func_fullName": "mrjob.spark.runner._default_opts( cls )"
                }
            ]
        },
        {
            "cluster_id": 7,
            "feature_id": 14,
            "feature_desc": "gamma=0.0550; k=15; a=0.25; combined=0.401; stability(ARI)=0.936; sep=0.153",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 245,
                    "func_name": "_add_step_args",
                    "func_desc": "_add_step_args",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _add_step_args(parser, include_deprecated=False):\\n    \"\"\"Add switches that determine what part of the job a MRJob runs.\"\"\"\\n    for dest, (args, kwargs) in _STEP_OPTS.items():\\n        if dest in _DEPRECATED_STEP_OPTS and not include_deprecated:\\n            continue\\n        kwargs = dict(dest=dest, **kwargs)\\n        parser.add_argument(*args, **kwargs)",
                    "func_fullName": "mrjob.options._add_step_args( parser, include_deprecated )"
                },
                {
                    "func_id": 600,
                    "func_name": "_get_steps",
                    "func_desc": "_get_steps",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_steps(self):\\n        \"\"\"Returns ``self._steps``.\\n        \"\"\"\\n        # TODO: remove this\\n        return self._steps",
                    "func_fullName": "mrjob.runner._get_steps( self )"
                },
                {
                    "func_id": 601,
                    "func_name": "_check_steps",
                    "func_desc": "_check_steps",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _check_steps(self, steps):\\n        \"\"\"Look at the step definition (*steps*). If it is not supported by\\n        the runner, raise :py:class:`NotImplementedError`. If it is not\\n        supported by mrjob, raise :py:class:`ValueError`.\\n        \"\"\"\\n        if not self._STEP_TYPES:\\n            # use __class__.__name__ because only MRJobRunner would\\n            # trigger this\\n            raise NotImplementedError(\\n                '%s cannot run steps!' % self.__class__.__name__)\\n\\n        for step_num, step in enumerate(steps):\\n            self._check_step(step, step_num)",
                    "func_fullName": "mrjob.runner._check_steps( self, steps )"
                },
                {
                    "func_id": 602,
                    "func_name": "_check_step",
                    "func_desc": "_check_step",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _check_step(self, step, step_num):\\n        \"\"\"Raise an exception if the given step is invalid\\n        (:py:class:`ValueError`) or not handled by this runner\\n        (:py:class:`NotImplementedError`).\\n\\n        By default, we check that *step* has a support step type,\\n        only uses an input manifest if it's the first step, and that\\n        :py:attr:`_script_path` exists if necessary. You can re-define\\n        this in your subclass.\\n        \"\"\"\\n        if step.get('type') not in self._STEP_TYPES:\\n            raise NotImplementedError(\\n                'step %d has type %r, but %s runner only supports:'\\n                ' %s' % (step_num, step.get('type'), self.alias,\\n                         ', '.join(sorted(self._STEP_TYPES))))\\n\\n        if step.get('input_manifest') and step_num != 0:\\n            raise ValueError(\\n                'step %d may not take an input manifest (only'\\n                ' first step can' % step_num)\\n\\n        # some step types assume a MRJob script\\n        if not self._script_path:\\n            if step['type'] == 'spark':\\n                raise ValueError(\\n                    \"SparkStep (step %d) can't run without a MRJob script\"\\n                    \" (try SparkScriptStep instead)\" % step_num)\\n\\n            elif step['type'] == 'streaming':\\n                for mrc in ('mapper', 'combiner', 'reducer'):\\n                    if not step.get(mrc):\\n                        continue\\n\\n                    substep = step[mrc]\\n                    if substep['type'] == 'script':\\n                        raise ValueError(\\n                            \"%s (step %d) can't run without a MRJob\"\\n                            \" script\" % (mrc, step_num))",
                    "func_fullName": "mrjob.runner._check_step( self, step, step_num )"
                },
                {
                    "func_id": 603,
                    "func_name": "_get_step",
                    "func_desc": "_get_step",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_step(self, step_num):\\n        \"\"\"Get a single step (calls :py:meth:`_get_steps`).\"\"\"\\n        return self._get_steps()[step_num]",
                    "func_fullName": "mrjob.runner._get_step( self, step_num )"
                },
                {
                    "func_id": 604,
                    "func_name": "_num_steps",
                    "func_desc": "_num_steps",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _num_steps(self):\\n        \"\"\"Get the number of steps (calls :py:meth:`get_steps`).\"\"\"\\n        return len(self._get_steps())",
                    "func_fullName": "mrjob.runner._num_steps( self )"
                },
                {
                    "func_id": 619,
                    "func_name": "_interpolate_step_args",
                    "func_desc": "_interpolate_step_args",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _interpolate_step_args(self, args, step_num):\\n        \"\"\"Replace :py:data:`~mrjob.step.INPUT` and\\n        :py:data:`~mrjob.step.OUTPUT` in arguments to a jar or Spark\\n        step.\\n        \"\"\"\\n        result = []\\n\\n        for arg in args:\\n            if arg == INPUT:\\n                result.append(\\n                    ','.join(self._step_input_uris(step_num)))\\n\\n            elif arg == OUTPUT:\\n                result.append(\\n                    self._step_output_uri(step_num))\\n\\n            else:\\n                result.append(arg)\\n\\n        return result",
                    "func_fullName": "mrjob.runner._interpolate_step_args( self, args, step_num )"
                },
                {
                    "func_id": 635,
                    "func_name": "_upload_part_size",
                    "func_desc": "_upload_part_size",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _upload_part_size(self):\\n        \"\"\"Part size for uploads, in bytes, or ``None``,\\n        from :mrjob-opt:`cloud_part_size_mb`\"\"\"\\n        if self._opts.get('cloud_part_size_mb'):\\n            return int(self._opts['cloud_part_size_mb'] * 1024 * 1024)\\n        else:\\n            return None",
                    "func_fullName": "mrjob.runner._upload_part_size( self )"
                },
                {
                    "func_id": 638,
                    "func_name": "_step_input_uris",
                    "func_desc": "_step_input_uris",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _step_input_uris(self, step_num):\\n        \"\"\"A list of URIs to use as input for the given step. For all\\n        except the first step, this list will have a single item (a\\n        directory).\"\"\"\\n        if step_num == 0:\\n            return [self._upload_mgr.uri(path) if self._upload_mgr\\n                    else to_uri(path)\\n                    for path in self._get_input_paths()]\\n        else:\\n            return [to_uri(self._intermediate_output_dir(step_num - 1))]",
                    "func_fullName": "mrjob.runner._step_input_uris( self, step_num )"
                },
                {
                    "func_id": 639,
                    "func_name": "_step_output_uri",
                    "func_desc": "_step_output_uri",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _step_output_uri(self, step_num):\\n        \"\"\"URI to use as output for the given step. This is either an\\n        intermediate dir (see :py:meth:`intermediate_output_uri`) or\\n        ``self._output_dir`` for the final step.\"\"\"\\n        if step_num == len(self._get_steps()) - 1:\\n            return to_uri(self._output_dir)\\n        else:\\n            return to_uri(self._intermediate_output_dir(step_num))",
                    "func_fullName": "mrjob.runner._step_output_uri( self, step_num )"
                },
                {
                    "func_id": 641,
                    "func_name": "_jobconf_for_step",
                    "func_desc": "_jobconf_for_step",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _jobconf_for_step(self, step_num):\\n        \"\"\"Get the jobconf dictionary, optionally including step-specific\\n        jobconf info.\\n\\n        Also translate jobconfs to the current Hadoop version, if necessary.\\n        \"\"\"\\n\\n        step = self._get_step(step_num)\\n\\n        # _sort_values_jobconf() isn't relevant to Spark,\\n        # but it doesn't do any harm either\\n\\n        jobconf = combine_jobconfs(self._sort_values_jobconf(),\\n                                   self._opts['jobconf'],\\n                                   step.get('jobconf'))\\n\\n        # if user is using the wrong jobconfs, add in the correct ones\\n        # and log a warning\\n        hadoop_version = self.get_hadoop_version()\\n        if hadoop_version:\\n            jobconf = translate_jobconf_dict(jobconf, hadoop_version)\\n\\n        return jobconf",
                    "func_fullName": "mrjob.runner._jobconf_for_step( self, step_num )"
                },
                {
                    "func_id": 1439,
                    "func_name": "_emr_proof_steps_desc",
                    "func_desc": "_emr_proof_steps_desc",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _emr_proof_steps_desc(steps_desc):\\n    # EMR's command-runner.jar does some very strange things to\\n    # arguments, including deleting empty args and deleting\\n    # '}}' from arguments. See #2070\\n    return _CLOSE_BRACE_AFTER_CLOSE_BRACE_RE.sub(' }', steps_desc)",
                    "func_fullName": "mrjob.spark.runner._emr_proof_steps_desc( steps_desc )"
                },
                {
                    "func_id": 1442,
                    "func_name": "_check_step",
                    "func_desc": "_check_step",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _check_step(self, step, step_num):\\n        \"\"\"Don't try to run steps that include commands or use manifests.\"\"\"\\n        super(SparkMRJobRunner, self)._check_step(step, step_num)\\n\\n        if step.get('input_manifest'):\\n            raise NotImplementedError(\\n                'spark runner does not support input manifests')\\n\\n        # we don't currently support commands, but we *could* (see #1956).\\n        if step['type'] == 'streaming':\\n            if not self._mrjob_cls:\\n                raise ValueError(\\n                    'You must set mrjob_cls to run streaming steps')\\n\\n            for mrc in ('mapper', 'combiner', 'reducer'):\\n                if step.get(mrc):\\n                    if 'command' in step[mrc] or 'pre_filter' in step[mrc]:\\n                        raise NotImplementedError(\\n                            \"step %d's %s runs a command, but spark\"\\n                            \" runner does not support commands\" % (\\n                                step_num, mrc))",
                    "func_fullName": "mrjob.spark.runner._check_step( self, step, step_num )"
                },
                {
                    "func_id": 1454,
                    "func_name": "_group_steps",
                    "func_desc": "_group_steps",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _group_steps(self, steps):\\n        \"\"\"Group streaming steps together.\"\"\"\\n        # a list of dicts with:\\n        #\\n        # type -- shared type of steps\\n        # steps -- list of steps in group\\n        # step_num -- (0-indexed) number of first step\\n        groups = []\\n\\n        for step_num, step in enumerate(steps):\\n            # should we add *step* to existing group of streaming steps?\\n            if (step['type'] == 'streaming' and groups and\\n                    groups[-1]['type'] == 'streaming' and\\n                    step.get('jobconf') ==\\n                    groups[-1]['steps'][0].get('jobconf')):\\n                groups[-1]['steps'].append(step)\\n            else:\\n                # start a new step group\\n                groups.append(dict(\\n                    type=step['type'],\\n                    steps=[step],\\n                    step_num=step_num))\\n\\n        return groups",
                    "func_fullName": "mrjob.spark.runner._group_steps( self, steps )"
                },
                {
                    "func_id": 1461,
                    "func_name": "_has_streaming_steps",
                    "func_desc": "_has_streaming_steps",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _has_streaming_steps(self):\\n        \"\"\"Are any of our steps \"streaming\" steps that would normally run\\n        on Hadoop Streaming?\"\"\"\\n        return any(step['type'] == 'streaming' for step in self._get_steps())",
                    "func_fullName": "mrjob.spark.runner._has_streaming_steps( self )"
                },
                {
                    "func_id": 1468,
                    "func_name": "_run_step",
                    "func_desc": "_run_step",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _run_step(\\n        step, step_num, rdd, make_mrc_job,\\n        num_reducers=None, sort_values=None,\\n        emulate_map_input_file=False,\\n        skip_internal_protocol=False):\\n    \"\"\"Run the given step on the RDD and return the transformed RDD.\"\"\"\\n    _check_step(step, step_num)\\n\\n    # we try to avoid initializing job instances here in the driver (see #2044\\n    # for why). However, while we can get away with initializing one instance\\n    # per partition in the mapper and reducer, that would be too inefficient\\n    # for combiners, which run on *two* key-value pairs at a time.\\n    #\\n    # but combiners are optional! if we can't initialize a combiner job\\n    # instance, we can just skip it!\\n\\n    # mapper\\n    if step.get('mapper'):\\n        rdd_includes_input_path = (emulate_map_input_file and step_num == 0)\\n\\n        rdd = _run_mapper(\\n            make_mrc_job, step_num, rdd, rdd_includes_input_path)\\n\\n    # combiner/shuffle-and-sort\\n    combiner_job = None\\n    if step.get('combiner'):\\n        try:\\n            _check_substep(step, step_num, 'combiner')\\n            combiner_job = make_mrc_job('combiner', step_num)\\n        except Exception:\\n            # if combiner needs to run subprocesses, or we can't\\n            # initialize a job instance, just skip combiners\\n            pass\\n\\n    if combiner_job:\\n        # _run_combiner() includes shuffle-and-sort\\n        rdd = _run_combiner(\\n            combiner_job, rdd,\\n            sort_values=sort_values,\\n            num_reducers=num_reducers)\\n    elif step.get('reducer'):\\n        rdd = _shuffle_and_sort(\\n            rdd, sort_values=sort_values, num_reducers=num_reducers,\\n            skip_internal_protocol=skip_internal_protocol)\\n\\n    # reducer\\n    if step.get('reducer'):\\n        rdd = _run_reducer(\\n            make_mrc_job, step_num, rdd, num_reducers=num_reducers)\\n\\n    return rdd",
                    "func_fullName": "mrjob.spark.harness._run_step( step, step_num, rdd, make_mrc_job, num_reducers, sort_values, emulate_map_input_file, skip_internal_protocol )"
                },
                {
                    "func_id": 1474,
                    "func_name": "_check_step",
                    "func_desc": "_check_step",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _check_step(step, step_num):\\n    \"\"\"Check that the given step description is for a MRStep\\n    with no input manifest\"\"\"\\n    if step.get('type') != 'streaming':\\n        raise ValueError(\\n            'step %d has unexpected type: %r' % (\\n                step_num, step.get('type')))\\n\\n    if step.get('input_manifest'):\\n        raise NotImplementedError(\\n            'step %d uses an input manifest, which is unsupported')\\n\\n    for mrc in ('mapper', 'reducer'):\\n        _check_substep(step, step_num, mrc)",
                    "func_fullName": "mrjob.spark.harness._check_step( step, step_num )"
                },
                {
                    "func_id": 1475,
                    "func_name": "_check_substep",
                    "func_desc": "_check_substep",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _check_substep(step, step_num, mrc):\\n    \"\"\"Raise :py:class:`NotImplementedError` if the given substep\\n    (e.g. ``'mapper'``) runs subprocesses.\"\"\"\\n    substep = step.get(mrc)\\n    if not substep:\\n        return\\n\\n    if substep.get('type') != 'script':\\n        raise NotImplementedError(\\n            \"step %d's %s has unexpected type: %r\" % (\\n                step_num, mrc, substep.get('type')))\\n\\n    if substep.get('pre_filter'):\\n        raise NotImplementedError(\\n            \"step %d's %s has pre-filter, which is unsupported\" % (\\n                step_num, mrc))",
                    "func_fullName": "mrjob.spark.harness._check_substep( step, step_num, mrc )"
                },
                {
                    "func_id": 1484,
                    "func_name": "increment_counter",
                    "func_desc": "increment_counter",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def increment_counter(group, counter, amount=1):\\n            counter_accumulator.add({group: {counter: amount}})",
                    "func_fullName": "mrjob.spark.harness.increment_counter( group, counter, amount )"
                },
                {
                    "func_id": 1841,
                    "func_name": "_get_step",
                    "func_desc": "_get_step",
                    "func_file": "spark_submit",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _get_step(options, parser, cl_args):\\n    \"\"\"Extract the step from the runner options.\"\"\"\\n    args = options.args\\n    main_class = options.main_class\\n    spark_args = _get_spark_args(parser, cl_args)\\n    script_or_jar = options.script_or_jar\\n\\n    if script_or_jar.lower().endswith('.jar'):\\n        return SparkJarStep(args=args,\\n                            jar=script_or_jar,\\n                            main_class=main_class,\\n                            spark_args=spark_args)\\n    elif script_or_jar.lower().split('.')[-1].startswith('py'):\\n        return SparkScriptStep(args=args,\\n                               script=script_or_jar,\\n                               spark_args=spark_args)\\n    else:\\n        raise ValueError('%s appears not to be a JAR or Python script' %\\n                         options.script_or_jar)",
                    "func_fullName": "mrjob.tools.spark_submit._get_step( options, parser, cl_args )"
                }
            ]
        },
        {
            "cluster_id": 7,
            "feature_id": 15,
            "feature_desc": "gamma=0.0550; k=15; a=0.25; combined=0.401; stability(ARI)=0.936; sep=0.153",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 246,
                    "func_name": "_print_help_for_runner",
                    "func_desc": "_print_help_for_runner",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _print_help_for_runner(opt_names, include_deprecated=False):\\n    help_parser = ArgumentParser(usage=SUPPRESS, add_help=False)\\n\\n    _add_runner_args(help_parser, opt_names,\\n                     include_deprecated=include_deprecated)\\n\\n    help_parser.print_help()",
                    "func_fullName": "mrjob.options._print_help_for_runner( opt_names, include_deprecated )"
                },
                {
                    "func_id": 247,
                    "func_name": "_print_basic_help",
                    "func_desc": "_print_basic_help",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _print_basic_help(option_parser, usage, include_deprecated=False,\\n                      include_steps=False):\\n    \"\"\"Print all help for the parser. Unlike similar functions, this needs a\\n    parser so that it can include custom options added by a\\n    :py:class:`~mrjob.job.MRJob`.\\n    \"\"\"\\n    help_parser = ArgumentParser(usage=usage, add_help=False)\\n\\n    _add_basic_args(help_parser)\\n    _add_job_args(help_parser, include_deprecated=include_deprecated,\\n                  include_steps=include_steps)\\n\\n    basic_dests = {action.dest for action in help_parser._actions}\\n\\n    # add other custom args added by the user\\n    for action in option_parser._actions:\\n        # option_parser already includes deprecated option dests\\n\\n        # this excludes deprecated switch aliases (e.g. --no-output)\\n        if action.dest in basic_dests:\\n            continue\\n\\n        # this excludes the --deprecated switch (which is explained below)\\n        if action.dest in _DEPRECATED_NON_RUNNER_OPTS:\\n            continue\\n\\n        # this excludes options that are shown with --help -r <runner>\\n        if action.dest in _RUNNER_OPTS:\\n            continue\\n\\n        # don't include steps if *include_steps* isn't set\\n        if action.dest in _STEP_OPTS and not include_steps:\\n            continue\\n\\n        # this excludes the ARGS option, which is already covered by usage\\n        if not action.option_strings:\\n            continue\\n\\n        # found a custom option. thanks, library user!\\n        help_parser._add_action(action)\\n\\n    help_parser.print_help()\\n\\n    print()\\n    print('To see help for a specific runner, use --help -r <runner name>')\\n    print()\\n    if not include_steps:\\n        print('To include switches that control what part of a job runs,'\\n              ' use --help -v')\\n        print()\\n    if not include_deprecated:\\n        print('To include help for deprecated options, add --deprecated')\\n        print()",
                    "func_fullName": "mrjob.options._print_basic_help( option_parser, usage, include_deprecated, include_steps )"
                },
                {
                    "func_id": 566,
                    "func_name": "_runner_class",
                    "func_desc": "_runner_class",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _runner_class(alias):\\n    \"\"\"Get the runner subclass corresponding to the given alias\\n    (importing code only as needed).\"\"\"\\n    if alias == 'dataproc':\\n        from mrjob.dataproc import DataprocJobRunner\\n        return DataprocJobRunner\\n\\n    elif alias == 'emr':\\n        from mrjob.emr import EMRJobRunner\\n        return EMRJobRunner\\n\\n    elif alias == 'hadoop':\\n        from mrjob.hadoop import HadoopJobRunner\\n        return HadoopJobRunner\\n\\n    elif alias == 'inline':\\n        from mrjob.inline import InlineMRJobRunner\\n        return InlineMRJobRunner\\n\\n    elif alias == 'local':\\n        from mrjob.local import LocalMRJobRunner\\n        return LocalMRJobRunner\\n\\n    elif alias == 'spark':\\n        from mrjob.spark.runner import SparkMRJobRunner\\n        return SparkMRJobRunner\\n\\n    else:\\n        raise ValueError('bad runner alias: %s' % alias)",
                    "func_fullName": "mrjob.runner._runner_class( alias )"
                },
                {
                    "func_id": 578,
                    "func_name": "run",
                    "func_desc": "run",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def run(self):\\n        \"\"\"Run the job, and block until it finishes.\\n\\n        Raise :py:class:`~mrjob.step.StepFailedException` if there\\n        are any problems (except on\\n        :py:class:`~mrjob.inline.InlineMRJobRunner`, where we raise the\\n        actual exception that caused the step to fail).\\n        \"\"\"\\n        if self._ran_job:\\n            raise ValueError('Job already ran!')\\n\\n        if self._num_steps() == 0:\\n            raise ValueError('Job has no steps!')\\n\\n        self._create_dir_archives()\\n        # TODO: no point in checking input paths if we're going to\\n        # make a manifest out of them\\n        self._check_input_paths()\\n        self._add_input_files_for_upload()\\n        self._create_input_manifest_if_needed()\\n        self._run()\\n        self._ran_job = True\\n\\n        last_step = self._get_steps()[-1]\\n\\n        # only print this message if the last step uses our output dir\\n        if 'args' not in last_step or OUTPUT in last_step['args']:\\n            log.info('job output is in %s' % self._output_dir)",
                    "func_fullName": "mrjob.runner.run( self )"
                },
                {
                    "func_id": 579,
                    "func_name": "cat_output",
                    "func_desc": "cat_output",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def cat_output(self):\\n        \"\"\"Stream the job's output, as a stream of ``bytes``. If there are\\n        multiple output files, there will be an empty bytestring\\n        (``b''``) between them.\\n\\n        Like Hadoop input formats, we ignore files and subdirectories whose\\n        names start with ``\"_\"`` or ``\".\"`` (e.g. ``_SUCCESS``, ``_logs/``,\\n        ``.part-00000.crc``.\\n\\n        .. versionchanged:: 0.6.8\\n\\n           Ignore file/dirnames starting with ``\".\"`` as well as ``\"_\"``.\\n        \"\"\"\\n        output_dir = self.get_output_dir()\\n        if output_dir is None:\\n            raise ValueError('Run the job before streaming output')\\n\\n        if self._closed is True:\\n            log.warning(\\n                'WARNING! Trying to stream output from a closed runner, output'\\n                ' will probably be empty.')\\n\\n        log.info('Streaming final output from %s...' % output_dir)\\n\\n        def split_path(path):\\n            while True:\\n                base, name = os.path.split(path)\\n\\n                # no more elements\\n                if not name:\\n                    break\\n\\n                yield name\\n\\n                path = base\\n\\n        def ls_output():\\n            for filename in self.fs.ls(output_dir):\\n                subpath = filename[len(output_dir):]\\n                # Hadoop ignores files and dirs inside the output dir\\n                # whose names start with '_' or '.'. See #1337.\\n                if not (any(name[0] in '_.'\\n                            for name in split_path(subpath))):\\n                    yield filename\\n\\n        for i, filename in enumerate(ls_output()):\\n            if i > 0:\\n                yield b''  # EOF of previous file\\n\\n            for chunk in self.fs._cat_file(filename):\\n                yield chunk",
                    "func_fullName": "mrjob.runner.cat_output( self )"
                },
                {
                    "func_id": 649,
                    "func_name": "_write_script",
                    "func_desc": "_write_script",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _write_script(self, lines, path, description):\\n        \"\"\"Write text of a setup script, input manifest, etc. to the given\\n        file.\\n\\n        By default, this writes binary data. Redefine :py:meth:`write_lines`\\n        to use other line endings.\\n\\n        :param lines: a list of lines as ``str``\\n        :param path: path of file to write to\\n        :param description: what we're writing to, for debug messages\\n        \"\"\"\\n        log.debug('Writing %s to %s:' % (description, path))\\n        for line in lines:\\n            log.debug('  ' + line)\\n\\n        self._write_script_lines(lines, path)",
                    "func_fullName": "mrjob.runner._write_script( self, lines, path, description )"
                },
                {
                    "func_id": 650,
                    "func_name": "_write_script_lines",
                    "func_desc": "_write_script_lines",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _write_script_lines(self, lines, path):\\n        \"\"\"Write text to the given file. By default, this writes\\n        binary data, but can be redefined to use local line endings.\"\"\"\\n        with open(path, 'wb') as f:\\n            for line in lines:\\n                f.write((line + '\\n').encode('utf-8'))",
                    "func_fullName": "mrjob.runner._write_script_lines( self, lines, path )"
                },
                {
                    "func_id": 653,
                    "func_name": "ls_output",
                    "func_desc": "ls_output",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def ls_output():\\n            for filename in self.fs.ls(output_dir):\\n                subpath = filename[len(output_dir):]\\n                # Hadoop ignores files and dirs inside the output dir\\n                # whose names start with '_' or '.'. See #1337.\\n                if not (any(name[0] in '_.'\\n                            for name in split_path(subpath))):\\n                    yield filename",
                    "func_fullName": "mrjob.runner.ls_output(  )"
                },
                {
                    "func_id": 1839,
                    "func_name": "main",
                    "func_desc": "main",
                    "func_file": "spark_submit",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def main(cl_args=None):\\n    parser = _make_arg_parser()\\n    options = parser.parse_args(cl_args)\\n\\n    runner_alias = options.runner or _DEFAULT_RUNNER\\n    runner_class = _runner_class(runner_alias)\\n\\n    if options.help or not options.script_or_jar:\\n        _print_help(options, runner_class)\\n        sys.exit(0)\\n\\n    MRJob.set_up_logging(\\n        quiet=options.quiet,\\n        verbose=options.verbose,\\n    )\\n\\n    kwargs = _get_runner_opt_kwargs(options, runner_class)\\n    kwargs.update(_HARD_CODED_OPTS)\\n\\n    kwargs['input_paths'] = [os.devnull]\\n\\n    step = _get_step(options, parser, cl_args)\\n    kwargs['steps'] = [step.description()]\\n\\n    runner = runner_class(**kwargs)\\n\\n    try:\\n        runner.run()\\n    finally:\\n        runner.cleanup()",
                    "func_fullName": "mrjob.tools.spark_submit.main( cl_args )"
                },
                {
                    "func_id": 1840,
                    "func_name": "_get_runner_opt_kwargs",
                    "func_desc": "_get_runner_opt_kwargs",
                    "func_file": "spark_submit",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _get_runner_opt_kwargs(options, runner_class):\\n    \"\"\"Extract the options for the given runner class from *options*.\"\"\"\\n    return {opt_name: getattr(options, opt_name)\\n            for opt_name in runner_class.OPT_NAMES\\n            if hasattr(options, opt_name)}",
                    "func_fullName": "mrjob.tools.spark_submit._get_runner_opt_kwargs( options, runner_class )"
                },
                {
                    "func_id": 1848,
                    "func_name": "_print_help",
                    "func_desc": "_print_help",
                    "func_file": "spark_submit",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _print_help(options, runner_class):\\n    if options.help and options.runner:\\n        # if user specifies -r without -h, show basic help\\n        _print_help_for_runner(runner_class,\\n                               include_deprecated=options.deprecated)\\n    else:\\n        _print_basic_help(include_deprecated=options.deprecated)",
                    "func_fullName": "mrjob.tools.spark_submit._print_help( options, runner_class )"
                },
                {
                    "func_id": 1849,
                    "func_name": "_print_help_for_runner",
                    "func_desc": "_print_help_for_runner",
                    "func_file": "spark_submit",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _print_help_for_runner(runner_class, include_deprecated=False):\\n    help_parser = ArgumentParser(usage=SUPPRESS, add_help=False)\\n\\n    arg_group = help_parser.add_argument_group(\\n        'optional arguments for %s runner' % runner_class.alias)\\n\\n    # don't include hard-coded opts or opts in basic help\\n    opt_names = runner_class.OPT_NAMES - set(_HARD_CODED_OPTS)\\n\\n    # don't include switches already in basic help\\n    suppress_switches = set(_SPARK_SUBMIT_SWITCHES.values())\\n\\n    # simplify description of aliases of switches in basic help\\n    customize_switches = {\\n        v: dict(help='Alias for %s' % k)\\n        for k, v in _SWITCH_ALIASES.items()\\n    }\\n\\n    _add_runner_args(arg_group, opt_names,\\n                     include_deprecated=include_deprecated,\\n                     customize_switches=customize_switches,\\n                     suppress_switches=suppress_switches)\\n\\n    help_parser.print_help()",
                    "func_fullName": "mrjob.tools.spark_submit._print_help_for_runner( runner_class, include_deprecated )"
                },
                {
                    "func_id": 1850,
                    "func_name": "_print_basic_help",
                    "func_desc": "_print_basic_help",
                    "func_file": "spark_submit",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _print_basic_help(include_deprecated=False):\\n    _make_basic_help_parser(include_deprecated).print_help()\\n\\n    if not include_deprecated:\\n        print()\\n        print(_DEPRECATED_OPT_HELP)",
                    "func_fullName": "mrjob.tools.spark_submit._print_basic_help( include_deprecated )"
                }
            ]
        },
        {
            "cluster_id": 7,
            "feature_id": 16,
            "feature_desc": "gamma=0.0550; k=15; a=0.25; combined=0.401; stability(ARI)=0.936; sep=0.153",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 250,
                    "func_name": "__call__",
                    "func_desc": "__call__",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __call__(self, parser, namespace, value, option_string=None):\\n        try:\\n            k, v = value.split('=', 1)\\n        except ValueError:\\n            parser.error('%s argument %r is not of the form KEY=VALUE' % (\\n                option_string, value))\\n\\n        _default_to(namespace, self.dest, {})\\n        getattr(namespace, self.dest)[k] = v",
                    "func_fullName": "mrjob.options.__call__( self, parser, namespace, value, option_string )"
                },
                {
                    "func_id": 251,
                    "func_name": "__call__",
                    "func_desc": "__call__",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __call__(self, parser, namespace, value, option_string=None):\\n        _default_to(namespace, self.dest, {})\\n        getattr(namespace, self.dest)[value] = None",
                    "func_fullName": "mrjob.options.__call__( self, parser, namespace, value, option_string )"
                },
                {
                    "func_id": 252,
                    "func_name": "__call__",
                    "func_desc": "__call__",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __call__(self, parser, namespace, value, option_string=None):\\n        result = []\\n\\n        for choice in value.split(','):\\n            if choice in CLEANUP_CHOICES:\\n                result.append(choice)\\n            else:\\n                parser.error(\\n                    '%s got %s, which is not one of: %s' %\\n                    (option_string, choice, ', '.join(CLEANUP_CHOICES)))\\n\\n        if 'NONE' in result and len(set(result)) > 1:\\n            parser.error(\\n                '%s: Cannot clean up both nothing and something!' %\\n                option_string)\\n\\n        setattr(namespace, self.dest, result)",
                    "func_fullName": "mrjob.options.__call__( self, parser, namespace, value, option_string )"
                },
                {
                    "func_id": 253,
                    "func_name": "__call__",
                    "func_desc": "__call__",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __call__(self, parser, namespace, value, option_string=None):\\n        items = [s.strip() for s in value.split(',') if s]\\n\\n        setattr(namespace, self.dest, items)",
                    "func_fullName": "mrjob.options.__call__( self, parser, namespace, value, option_string )"
                },
                {
                    "func_id": 254,
                    "func_name": "__call__",
                    "func_desc": "__call__",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __call__(self, parser, namespace, value, option_string=None):\\n        _default_to(namespace, self.dest, [])\\n\\n        items = [s.strip() for s in value.split(',') if s]\\n\\n        getattr(namespace, self.dest).extend(items)",
                    "func_fullName": "mrjob.options.__call__( self, parser, namespace, value, option_string )"
                },
                {
                    "func_id": 255,
                    "func_name": "__call__",
                    "func_desc": "__call__",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __call__(self, parser, namespace, value, option_string=None):\\n        _default_to(namespace, self.dest, [])\\n\\n        args = shlex_split(value)\\n\\n        getattr(namespace, self.dest).extend(args)",
                    "func_fullName": "mrjob.options.__call__( self, parser, namespace, value, option_string )"
                },
                {
                    "func_id": 256,
                    "func_name": "__call__",
                    "func_desc": "__call__",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __call__(self, parser, namespace, value, option_string=None):\\n        _default_to(namespace, self.dest, [])\\n\\n        try:\\n            j = json.loads(value)\\n        except ValueError as e:\\n            parser.error('Malformed JSON passed to %s: %s' % (\\n                option_string, str(e)))\\n\\n        getattr(namespace, self.dest).append(j)",
                    "func_fullName": "mrjob.options.__call__( self, parser, namespace, value, option_string )"
                },
                {
                    "func_id": 257,
                    "func_name": "__call__",
                    "func_desc": "__call__",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __call__(self, parser, namespace, value, option_string=None):\\n        try:\\n            k, v = value.split('=', 1)\\n        except ValueError:\\n            parser.error('%s argument %r is not of the form KEY=VALUE' % (\\n                option_string, value))\\n\\n        try:\\n            v = json.loads(v)\\n        except ValueError:\\n            if _PROBABLY_JSON_RE.match(v):\\n                parser.error('%s argument %r is not valid JSON' % (\\n                    option_string, value))\\n\\n        _default_to(namespace, self.dest, {})\\n        getattr(namespace, self.dest)[k] = v",
                    "func_fullName": "mrjob.options.__call__( self, parser, namespace, value, option_string )"
                },
                {
                    "func_id": 258,
                    "func_name": "__call__",
                    "func_desc": "__call__",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __call__(self, parser, namespace, value, option_string=None):\\n        try:\\n            j = json.loads(value)\\n        except ValueError as e:\\n            parser.error('Malformed JSON passed to %s: %s' % (\\n                option_string, str(e)))\\n\\n        setattr(namespace, self.dest, j)",
                    "func_fullName": "mrjob.options.__call__( self, parser, namespace, value, option_string )"
                },
                {
                    "func_id": 259,
                    "func_name": "__call__",
                    "func_desc": "__call__",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __call__(self, parser, namespace, value, option_string=None):\\n        try:\\n            ports = _parse_port_range_list(value)\\n        except ValueError as e:\\n            parser.error('%s: invalid port range list %r: \\n%s' %\\n                         (option_string, value, e.args[0]))\\n\\n        setattr(namespace, self.dest, ports)",
                    "func_fullName": "mrjob.options.__call__( self, parser, namespace, value, option_string )"
                },
                {
                    "func_id": 260,
                    "func_name": "suppressed",
                    "func_desc": "suppressed",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def suppressed(switches):\\n        return any(sw in suppress_switches for sw in switches)",
                    "func_fullName": "mrjob.options.suppressed( switches )"
                },
                {
                    "func_id": 262,
                    "func_name": "error",
                    "func_desc": "error",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def error(msg):\\n        raise ValueError(msg)",
                    "func_fullName": "mrjob.options.error( msg )"
                },
                {
                    "func_id": 263,
                    "func_name": "__call__",
                    "func_desc": "__call__",
                    "func_file": "options",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def __call__(self, parser, namespace, values, option_string=None):\\n            # ignore *namespace*, append to *results*\\n            results.append((self.dest, option_string, values))",
                    "func_fullName": "mrjob.options.__call__( self, parser, namespace, values, option_string )"
                },
                {
                    "func_id": 567,
                    "func_name": "_basename",
                    "func_desc": "_basename",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _basename(path_or_uri):\\n    if is_uri(path_or_uri):\\n        return posixpath.basename(path_or_uri)\\n    else:\\n        return os.path.basename(path_or_uri)",
                    "func_fullName": "mrjob.runner._basename( path_or_uri )"
                },
                {
                    "func_id": 568,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, mr_job_script=None, conf_paths=None,\\n                 extra_args=None,\\n                 hadoop_input_format=None, hadoop_output_format=None,\\n                 input_paths=None, output_dir=None, partitioner=None,\\n                 sort_values=None, stdin=None, steps=None,\\n                 step_output_dir=None,\\n                 **opts):\\n        \"\"\"All runners take the following keyword arguments:\\n\\n        :type mr_job_script: str\\n        :param mr_job_script: the path of the ``.py`` file containing the\\n                              :py:class:`~mrjob.job.MRJob`. If this is None,\\n                              you won't actually be able to :py:meth:`run` the\\n                              job, but other utilities (e.g. :py:meth:`ls`)\\n                              will work.\\n        :type conf_paths: None or list\\n        :param conf_paths: List of config files to combine and use, or None to\\n                           search for mrjob.conf in the default locations.\\n        :type extra_args: list of str\\n        :param extra_args: a list of extra cmd-line arguments to pass to the\\n                           mr_job script. This is a hook to allow jobs to take\\n                           additional arguments.\\n        :type hadoop_input_format: str\\n        :param hadoop_input_format: name of an optional Hadoop ``InputFormat``\\n                                    class. Passed to Hadoop along with your\\n                                    first step with the ``-inputformat``\\n                                    option. Note that if you write your own\\n                                    class, you'll need to include it in your\\n                                    own custom streaming jar (see\\n                                    :mrjob-opt:`hadoop_streaming_jar`).\\n        :type hadoop_output_format: str\\n        :param hadoop_output_format: name of an optional Hadoop\\n                                     ``OutputFormat`` class. Passed to Hadoop\\n                                     along with your first step with the\\n                                     ``-outputformat`` option. Note that if you\\n                                     write your own class, you'll need to\\n                                     include it in your own custom streaming\\n                                     jar (see\\n                                     :mrjob-opt:`hadoop_streaming_jar`).\\n        :type input_paths: list of str\\n        :param input_paths: Input files for your job. Supports globs and\\n                            recursively walks directories (e.g.\\n                            ``['data/common/', 'data/training/*.gz']``). If\\n                            this is left blank, we'll read from stdin\\n        :type output_dir: str\\n        :param output_dir: An empty/non-existent directory where Hadoop\\n                           should put the final output from the job.\\n                           If you don't specify an output directory, we'll\\n                           output into a subdirectory of this job's temporary\\n                           directory. You can control this from the command\\n                           line with ``--output-dir``. This option cannot be\\n                           set from configuration files. If used with the\\n                           hadoop runner, this path does not need to be fully\\n                           qualified with ``hdfs://`` URIs because it's\\n                           understood that it has to be on HDFS.\\n        :type partitioner: str\\n        :param partitioner: Optional name of a Hadoop partitioner class, e.g.\\n                            ``'org.apache.hadoop.mapred.lib.HashPartitioner'``.\\n                            Hadoop streaming will use this to determine how\\n                            mapper output should be sorted and distributed\\n                            to reducers.\\n        :type sort_values: bool\\n        :param sort_values: if true, set partitioners and jobconf variables\\n                            so that reducers to receive the values\\n                            associated with any key in sorted order (sorted by\\n                            their *encoded* value). Also known as secondary\\n                            sort.\\n        :param stdin: an iterable (can be a ``BytesIO`` or even a list) to use\\n                      as stdin. This is a hook for testing; if you set\\n                      ``stdin`` via :py:meth:`~mrjob.job.MRJob.sandbox`, it'll\\n                      get passed through to the runner. If for some reason\\n                      your lines are missing newlines, we'll add them;\\n                      this makes it easier to write automated tests.\\n        :param steps: a list of descriptions of steps to run (see :doc:`step`\\n                      for description formats)\\n        :type step_output_dir: str\\n        :param step_output_dir: An empty/non-existent directory where Hadoop\\n                                should put output from all steps other than\\n                                the last one (this only matters for multi-step\\n                                jobs). Currently ignored by local runners.\\n        \"\"\"\\n        self._ran_job = False\\n\\n        # opts are made from:\\n        #\\n        # empty defaults (everything set to None)\\n        # runner-specific defaults\\n        # opts from config file(s)\\n        # opts from command line\\n        self._opts = self._combine_confs(\\n            [(None, {key: None for key in self.OPT_NAMES})] +\\n            [(None, self._default_opts())] +\\n            load_opts_from_mrjob_confs(self.alias, conf_paths) +\\n            [('the command line', opts)]\\n        )\\n\\n        log.debug('Active configuration:')\\n        log.debug(pprint.pformat({\\n            opt_key: self._obfuscate_opt(opt_key, opt_value)\\n            for opt_key, opt_value in self._opts.items()\\n        }))\\n\\n        self._fs = None\\n\\n        # a local tmp directory that will be cleaned up when we're done\\n        # access/make this using self._get_local_tmp_dir()\\n        self._local_tmp_dir = None\\n\\n        if self._emulate_archives_on_spark():\\n            # keep Spark from auto-uncompressing tarballs\\n            archive_file_suffix = '.file'\\n        else:\\n            # otherwise, leave as-is, so that --archive will\\n            # work properly\\n            archive_file_suffix = ''\\n\\n        self._working_dir_mgr = WorkingDirManager(\\n            archive_file_suffix=archive_file_suffix)\\n\\n        # mapping from dir to path for corresponding archive. we pick\\n        # paths during init(), but don't actually create the archives\\n        # until self._create_dir_archives() is called\\n        self._dir_to_archive_path = {}\\n        # dir archive names (the filename minus \".tar.gz\") already taken\\n        self._dir_archive_names_taken = set()\\n        # set of dir_archives that have actually been created\\n        self._dir_archives_created = set()\\n\\n        # set this to an :py:class:`~mrjob.setup.UploadDirManager` in\\n        # runners that upload files to HDFS, S3, etc.\\n        #\\n        # this manager should not handle files belonging to\\n        # self._working_dir_mgr,\\n        # which, if they are uploaded, will go into self._wd_upload_dir()\\n        self._upload_mgr = None\\n\\n        self._script_path = mr_job_script\\n        if self._script_path:\\n            self._working_dir_mgr.add('file', self._script_path)\\n\\n        # give this job a unique name\\n        self._job_key = self._make_unique_job_key()\\n\\n        # extra args to our job\\n        self._extra_args = list(extra_args) if extra_args else []\\n        for extra_arg in self._extra_args:\\n            if isinstance(extra_arg, dict):\\n                if extra_arg.get('type') != 'file':\\n                    raise NotImplementedError\\n                self._working_dir_mgr.add(**extra_arg)\\n\\n        # set up uploading\\n        for hash_path in self._opts['upload_files']:\\n            uf = parse_legacy_hash_path('file', hash_path,\\n                                        must_name='upload_files')\\n            self._working_dir_mgr.add(**uf)\\n\\n        for hash_path in self._opts['upload_archives']:\\n            ua = parse_legacy_hash_path('archive', hash_path,\\n                                        must_name='upload_archives')\\n            self._working_dir_mgr.add(**ua)\\n\\n        for hash_path in self._opts['upload_dirs']:\\n            # pick name based on directory path\\n            ud = parse_legacy_hash_path('dir', hash_path,\\n                                        must_name='upload_archives')\\n            # but feed working_dir_mgr the archive's path\\n            archive_path = self._dir_archive_path(ud['path'])\\n            self._working_dir_mgr.add(\\n                'archive', archive_path, name=ud['name'])\\n\\n        # Where to read input from (log files, etc.)\\n        self._input_paths = input_paths or ['-']  # by default read from stdin\\n        if PY2:\\n            self._stdin = stdin or sys.stdin\\n        else:\\n            self._stdin = stdin or sys.stdin.buffer\\n        self._stdin_path = None  # temp file containing dump from stdin\\n\\n        # where to keep the input manifest\\n        self._input_manifest_path = None\\n\\n        # store output_dir\\n        self._output_dir = output_dir\\n\\n        # store partitioner\\n        self._partitioner = partitioner\\n\\n        # store sort_values\\n        self._sort_values = sort_values\\n\\n        # store step_output_dir\\n        self._step_output_dir = step_output_dir\\n\\n        # store hadoop input and output formats\\n        self._hadoop_input_format = hadoop_input_format\\n        self._hadoop_output_format = hadoop_output_format\\n\\n        # check and store *steps*\\n        self._steps = []\\n        if steps:\\n            self._check_steps(steps)\\n            self._steps = copy.deepcopy(steps)\\n\\n        # this variable marks whether a cleanup has happened and this runner's\\n        # output stream is no longer available.\\n        self._closed = False",
                    "func_fullName": "mrjob.runner.__init__( self, mr_job_script, conf_paths, extra_args, hadoop_input_format, hadoop_output_format, input_paths, output_dir, partitioner, sort_values, stdin, steps, step_output_dir, **opts )"
                },
                {
                    "func_id": 588,
                    "func_name": "counters",
                    "func_desc": "counters",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def counters(self):\\n        \"\"\"Get counters associated with this run in this form::\\n\\n            [{'group name': {'counter1': 1, 'counter2': 2}},\\n             {'group name': ...}]\\n\\n        The list contains an entry for every step of the current job.\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.runner.counters( self )"
                },
                {
                    "func_id": 589,
                    "func_name": "__enter__",
                    "func_desc": "__enter__",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __enter__(self):\\n        \"\"\"Don't do anything special at start of with block\"\"\"\\n        return self",
                    "func_fullName": "mrjob.runner.__enter__( self )"
                },
                {
                    "func_id": 590,
                    "func_name": "__exit__",
                    "func_desc": "__exit__",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __exit__(self, type, value, traceback):\\n        \"\"\"Call self.cleanup() at end of with block.\"\"\"\\n        self.cleanup()",
                    "func_fullName": "mrjob.runner.__exit__( self, type, value, traceback )"
                },
                {
                    "func_id": 595,
                    "func_name": "_run",
                    "func_desc": "_run",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run(self):\\n        \"\"\"Run the job.\"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.runner._run( self )"
                },
                {
                    "func_id": 598,
                    "func_name": "_label",
                    "func_desc": "_label",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _label(self):\\n        \"\"\"Return *label* opt, or if not set, the name of the file\\n        containing the MRJob, minus extension, or if none, ``'no_script'``\"\"\"\\n        if self._opts['label']:\\n            return self._opts['label']\\n        elif self._script_path:\\n            return os.path.basename(self._script_path).split('.')[0]\\n        else:\\n            return 'no_script'",
                    "func_fullName": "mrjob.runner._label( self )"
                },
                {
                    "func_id": 1440,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, max_output_files=None, mrjob_cls=None, **kwargs):\\n        \"\"\"Create a Spark runner.\\n\\n        :param max_output_files: limit on number of output files when\\n                                 running streaming jobs. Can only be\\n                                 set on command line (not config file)\\n        :param mrjob_cls: class of the job you want to run. Used for\\n                          running streaming steps in Spark\\n        \"\"\"\\n        # need to set this before checking steps in superclass __init__()\\n        self._mrjob_cls = mrjob_cls\\n\\n        super(SparkMRJobRunner, self).__init__(**kwargs)\\n\\n        self._max_output_files = max_output_files\\n\\n        if self._opts['spark_tmp_dir']:\\n            self._check_spark_tmp_dir_opt()\\n\\n        self._spark_tmp_dir = self._pick_spark_tmp_dir()\\n\\n        # where local files are uploaded into Spark\\n        if is_uri(self._spark_tmp_dir):\\n            spark_files_dir = posixpath.join(self._spark_tmp_dir, 'files', '')\\n            self._upload_mgr = UploadDirManager(spark_files_dir)\\n\\n        # where to put job output (if not set explicitly)\\n        if not self._output_dir:\\n            self._output_dir = self.fs.join(self._spark_tmp_dir, 'output')\\n\\n        # keep track of where the spark-submit binary is\\n        self._spark_submit_bin = self._opts['spark_submit_bin']\\n\\n        # where to store a .zip file containing the MRJob, with a unique\\n        # module name\\n        self._job_script_zip_path = None\\n\\n        # counters, one per job step. (Counters will be {} for non-streaming\\n        # steps because Spark doesn't have counters).\\n        self._counters = []",
                    "func_fullName": "mrjob.spark.runner.__init__( self, max_output_files, mrjob_cls, **kwargs )"
                },
                {
                    "func_id": 1444,
                    "func_name": "_run",
                    "func_desc": "_run",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run(self):\\n        self.get_spark_submit_bin()  # find spark-submit up front\\n        self._create_setup_wrapper_scripts()\\n        self._upload_local_files()\\n        self._run_steps_on_spark()",
                    "func_fullName": "mrjob.spark.runner._run( self )"
                },
                {
                    "func_id": 1448,
                    "func_name": "counters",
                    "func_desc": "counters",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def counters(self):\\n        return deepcopy(self._counters)",
                    "func_fullName": "mrjob.spark.runner.counters( self )"
                }
            ]
        },
        {
            "cluster_id": 7,
            "feature_id": 17,
            "feature_desc": "gamma=0.0550; k=15; a=0.25; combined=0.401; stability(ARI)=0.936; sep=0.153",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 575,
                    "func_name": "_fix_cleanup_opt",
                    "func_desc": "_fix_cleanup_opt",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _fix_cleanup_opt(self, opt_key, opt_value, source):\\n        \"\"\"Fix a cleanup option, or raise ValueError.\"\"\"\\n        if isinstance(opt_value, string_types):\\n            opt_value = [opt_value]\\n\\n        if 'NONE' in opt_value and len(set(opt_value)) > 1:\\n            raise ValueError(\\n                'Cannot clean up both nothing and something!'\\n                ' (%s option from %s)' % (opt_key, source))\\n\\n        for cleanup_type in opt_value:\\n            if cleanup_type not in CLEANUP_CHOICES:\\n                raise ValueError(\\n                    '%s must be one of %s, not %s (from %s)' % (\\n                        opt_key, ', '.join(CLEANUP_CHOICES), opt_value,\\n                        source))\\n\\n        return opt_value",
                    "func_fullName": "mrjob.runner._fix_cleanup_opt( self, opt_key, opt_value, source )"
                },
                {
                    "func_id": 580,
                    "func_name": "_cleanup_mode",
                    "func_desc": "_cleanup_mode",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cleanup_mode(self, mode=None):\\n        \"\"\"Actual cleanup action to take based on various options\"\"\"\\n        if self._script_path and not self._ran_job:\\n            return mode or self._opts['cleanup_on_failure']\\n        else:\\n            return mode or self._opts['cleanup']",
                    "func_fullName": "mrjob.runner._cleanup_mode( self, mode )"
                },
                {
                    "func_id": 581,
                    "func_name": "_cleanup_cloud_tmp",
                    "func_desc": "_cleanup_cloud_tmp",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cleanup_cloud_tmp(self):\\n        \"\"\"Cleanup any files/directories on cloud storage (e.g. S3) we created\\n        while running this job. Should be safe to run this at any time, or\\n        multiple times.\\n        \"\"\"\\n        pass  # only EMR runner does this",
                    "func_fullName": "mrjob.runner._cleanup_cloud_tmp( self )"
                },
                {
                    "func_id": 582,
                    "func_name": "_cleanup_hadoop_tmp",
                    "func_desc": "_cleanup_hadoop_tmp",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cleanup_hadoop_tmp(self):\\n        \"\"\"Cleanup any files/directories on HDFS we created\\n        while running this job. Should be safe to run this at any time, or\\n        multiple times.\\n        \"\"\"\\n        pass  # only Hadoop runner does this",
                    "func_fullName": "mrjob.runner._cleanup_hadoop_tmp( self )"
                },
                {
                    "func_id": 583,
                    "func_name": "_cleanup_local_tmp",
                    "func_desc": "_cleanup_local_tmp",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cleanup_local_tmp(self):\\n        \"\"\"Cleanup any files/directories on the local machine we created while\\n        running this job. Should be safe to run this at any time, or multiple\\n        times.\\n\\n        This particular function removes any local tmp directories\\n        added to the list self._local_tmp_dirs\\n\\n        This won't remove output_dir if it's outside of our tmp dir.\\n        \"\"\"\\n        if self._local_tmp_dir:\\n            log.info('Removing temp directory %s...' % self._local_tmp_dir)\\n            try:\\n                rmtree(self._local_tmp_dir)\\n            except OSError as e:\\n                log.exception(e)\\n\\n        self._local_tmp_dir = None",
                    "func_fullName": "mrjob.runner._cleanup_local_tmp( self )"
                },
                {
                    "func_id": 584,
                    "func_name": "_cleanup_cluster",
                    "func_desc": "_cleanup_cluster",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cleanup_cluster(self):\\n        \"\"\"Terminate the cluster if there is one.\"\"\"\\n        pass  # this only happens on EMR",
                    "func_fullName": "mrjob.runner._cleanup_cluster( self )"
                },
                {
                    "func_id": 585,
                    "func_name": "_cleanup_logs",
                    "func_desc": "_cleanup_logs",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cleanup_logs(self):\\n        \"\"\"Cleanup any log files that are created as a side-effect of the job.\\n        \"\"\"\\n        pass  # this only happens on EMR",
                    "func_fullName": "mrjob.runner._cleanup_logs( self )"
                },
                {
                    "func_id": 586,
                    "func_name": "_cleanup_job",
                    "func_desc": "_cleanup_job",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cleanup_job(self):\\n        \"\"\"Stop any jobs that we created that are still running.\"\"\"\\n        pass  # currently disabled (see #1241)",
                    "func_fullName": "mrjob.runner._cleanup_job( self )"
                },
                {
                    "func_id": 587,
                    "func_name": "cleanup",
                    "func_desc": "cleanup",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def cleanup(self, mode=None):\\n        \"\"\"Clean up running jobs, temp files, and logs, subject to the\\n        *cleanup* option passed to the constructor.\\n\\n        If you create your runner in a ``with`` block,\\n        :py:meth:`cleanup` will be called automatically::\\n\\n            with mr_job.make_runner() as runner:\\n                ...\\n\\n            # cleanup() called automatically here\\n\\n        :param mode: override *cleanup* passed into the constructor. Should be\\n                     a list of strings from\\n                     :py:data:`~mrjob.options.CLEANUP_CHOICES`\\n        \"\"\"\\n        mode = self._cleanup_mode(mode)\\n\\n        def mode_has(*args):\\n            return any((choice in mode) for choice in args)\\n\\n        if self._script_path and not self._ran_job:\\n            if mode_has('CLUSTER', 'ALL'):\\n                self._cleanup_cluster()\\n\\n            if mode_has('JOB', 'ALL'):\\n                self._cleanup_job()\\n\\n        if mode_has('ALL', 'TMP', 'CLOUD_TMP'):\\n            self._cleanup_cloud_tmp()\\n\\n        if mode_has('ALL', 'TMP', 'HADOOP_TMP'):\\n            self._cleanup_hadoop_tmp()\\n\\n        if mode_has('ALL', 'TMP', 'LOCAL_TMP'):\\n            self._cleanup_local_tmp()\\n\\n        if mode_has('ALL', 'LOGS'):\\n            self._cleanup_logs()\\n\\n        self._closed = True",
                    "func_fullName": "mrjob.runner.cleanup( self, mode )"
                }
            ]
        },
        {
            "cluster_id": 7,
            "feature_id": 18,
            "feature_desc": "gamma=0.0550; k=15; a=0.25; combined=0.401; stability(ARI)=0.936; sep=0.153",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 593,
                    "func_name": "get_output_dir",
                    "func_desc": "get_output_dir",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_output_dir(self):\\n        \"\"\"Find the directory containing the job output. If the job hasn't\\n        run yet, returns None\"\"\"\\n        if self._script_path and not self._ran_job:\\n            return None\\n\\n        return self._output_dir",
                    "func_fullName": "mrjob.runner.get_output_dir( self )"
                },
                {
                    "func_id": 596,
                    "func_name": "_get_local_tmp_dir",
                    "func_desc": "_get_local_tmp_dir",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_local_tmp_dir(self):\\n        \"\"\"Create a tmp directory on the local filesystem that will be\\n        cleaned up by self.cleanup()\"\"\"\\n        if not self._local_tmp_dir:\\n            tmp_dir = (self._opts['local_tmp_dir'] or\\n                       tempfile.gettempdir())\\n\\n            path = os.path.join(tmp_dir, self._job_key)\\n            log.info('Creating temp directory %s' % path)\\n            if os.path.isdir(path):\\n                rmtree(path)\\n            os.makedirs(path)\\n            self._local_tmp_dir = path\\n\\n        return self._local_tmp_dir",
                    "func_fullName": "mrjob.runner._get_local_tmp_dir( self )"
                },
                {
                    "func_id": 620,
                    "func_name": "_dir_archive_path",
                    "func_desc": "_dir_archive_path",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _dir_archive_path(self, dir_path):\\n        \"\"\"Assign a path for the archive of *dir_path* but don't\\n        actually create anything.\"\"\"\\n        if dir_path not in self._dir_to_archive_path:\\n            # we can check local paths now\\n            if not (is_uri(dir_path) or os.path.isdir(dir_path)):\\n                raise OSError('%s is not a directory!' % dir_path)\\n\\n            name = name_uniquely(\\n                dir_path, names_taken=self._dir_archive_names_taken)\\n            self._dir_archive_names_taken.add(name)\\n\\n            self._dir_to_archive_path[dir_path] = os.path.join(\\n                self._get_local_tmp_dir(), 'archives', name + '.tar.gz')\\n\\n        return self._dir_to_archive_path[dir_path]",
                    "func_fullName": "mrjob.runner._dir_archive_path( self, dir_path )"
                },
                {
                    "func_id": 621,
                    "func_name": "_create_dir_archives",
                    "func_desc": "_create_dir_archives",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _create_dir_archives(self):\\n        \"\"\"Call this to create all dir archives\"\"\"\\n        for dir_path in sorted(set(self._dir_to_archive_path)):\\n            self._create_dir_archive(dir_path)",
                    "func_fullName": "mrjob.runner._create_dir_archives( self )"
                },
                {
                    "func_id": 622,
                    "func_name": "_create_dir_archive",
                    "func_desc": "_create_dir_archive",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _create_dir_archive(self, dir_path):\\n        \"\"\"Helper for :py:meth:`archive_dir`\"\"\"\\n        if not self.fs.exists(dir_path):\\n            raise OSError('%s does not exist')\\n\\n        tar_gz_path = self._dir_archive_path(dir_path)\\n\\n        if tar_gz_path in self._dir_archives_created:\\n            return  # already created\\n\\n        if not os.path.isdir(os.path.dirname(tar_gz_path)):\\n            os.makedirs(os.path.dirname(tar_gz_path))\\n\\n        # for remote files\\n        tmp_download_path = os.path.join(\\n            self._get_local_tmp_dir(), 'tmp-download')\\n\\n        log.info('Archiving %s -> %s' % (dir_path, tar_gz_path))\\n\\n        with tarfile.open(tar_gz_path, mode='w:gz') as tar_gz:\\n            for path in self.fs.ls(dir_path):\\n                # fs.ls() only lists files\\n                if path == dir_path:\\n                    raise OSError('%s is a file, not a directory!' % dir_path)\\n\\n                # TODO: do we need this?\\n                if os.path.realpath(path) == os.path.realpath(tar_gz_path):\\n                    raise OSError(\\n                        'attempted to archive %s into itself!' % tar_gz_path)\\n\\n                if is_uri(path):\\n                    path_in_tar_gz = path[len(dir_path):].lstrip('/')\\n\\n                    log.info('  downloading %s -> %s' % (\\n                        path, tmp_download_path))\\n                    with open(tmp_download_path, 'wb') as f:\\n                        for chunk in self.fs.cat(path):\\n                            f.write(chunk)\\n                    local_path = tmp_download_path\\n                else:\\n                    path_in_tar_gz = path[len(dir_path):].lstrip(os.sep)\\n                    local_path = path\\n\\n                log.debug('  adding %s to %s' % (path, tar_gz_path))\\n                tar_gz.add(local_path, path_in_tar_gz, recursive=False)\\n\\n        self._dir_archives_created.add(tar_gz_path)",
                    "func_fullName": "mrjob.runner._create_dir_archive( self, dir_path )"
                },
                {
                    "func_id": 624,
                    "func_name": "_get_input_paths",
                    "func_desc": "_get_input_paths",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_input_paths(self):\\n        \"\"\"Get the paths to input files, dumping STDIN to a local\\n        file if need be.\"\"\"\\n        if self._input_manifest_path:\\n            return [self._input_manifest_path]\\n\\n        if '-' in self._input_paths:\\n            if self._stdin_path is None:\\n                # prompt user, so they don't think the process has stalled\\n                log.info('reading from STDIN')\\n\\n                stdin_path = os.path.join(self._get_local_tmp_dir(), 'STDIN')\\n                log.debug('dumping stdin to local file %s' % stdin_path)\\n                with open(stdin_path, 'wb') as stdin_file:\\n                    for line in self._stdin:\\n                        # catch missing newlines (often happens with test data)\\n                        if not line.endswith(b'\\n'):\\n                            line += b'\\n'\\n                        stdin_file.write(line)\\n\\n                self._stdin_path = stdin_path\\n\\n        return [self._stdin_path if p == '-' else p for p in self._input_paths]",
                    "func_fullName": "mrjob.runner._get_input_paths( self )"
                },
                {
                    "func_id": 626,
                    "func_name": "_check_input_paths",
                    "func_desc": "_check_input_paths",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _check_input_paths(self):\\n        \"\"\"Check that input exists prior to running the job, if the\\n        `check_input_paths` option is true.\"\"\"\\n        if not self._opts['check_input_paths']:\\n            return\\n\\n        for path in self._input_paths:\\n            self._check_input_path(path)",
                    "func_fullName": "mrjob.runner._check_input_paths( self )"
                },
                {
                    "func_id": 627,
                    "func_name": "_check_input_path",
                    "func_desc": "_check_input_path",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _check_input_path(self, path):\\n        \"\"\"Raise :py:class:`IOError` if the given input does not exist or\\n        is otherwise invalid. Override this to provide custom check\\n        behavior.\"\"\"\\n        if path == '-':\\n            return  # STDIN always exists\\n\\n        if not self.fs.can_handle_path(path):\\n            return  # no way to check (e.g. non-S3 URIs on EMR)\\n\\n        if not self.fs.exists(path):\\n            raise IOError(\\n                'Input path %s does not exist!' % (path,))",
                    "func_fullName": "mrjob.runner._check_input_path( self, path )"
                },
                {
                    "func_id": 628,
                    "func_name": "_add_input_files_for_upload",
                    "func_desc": "_add_input_files_for_upload",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _add_input_files_for_upload(self):\\n        \"\"\"If there is an upload manager, add input files to it.\"\"\"\\n        if self._upload_mgr:\\n            for path in self._get_input_paths():\\n                self._upload_mgr.add(path)",
                    "func_fullName": "mrjob.runner._add_input_files_for_upload( self )"
                },
                {
                    "func_id": 629,
                    "func_name": "_upload_local_files",
                    "func_desc": "_upload_local_files",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _upload_local_files(self):\\n        self._copy_files_to_wd_mirror()\\n\\n        if self._upload_mgr:\\n            self.fs.mkdir(self._upload_mgr.prefix)\\n\\n            log.info('Copying other local files to %s' %\\n                     self._upload_mgr.prefix)\\n            for src_path, uri in self._upload_mgr.path_to_uri().items():\\n                log.debug('  %s -> %s' % (src_path, uri))\\n                self.fs.put(src_path, uri)",
                    "func_fullName": "mrjob.runner._upload_local_files( self )"
                },
                {
                    "func_id": 631,
                    "func_name": "_wd_filenames_must_match",
                    "func_desc": "_wd_filenames_must_match",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _wd_filenames_must_match(self):\\n        \"\"\"When we tell Hadoop/Spark to put files in the working directory,\\n        must they have the same names as the files in the working dir?\\n\\n        This basically only happens with Spark on non-YARN masters. YARN/Hadoop\\n        allows you to specify a name for each file (``path#name_in_wd``).\\n        \"\"\"\\n        return self._has_spark_steps() and self._spark_master() != 'yarn'",
                    "func_fullName": "mrjob.runner._wd_filenames_must_match( self )"
                },
                {
                    "func_id": 633,
                    "func_name": "_copy_file_to_wd_mirror",
                    "func_desc": "_copy_file_to_wd_mirror",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _copy_file_to_wd_mirror(self, path, name):\\n        \"\"\"Upload/copy *path* to the appropriate place in the working dir\\n        mirror, if necessary.\\n\\n        We don't track whether something has already been uploaded.\\n        \"\"\"\\n        dest = self._dest_in_wd_mirror(path, name)\\n        if not dest:\\n            return\\n\\n        if is_uri(path):\\n            # file is visible to non-YARN Spark, but has the wrong name, so\\n            # download and re-upload it\\n            wd_tmp = os.path.join(self._get_local_tmp_dir(), 'wd-mirror')\\n            self.fs.mkdir(wd_tmp)\\n\\n            tmp_path = os.path.join(wd_tmp, name)\\n\\n            log.debug('  %s <- %s' % (tmp_path, path))\\n            try:\\n                with open(tmp_path, 'wb') as tmp_f:\\n                    for chunk in self.fs.cat(path):\\n                        tmp_f.write(chunk)\\n\\n                log.debug('  %s -> %s' % (tmp_path, dest))\\n                self.fs.put(tmp_path, dest)\\n            finally:\\n                os.remove(tmp_path)\\n        else:\\n            # upload it\\n            log.debug('  %s -> %s' % (path, dest))\\n            self.fs.put(path, dest)",
                    "func_fullName": "mrjob.runner._copy_file_to_wd_mirror( self, path, name )"
                },
                {
                    "func_id": 634,
                    "func_name": "_copy_files_to_wd_mirror",
                    "func_desc": "_copy_files_to_wd_mirror",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _copy_files_to_wd_mirror(self):\\n        \"\"\"Upload working dir files to the working dir mirror, if necessary.\\n\\n        This does not handle archives, which we always rename with\\n        hash paths anyhow (see #2059).\\n        \"\"\"\\n        wd_mirror = self._wd_mirror()\\n        if not wd_mirror:\\n            return\\n\\n        self.fs.mkdir(wd_mirror)\\n\\n        log.info('%s working dir files to %s...' %\\n                 ('uploading' if is_uri(wd_mirror) else 'copying', wd_mirror))\\n\\n        for name, path in sorted(\\n                self._working_dir_mgr.name_to_path('file').items()):\\n            self._copy_file_to_wd_mirror(path, name)\\n\\n        for name, path in sorted(\\n                self._working_dir_mgr.name_to_path('archive_file').items()):\\n            self._copy_file_to_wd_mirror(path, name)",
                    "func_fullName": "mrjob.runner._copy_files_to_wd_mirror( self )"
                },
                {
                    "func_id": 636,
                    "func_name": "_intermediate_output_dir",
                    "func_desc": "_intermediate_output_dir",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _intermediate_output_dir(self, step_num, local=False):\\n        \"\"\"A directory for intermediate output for the given step number.\"\"\"\\n        join = os.path.join if local else posixpath.join\\n\\n        return join(\\n            self._step_output_dir or self._default_step_output_dir(),\\n            '%04d' % step_num)",
                    "func_fullName": "mrjob.runner._intermediate_output_dir( self, step_num, local )"
                },
                {
                    "func_id": 637,
                    "func_name": "_default_step_output_dir",
                    "func_desc": "_default_step_output_dir",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _default_step_output_dir(self):\\n        \"\"\"Where to put output for steps other than the last one,\\n        if not specified by the *output_dir* constructor keyword.\\n        Usually you want this to be on HDFS (most efficient).\\n\\n        Define this in your runner subclass.\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.runner._default_step_output_dir( self )"
                },
                {
                    "func_id": 646,
                    "func_name": "_file_arg_hash_paths",
                    "func_desc": "_file_arg_hash_paths",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _file_arg_hash_paths(self, named_paths=None, always_use_hash=True):\\n        \"\"\"Helper function for the *upload_args methods. The names of all\\n        arguments to ``-files`` (or ``--files`` on Spark).\\n\\n        If *always_use_hash* is false, only use ``path#name`` syntax\\n        when the name is different.\\n        \"\"\"\\n        if named_paths is None:\\n            # just return every file managed by _working_dir_mgr\\n            named_paths = sorted(\\n                self._working_dir_mgr.name_to_path('file').items())\\n\\n        for name, path in named_paths:\\n            if not name:\\n                name = self._working_dir_mgr.name('file', path)\\n\\n            uri = self._dest_in_wd_mirror(path, name) or path\\n\\n            if not always_use_hash and _basename(uri) == name:\\n                yield uri\\n            else:\\n                yield '%s#%s' % (uri, name)",
                    "func_fullName": "mrjob.runner._file_arg_hash_paths( self, named_paths, always_use_hash )"
                },
                {
                    "func_id": 647,
                    "func_name": "_file_archive_hash_paths",
                    "func_desc": "_file_archive_hash_paths",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _file_archive_hash_paths(self, named_paths=None):\\n        \"\"\"Helper function for the *upload_args methods. The names of\\n        archives to pass to the ``--files`` switch of ``spark-submit``,\\n        since we can't use ``--archives``.\\n\\n        The names in *named_paths* should be of the archive destination\\n        (the 'archive' type in WorkingDirManager)\\n        not of the filename we're going to copy the archive to before\\n        unpacking it into its destination (the 'archive_file' type).\\n        \"\"\"\\n        if named_paths is None:\\n            named_paths = sorted(\\n                self._working_dir_mgr.name_to_path('archive').items())\\n\\n        for name, path in named_paths:\\n            if not name:\\n                name = self._working_dir_mgr.name('archive', path)\\n\\n            archive_file_name = self._working_dir_mgr.name(\\n                'archive_file', path)\\n\\n            uri = self._dest_in_wd_mirror(path, archive_file_name) or path\\n\\n            yield uri",
                    "func_fullName": "mrjob.runner._file_archive_hash_paths( self, named_paths )"
                },
                {
                    "func_id": 648,
                    "func_name": "_archive_arg_hash_paths",
                    "func_desc": "_archive_arg_hash_paths",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _archive_arg_hash_paths(self, named_paths=None):\\n        \"\"\"Helper function for the *upload_args methods. The names of all\\n        arguments to ``-archives`` (or ``--archives`` on Spark).\\n        \"\"\"\\n        # we always use path#name syntax, even on Spark, because unlike\\n        # with --files, Spark will either accept that syntax with --archives\\n        # (if we're on YARN) or ignore --archives completely (if we're on\\n        # any other Spark master)\\n        if named_paths is None:\\n            # just return every archive managed by _working_dir_mgr\\n            named_paths = sorted(\\n                self._working_dir_mgr.name_to_path('archive').items())\\n\\n        for name, path in named_paths:\\n            if not name:\\n                name = self._working_dir_mgr.name('archive', path)\\n\\n            # archives are uploaded to the working dir mirror by the\\n            # name of the original archive file, not the dir it unpacks into\\n            archive_file_name = self._working_dir_mgr.name(\\n                'archive_file', path)\\n\\n            uri = self._dest_in_wd_mirror(path, archive_file_name) or path\\n\\n            yield '%s#%s' % (uri, name)",
                    "func_fullName": "mrjob.runner._archive_arg_hash_paths( self, named_paths )"
                },
                {
                    "func_id": 652,
                    "func_name": "split_path",
                    "func_desc": "split_path",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def split_path(path):\\n            while True:\\n                base, name = os.path.split(path)\\n\\n                # no more elements\\n                if not name:\\n                    break\\n\\n                yield name\\n\\n                path = base",
                    "func_fullName": "mrjob.runner.split_path( path )"
                },
                {
                    "func_id": 1446,
                    "func_name": "_default_step_output_dir",
                    "func_desc": "_default_step_output_dir",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _default_step_output_dir(self):\\n        return self.fs.join(self._spark_tmp_dir, 'step-output')",
                    "func_fullName": "mrjob.spark.runner._default_step_output_dir( self )"
                },
                {
                    "func_id": 1447,
                    "func_name": "_counter_output_dir",
                    "func_desc": "_counter_output_dir",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _counter_output_dir(self, step_num):\\n        return self.fs.join(\\n            self._spark_tmp_dir, 'counter-output-step-%d' % step_num)",
                    "func_fullName": "mrjob.spark.runner._counter_output_dir( self, step_num )"
                },
                {
                    "func_id": 1452,
                    "func_name": "_py_files",
                    "func_desc": "_py_files",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _py_files(self):\\n        \"\"\"Patch in :py:attr:`_job_script_zip_path`, if running streaming\\n        steps.\"\"\"\\n        py_files = super(SparkMRJobRunner, self)._py_files()\\n\\n        if self._has_streaming_steps():\\n            py_files.append(self._create_job_script_zip())\\n\\n        return py_files",
                    "func_fullName": "mrjob.spark.runner._py_files( self )"
                },
                {
                    "func_id": 1467,
                    "func_name": "_text_file_with_path",
                    "func_desc": "_text_file_with_path",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _text_file_with_path(sc, path):\\n    \"\"\"Return an RDD that yields (path, line) for each line in the file.\\n\\n    *path* must be a single path, not a comma-separated list of paths\\n    \"\"\"\\n    from pyspark.sql import SparkSession\\n    from pyspark.sql import functions as F\\n\\n    spark = SparkSession(sc)\\n\\n    df = spark.read.text(path).select([\\n        F.input_file_name().alias('input_file_name'),\\n        F.col('value')\\n    ])\\n\\n    return df.rdd.map(\\n        lambda row: (row.input_file_name,\\n                     (row.value if isinstance(row.value, bytes)\\n                      else row.value.encode('utf_8')))\\n    )",
                    "func_fullName": "mrjob.spark.harness._text_file_with_path( sc, path )"
                }
            ]
        },
        {
            "cluster_id": 7,
            "feature_id": 19,
            "feature_desc": "gamma=0.0550; k=15; a=0.25; combined=0.401; stability(ARI)=0.936; sep=0.153",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 594,
                    "func_name": "get_hadoop_version",
                    "func_desc": "get_hadoop_version",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_hadoop_version(self):\\n        \"\"\"Return the version number of the Hadoop environment as a string if\\n        Hadoop is being used or simulated. Return None if not applicable.\\n\\n        :py:class:`~mrjob.emr.EMRJobRunner` infers this from the cluster.\\n        :py:class:`~mrjob.hadoop.HadoopJobRunner` gets this from\\n        ``hadoop version``. :py:class:`~mrjob.local.LocalMRJobRunner` has an\\n        additional `hadoop_version` option to specify which version it\\n        simulates.\\n        :py:class:`~mrjob.inline.InlineMRJobRunner` does not simulate Hadoop at\\n        all.\\n        \"\"\"\\n        return None",
                    "func_fullName": "mrjob.runner.get_hadoop_version( self )"
                },
                {
                    "func_id": 606,
                    "func_name": "_has_hadoop_streaming_steps",
                    "func_desc": "_has_hadoop_streaming_steps",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _has_hadoop_streaming_steps(self):\\n        \"\"\"Are any of our steps Hadoop Streaming steps?\"\"\"\\n        return any(step['type'] == 'streaming'\\n                   for step in self._get_steps())",
                    "func_fullName": "mrjob.runner._has_hadoop_streaming_steps( self )"
                },
                {
                    "func_id": 607,
                    "func_name": "_has_spark_steps",
                    "func_desc": "_has_spark_steps",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _has_spark_steps(self):\\n        \"\"\"Are any of our steps Spark steps? (e.g. spark, spark_jar,\\n        spark_script)\\n\\n        Generally used to determine if we need to install Spark on a cluster.\\n        \"\"\"\\n        return any(self._step_type_uses_spark(step['type'])\\n                   for step in self._get_steps())",
                    "func_fullName": "mrjob.runner._has_spark_steps( self )"
                },
                {
                    "func_id": 608,
                    "func_name": "_has_pyspark_steps",
                    "func_desc": "_has_pyspark_steps",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _has_pyspark_steps(self):\\n        \"\"\"Do any of our steps involve running Python on Spark?\\n        Includes spark and spark_script types, but not spark_jar.\\n\\n        Generally used to tell if we need a Spark setup script.\\n        \"\"\"\\n        return any(self._step_type_uses_pyspark(step['type'])\\n                   for step in self._get_steps())",
                    "func_fullName": "mrjob.runner._has_pyspark_steps( self )"
                },
                {
                    "func_id": 609,
                    "func_name": "_step_type_uses_spark",
                    "func_desc": "_step_type_uses_spark",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _step_type_uses_spark(self, step_type):\\n        \"\"\"Does this step run on Spark?\\n\\n        (This is re-defined in the Spark runner to include\\n        streaming steps, and used by mrjob.logs.mixin)\\n        \"\"\"\\n        return _is_spark_step_type(step_type)",
                    "func_fullName": "mrjob.runner._step_type_uses_spark( self, step_type )"
                },
                {
                    "func_id": 610,
                    "func_name": "_step_type_uses_pyspark",
                    "func_desc": "_step_type_uses_pyspark",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _step_type_uses_pyspark(self, step_type):\\n        \"\"\"Does this step involve running Python on Spark?\\n\\n        (This is re-defined in the Spark runner to include\\n        streaming steps, and used by mrjob.logs.mixin)\\n        \"\"\"\\n        return _is_pyspark_step_type(step_type)",
                    "func_fullName": "mrjob.runner._step_type_uses_pyspark( self, step_type )"
                },
                {
                    "func_id": 611,
                    "func_name": "_spark_master",
                    "func_desc": "_spark_master",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_master(self):\\n        return self._opts.get('spark_master') or 'local[*]'",
                    "func_fullName": "mrjob.runner._spark_master( self )"
                },
                {
                    "func_id": 612,
                    "func_name": "_spark_deploy_mode",
                    "func_desc": "_spark_deploy_mode",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_deploy_mode(self):\\n        return self._opts.get('spark_deploy_mode') or 'client'",
                    "func_fullName": "mrjob.runner._spark_deploy_mode( self )"
                },
                {
                    "func_id": 613,
                    "func_name": "_spark_driver_has_own_wd",
                    "func_desc": "_spark_driver_has_own_wd",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_driver_has_own_wd(self):\\n        \"\"\"Does the spark driver have a working directory different\\n        from the one *spark-submit* was run in?\\n\\n        (Only true in cluster mode.)\\n        \"\"\"\\n        return (self._spark_deploy_mode() == 'cluster' and\\n                self._spark_executors_have_own_wd())",
                    "func_fullName": "mrjob.runner._spark_driver_has_own_wd( self )"
                },
                {
                    "func_id": 614,
                    "func_name": "_spark_executors_have_own_wd",
                    "func_desc": "_spark_executors_have_own_wd",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_executors_have_own_wd(self):\\n        \"\"\"Do spark executors have a working directory different\\n        from the one *spark-submit* was run in?\\n\\n        (True on everything but local.)\\n        \"\"\"\\n        # note: local-cluster[...] master does in fact have working dirs\\n        return self._spark_master().split('[')[0] != 'local'",
                    "func_fullName": "mrjob.runner._spark_executors_have_own_wd( self )"
                },
                {
                    "func_id": 615,
                    "func_name": "_emulate_archives_on_spark",
                    "func_desc": "_emulate_archives_on_spark",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _emulate_archives_on_spark(self):\\n        \"\"\"True if spark-submit's --archives doesn't work on the given Spark\\n        master, which means we'll need to emulate archives in setup scripts.\\n        \"\"\"\\n        return self._spark_master() != 'yarn'",
                    "func_fullName": "mrjob.runner._emulate_archives_on_spark( self )"
                },
                {
                    "func_id": 618,
                    "func_name": "_spark_script_args",
                    "func_desc": "_spark_script_args",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_script_args(self, step_num, last_step_num=None):\\n        \"\"\"A list of args to the spark script/jar/MRJob, used by\\n        _args_for_spark_step().\\n\\n        *last_step_num* is only used by the Spark runner, where multiple\\n        streaming steps are run in a single Spark job.\"\"\"\\n        step = self._get_step(step_num)\\n\\n        if step['type'] == 'spark':\\n            # if on local[*] master, keep file upload args as-is (see #2031)\\n            local = not self._spark_executors_have_own_wd()\\n\\n            args = (\\n                [\\n                    '--step-num=%d' % step_num,\\n                    '--spark',\\n                ] + self._mr_job_extra_args(local=local) + [\\n                    INPUT,\\n                    OUTPUT,\\n                ]\\n            )\\n        elif step['type'] in ('spark_jar', 'spark_script'):\\n            args = step['args']\\n        else:\\n            raise TypeError('Bad step type: %r' % step['type'])\\n\\n        return self._interpolate_step_args(args, step_num)",
                    "func_fullName": "mrjob.runner._spark_script_args( self, step_num, last_step_num )"
                },
                {
                    "func_id": 1441,
                    "func_name": "_check_spark_tmp_dir_opt",
                    "func_desc": "_check_spark_tmp_dir_opt",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _check_spark_tmp_dir_opt(self):\\n        # warn if spark_tmp_dir isn't actually visible to Spark executors\\n        # (see #2062)\\n        tmp_dir_is_local = to_uri(\\n            self._opts['spark_tmp_dir']).startswith('file://')\\n        spark_master_is_local = self._spark_master().startswith('local')\\n\\n        if tmp_dir_is_local != spark_master_is_local:\\n            log.warning(\\n                'Warning: executors on Spark master %s may not be able to'\\n                ' access spark_tmp_dir %s' %\\n                (self._spark_master(), self._opts['spark_tmp_dir']))",
                    "func_fullName": "mrjob.spark.runner._check_spark_tmp_dir_opt( self )"
                },
                {
                    "func_id": 1445,
                    "func_name": "_pick_spark_tmp_dir",
                    "func_desc": "_pick_spark_tmp_dir",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _pick_spark_tmp_dir(self):\\n        if self._opts['spark_tmp_dir']:\\n            return self.fs.join(self._opts['spark_tmp_dir'], self._job_key)\\n        else:\\n            master = self._spark_master() or 'local'\\n            if master.startswith('local'):  # including local-cluster\\n                # need a local temp dir\\n                # add \"-spark\" so we don't collide with default local temp dir\\n                return os.path.join(\\n                    gettempdir(), self._job_key + '-spark')\\n            else:\\n                # use HDFS (same default as HadoopJobRunner)\\n                return posixpath.join(\\n                    fully_qualify_hdfs_path('tmp/mrjob'), self._job_key)",
                    "func_fullName": "mrjob.spark.runner._pick_spark_tmp_dir( self )"
                },
                {
                    "func_id": 1453,
                    "func_name": "_run_steps_on_spark",
                    "func_desc": "_run_steps_on_spark",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run_steps_on_spark(self):\\n        steps = self._get_steps()\\n\\n        for group in self._group_steps(steps):\\n            step_num = group['step_num']\\n            last_step_num = step_num + len(group['steps']) - 1\\n\\n            # the Spark harness can run several streaming steps in one job\\n            if step_num == last_step_num:\\n                step_desc = 'step %d' % (step_num + 1)\\n            else:\\n                step_desc = 'steps %d-%d' % (step_num + 1, last_step_num + 1)\\n\\n            log.info('Running %s of %d' % (step_desc, len(steps)))\\n\\n            self._run_step_on_spark(group['steps'][0], step_num, last_step_num)",
                    "func_fullName": "mrjob.spark.runner._run_steps_on_spark( self )"
                },
                {
                    "func_id": 1455,
                    "func_name": "_run_step_on_spark",
                    "func_desc": "_run_step_on_spark",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run_step_on_spark(self, step, step_num, last_step_num=None):\\n        spark_submit_args = self._args_for_spark_step(step_num, last_step_num)\\n\\n        env = dict(os.environ)\\n        env.update(self._spark_cmdenv(step_num))\\n\\n        returncode, step_interpretation = self._run_spark_submit(\\n            spark_submit_args, env, record_callback=_log_log4j_record)\\n\\n        counters = None\\n        if step['type'] == 'streaming':\\n            counter_file = self.fs.join(\\n                self._counter_output_dir(step_num), 'part-*')\\n            counter_json = b''.join(self.fs.cat(counter_file))\\n            if counter_json.strip():\\n                # json.loads() on Python 3.4/3.5 can't take bytes\\n                counters = json.loads(to_unicode(counter_json))\\n\\n        if isinstance(counters, list):\\n            self._counters.extend(counters)\\n\\n            # desc_num is 1-indexed user-readable step num\\n            for desc_num, counter_dict in enumerate(\\n                    counters, start=(step_num + 1)):\\n                if counter_dict:\\n                    log.info(_format_counters(\\n                        counter_dict,\\n                        desc=('Counters for step %d' % desc_num)))\\n\\n        # for non-streaming steps, there are no counters.\\n        # pad self._counters to match number of steps\\n        while len(self._counters) < (last_step_num or step_num) + 1:\\n            self._counters.append({})\\n\\n        if returncode:\\n            error = _pick_error(dict(step=step_interpretation))\\n            if error:\\n                _log_probable_cause_of_failure(log, error)\\n\\n            reason = str(CalledProcessError(returncode, spark_submit_args))\\n            raise StepFailedException(\\n                reason=reason, step_num=step_num, last_step_num=last_step_num,\\n                num_steps=self._num_steps())",
                    "func_fullName": "mrjob.spark.runner._run_step_on_spark( self, step, step_num, last_step_num )"
                },
                {
                    "func_id": 1456,
                    "func_name": "_spark_script_path",
                    "func_desc": "_spark_script_path",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_script_path(self, step_num):\\n        \"\"\"For streaming steps, return the path of the harness script\\n        (and handle other spark step types the usual way).\"\"\"\\n        step = self._get_step(step_num)\\n\\n        if step['type'] == 'streaming':\\n            return self._spark_harness_path()\\n        else:\\n            return super(SparkMRJobRunner, self)._spark_script_path(step_num)",
                    "func_fullName": "mrjob.spark.runner._spark_script_path( self, step_num )"
                },
                {
                    "func_id": 1457,
                    "func_name": "_spark_script_args",
                    "func_desc": "_spark_script_args",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_script_args(self, step_num, last_step_num=None):\\n        \"\"\"Generate spark harness args for streaming steps (and handle\\n        other spark step types the usual way).\\n        \"\"\"\\n        if last_step_num is None:\\n            last_step_num = step_num\\n\\n        steps = self._get_steps()[step_num:last_step_num + 1]\\n\\n        if steps[0]['type'] != 'streaming':\\n            return super(SparkMRJobRunner, self)._spark_script_args(\\n                step_num, last_step_num)\\n\\n        args = []\\n\\n        # class name\\n        args.append('%s.%s' % (self._job_script_module_name(),\\n                               self._mrjob_cls.__name__))\\n\\n        # INPUT\\n        args.append(\\n            ','.join(self._step_input_uris(step_num)))\\n\\n        # OUTPUT\\n        # note that we use the output dir for the *last* step\\n        args.append(\\n            self._step_output_uri(last_step_num))\\n\\n        # --hadoop-input-format\\n        if self._hadoop_input_format:\\n            args.extend(['--hadoop-input-format', self._hadoop_input_format])\\n        else:\\n            # you can't pass --hadoop-input-format '' to EMR's script runner,\\n            # so pass something that doesn't use an empty string (see #2055)\\n            args.append('--no-hadoop-input-format')\\n\\n        # --hadoop-output-format\\n        if self._hadoop_output_format:\\n            args.extend(['--hadoop-output-format', self._hadoop_output_format])\\n        else:\\n            # alternative to --hadoop-output-format '' (see #2055)\\n            args.append('--no-hadoop-output-format')\\n\\n        # --sort-values\\n        if self._sort_values:\\n            args.append('--sort-values')\\n        else:\\n            args.append('--no-sort-values')\\n\\n        # --steps-desc\\n        args.extend(['--steps-desc',\\n                     _emr_proof_steps_desc(json.dumps(steps))])\\n\\n        # --counter-output-dir, to simulate counters\\n        args.extend(['--counter-output-dir',\\n                     self._counter_output_dir(step_num)])\\n\\n        # --first-step-num, --last-step-num (step range)\\n        args.extend(['--first-step-num', str(step_num),\\n                     '--last-step-num', str(last_step_num)])\\n\\n        # --job-args (passthrough args)\\n\\n        # if on local[*] master, keep file upload args as-is (see #2031)\\n        job_args = self._mr_job_extra_args(\\n            local=not self._spark_executors_have_own_wd())\\n\\n        if job_args:\\n            args.extend(['--job-args', cmd_line(job_args)])\\n\\n        # --compression-codec\\n        jobconf = self._jobconf_for_step(step_num)\\n\\n        compress_conf = jobconf_from_dict(\\n            jobconf, 'mapreduce.output.fileoutputformat.compress')\\n        codec_conf = jobconf_from_dict(\\n            jobconf, 'mapreduce.output.fileoutputformat.compress.codec')\\n\\n        if compress_conf and compress_conf != 'false' and codec_conf:\\n            args.extend(['--compression-codec', codec_conf])\\n\\n        # --num-reducers\\n        num_reducers = jobconf_from_dict(jobconf, 'mapreduce.job.reduces')\\n        if num_reducers and int(num_reducers) > 0:\\n            args.extend(['--num-reducers', str(num_reducers)])\\n\\n        # --max-output-files\\n        if self._max_output_files:\\n            args.extend(['--max-output-files',\\n                         str(self._max_output_files)])\\n\\n        # --emulate-map-input-file\\n        if self._opts['emulate_map_input_file']:\\n            args.append('--emulate-map-input-file')\\n\\n        # --skip_internal-protocol\\n        if self._opts['skip_internal_protocol']:\\n            args.append('--skip-internal-protocol')\\n\\n        return args",
                    "func_fullName": "mrjob.spark.runner._spark_script_args( self, step_num, last_step_num )"
                },
                {
                    "func_id": 1458,
                    "func_name": "_spark_harness_path",
                    "func_desc": "_spark_harness_path",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_harness_path(self):\\n        \"\"\"Where to find the Spark harness.\"\"\"\\n        # harness requires pyspark, which may be in spark-submit's PYTHONPATH\\n        # but not ours. So don't import the harness unless we need it.\\n        # (See #2091)\\n        import mrjob.spark.harness\\n        path = mrjob.spark.harness.__file__\\n        if path.endswith('.pyc'):\\n            path = path[:-1]\\n        return path",
                    "func_fullName": "mrjob.spark.runner._spark_harness_path( self )"
                },
                {
                    "func_id": 1459,
                    "func_name": "_has_spark_steps",
                    "func_desc": "_has_spark_steps",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _has_spark_steps(self):\\n        \"\"\"Treat streaming steps as Spark steps.\"\"\"\\n        return (super(SparkMRJobRunner, self)._has_spark_steps() or\\n                self._has_streaming_steps())",
                    "func_fullName": "mrjob.spark.runner._has_spark_steps( self )"
                },
                {
                    "func_id": 1460,
                    "func_name": "_has_hadoop_streaming_steps",
                    "func_desc": "_has_hadoop_streaming_steps",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _has_hadoop_streaming_steps(self):\\n        # the Spark runner doesn't run \"streaming\" steps on Hadoop\\n        return False",
                    "func_fullName": "mrjob.spark.runner._has_hadoop_streaming_steps( self )"
                },
                {
                    "func_id": 1462,
                    "func_name": "_step_type_uses_pyspark",
                    "func_desc": "_step_type_uses_pyspark",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _step_type_uses_pyspark(self, step_type):\\n        \"\"\"Treat streaming steps as Spark steps that use Python.\"\"\"\\n        return (\\n            super(SparkMRJobRunner, self)._step_type_uses_pyspark(step_type) or\\n            step_type == 'streaming')",
                    "func_fullName": "mrjob.spark.runner._step_type_uses_pyspark( self, step_type )"
                },
                {
                    "func_id": 1463,
                    "func_name": "_step_type_uses_spark",
                    "func_desc": "_step_type_uses_spark",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _step_type_uses_spark(self, step_type):\\n        \"\"\"Treat streaming steps as Spark steps that use Python.\"\"\"\\n        return (\\n            super(SparkMRJobRunner, self)._step_type_uses_spark(step_type) or\\n            step_type == 'streaming')",
                    "func_fullName": "mrjob.spark.runner._step_type_uses_spark( self, step_type )"
                },
                {
                    "func_id": 1780,
                    "func_name": "_parse_spark_log",
                    "func_desc": "_parse_spark_log",
                    "func_file": "spark",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_spark_log(lines, record_callback=None):\\n    \"\"\"Parse a Spark log, looking for errors and application_id\"\"\"\\n    def yield_records():\\n        for record in _parse_hadoop_log4j_records(lines):\\n            if record_callback:\\n                record_callback(record)\\n            yield record\\n\\n    return _parse_spark_log_from_log4j_records(yield_records())",
                    "func_fullName": "mrjob.logs.spark._parse_spark_log( lines, record_callback )"
                },
                {
                    "func_id": 1781,
                    "func_name": "_parse_spark_log_from_log4j_records",
                    "func_desc": "_parse_spark_log_from_log4j_records",
                    "func_file": "spark",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_spark_log_from_log4j_records(records):\\n    \"\"\"Helper for _parse_spark_log()\"\"\"\\n\\n    # make sure *records* is a generator\\n    records = iter(records)\\n\\n    result = {}\\n\\n    for record in records:\\n        message = record['message']\\n\\n        m = _SUBMITTED_APPLICATION_RE.match(message)\\n        if m:\\n            # need this on YARN or we won't be able to find container logs\\n            result['application_id'] = m.group('application_id')\\n            continue\\n\\n        if record['level'] in ('WARN', 'ERROR'):\\n            # only interested in multi-line warnings\\n            if record['level'] == 'WARN' and record['num_lines'] == 1:\\n                continue\\n\\n            error = dict(\\n                spark_error=dict(\\n                    message=message,\\n                    start_line=record['start_line'],\\n                    num_lines=record['num_lines'],\\n                )\\n            )\\n\\n            if not result.get('errors'):\\n                result['errors'] = []\\n\\n            result['errors'].append(error)\\n            continue\\n\\n    return result",
                    "func_fullName": "mrjob.logs.spark._parse_spark_log_from_log4j_records( records )"
                },
                {
                    "func_id": 1782,
                    "func_name": "_interpret_spark_logs",
                    "func_desc": "_interpret_spark_logs",
                    "func_file": "spark",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _interpret_spark_logs(fs, matches, partial=True, log_callback=None):\\n    result = {}\\n    errors = []\\n\\n    for match in matches:\\n        stop_if_partial = False\\n\\n        path = match['path']\\n        if log_callback:\\n            log_callback(path)\\n\\n        interpretation = _parse_spark_log(_cat_log_lines(fs, path))\\n\\n        result.update(interpretation)\\n        # don't _add_implied_job_id() because it doesn't work that way on Spark\\n\\n        for error in interpretation.get('errors') or ():\\n            if 'spark_error' in error:\\n                error['spark_error']['path'] = path\\n                if error['spark_error']['num_lines'] > 1:\\n                    stop_if_partial = True\\n                    # still worth parsing all the errors in this log\\n\\n            for id_key in 'attempt_id', 'container_id':\\n                if id_key in match:\\n                    error[id_key] = match[id_key]\\n            _add_implied_task_id(error)\\n\\n            errors.append(error)\\n\\n        if partial and stop_if_partial:\\n            result['partial'] = True\\n            break\\n\\n    if errors:\\n        result['errors'] = errors\\n\\n    return result",
                    "func_fullName": "mrjob.logs.spark._interpret_spark_logs( fs, matches, partial, log_callback )"
                },
                {
                    "func_id": 1842,
                    "func_name": "_get_spark_args",
                    "func_desc": "_get_spark_args",
                    "func_file": "spark_submit",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _get_spark_args(parser, cl_args):\\n    raw_args = _parse_raw_args(parser, cl_args)\\n\\n    spark_args = []\\n\\n    for dest, option_string, args in raw_args:\\n        if dest in _SPARK_ARG_OPT_NAMES:\\n            spark_args.append(option_string)\\n            spark_args.extend(args)\\n\\n    return spark_args",
                    "func_fullName": "mrjob.tools.spark_submit._get_spark_args( parser, cl_args )"
                },
                {
                    "func_id": 1843,
                    "func_name": "_add_spark_submit_arg",
                    "func_desc": "_add_spark_submit_arg",
                    "func_file": "spark_submit",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _add_spark_submit_arg(parser, opt_name):\\n    opt_string = _SPARK_SUBMIT_SWITCHES[opt_name]\\n\\n    kwargs = dict(dest=opt_name)\\n\\n    # if opt_name is a mrjob opt, parse args like a MRJob would\\n    if opt_name in _RUNNER_OPTS:\\n        opt_alias = _SWITCH_ALIASES.get(opt_string, opt_string)\\n\\n        for opt_strings, opt_kwargs in _RUNNER_OPTS[opt_name]['switches']:\\n            if opt_alias in opt_strings:\\n                kwargs.update(opt_kwargs)\\n\\n    kwargs['help'] = _SPARK_SUBMIT_ARG_HELP[opt_name]\\n    kwargs.update(_SPARK_SUBMIT_ARG_KWARGS.get(opt_name) or {})\\n\\n    parser.add_argument(opt_string, **kwargs)",
                    "func_fullName": "mrjob.tools.spark_submit._add_spark_submit_arg( parser, opt_name )"
                }
            ]
        },
        {
            "cluster_id": 7,
            "feature_id": 20,
            "feature_desc": "gamma=0.0550; k=15; a=0.25; combined=0.401; stability(ARI)=0.936; sep=0.153",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 605,
                    "func_name": "_uses_input_manifest",
                    "func_desc": "_uses_input_manifest",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _uses_input_manifest(self):\\n        \"\"\"Does the first step take an input manifest?\"\"\"\\n        return bool(self._get_step(0).get('input_manifest'))",
                    "func_fullName": "mrjob.runner._uses_input_manifest( self )"
                },
                {
                    "func_id": 625,
                    "func_name": "_create_input_manifest_if_needed",
                    "func_desc": "_create_input_manifest_if_needed",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _create_input_manifest_if_needed(self):\\n        \"\"\"Create a file with a list of URIs of input files.\"\"\"\\n        if self._input_manifest_path or not self._uses_input_manifest():\\n            return\\n\\n        uris = []\\n\\n        log.info('finding input files to add to manifest...')\\n\\n        for path in self._get_input_paths():\\n            log.debug('  in %s' % path)\\n            if is_uri(path):\\n                # URIs might be globs\\n                for uri in self.fs.ls(path):\\n                    uris.append(uri)\\n            else:\\n                # local paths are expected to be single files\\n                # (shell would resolve globs)\\n                if self._upload_mgr:\\n                    uris.append(self._upload_mgr.uri(path))\\n                else:\\n                    # just make sure job can find files from its working dir\\n                    uris.append(os.path.abspath(path))\\n\\n        log.info('found %d input files' % len(uris))\\n\\n        path = os.path.join(self._get_local_tmp_dir(), 'input-manifest.txt')\\n        self._write_script(uris, path, 'input manifest')\\n\\n        self._input_manifest_path = path\\n        if self._upload_mgr:\\n            self._upload_mgr.add(self._input_manifest_path)",
                    "func_fullName": "mrjob.runner._create_input_manifest_if_needed( self )"
                },
                {
                    "func_id": 1478,
                    "func_name": "addInPlace",
                    "func_desc": "addInPlace",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def addInPlace(self, value1, value2):\\n        for group in value2:\\n            for key in value2[group]:\\n                if key not in value1[group]:\\n                    value1[group][key] = value2[group][key]\\n                else:\\n                    value1[group][key] += value2[group][key]\\n        return value1",
                    "func_fullName": "mrjob.spark.harness.addInPlace( self, value1, value2 )"
                }
            ]
        },
        {
            "cluster_id": 7,
            "feature_id": 21,
            "feature_desc": "gamma=0.0550; k=15; a=0.25; combined=0.401; stability(ARI)=0.936; sep=0.153",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 630,
                    "func_name": "_wd_mirror",
                    "func_desc": "_wd_mirror",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _wd_mirror(self):\\n        \"\"\"A directory to upload files belonging to\\n        :py:attr:`_working_dir_mgr`. This will be a subdir of\\n        ``self._upload_mgr.prefix`, if it exists, and otherwise will\\n        be ``None``.\"\"\"\\n        if self._upload_mgr and is_uri(self._upload_mgr.prefix):\\n            return posixpath.join(self._upload_mgr.prefix, 'wd')\\n        elif (self._has_spark_steps() and self._spark_executors_have_own_wd()):\\n            return os.path.join(self._get_local_tmp_dir(), 'wd-mirror')\\n        else:\\n            return None",
                    "func_fullName": "mrjob.runner._wd_mirror( self )"
                },
                {
                    "func_id": 632,
                    "func_name": "_dest_in_wd_mirror",
                    "func_desc": "_dest_in_wd_mirror",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _dest_in_wd_mirror(self, path, name):\\n        \"\"\"Return the URI of where to upload *path* so it can appear in the\\n        working dir as *name*, or ``None`` if it doesn't need to be uploaded.\\n        \"\"\"\\n        dest_dir = self._wd_mirror()\\n        if not dest_dir:\\n            return None\\n\\n        # the only reason to re-upload a URI is if it has the wrong name\\n        #\\n        # similarly, the only point of a local working dir mirror is\\n        # to rename things\\n        if (is_uri(path) or not is_uri(dest_dir)) and (\\n                posixpath.basename(path) == name or\\n                not self._wd_filenames_must_match()):\\n            return None\\n\\n        return posixpath.join(dest_dir, name)",
                    "func_fullName": "mrjob.runner._dest_in_wd_mirror( self, path, name )"
                }
            ]
        },
        {
            "cluster_id": 7,
            "feature_id": 22,
            "feature_desc": "gamma=0.0550; k=15; a=0.25; combined=0.401; stability(ARI)=0.936; sep=0.153",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 651,
                    "func_name": "_to_str",
                    "func_desc": "_to_str",
                    "func_file": "runner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _to_str(s):\\n        if isinstance(s, string_types) and not isinstance(s, str):\\n            return s.encode('utf_8')\\n        else:\\n            return s",
                    "func_fullName": "mrjob.runner._to_str( s )"
                },
                {
                    "func_id": 1481,
                    "func_name": "map_lines",
                    "func_desc": "map_lines",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def map_lines(lines):\\n        job = make_mrc_job('mapper', step_num)\\n\\n        read, write = job.pick_protocols(step_num, 'mapper')\\n\\n        if rdd_includes_input_path:\\n            # rdd actually contains pairs of (input_path, line), convert\\n            path_line_pairs = lines\\n\\n            # emulate the mapreduce.map.input.file config property\\n            # set in Hadoop\\n            #\\n            # do this first so that mapper_init() etc. will work.\\n            # we can assume *rdd* contains at least one record.\\n            input_path, first_line = next(path_line_pairs)\\n            os.environ['mapreduce_map_input_file'] = input_path\\n\\n            # reconstruct *lines* (without dumping to memory)\\n            lines = chain([first_line], (line for _, line in path_line_pairs))\\n\\n        # decode lines into key-value pairs (as a generator, not a list)\\n        #\\n        # line -> (k, v)\\n        if read:\\n            pairs = (read(line) for line in lines)\\n        else:\\n            pairs = lines  # was never encoded\\n\\n        # reduce_pairs() runs key-value pairs through mapper\\n        #\\n        # (k, v), ... -> (k, v), ...\\n        for k, v in job.map_pairs(pairs, step_num):\\n            # encode key-value pairs back into lines\\n            #\\n            # (k, v) -> line\\n            if write:\\n                yield write(k, v)\\n            else:\\n                yield k, v",
                    "func_fullName": "mrjob.spark.harness.map_lines( lines )"
                },
                {
                    "func_id": 1485,
                    "func_name": "key_func",
                    "func_desc": "key_func",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def key_func(k_v):\\n            return k_v[0]",
                    "func_fullName": "mrjob.spark.harness.key_func( k_v )"
                },
                {
                    "func_id": 1486,
                    "func_name": "key_func",
                    "func_desc": "key_func",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def key_func(line):\\n            return line.split(b'\\t')[0]",
                    "func_fullName": "mrjob.spark.harness.key_func( line )"
                },
                {
                    "func_id": 1487,
                    "func_name": "map_f",
                    "func_desc": "map_f",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def map_f(key_and_lines):\\n            return sorted(key_and_lines[1])",
                    "func_fullName": "mrjob.spark.harness.map_f( key_and_lines )"
                },
                {
                    "func_id": 1488,
                    "func_name": "map_f",
                    "func_desc": "map_f",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def map_f(key_and_lines):\\n            return key_and_lines[1]",
                    "func_fullName": "mrjob.spark.harness.map_f( key_and_lines )"
                }
            ]
        },
        {
            "cluster_id": 7,
            "feature_id": 23,
            "feature_desc": "gamma=0.0550; k=15; a=0.25; combined=0.401; stability(ARI)=0.936; sep=0.153",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1466,
                    "func_name": "main",
                    "func_desc": "main",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def main(cmd_line_args=None):\\n    if cmd_line_args is None:\\n        cmd_line_args = sys.argv[1:]\\n\\n    parser = _make_arg_parser()\\n    args = parser.parse_args(cmd_line_args)\\n\\n    if args.num_reducers is not None and args.num_reducers <= 0:\\n        raise ValueError(\\n            'You can only configure num_reducers to positive number.')\\n\\n    # get job_class\\n    job_module_name, job_class_name = args.job_class.rsplit('.', 1)\\n    job_module = import_module(job_module_name)\\n    job_class = getattr(job_module, job_class_name)\\n\\n    # load initial data\\n    from pyspark import SparkContext\\n\\n    if args.job_args:\\n        job_args = shlex_split(args.job_args)\\n    else:\\n        job_args = []\\n\\n    # determine hadoop_*_format, steps\\n    # try to avoid instantiating a job in the driver; see #2044\\n    job = None\\n\\n    if args.hadoop_input_format is None:\\n        job = job or job_class(job_args)\\n        hadoop_input_format = job.hadoop_input_format()\\n    else:\\n        hadoop_input_format = args.hadoop_input_format or None\\n\\n    if args.hadoop_output_format is None:\\n        job = job or job_class(job_args)\\n        hadoop_output_format = job.hadoop_output_format()\\n    else:\\n        hadoop_output_format = args.hadoop_output_format or None\\n\\n    if args.sort_values is None:\\n        job = job or job_class(job_args)\\n        sort_values = job.sort_values()\\n    else:\\n        sort_values = args.sort_values\\n\\n    if args.steps_desc is None:\\n        job = job or job_class(job_args)\\n        steps = [step.description(step_num)\\n                 for step_num, step in enumerate(job.steps())]\\n    else:\\n        steps = json.loads(args.steps_desc)\\n\\n    # pick steps\\n    start = args.first_step_num or 0\\n    end = None if args.last_step_num is None else args.last_step_num + 1\\n    steps_to_run = list(enumerate(steps))[start:end]\\n\\n    sc = SparkContext()\\n\\n    # keep track of one set of counters per job step\\n    counter_accumulators = [\\n        sc.accumulator(defaultdict(dict), CounterAccumulator())\\n        for _ in steps_to_run\\n    ]\\n\\n    def make_increment_counter(step_num):\\n        counter_accumulator = counter_accumulators[step_num - start]\\n\\n        def increment_counter(group, counter, amount=1):\\n            counter_accumulator.add({group: {counter: amount}})\\n\\n        return increment_counter\\n\\n    def make_mrc_job(mrc, step_num):\\n        j = job_class(job_args + [\\n            '--%s' % mrc, '--step-num=%d' % step_num\\n        ])\\n\\n        # patch increment_counter() to update the accumulator for this step\\n        j.increment_counter = make_increment_counter(step_num)\\n\\n        # if skip_internal_protocol is true, patch internal_protocol() to\\n        # return an object whose *read* and *write* attributes are ``None``\\n        if args.skip_internal_protocol:\\n            j.internal_protocol = lambda: _NO_INTERNAL_PROTOCOL\\n\\n        return j\\n\\n    # --emulate-map-input-file doesn't work with hadoop_input_format\\n    emulate_map_input_file = (\\n        args.emulate_map_input_file and not hadoop_input_format)\\n\\n    try:\\n        if emulate_map_input_file:\\n            # load an rdd with pairs of (input_path, line). *path* here\\n            # has to be a single path, not a comma-separated list\\n            rdd = sc.union([_text_file_with_path(sc, path)\\n                            for path in args.input_path.split(',')])\\n\\n        elif hadoop_input_format:\\n            rdd = sc.hadoopFile(\\n                args.input_path,\\n                inputFormatClass=hadoop_input_format,\\n                keyClass='org.apache.hadoop.io.Text',\\n                valueClass='org.apache.hadoop.io.Text')\\n\\n            # hadoopFile loads each line as a key-value pair in which the\\n            # contents of the line are the key and the value is an empty\\n            # string. Convert to an rdd of just lines, encoded as bytes.\\n            rdd = rdd.map(lambda kv: kv[0].encode('utf-8'))\\n\\n        else:\\n            rdd = sc.textFile(args.input_path, use_unicode=False)\\n\\n        # run steps\\n        for step_num, step in steps_to_run:\\n            rdd = _run_step(\\n                step, step_num, rdd,\\n                make_mrc_job,\\n                args.num_reducers, sort_values,\\n                emulate_map_input_file,\\n                args.skip_internal_protocol)\\n\\n        # max_output_files: limit number of partitions\\n        if args.max_output_files:\\n            rdd = rdd.coalesce(args.max_output_files)\\n\\n        # write the results\\n        if hadoop_output_format:\\n            # saveAsHadoopFile takes an rdd of key-value pairs, so convert to\\n            # that format\\n            rdd = rdd.map(lambda line: tuple(\\n                x.decode('utf-8') for x in line.split(b'\\t', 1)))\\n            rdd.saveAsHadoopFile(\\n                args.output_path,\\n                outputFormatClass=hadoop_output_format,\\n                compressionCodecClass=args.compression_codec)\\n        else:\\n            rdd.saveAsTextFile(\\n                args.output_path, compressionCodecClass=args.compression_codec)\\n    finally:\\n        if args.counter_output_dir is not None:\\n            counters = [ca.value for ca in counter_accumulators]\\n\\n            # If the given path is an s3 path, use s3.parallelize,\\n            # otherwise just write them directly to the local dir\\n            if is_uri(args.counter_output_dir):\\n                sc.parallelize(\\n                    [json.dumps(counters)],\\n                    numSlices=1\\n                ).saveAsTextFile(\\n                    args.counter_output_dir\\n                )\\n            else:\\n                # Use regular python built-in file writer if the part-* file\\n                # is not created\\n                path = os.path.join(args.counter_output_dir, \"part-00000\")\\n                if not os.path.exists(args.counter_output_dir):\\n                    os.mkdir(args.counter_output_dir)\\n                with open(path, 'w') as wb:\\n                    wb.write(str(json.dumps(counters)))",
                    "func_fullName": "mrjob.spark.harness.main( cmd_line_args )"
                },
                {
                    "func_id": 1477,
                    "func_name": "zero",
                    "func_desc": "zero",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def zero(self, value):\\n        return value",
                    "func_fullName": "mrjob.spark.harness.zero( self, value )"
                },
                {
                    "func_id": 1479,
                    "func_name": "make_increment_counter",
                    "func_desc": "make_increment_counter",
                    "func_file": "harness",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def make_increment_counter(step_num):\\n        counter_accumulator = counter_accumulators[step_num - start]\\n\\n        def increment_counter(group, counter, amount=1):\\n            counter_accumulator.add({group: {counter: amount}})\\n\\n        return increment_counter",
                    "func_fullName": "mrjob.spark.harness.make_increment_counter( step_num )"
                }
            ]
        },
        {
            "cluster_id": 12,
            "feature_id": 24,
            "feature_desc": "gamma=0.0822; k=3; a=0.25; combined=0.457; stability(ARI)=1.000; sep=0.114",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 264,
                    "func_name": "fully_qualify_hdfs_path",
                    "func_desc": "fully_qualify_hdfs_path",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def fully_qualify_hdfs_path(path):\\n    \"\"\"If path isn't an ``hdfs://`` URL, turn it into one.\"\"\"\\n    from mrjob.parse import is_uri\\n    if is_uri(path):\\n        return path\\n    elif path.startswith('/'):\\n        return 'hdfs://' + path\\n    else:\\n        return 'hdfs:///user/%s/%s' % (getpass.getuser(), path)",
                    "func_fullName": "mrjob.hadoop.fully_qualify_hdfs_path( path )"
                },
                {
                    "func_id": 292,
                    "func_name": "_hadoop_prefix_from_bin",
                    "func_desc": "_hadoop_prefix_from_bin",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _hadoop_prefix_from_bin(hadoop_bin):\\n    \"\"\"Given a path to the hadoop binary, return the path of the implied\\n    hadoop home, or None if we don't know.\\n\\n    Don't return the parent directory of directories in the default\\n    path (not ``/``, ``/usr``, or ``/usr/local``).\\n    \"\"\"\\n    # resolve unqualified binary name (relative paths are okay)\\n    if '/' not in hadoop_bin:\\n        hadoop_bin = which(hadoop_bin)\\n        if not hadoop_bin:\\n            return None\\n\\n    # use parent of hadoop_bin's directory\\n    hadoop_home = posixpath.abspath(\\n        posixpath.join(posixpath.realpath(posixpath.dirname(hadoop_bin)), '..')\\n    )\\n\\n    if hadoop_home in _BAD_HADOOP_HOMES:\\n        return None\\n\\n    return hadoop_home",
                    "func_fullName": "mrjob.hadoop._hadoop_prefix_from_bin( hadoop_bin )"
                },
                {
                    "func_id": 293,
                    "func_name": "_log_record_from_hadoop",
                    "func_desc": "_log_record_from_hadoop",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _log_record_from_hadoop(record):\\n    \"\"\"Log log4j record parsed from hadoop stderr.\"\"\"\\n    if not _is_counter_log4j_record(record):  # counters are printed separately\\n        _log_log4j_record(record)",
                    "func_fullName": "mrjob.hadoop._log_record_from_hadoop( record )"
                },
                {
                    "func_id": 297,
                    "func_name": "get_hadoop_version",
                    "func_desc": "get_hadoop_version",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_hadoop_version(self):\\n        \"\"\"Invoke the hadoop executable to determine its version\"\"\"\\n        return self.fs.hadoop.get_hadoop_version()",
                    "func_fullName": "mrjob.hadoop.get_hadoop_version( self )"
                },
                {
                    "func_id": 298,
                    "func_name": "get_hadoop_bin",
                    "func_desc": "get_hadoop_bin",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_hadoop_bin(self):\\n        \"\"\"Find the hadoop binary. A list: binary followed by arguments.\"\"\"\\n        return self.fs.hadoop.get_hadoop_bin()",
                    "func_fullName": "mrjob.hadoop.get_hadoop_bin( self )"
                },
                {
                    "func_id": 299,
                    "func_name": "get_hadoop_streaming_jar",
                    "func_desc": "get_hadoop_streaming_jar",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_hadoop_streaming_jar(self):\\n        \"\"\"Find the path of the hadoop streaming jar, or None if not found.\"\"\"\\n        if not (self._hadoop_streaming_jar or\\n                self._searched_for_hadoop_streaming_jar):\\n\\n            self._hadoop_streaming_jar = self._find_hadoop_streaming_jar()\\n\\n            if self._hadoop_streaming_jar:\\n                log.info('Found Hadoop streaming jar: %s' %\\n                         self._hadoop_streaming_jar)\\n            else:\\n                log.warning('Hadoop streaming jar not found. Use'\\n                            ' --hadoop-streaming-jar')\\n\\n            self._searched_for_hadoop_streaming_jar = True\\n\\n        return self._hadoop_streaming_jar",
                    "func_fullName": "mrjob.hadoop.get_hadoop_streaming_jar( self )"
                },
                {
                    "func_id": 300,
                    "func_name": "_find_hadoop_streaming_jar",
                    "func_desc": "_find_hadoop_streaming_jar",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _find_hadoop_streaming_jar(self):\\n        \"\"\"Search for the hadoop streaming jar. See\\n        :py:meth:`_hadoop_streaming_jar_dirs` for where we search.\"\"\"\\n        for path in unique(self._hadoop_streaming_jar_dirs()):\\n            log.info('Looking for Hadoop streaming jar in %s...' % path)\\n\\n            streaming_jars = []\\n            for path in self.fs.ls(path):\\n                if _HADOOP_STREAMING_JAR_RE.match(posixpath.basename(path)):\\n                    streaming_jars.append(path)\\n\\n            if streaming_jars:\\n                # prefer shorter names and shallower paths\\n                def sort_key(p):\\n                    return (len(p.split('/')),\\n                            len(posixpath.basename(p)),\\n                            p)\\n\\n                streaming_jars.sort(key=sort_key)\\n\\n                return streaming_jars[0]\\n\\n        return None",
                    "func_fullName": "mrjob.hadoop._find_hadoop_streaming_jar( self )"
                },
                {
                    "func_id": 301,
                    "func_name": "_hadoop_dirs",
                    "func_desc": "_hadoop_dirs",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _hadoop_dirs(self):\\n        \"\"\"Yield all possible hadoop directories (used for streaming jar\\n        and logs). May yield duplicates\"\"\"\\n        for name in ('HADOOP_PREFIX', 'HADOOP_HOME', 'HADOOP_INSTALL',\\n                     'HADOOP_MAPRED_HOME'):\\n            path = os.environ.get(name)\\n            if path:\\n                yield path\\n\\n        # guess it from the path of the Hadoop binary\\n        hadoop_home = _hadoop_prefix_from_bin(self.get_hadoop_bin()[0])\\n        if hadoop_home:\\n            yield hadoop_home\\n\\n        # try HADOOP_*_HOME\\n        for name, path in sorted(os.environ.items()):\\n            if name.startswith('HADOOP_') and name.endswith('_HOME'):\\n                yield path",
                    "func_fullName": "mrjob.hadoop._hadoop_dirs( self )"
                },
                {
                    "func_id": 302,
                    "func_name": "_hadoop_streaming_jar_dirs",
                    "func_desc": "_hadoop_streaming_jar_dirs",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _hadoop_streaming_jar_dirs(self):\\n        \"\"\"Yield all possible places to look for the Hadoop streaming jar.\\n        May yield duplicates.\\n        \"\"\"\\n        for hadoop_dir in self._hadoop_dirs():\\n            yield hadoop_dir\\n\\n        # use hard-coded paths to work out-of-the-box on EMR\\n        for path in _EMR_HADOOP_STREAMING_JAR_DIRS:\\n            yield path",
                    "func_fullName": "mrjob.hadoop._hadoop_streaming_jar_dirs( self )"
                },
                {
                    "func_id": 303,
                    "func_name": "_hadoop_log_dirs",
                    "func_desc": "_hadoop_log_dirs",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _hadoop_log_dirs(self, output_dir=None):\\n        \"\"\"Yield all possible places to look for hadoop logs.\"\"\"\\n        # hadoop_log_dirs opt overrides all this\\n        if self._opts['hadoop_log_dirs']:\\n            for path in self._opts['hadoop_log_dirs']:\\n                yield path\\n            return\\n\\n        hadoop_log_dir = os.environ.get('HADOOP_LOG_DIR')\\n        if hadoop_log_dir:\\n            yield hadoop_log_dir\\n\\n        yarn = uses_yarn(self.get_hadoop_version())\\n\\n        if yarn:\\n            yarn_log_dir = os.environ.get('YARN_LOG_DIR')\\n            if yarn_log_dir:\\n                yield yarn_log_dir\\n\\n            yield _DEFAULT_YARN_HDFS_LOG_DIR\\n\\n        if output_dir:\\n            # Cloudera style of logging\\n            yield posixpath.join(output_dir, '_logs')\\n\\n        for hadoop_dir in self._hadoop_dirs():\\n            yield posixpath.join(hadoop_dir, 'logs')\\n\\n        # hard-coded fallback paths\\n        if yarn:\\n            for path in _FALLBACK_HADOOP_YARN_LOG_DIRS:\\n                yield path\\n\\n        for path in _FALLBACK_HADOOP_LOG_DIRS:\\n            yield path",
                    "func_fullName": "mrjob.hadoop._hadoop_log_dirs( self, output_dir )"
                },
                {
                    "func_id": 305,
                    "func_name": "_find_binaries_and_jars",
                    "func_desc": "_find_binaries_and_jars",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _find_binaries_and_jars(self):\\n        \"\"\"Find hadoop and (if needed) spark-submit bin up-front, before\\n        continuing with the job.\\n\\n        (This is just for user-interaction purposes; these would otherwise\\n        lazy-load as needed.)\\n        \"\"\"\\n        # this triggers looking for Hadoop binary\\n        self.get_hadoop_version()\\n\\n        if self._has_hadoop_streaming_steps():\\n            self.get_hadoop_streaming_jar()\\n\\n        if self._has_spark_steps():\\n            self.get_spark_submit_bin()",
                    "func_fullName": "mrjob.hadoop._find_binaries_and_jars( self )"
                },
                {
                    "func_id": 308,
                    "func_name": "_run_job_in_hadoop",
                    "func_desc": "_run_job_in_hadoop",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run_job_in_hadoop(self):\\n        for step_num, step in enumerate(self._get_steps()):\\n            step_type = step['type']\\n            step_args = self._args_for_step(step_num)\\n            env = _fix_env(self._env_for_step(step_num))\\n\\n            # log this *after* _args_for_step(), which can start a search\\n            # for the Hadoop streaming jar\\n            log.info('Running step %d of %d...' %\\n                     (step_num + 1, self._num_steps()))\\n            log.debug('> %s' % cmd_line(step_args))\\n            log.debug('  with environment: %r' % sorted(env.items()))\\n\\n            log_interpretation = {}\\n            self._log_interpretations.append(log_interpretation)\\n\\n            if self._step_type_uses_spark(step_type):\\n                returncode, step_interpretation = self._run_spark_submit(\\n                    step_args, env, record_callback=_log_log4j_record)\\n            else:\\n                returncode, step_interpretation = self._run_hadoop(\\n                    step_args, env, record_callback=_log_record_from_hadoop)\\n\\n            # make sure output_dir is filled (used for history log)\\n            if 'output_dir' not in step_interpretation:\\n                step_interpretation['output_dir'] = (\\n                    self._step_output_uri(step_num))\\n\\n            log_interpretation['step'] = step_interpretation\\n\\n            self._log_counters(log_interpretation, step_num)\\n\\n            if returncode:\\n                error = self._pick_error(log_interpretation, step_type)\\n                if error:\\n                    _log_probable_cause_of_failure(log, error)\\n\\n                # use CalledProcessError's well-known message format\\n                reason = str(CalledProcessError(returncode, step_args))\\n                raise StepFailedException(\\n                    reason=reason, step_num=step_num,\\n                    num_steps=self._num_steps())",
                    "func_fullName": "mrjob.hadoop._run_job_in_hadoop( self )"
                },
                {
                    "func_id": 309,
                    "func_name": "_run_hadoop",
                    "func_desc": "_run_hadoop",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run_hadoop(self, hadoop_args, env, record_callback):\\n        # try to use a PTY if it's available\\n        try:\\n            pid, master_fd = pty.fork()\\n        except (AttributeError, OSError):\\n            # no PTYs, just use Popen\\n\\n            # user won't get much feedback for a while, so tell them\\n            # Hadoop is running\\n            log.debug('No PTY available, using Popen() to invoke Hadoop')\\n\\n            step_proc = Popen(hadoop_args, stdout=PIPE, stderr=PIPE, env=env)\\n\\n            step_interpretation = _interpret_hadoop_jar_command_stderr(\\n                step_proc.stderr,\\n                record_callback=_log_record_from_hadoop)\\n\\n            # there shouldn't be much output to STDOUT\\n            for line in step_proc.stdout:\\n                _log_line_from_driver(to_unicode(line).strip('\\r\\n'))\\n\\n            step_proc.stdout.close()\\n            step_proc.stderr.close()\\n\\n            returncode = step_proc.wait()\\n        else:\\n            # we have PTYs\\n            if pid == 0:  # we are the child process\\n                try:\\n                    os.execvpe(hadoop_args[0], hadoop_args, env)\\n                    # now we are no longer Python\\n                except OSError as ex:\\n                    # use _exit() so we don't do cleanup, etc. that's\\n                    # the parent process's job\\n                    os._exit(ex.errno)\\n                finally:\\n                    # if we got some other exception, still exit hard\\n                    os._exit(-1)\\n            else:\\n                log.debug('Invoking Hadoop via PTY')\\n\\n                with os.fdopen(master_fd, 'rb') as master:\\n                    # reading from master gives us the subprocess's\\n                    # stderr and stdout (it's a fake terminal)\\n                    step_interpretation = (\\n                        _interpret_hadoop_jar_command_stderr(\\n                            _eio_to_eof(master),\\n                            record_callback=_log_record_from_hadoop))\\n                    _, returncode = os.waitpid(pid, 0)\\n\\n        return returncode, step_interpretation",
                    "func_fullName": "mrjob.hadoop._run_hadoop( self, hadoop_args, env, record_callback )"
                },
                {
                    "func_id": 313,
                    "func_name": "_args_for_jar_step",
                    "func_desc": "_args_for_jar_step",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _args_for_jar_step(self, step_num):\\n        step = self._get_step(step_num)\\n\\n        args = []\\n\\n        args.extend(self.get_hadoop_bin())\\n\\n        # special case for consistency with EMR runner.\\n        #\\n        # This might look less like duplicated code if we ever\\n        # implement #780 (fetching jars from URIs)\\n        if step['jar'].startswith('file:///'):\\n            jar = step['jar'][7:]  # keep leading slash\\n        else:\\n            jar = step['jar']\\n\\n        args.extend(['jar', jar])\\n\\n        if step.get('main_class'):\\n            args.append(step['main_class'])\\n\\n        if step.get('args'):\\n            args.extend(\\n                self._interpolate_jar_step_args(step['args'], step_num))\\n\\n        return args",
                    "func_fullName": "mrjob.hadoop._args_for_jar_step( self, step_num )"
                },
                {
                    "func_id": 316,
                    "func_name": "_cleanup_hadoop_tmp",
                    "func_desc": "_cleanup_hadoop_tmp",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cleanup_hadoop_tmp(self):\\n        if self._hadoop_tmp_dir:\\n            log.info('Removing HDFS temp directory %s...' %\\n                     self._hadoop_tmp_dir)\\n            try:\\n                self.fs.rm(self._hadoop_tmp_dir)\\n            except Exception as e:\\n                log.exception(e)",
                    "func_fullName": "mrjob.hadoop._cleanup_hadoop_tmp( self )"
                },
                {
                    "func_id": 1572,
                    "func_name": "get_hadoop_bin",
                    "func_desc": "get_hadoop_bin",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_hadoop_bin(self):\\n        \"\"\"Return the hadoop binary, searching for it if need be.\"\"\"\\n        if self._hadoop_bin is None:\\n            self._hadoop_bin = self._find_hadoop_bin()\\n        return self._hadoop_bin",
                    "func_fullName": "mrjob.fs.hadoop.get_hadoop_bin( self )"
                },
                {
                    "func_id": 1573,
                    "func_name": "set_hadoop_bin",
                    "func_desc": "set_hadoop_bin",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def set_hadoop_bin(self, hadoop_bin):\\n        \"\"\"Manually set the hadoop binary, as a list of args.\"\"\"\\n        self._hadoop_bin = hadoop_bin",
                    "func_fullName": "mrjob.fs.hadoop.set_hadoop_bin( self, hadoop_bin )"
                },
                {
                    "func_id": 1574,
                    "func_name": "_find_hadoop_bin",
                    "func_desc": "_find_hadoop_bin",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _find_hadoop_bin(self):\\n        \"\"\"Look for the hadoop binary in any plausible place. If all\\n        else fails, return ``['hadoop']``.\\n        \"\"\"\\n        def yield_paths():\\n            for name in 'HADOOP_PREFIX', 'HADOOP_HOME', 'HADOOP_INSTALL':\\n                path = os.environ.get(name)\\n                if path:\\n                    yield os.path.join(path, 'bin')\\n\\n            # They use $HADOOP_INSTALL/hadoop/bin here:\\n            # https://wiki.apache.org/hadoop/GettingStartedWithHadoop\\n            if os.environ.get('HADOOP_INSTALL'):\\n                yield os.path.join(\\n                    os.environ['HADOOP_INSTALL'], 'hadoop', 'bin')\\n\\n            yield None  # use $PATH\\n\\n            # Maybe it's in $HADOOP_MAPRED_HOME? $HADOOP_YARN_HOME? Don't give\\n            # up. Don't worry about duplicates; they're de-duplicated below\\n            for name, path in sorted(os.environ.items()):\\n                if name.startswith('HADOOP_') and name.endswith('_HOME'):\\n                    yield os.path.join(path, 'bin')\\n\\n        for path in unique(yield_paths()):\\n            log.info('Looking for hadoop binary in %s...' % (path or '$PATH'))\\n\\n            hadoop_bin = which('hadoop', path=path)\\n\\n            if hadoop_bin:\\n                log.info('Found hadoop binary: %s' % hadoop_bin)\\n                return [hadoop_bin]\\n        else:\\n            log.info(\"Falling back to 'hadoop'\")\\n            return ['hadoop']",
                    "func_fullName": "mrjob.fs.hadoop._find_hadoop_bin( self )"
                },
                {
                    "func_id": 1575,
                    "func_name": "get_hadoop_version",
                    "func_desc": "get_hadoop_version",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_hadoop_version(self):\\n        \"\"\"Invoke the hadoop executable to determine its version\"\"\"\\n        # mkdir() needs this\\n        if not self._hadoop_version:\\n            stdout = self.invoke_hadoop(['version'], return_stdout=True)\\n            if stdout:\\n                first_line = stdout.split(b'\\n')[0]\\n                m = _HADOOP_VERSION_RE.match(first_line)\\n                if m:\\n                    self._hadoop_version = to_unicode(m.group('version'))\\n                    log.info(\"Using Hadoop version %s\" % self._hadoop_version)\\n                else:\\n                    raise Exception('Unable to determine Hadoop version.')\\n\\n        return self._hadoop_version",
                    "func_fullName": "mrjob.fs.hadoop.get_hadoop_version( self )"
                },
                {
                    "func_id": 1576,
                    "func_name": "invoke_hadoop",
                    "func_desc": "invoke_hadoop",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def invoke_hadoop(self, args, ok_returncodes=None, ok_stderr=None,\\n                      return_stdout=False):\\n        \"\"\"Run the given hadoop command, raising an exception on non-zero\\n        return code. This only works for commands whose output we don't\\n        care about.\\n\\n        Args:\\n        ok_returncodes -- a list/tuple/set of return codes we expect to\\n            get back from hadoop (e.g. [0,1]). By default, we only expect 0.\\n            If we get an unexpected return code, we raise a CalledProcessError.\\n        ok_stderr -- don't log STDERR or raise CalledProcessError if stderr\\n            matches a regex in this list (even if the returncode is bad)\\n        return_stdout -- return the stdout from the hadoop command rather\\n            than logging it. If this is False, we return the returncode\\n            instead.\\n        \"\"\"\\n        args = self.get_hadoop_bin() + args\\n\\n        log.debug('> %s' % cmd_line(args))\\n\\n        proc = Popen(args, stdout=PIPE, stderr=PIPE)\\n        stdout, stderr = proc.communicate()\\n\\n        log_func = log.debug if proc.returncode == 0 else log.error\\n        if not return_stdout:\\n            for line in BytesIO(stdout):\\n                log_func('STDOUT: ' + to_unicode(line.rstrip(b'\\r\\n')))\\n\\n        # check if STDERR is okay\\n        stderr_is_ok = False\\n        if ok_stderr:\\n            for stderr_re in ok_stderr:\\n                if stderr_re.match(stderr):\\n                    stderr_is_ok = True\\n                    break\\n\\n        if not stderr_is_ok:\\n            for line in BytesIO(stderr):\\n                log_func('STDERR: ' + to_unicode(line.rstrip(b'\\r\\n')))\\n\\n        ok_returncodes = ok_returncodes or [0]\\n\\n        if not stderr_is_ok and proc.returncode not in ok_returncodes:\\n            raise CalledProcessError(proc.returncode, args)\\n\\n        if return_stdout:\\n            return stdout\\n        else:\\n            return proc.returncode",
                    "func_fullName": "mrjob.fs.hadoop.invoke_hadoop( self, args, ok_returncodes, ok_stderr, return_stdout )"
                },
                {
                    "func_id": 1784,
                    "func_name": "_parse_hadoop_log4j_records",
                    "func_desc": "_parse_hadoop_log4j_records",
                    "func_file": "log4j",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_hadoop_log4j_records(lines, pre_filter=None):\\n    \"\"\"Parse lines from a hadoop log into log4j records.\\n\\n    Yield dictionaries with the following keys:\\n    caller_location -- e.g. 'YarnClientImpl.java:submitApplication(251)'\\n    level -- e.g. 'INFO'\\n    logger -- e.g. 'amazon.emr.metrics.MetricsSaver'\\n    message -- the actual message. If this is a multi-line message (e.g.\\n        for counters), the lines will be joined by '\\n'\\n    num_lines -- how many lines made up the message\\n    start_line -- which line the message started on (0-indexed)\\n    thread -- e.g. 'main'. Defaults to ''\\n    timestamp -- unparsed timestamp, e.g. '15/12/07 20:49:28',\\n        '2015-08-22 00:46:18,411'\\n\\n    Lines will be converted to unicode, and trailing \\r and \\n will be stripped\\n    from lines.\\n\\n    If set, *pre_filter* will be applied to stripped lines. If it\\n    returns true, we'll return a fake record with message set to the line,\\n    num_lines and start_line set as normal, and everything else set to ''.\\n\\n    Also yields fake records for leading non-log4j lines (trailing non-log4j\\n    lines are assumed to be part of a multiline message if not pre-filtered).\\n    \"\"\"\\n    last_record = None\\n\\n    for line_num, line in enumerate(lines):\\n        # convert from bytes to unicode, if needed, and strip trailing newlines\\n        line = to_unicode(line).rstrip('\\r\\n')\\n\\n        def fake_record():\\n            return dict(\\n                caller_location='',\\n                level='',\\n                logger='',\\n                message=line,\\n                num_lines=1,\\n                start_line=line_num,\\n                thread='',\\n                timestamp='')\\n\\n        # had to patch this in here to get _parse_hadoop_jar_command_stderr()'s\\n        # record_callback to fire on the correct line. The problem is that\\n        # we don't emit records until we see the next line (to handle\\n        # multiline records), so the callback would fire in the wrong order\\n        if pre_filter:\\n            if pre_filter(line):\\n                if last_record:\\n                    last_record['num_lines'] = (\\n                        line_num - last_record['start_line'])\\n                    yield last_record\\n\\n                yield fake_record()\\n\\n                last_record = None\\n                continue\\n\\n        m = (_HADOOP_LOG4J_LINE_RE.match(line) or\\n             _HADOOP_LOG4J_LINE_ALTERNATE_RE.match(line))\\n\\n        if m:\\n            if last_record:\\n                last_record['num_lines'] = (\\n                    line_num - last_record['start_line'])\\n                yield last_record\\n\\n            last_record = m.groupdict()\\n            last_record.setdefault('caller_location', '')\\n            last_record['thread'] = last_record['thread'] or ''\\n            last_record['start_line'] = line_num\\n        else:\\n            # add on to previous record\\n            if last_record:\\n                last_record['message'] += '\\n' + line\\n            else:\\n                yield fake_record()\\n\\n    if last_record:\\n        last_record['num_lines'] = (\\n            line_num + 1 - last_record['start_line'])\\n        yield last_record",
                    "func_fullName": "mrjob.logs.log4j._parse_hadoop_log4j_records( lines, pre_filter )"
                }
            ]
        },
        {
            "cluster_id": 12,
            "feature_id": 25,
            "feature_desc": "gamma=0.0822; k=3; a=0.25; combined=0.457; stability(ARI)=1.000; sep=0.114",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 294,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, **kwargs):\\n        \"\"\":py:class:`~mrjob.hadoop.HadoopJobRunner` takes the same arguments\\n        as :py:class:`~mrjob.runner.MRJobRunner`, plus some additional options\\n        which can be defaulted in :ref:`mrjob.conf <mrjob.conf>`.\\n        \"\"\"\\n        super(HadoopJobRunner, self).__init__(**kwargs)\\n\\n        self._hadoop_tmp_dir = fully_qualify_hdfs_path(\\n            posixpath.join(\\n                self._opts['hadoop_tmp_dir'], self._job_key))\\n\\n        # Keep track of local files to upload to HDFS. We'll add them\\n        # to this manager just before we need them.\\n        hdfs_files_dir = posixpath.join(self._hadoop_tmp_dir, 'files', '')\\n        self._upload_mgr = UploadDirManager(hdfs_files_dir)\\n\\n        # Set output dir if it wasn't set explicitly\\n        self._output_dir = fully_qualify_hdfs_path(\\n            self._output_dir or\\n            posixpath.join(self._hadoop_tmp_dir, 'output'))\\n\\n        # Fully qualify step_output_dir, if set\\n        if self._step_output_dir:\\n            self._step_output_dir = fully_qualify_hdfs_path(\\n                self._step_output_dir)\\n\\n        # Track job and (YARN) application ID to enable log parsing\\n        self._application_id = None\\n        self._job_id = None\\n\\n        # Keep track of where the hadoop streaming jar is\\n        self._hadoop_streaming_jar = self._opts['hadoop_streaming_jar']\\n        self._searched_for_hadoop_streaming_jar = False\\n\\n        # List of dicts (one for each step) potentially containing\\n        # the keys 'history', 'step', and 'task' ('step' will always\\n        # be filled because it comes from the hadoop jar command output,\\n        # others will be filled as needed)\\n        self._log_interpretations = []",
                    "func_fullName": "mrjob.hadoop.__init__( self, **kwargs )"
                },
                {
                    "func_id": 295,
                    "func_name": "_default_opts",
                    "func_desc": "_default_opts",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _default_opts(cls):\\n        return combine_dicts(\\n            super(HadoopJobRunner, cls)._default_opts(),\\n            dict(\\n                hadoop_tmp_dir='tmp/mrjob',\\n            )\\n        )",
                    "func_fullName": "mrjob.hadoop._default_opts( cls )"
                },
                {
                    "func_id": 296,
                    "func_name": "fs",
                    "func_desc": "fs",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def fs(self):\\n        \"\"\":py:class:`mrjob.fs.base.Filesystem` object for HDFS and the local\\n        filesystem.\\n        \"\"\"\\n        from mrjob.fs.local import LocalFilesystem\\n        from mrjob.fs.hadoop import HadoopFilesystem\\n        if self._fs is None:\\n            self._fs = CompositeFilesystem()\\n\\n            # don't pass [] to fs; this means not to use hadoop until\\n            # fs.set_hadoop_bin() is called (used for running hadoop over SSH).\\n            hadoop_bin = self._opts['hadoop_bin'] or None\\n\\n            self._fs.add_fs('hadoop',\\n                            HadoopFilesystem(hadoop_bin))\\n            self._fs.add_fs('local', LocalFilesystem())\\n\\n        return self._fs",
                    "func_fullName": "mrjob.hadoop.fs( self )"
                },
                {
                    "func_id": 304,
                    "func_name": "_run",
                    "func_desc": "_run",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run(self):\\n        self._find_binaries_and_jars()\\n        self._create_setup_wrapper_scripts()\\n        self._add_job_files_for_upload()\\n        self._upload_local_files()\\n        self._run_job_in_hadoop()",
                    "func_fullName": "mrjob.hadoop._run( self )"
                },
                {
                    "func_id": 310,
                    "func_name": "_spark_master",
                    "func_desc": "_spark_master",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_master(self):\\n        return self._opts['spark_master'] or 'yarn'",
                    "func_fullName": "mrjob.hadoop._spark_master( self )"
                },
                {
                    "func_id": 320,
                    "func_name": "counters",
                    "func_desc": "counters",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def counters(self):\\n        return [_pick_counters(log_interpretation)\\n                for log_interpretation in self._log_interpretations]",
                    "func_fullName": "mrjob.hadoop.counters( self )"
                },
                {
                    "func_id": 321,
                    "func_name": "sort_key",
                    "func_desc": "sort_key",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "                def sort_key(p):\\n                    return (len(p.split('/')),\\n                            len(posixpath.basename(p)),\\n                            p)",
                    "func_fullName": "mrjob.hadoop.sort_key( p )"
                },
                {
                    "func_id": 1570,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, hadoop_bin=None):\\n        \"\"\"Create a Hadoop filesystem\\n\\n        :param hadoop_bin: ``hadoop`` binary, as a list of args. If set to\\n                           ``None``, we'll auto-detect the Hadoop binary.\\n                           If set to ``[]``, this FS will be disabled\\n                           until you call :py:meth:`set_hadoop_bin`.\\n        \"\"\"\\n        super(HadoopFilesystem, self).__init__()\\n        self._hadoop_bin = hadoop_bin\\n        self._hadoop_version = None  # cache for get_hadoop_version()",
                    "func_fullName": "mrjob.fs.hadoop.__init__( self, hadoop_bin )"
                },
                {
                    "func_id": 1577,
                    "func_name": "du",
                    "func_desc": "du",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def du(self, path_glob):\\n        \"\"\"Get the size of a file or directory (recursively), or 0\\n        if it doesn't exist.\"\"\"\\n        try:\\n            stdout = self.invoke_hadoop(['fs', '-du', path_glob],\\n                                        return_stdout=True,\\n                                        ok_returncodes=[0, 1, 255])\\n        except CalledProcessError:\\n            return 0\\n\\n        try:\\n            return sum(int(line.split()[0])\\n                       for line in stdout.split(b'\\n')\\n                       if line.strip())\\n        except (ValueError, TypeError, IndexError):\\n            raise IOError(\\n                'Unexpected output from hadoop fs -du: %r' % stdout)",
                    "func_fullName": "mrjob.fs.hadoop.du( self, path_glob )"
                },
                {
                    "func_id": 1578,
                    "func_name": "ls",
                    "func_desc": "ls",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def ls(self, path_glob):\\n        components = urlparse(path_glob)\\n        hdfs_prefix = '%s://%s' % (components.scheme, components.netloc)\\n\\n        version = self.get_hadoop_version()\\n\\n        # use ls -R on Hadoop 2 (see #1152)\\n        if uses_yarn(version):\\n            args = ['fs', '-ls', '-R', path_glob]\\n        else:\\n            args = ['fs', '-lsr', path_glob]\\n\\n        try:\\n            stdout = self.invoke_hadoop(args, return_stdout=True,\\n                                        ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\\n        except CalledProcessError:\\n            raise IOError(\"Could not ls %s\" % path_glob)\\n\\n        for line in BytesIO(stdout):\\n            line = line.rstrip(b'\\r\\n')\\n\\n            # ignore total item count\\n            if line.startswith(b'Found '):\\n                continue\\n\\n            fields = line.split(b' ')\\n\\n            # Throw out directories\\n            if fields[0].startswith(b'd'):\\n                continue\\n\\n            # Try to figure out which part of the line is the path\\n            # Expected lines:\\n            #\\n            # HDFS:\\n            # -rw-r--r--   3 dave users       3276 2010-01-13 14:00 /foo/bar\\n            #\\n            # S3:\\n            # -rwxrwxrwx   1          3276 010-01-13 14:00 /foo/bar\\n            path_index = None\\n            for index, field in enumerate(fields):\\n                # look for time field, and pick one after that\\n                # (can't use field[2] because that's an int in Python 3)\\n                if len(field) == 5 and field[2:3] == b':':\\n                    path_index = (index + 1)\\n            if not path_index:\\n                raise IOError(\"Could not locate path in string %r\" % line)\\n\\n            path = to_unicode(line.split(b' ', path_index)[-1])\\n            # handle fully qualified URIs from newer versions of Hadoop ls\\n            # (see Pull Request #577)\\n            if is_uri(path):\\n                yield path\\n            else:\\n                yield hdfs_prefix + path",
                    "func_fullName": "mrjob.fs.hadoop.ls( self, path_glob )"
                },
                {
                    "func_id": 1579,
                    "func_name": "_cat_file",
                    "func_desc": "_cat_file",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cat_file(self, path):\\n        # stream from HDFS\\n        cat_args = self.get_hadoop_bin() + ['fs', '-cat', path]\\n        log.debug('> %s' % cmd_line(cat_args))\\n\\n        cat_proc = Popen(cat_args, stdout=PIPE, stderr=PIPE)\\n\\n        for chunk in decompress(cat_proc.stdout, path):\\n            yield chunk\\n\\n        # this does someties happen; see #1396\\n        for line in cat_proc.stderr:\\n            log.error('STDERR: ' + to_unicode(line.rstrip(b'\\r\\n')))\\n\\n        cat_proc.stdout.close()\\n        cat_proc.stderr.close()\\n\\n        returncode = cat_proc.wait()\\n\\n        if returncode != 0:\\n            raise IOError(\"Could not stream %s\" % path)",
                    "func_fullName": "mrjob.fs.hadoop._cat_file( self, path )"
                },
                {
                    "func_id": 1580,
                    "func_name": "mkdir",
                    "func_desc": "mkdir",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mkdir(self, path):\\n        version = self.get_hadoop_version()\\n\\n        # use -p on Hadoop 2 (see #991, #845)\\n        if uses_yarn(version):\\n            args = ['fs', '-mkdir', '-p', path]\\n        else:\\n            args = ['fs', '-mkdir', path]\\n\\n        try:\\n            self.invoke_hadoop(args, ok_stderr=[_HADOOP_FILE_EXISTS_RE])\\n        except CalledProcessError:\\n            raise IOError(\"Could not mkdir %s\" % path)",
                    "func_fullName": "mrjob.fs.hadoop.mkdir( self, path )"
                },
                {
                    "func_id": 1581,
                    "func_name": "exists",
                    "func_desc": "exists",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def exists(self, path_glob):\\n        \"\"\"Does the given path exist?\\n\\n        If dest is a directory (ends with a \"/\"), we check if there are\\n        any files starting with that path.\\n        \"\"\"\\n        try:\\n            return_code = self.invoke_hadoop(\\n                ['fs', '-ls', path_glob],\\n                ok_returncodes=[0, -1, 255],\\n                ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\\n\\n            return (return_code == 0)\\n        except CalledProcessError:\\n            raise IOError(\"Could not check path %s\" % path_glob)",
                    "func_fullName": "mrjob.fs.hadoop.exists( self, path_glob )"
                },
                {
                    "func_id": 1582,
                    "func_name": "put",
                    "func_desc": "put",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def put(self, src, path):\\n        # don't inadvertently support cp syntax\\n        if path.endswith('/'):\\n            raise ValueError('put() destination may not be a directory')\\n\\n        self.invoke_hadoop(['fs', '-put', src, path])",
                    "func_fullName": "mrjob.fs.hadoop.put( self, src, path )"
                },
                {
                    "func_id": 1583,
                    "func_name": "rm",
                    "func_desc": "rm",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def rm(self, path_glob):\\n        if not is_uri(path_glob):\\n            super(HadoopFilesystem, self).rm(path_glob)\\n\\n        version = self.get_hadoop_version()\\n        if uses_yarn(version):\\n            args = ['fs', '-rm', '-R', '-f', '-skipTrash', path_glob]\\n        else:\\n            args = ['fs', '-rmr', '-skipTrash', path_glob]\\n\\n        try:\\n            self.invoke_hadoop(\\n                args,\\n                return_stdout=True, ok_stderr=[_HADOOP_RM_NO_SUCH_FILE])\\n        except CalledProcessError:\\n            raise IOError(\"Could not rm %s\" % path_glob)",
                    "func_fullName": "mrjob.fs.hadoop.rm( self, path_glob )"
                },
                {
                    "func_id": 1584,
                    "func_name": "touchz",
                    "func_desc": "touchz",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def touchz(self, path):\\n        try:\\n            self.invoke_hadoop(['fs', '-touchz', path])\\n        except CalledProcessError:\\n            raise IOError(\"Could not touchz %s\" % path)",
                    "func_fullName": "mrjob.fs.hadoop.touchz( self, path )"
                },
                {
                    "func_id": 1785,
                    "func_name": "fake_record",
                    "func_desc": "fake_record",
                    "func_file": "log4j",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def fake_record():\\n            return dict(\\n                caller_location='',\\n                level='',\\n                logger='',\\n                message=line,\\n                num_lines=1,\\n                start_line=line_num,\\n                thread='',\\n                timestamp='')",
                    "func_fullName": "mrjob.logs.log4j.fake_record(  )"
                }
            ]
        },
        {
            "cluster_id": 12,
            "feature_id": 26,
            "feature_desc": "gamma=0.0822; k=3; a=0.25; combined=0.457; stability(ARI)=1.000; sep=0.114",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 306,
                    "func_name": "_add_job_files_for_upload",
                    "func_desc": "_add_job_files_for_upload",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _add_job_files_for_upload(self):\\n        \"\"\"Add files needed for running the job (setup and input)\\n        to self._upload_mgr.\"\"\"\\n        for path in self._py_files():\\n            self._upload_mgr.add(path)",
                    "func_fullName": "mrjob.hadoop._add_job_files_for_upload( self )"
                },
                {
                    "func_id": 307,
                    "func_name": "_dump_stdin_to_local_file",
                    "func_desc": "_dump_stdin_to_local_file",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _dump_stdin_to_local_file(self):\\n        \"\"\"Dump sys.stdin to a local file, and return the path to it.\"\"\"\\n        stdin_path = posixpath.join(self._get_local_tmp_dir(), 'STDIN')\\n         # prompt user, so they don't think the process has stalled\\n        log.info('reading from STDIN')\\n\\n        log.debug('dumping stdin to local file %s...' % stdin_path)\\n        stdin_file = open(stdin_path, 'wb')\\n        for line in self._stdin:\\n            stdin_file.write(line)\\n\\n        return stdin_path",
                    "func_fullName": "mrjob.hadoop._dump_stdin_to_local_file( self )"
                },
                {
                    "func_id": 311,
                    "func_name": "_args_for_step",
                    "func_desc": "_args_for_step",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _args_for_step(self, step_num):\\n        step = self._get_step(step_num)\\n\\n        if step['type'] == 'streaming':\\n            return self._args_for_streaming_step(step_num)\\n        elif step['type'] == 'jar':\\n            return self._args_for_jar_step(step_num)\\n        elif _is_spark_step_type(step['type']):\\n            return self._args_for_spark_step(step_num)\\n        else:\\n            raise ValueError('Bad step type: %r' % (step['type'],))",
                    "func_fullName": "mrjob.hadoop._args_for_step( self, step_num )"
                },
                {
                    "func_id": 312,
                    "func_name": "_args_for_streaming_step",
                    "func_desc": "_args_for_streaming_step",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _args_for_streaming_step(self, step_num):\\n        hadoop_streaming_jar = self.get_hadoop_streaming_jar()\\n        if not hadoop_streaming_jar:\\n            raise Exception('no Hadoop streaming jar')\\n\\n        return (self.get_hadoop_bin() + ['jar', hadoop_streaming_jar] +\\n                self._hadoop_streaming_jar_args(step_num))",
                    "func_fullName": "mrjob.hadoop._args_for_streaming_step( self, step_num )"
                },
                {
                    "func_id": 314,
                    "func_name": "_env_for_step",
                    "func_desc": "_env_for_step",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _env_for_step(self, step_num):\\n        step = self._get_step(step_num)\\n\\n        env = dict(os.environ)\\n\\n        # when running spark-submit, set its environment directly. See #1464\\n        if _is_spark_step_type(step['type']):\\n            env.update(self._spark_cmdenv(step_num))\\n\\n        return env",
                    "func_fullName": "mrjob.hadoop._env_for_step( self, step_num )"
                },
                {
                    "func_id": 315,
                    "func_name": "_default_step_output_dir",
                    "func_desc": "_default_step_output_dir",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _default_step_output_dir(self):\\n        return posixpath.join(self._hadoop_tmp_dir, 'step-output')",
                    "func_fullName": "mrjob.hadoop._default_step_output_dir( self )"
                },
                {
                    "func_id": 317,
                    "func_name": "_manifest_download_commands",
                    "func_desc": "_manifest_download_commands",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _manifest_download_commands(self):\\n        cp_to_local = self.get_hadoop_bin() + ['fs', '-copyToLocal']\\n\\n        return [\\n            ('*://*', cmd_line(cp_to_local)),\\n        ]",
                    "func_fullName": "mrjob.hadoop._manifest_download_commands( self )"
                },
                {
                    "func_id": 318,
                    "func_name": "_stream_history_log_dirs",
                    "func_desc": "_stream_history_log_dirs",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _stream_history_log_dirs(self, output_dir=None):\\n        \"\"\"Yield lists of directories to look for the history log in.\"\"\"\\n        if not self._read_logs():\\n            return\\n\\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\\n            if _logs_exist(self.fs, log_dir):\\n                log.info('Looking for history log in %s...' % log_dir)\\n                # logs aren't always in a subdir named history/\\n                yield [log_dir]",
                    "func_fullName": "mrjob.hadoop._stream_history_log_dirs( self, output_dir )"
                },
                {
                    "func_id": 319,
                    "func_name": "_stream_task_log_dirs",
                    "func_desc": "_stream_task_log_dirs",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _stream_task_log_dirs(self, application_id=None, output_dir=None):\\n        \"\"\"Yield lists of directories to look for the task logs in.\"\"\"\\n        # Note: this is unlikely to be super-helpful on \"real\" (multi-node)\\n        # pre-YARN Hadoop because task logs aren't generally shipped to a\\n        # local directory. It's a start, anyways. See #1201.\\n        if not self._read_logs():\\n            return\\n\\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\\n            if application_id:\\n                path = self.fs.join(log_dir, 'userlogs', application_id)\\n            else:\\n                path = self.fs.join(log_dir, 'userlogs')\\n\\n            if _logs_exist(self.fs, path):\\n                log.info('Looking for task syslogs in %s...' % path)\\n                yield [path]",
                    "func_fullName": "mrjob.hadoop._stream_task_log_dirs( self, application_id, output_dir )"
                },
                {
                    "func_id": 1571,
                    "func_name": "can_handle_path",
                    "func_desc": "can_handle_path",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def can_handle_path(self, path):\\n        if not (self._hadoop_bin or self._hadoop_bin is None):\\n            return False\\n\\n        return is_uri(path)",
                    "func_fullName": "mrjob.fs.hadoop.can_handle_path( self, path )"
                },
                {
                    "func_id": 1585,
                    "func_name": "yield_paths",
                    "func_desc": "yield_paths",
                    "func_file": "hadoop",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def yield_paths():\\n            for name in 'HADOOP_PREFIX', 'HADOOP_HOME', 'HADOOP_INSTALL':\\n                path = os.environ.get(name)\\n                if path:\\n                    yield os.path.join(path, 'bin')\\n\\n            # They use $HADOOP_INSTALL/hadoop/bin here:\\n            # https://wiki.apache.org/hadoop/GettingStartedWithHadoop\\n            if os.environ.get('HADOOP_INSTALL'):\\n                yield os.path.join(\\n                    os.environ['HADOOP_INSTALL'], 'hadoop', 'bin')\\n\\n            yield None  # use $PATH\\n\\n            # Maybe it's in $HADOOP_MAPRED_HOME? $HADOOP_YARN_HOME? Don't give\\n            # up. Don't worry about duplicates; they're de-duplicated below\\n            for name, path in sorted(os.environ.items()):\\n                if name.startswith('HADOOP_') and name.endswith('_HOME'):\\n                    yield os.path.join(path, 'bin')",
                    "func_fullName": "mrjob.fs.hadoop.yield_paths(  )"
                }
            ]
        },
        {
            "cluster_id": 39,
            "feature_id": 27,
            "feature_desc": "gamma=0.0135; k=2; a=0.25; combined=0.438; stability(ARI)=1.000; sep=0.054",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 367,
                    "func_name": "_chmod_u_rx",
                    "func_desc": "_chmod_u_rx",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _chmod_u_rx(path, recursive=False):\\n    \"\"\"make *path* user readable and executable. If *recursive* is true,\\n    make *path* and everything inside it executable.\"\"\"\\n    if recursive:\\n        for dir_name, _, file_names in os.walk(path, followlinks=True):\\n            for file_name in file_names:\\n                _chmod_u_rx(join(dir_name, file_name))\\n    else:\\n        # only available on Unix, causes rmtree() to fail on Windows; see #1847\\n        if hasattr(os, 'chmod') and platform.system() != \"Windows\":\\n            os.chmod(path, stat.S_IRUSR | stat.S_IXUSR)",
                    "func_fullName": "mrjob.sim._chmod_u_rx( path, recursive )"
                },
                {
                    "func_id": 370,
                    "func_name": "_run_task",
                    "func_desc": "_run_task",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _run_task(invoke_task,\\n              task_type, step_num, task_num,\\n              input_path, output_path, stderr_path, wd, env):\\n    \"\"\"Set up filehandles and call *invoke_task()*.\\n\\n    Helper for :py:meth:`SimMRJobRunner._run_task_func`.\\n    \"\"\"\\n    log.debug('running step %d, %s %d' % (step_num, task_type, task_num))\\n\\n    with open(input_path, 'rb') as stdin, \\\\n            open(output_path, 'wb') as stdout, \\\\n            open(stderr_path, 'wb') as stderr:\\n\\n        invoke_task(\\n            stdin, stdout, stderr, wd, env)",
                    "func_fullName": "mrjob.sim._run_task( invoke_task, task_type, step_num, task_num, input_path, output_path, stderr_path, wd, env )"
                },
                {
                    "func_id": 373,
                    "func_name": "_symlink_or_copy",
                    "func_desc": "_symlink_or_copy",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _symlink_or_copy(path, dest):\\n    \"\"\"Symlink from *dest* to *path*, using relative paths if possible.\\n\\n    If symlinks aren't available, copy path to dest instead.\\n    \"\"\"\\n    if hasattr(os, 'symlink'):\\n        log.debug('creating symlink %s <- %s' % (path, dest))\\n        try:\\n            os.symlink(relpath(path, dirname(dest)), dest)\\n            return\\n        except OSError as ex:\\n            log.debug('  %s' % ex)\\n\\n    log.debug('copying %s -> %s' % (dest, path))\\n    if isdir(path):\\n        copytree(path, dest)\\n    else:\\n        copy2(path, dest)",
                    "func_fullName": "mrjob.sim._symlink_or_copy( path, dest )"
                },
                {
                    "func_id": 374,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, **kwargs):\\n        super(SimMRJobRunner, self).__init__(**kwargs)\\n\\n        self._counters = []\\n\\n        # warn about ignored keyword arguments\\n        for key in self._IGNORED_HADOOP_KWARGS:\\n            value = kwargs.get(key)\\n            if value is not None:\\n                log.warning(\\n                    'ignoring %s keyword arg (requires real Hadoop): %r' %\\n                    (key, value))\\n\\n        # TODO: libjars should just not be an option for local runners\\n        #\\n        # however, the job class can still set it; might want to handle\\n        # this in the job itself\\n        for ignored_opt in self._IGNORED_HADOOP_OPTS:\\n            value = self._opts.get(ignored_opt)\\n            if value:  # ignore [], the default value of libjars\\n                log.warning(\\n                    'ignoring %s option (requires real Hadoop): %r' %\\n                    (ignored_opt, value))",
                    "func_fullName": "mrjob.sim.__init__( self, **kwargs )"
                },
                {
                    "func_id": 376,
                    "func_name": "_invoke_task_func",
                    "func_desc": "_invoke_task_func",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _invoke_task_func(self, task_type, step_num, task_num):\\n        \"\"\"Return a function that runs the given\\n        mapper/reducer. This needs to be pickleable if tasks are going\\n        to be invoked through multiprocessing (e.g. local mode).\\n\\n        The function takes the arguments *stdin*, *stdout*, *stderr*,\\n        *wd* (path of working directory), and *env* (environment dictionary).\\n\\n        The job's filehandle, working dir (*wd*) and environment\\n        will be provided.\\n        \"\"\"\\n        NotImplementedError",
                    "func_fullName": "mrjob.sim._invoke_task_func( self, task_type, step_num, task_num )"
                },
                {
                    "func_id": 377,
                    "func_name": "_run_multiple",
                    "func_desc": "_run_multiple",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run_multiple(self, funcs, num_processes=None):\\n        \"\"\"Run multiple no-args functions, possibly in parallel using\\n        :py:mod:`multiprocessing` (if so all *funcs* must be pickleable).\\n\\n        By default, we just call *funcs* one at a time\\n        \"\"\"\\n        for func in funcs:\\n            func()",
                    "func_fullName": "mrjob.sim._run_multiple( self, funcs, num_processes )"
                },
                {
                    "func_id": 378,
                    "func_name": "_log_cause_of_error",
                    "func_desc": "_log_cause_of_error",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _log_cause_of_error(self, ex):\\n        \"\"\"Log why the job failed.\"\"\"\\n        pass",
                    "func_fullName": "mrjob.sim._log_cause_of_error( self, ex )"
                },
                {
                    "func_id": 379,
                    "func_name": "_run_step_on_spark",
                    "func_desc": "_run_step_on_spark",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run_step_on_spark(self, step, step_num):\\n        \"\"\"Run a Step on Spark. Override this in your subclass. You can\\n        assume that setup wrapper scripts are created (if relevant)\\n        and that self._counters has a dictionary for that step already\"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.sim._run_step_on_spark( self, step, step_num )"
                },
                {
                    "func_id": 380,
                    "func_name": "_run",
                    "func_desc": "_run",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run(self):\\n        if not self._output_dir:\\n            self._output_dir = join(self._get_local_tmp_dir(), 'output')\\n\\n        if hasattr(self, '_create_setup_wrapper_scripts'):  # inline doesn't\\n            self._create_setup_wrapper_scripts()\\n\\n        # this does nothing in inline mode, since there's no _spark_master()\\n        self._copy_files_to_wd_mirror()\\n\\n        # run mapper, combiner, sort, reducer for each step\\n        for step_num, step in enumerate(self._get_steps()):\\n            log.info('Running step %d of %d...' % (\\n                step_num + 1, self._num_steps()))\\n\\n            self._counters.append({})\\n\\n            self._run_step(step, step_num)",
                    "func_fullName": "mrjob.sim._run( self )"
                },
                {
                    "func_id": 381,
                    "func_name": "_run_step",
                    "func_desc": "_run_step",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run_step(self, step, step_num):\\n        \"\"\"Run an individual step. You can assume that setup wrapper scripts\\n        are created and self._counters has a dictionary for that step already.\\n        \"\"\"\\n        if _is_spark_step_type(step['type']):\\n            self._run_step_on_spark(step, step_num)\\n        else:\\n            self._run_streaming_step(step, step_num)",
                    "func_fullName": "mrjob.sim._run_step( self, step, step_num )"
                },
                {
                    "func_id": 382,
                    "func_name": "_run_streaming_step",
                    "func_desc": "_run_streaming_step",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run_streaming_step(self, step, step_num):\\n        \"\"\"Run a Hadoop streaming step on simulated Hadoop.\"\"\"\\n        try:\\n            self._create_dist_cache_dir(step_num)\\n            self.fs.mkdir(self._output_dir_for_step(step_num))\\n\\n            map_splits = self._split_mapper_input(\\n                self._input_paths_for_step(step_num), step_num)\\n\\n            self._run_mappers_and_combiners(step_num, map_splits)\\n\\n            if 'reducer' in step:\\n                self._sort_reducer_input(step_num, len(map_splits))\\n                num_reducer_tasks = self._split_reducer_input(step_num)\\n\\n                self._run_reducers(step_num, num_reducer_tasks)\\n\\n            self._log_counters(step_num)\\n\\n        except Exception as ex:\\n            self._log_counters(step_num)\\n            self._log_cause_of_error(ex)\\n\\n            raise",
                    "func_fullName": "mrjob.sim._run_streaming_step( self, step, step_num )"
                },
                {
                    "func_id": 383,
                    "func_name": "_run_task_func",
                    "func_desc": "_run_task_func",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run_task_func(self, task_type, step_num, task_num, map_split=None):\\n        \"\"\"Returns a no-args function that runs one mapper, reducer, or\\n         combiner.\\n\\n        This sets up everything the task needs to run, then passes it off to\\n        :py:meth:`_invoke_task_func`.\\n        \"\"\"\\n        input_path = self._task_input_path(task_type, step_num, task_num)\\n        stderr_path = self._task_stderr_path(task_type, step_num, task_num)\\n        output_path = self._task_output_path(task_type, step_num, task_num)\\n        wd = self._setup_working_dir(task_type, step_num, task_num)\\n        env = _fix_env(\\n            self._env_for_task(task_type, step_num, task_num, map_split))\\n\\n        return partial(\\n            _run_task,\\n            self._invoke_task_func(task_type, step_num, task_num),\\n            task_type, step_num, task_num,\\n            input_path, output_path, stderr_path, wd, env)",
                    "func_fullName": "mrjob.sim._run_task_func( self, task_type, step_num, task_num, map_split )"
                },
                {
                    "func_id": 385,
                    "func_name": "_parse_task_counters",
                    "func_desc": "_parse_task_counters",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _parse_task_counters(self, task_type, step_num):\\n        \"\"\"Parse all stderr files from the given task (if any).\"\"\"\\n        # don't disable if read_logs=False; parsing counters is\\n        # internal to Hadoop, not something that happens in log files\\n        stderr_paths = self.fs.ls(self._task_stderr_paths_glob(\\n            task_type, step_num))\\n\\n        for stderr_path in stderr_paths:\\n            with open(stderr_path, 'rb') as stderr:\\n                parse_mr_job_stderr(stderr, counters=self._counters[step_num])",
                    "func_fullName": "mrjob.sim._parse_task_counters( self, task_type, step_num )"
                },
                {
                    "func_id": 386,
                    "func_name": "counters",
                    "func_desc": "counters",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def counters(self):\\n        return deepcopy(self._counters)",
                    "func_fullName": "mrjob.sim.counters( self )"
                },
                {
                    "func_id": 388,
                    "func_name": "_write_script_lines",
                    "func_desc": "_write_script_lines",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _write_script_lines(self, lines, path):\\n        \"\"\"Write text to the given file, using local line endings.\"\"\"\\n        with open(path, 'w') as f:\\n            for line in lines:\\n                f.write(line + '\\n')",
                    "func_fullName": "mrjob.sim._write_script_lines( self, lines, path )"
                },
                {
                    "func_id": 391,
                    "func_name": "_create_dist_cache_dir",
                    "func_desc": "_create_dist_cache_dir",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _create_dist_cache_dir(self, step_num):\\n        \"\"\"Copy working directory files into a shared directory,\\n        simulating the way Hadoop's Distributed Cache works on nodes.\"\"\"\\n        cache_dir = self._dist_cache_dir(step_num)\\n\\n        log.debug('creating simulated Distributed Cache dir: %s' % cache_dir)\\n        self.fs.mkdir(cache_dir)\\n\\n        for name, path in self._working_dir_mgr.name_to_path('file').items():\\n            path = _from_file_uri(path)  # might start with file://\\n            dest = self._path_in_dist_cache_dir(name, step_num)\\n            log.debug('copying %s -> %s' % (path, dest))\\n            shutil.copy(path, dest)\\n            _chmod_u_rx(dest)\\n\\n        for name, path in self._working_dir_mgr.name_to_path(\\n                'archive').items():\\n            path = _from_file_uri(path)  # might start with file://\\n            dest = self._path_in_dist_cache_dir(name, step_num)\\n\\n            log.debug('unarchiving %s -> %s' % (path, dest))\\n            unarchive(path, dest)\\n            _chmod_u_rx(dest, recursive=True)",
                    "func_fullName": "mrjob.sim._create_dist_cache_dir( self, step_num )"
                },
                {
                    "func_id": 392,
                    "func_name": "_env_for_task",
                    "func_desc": "_env_for_task",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _env_for_task(self, task_type, step_num, task_num, map_split=None):\\n        \"\"\"Set up environment variables for a subprocess (mapper, etc.)\\n\\n        This combines, in decreasing order of priority:\\n\\n        * environment variables set by the **cmdenv** option\\n        * **jobconf** environment variables set by our job (e.g.\\n          ``mapreduce.task.ismap`)\\n        * environment variables from **jobconf** options, translated to\\n          whatever version of Hadoop we're emulating\\n        * the current environment\\n        * PYTHONPATH set to current working directory\\n\\n        We use :py:func:`~mrjob.conf.combine_local_envs`, so ``PATH``\\n        environment variables are handled specially.\\n        \"\"\"\\n        user_jobconf = self._jobconf_for_step(step_num)\\n\\n        simulated_jobconf = self._simulate_jobconf_for_step(\\n            task_type, step_num, task_num, map_split)\\n\\n        def to_env(jobconf):\\n            return dict((k.replace('.', '_'), str(v))\\n                        for k, v in jobconf.items())\\n\\n        # keep the current environment because we need PATH to find binaries\\n        # and make PYTHONPATH work\\n        return combine_local_envs(os.environ,\\n                                  to_env(user_jobconf),\\n                                  to_env(simulated_jobconf),\\n                                  self._cmdenv())",
                    "func_fullName": "mrjob.sim._env_for_task( self, task_type, step_num, task_num, map_split )"
                },
                {
                    "func_id": 393,
                    "func_name": "_simulate_jobconf_for_step",
                    "func_desc": "_simulate_jobconf_for_step",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _simulate_jobconf_for_step(\\n            self, task_type, step_num, task_num, map_split=None):\\n        j = {}\\n\\n        # TODO: these are really poor imtations of Hadoop IDs. See #1254\\n        j['mapreduce.job.id'] = self._job_key\\n        j['mapreduce.task.id'] = 'task_%s_%s_%04d%d' % (\\n            self._job_key, task_type.lower(), step_num, task_num)\\n        j['mapreduce.task.attempt.id'] = 'attempt_%s_%s_%04d%d_0' % (\\n            self._job_key, task_type.lower(), step_num, task_num)\\n\\n        j['mapreduce.task.ismap'] = str(task_type == 'mapper').lower()\\n\\n        # TODO: is this the correct format?\\n        j['mapreduce.task.partition'] = str(task_num)\\n\\n        j['mapreduce.task.output.dir'] = self._output_dir_for_step(step_num)\\n\\n        working_dir = self._task_working_dir(task_type, step_num, task_num)\\n        j['mapreduce.job.local.dir'] = working_dir\\n\\n        for x in ('archive', 'file'):\\n            named_paths = sorted(self._working_dir_mgr.name_to_path(x).items())\\n\\n            # mapreduce.job.cache.archives\\n            # mapreduce.job.cache.files\\n            j['mapreduce.job.cache.%ss' % x] = ','.join(\\n                '%s#%s' % (path, name) for name, path in named_paths)\\n\\n            # mapreduce.job.cache.local.archives\\n            # mapreduce.job.cache.local.files\\n            j['mapreduce.job.cache.local.%ss' % x] = ','.join(\\n                join(working_dir, name) for name, path in named_paths)\\n\\n        if map_split:\\n            j['mapreduce.map.input.file'] = 'file://' + map_split['file']\\n            j['mapreduce.map.input.length'] = str(map_split['length'])\\n            j['mapreduce.map.input.start'] = str(map_split['start'])\\n\\n        # translate to correct version\\n\\n        # don't use translate_jobconf_dict(); that's meant to add keys\\n        # to user-supplied jobconf\\n        hadoop_version = self.get_hadoop_version()\\n\\n        if hadoop_version:\\n            return {translate_jobconf(k, hadoop_version): v\\n                    for k, v in j.items()}\\n        else:\\n            return {tk: v for k, v in j.items()\\n                    for tk in translate_jobconf_for_all_versions(k)}",
                    "func_fullName": "mrjob.sim._simulate_jobconf_for_step( self, task_type, step_num, task_num, map_split )"
                },
                {
                    "func_id": 394,
                    "func_name": "_num_cores",
                    "func_desc": "_num_cores",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _num_cores(self):\\n        return self._opts['num_cores'] or cpu_count()",
                    "func_fullName": "mrjob.sim._num_cores( self )"
                },
                {
                    "func_id": 401,
                    "func_name": "_setup_working_dir",
                    "func_desc": "_setup_working_dir",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _setup_working_dir(self, task_type, step_num, task_num):\\n        wd = self._task_working_dir(task_type, step_num, task_num)\\n        self.fs.mkdir(wd)\\n\\n        for type in ('archive', 'file'):\\n            for name, path in (\\n                    self._working_dir_mgr.name_to_path(type).items()):\\n                _symlink_or_copy(\\n                    self._path_in_dist_cache_dir(name, step_num),\\n                    join(wd, name))\\n\\n        return wd",
                    "func_fullName": "mrjob.sim._setup_working_dir( self, task_type, step_num, task_num )"
                },
                {
                    "func_id": 402,
                    "func_name": "_last_task_type_in_step",
                    "func_desc": "_last_task_type_in_step",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _last_task_type_in_step(self, step_num):\\n        step = self._get_step(step_num)\\n\\n        if step.get('reducer'):\\n            return 'reducer'\\n        elif step.get('combiner'):\\n            return 'combiner'\\n        else:\\n            return 'mapper'",
                    "func_fullName": "mrjob.sim._last_task_type_in_step( self, step_num )"
                },
                {
                    "func_id": 403,
                    "func_name": "_dist_cache_dir",
                    "func_desc": "_dist_cache_dir",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _dist_cache_dir(self, step_num):\\n        return join(self._step_dir(step_num), 'cache')",
                    "func_fullName": "mrjob.sim._dist_cache_dir( self, step_num )"
                },
                {
                    "func_id": 404,
                    "func_name": "_path_in_dist_cache_dir",
                    "func_desc": "_path_in_dist_cache_dir",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _path_in_dist_cache_dir(self, name, step_num):\\n        return join(self._dist_cache_dir(step_num), name)",
                    "func_fullName": "mrjob.sim._path_in_dist_cache_dir( self, name, step_num )"
                },
                {
                    "func_id": 405,
                    "func_name": "_step_dir",
                    "func_desc": "_step_dir",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _step_dir(self, step_num):\\n        return join(self._get_local_tmp_dir(), 'step', '%03d' % step_num)",
                    "func_fullName": "mrjob.sim._step_dir( self, step_num )"
                },
                {
                    "func_id": 406,
                    "func_name": "_input_paths_for_step",
                    "func_desc": "_input_paths_for_step",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _input_paths_for_step(self, step_num):\\n        if step_num == 0:\\n            return [\\n                _from_file_uri(path)  # *path* could be a file:// URI\\n                for input_path_glob in self._get_input_paths()\\n                for path in self.fs.ls(input_path_glob)\\n            ]\\n        else:\\n            return self.fs.ls(\\n                join(self._output_dir_for_step(step_num - 1), 'part-*'))",
                    "func_fullName": "mrjob.sim._input_paths_for_step( self, step_num )"
                },
                {
                    "func_id": 407,
                    "func_name": "_output_dir_for_step",
                    "func_desc": "_output_dir_for_step",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _output_dir_for_step(self, step_num):\\n        if step_num == self._num_steps() - 1:\\n            return self._output_dir\\n        else:\\n            return self._intermediate_output_dir(step_num, local=True)",
                    "func_fullName": "mrjob.sim._output_dir_for_step( self, step_num )"
                },
                {
                    "func_id": 408,
                    "func_name": "_default_step_output_dir",
                    "func_desc": "_default_step_output_dir",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _default_step_output_dir(self):\\n        return join(self._get_local_tmp_dir(), 'step-output')",
                    "func_fullName": "mrjob.sim._default_step_output_dir( self )"
                },
                {
                    "func_id": 409,
                    "func_name": "_task_dir",
                    "func_desc": "_task_dir",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _task_dir(self, task_type, step_num, task_num):\\n        return join(self._step_dir(step_num), task_type, '%05d' % task_num)",
                    "func_fullName": "mrjob.sim._task_dir( self, task_type, step_num, task_num )"
                },
                {
                    "func_id": 410,
                    "func_name": "_task_input_path",
                    "func_desc": "_task_input_path",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _task_input_path(self, task_type, step_num, task_num):\\n        return join(\\n            self._task_dir(task_type, step_num, task_num), 'input')",
                    "func_fullName": "mrjob.sim._task_input_path( self, task_type, step_num, task_num )"
                },
                {
                    "func_id": 411,
                    "func_name": "_task_stderr_path",
                    "func_desc": "_task_stderr_path",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _task_stderr_path(self, task_type, step_num, task_num):\\n        return join(\\n            self._task_dir(task_type, step_num, task_num), 'stderr')",
                    "func_fullName": "mrjob.sim._task_stderr_path( self, task_type, step_num, task_num )"
                },
                {
                    "func_id": 412,
                    "func_name": "_task_stderr_paths_glob",
                    "func_desc": "_task_stderr_paths_glob",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _task_stderr_paths_glob(self, task_type, step_num):\\n        return join(\\n            self._step_dir(step_num), task_type, '*', 'stderr')",
                    "func_fullName": "mrjob.sim._task_stderr_paths_glob( self, task_type, step_num )"
                },
                {
                    "func_id": 413,
                    "func_name": "_task_output_path",
                    "func_desc": "_task_output_path",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _task_output_path(self, task_type, step_num, task_num):\\n        \"\"\"Where to output data for the given task.\\n\\n        Usually this is just a file named \"output\" in the task's directory,\\n        but if it's the last task type in the step (usually the reducer),\\n        it outputs directly to part-XXXXX files in the step's output directory.\\n        \"\"\"\\n        if task_type == self._last_task_type_in_step(step_num):\\n            return join(\\n                self._output_dir_for_step(step_num), 'part-%05d' % task_num)\\n        else:\\n            return join(\\n                self._task_dir(task_type, step_num, task_num), 'output')",
                    "func_fullName": "mrjob.sim._task_output_path( self, task_type, step_num, task_num )"
                },
                {
                    "func_id": 414,
                    "func_name": "_task_working_dir",
                    "func_desc": "_task_working_dir",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _task_working_dir(self, task_type, step_num, task_num):\\n        return join(self._task_dir(task_type, step_num, task_num), 'wd')",
                    "func_fullName": "mrjob.sim._task_working_dir( self, task_type, step_num, task_num )"
                },
                {
                    "func_id": 418,
                    "func_name": "_log_counters",
                    "func_desc": "_log_counters",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _log_counters(self, step_num):\\n        counters = self.counters()[step_num]\\n        if counters:\\n            log.info('\\n%s\\n' % _format_counters(counters))",
                    "func_fullName": "mrjob.sim._log_counters( self, step_num )"
                },
                {
                    "func_id": 419,
                    "func_name": "to_env",
                    "func_desc": "to_env",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def to_env(jobconf):\\n            return dict((k.replace('.', '_'), str(v))\\n                        for k, v in jobconf.items())",
                    "func_fullName": "mrjob.sim.to_env( jobconf )"
                },
                {
                    "func_id": 700,
                    "func_name": "_im_func",
                    "func_desc": "_im_func",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _im_func(f):\\n    \"\"\"Wrapper to get at the underlying function belonging to a method.\\n\\n    Python 2 is slightly different because classes have \"unbound methods\"\\n    which wrap the underlying function, whereas on Python 3 they're just\\n    functions. (Methods work the same way on both versions.)\\n    \"\"\"\\n    # \"im_func\" is the old Python 2 name for __func__\\n    if hasattr(f, '__func__'):\\n        return f.__func__\\n    else:\\n        return f",
                    "func_fullName": "mrjob.job._im_func( f )"
                },
                {
                    "func_id": 779,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, args=None):\\n        \"\"\"Entry point for running your job from other Python code.\\n\\n        You can pass in command-line arguments, and the job will act the same\\n        way it would if it were run from the command line. For example, to\\n        run your job on EMR::\\n\\n            mr_job = MRYourJob(args=['-r', 'emr'])\\n            with mr_job.make_runner() as runner:\\n                ...\\n\\n        Passing in ``None`` is the same as passing in ``sys.argv[1:]``\\n\\n        For a full list of command-line arguments, run:\\n        ``python -m mrjob.job --help``\\n\\n        :param args: Arguments to your script (switches and input files)\\n\\n        .. versionchanged:: 0.7.0\\n\\n           Previously, *args* set to ``None`` was equivalent to ``[]``.\\n        \"\"\"\\n        # make sure we respect the $TZ (time zone) environment variable\\n        if hasattr(time, 'tzset'):\\n            time.tzset()\\n\\n        # argument dests for args to pass through\\n        self._passthru_arg_dests = set()\\n        self._file_arg_dests = set()\\n\\n        self.arg_parser = ArgumentParser(usage=self._usage(),\\n                                         add_help=False)\\n        self.configure_args()\\n\\n        if args is None:\\n            self._cl_args = sys.argv[1:]\\n        else:\\n            # don't pass sys.argv to self.arg_parser, and have it\\n            # raise an exception on error rather than printing to stderr\\n            # and exiting.\\n            self._cl_args = args\\n\\n            def error(msg):\\n                raise ValueError(msg)\\n\\n            self.arg_parser.error = error\\n\\n        self.load_args(self._cl_args)\\n\\n        # Make it possible to redirect stdin, stdout, and stderr, for testing\\n        # See stdin, stdout, stderr properties and sandbox(), below.\\n        self._stdin = None\\n        self._stdout = None\\n        self._stderr = None",
                    "func_fullName": "mrjob.job.__init__( self, args )"
                },
                {
                    "func_id": 780,
                    "func_name": "stdin",
                    "func_desc": "stdin",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def stdin(self):\\n        return self._stdin or getattr(sys.stdin, 'buffer', sys.stdin)",
                    "func_fullName": "mrjob.job.stdin( self )"
                },
                {
                    "func_id": 781,
                    "func_name": "stdout",
                    "func_desc": "stdout",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def stdout(self):\\n        return self._stdout or getattr(sys.stdout, 'buffer', sys.stdout)",
                    "func_fullName": "mrjob.job.stdout( self )"
                },
                {
                    "func_id": 782,
                    "func_name": "stderr",
                    "func_desc": "stderr",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def stderr(self):\\n        return self._stderr or getattr(sys.stderr, 'buffer', sys.stderr)",
                    "func_fullName": "mrjob.job.stderr( self )"
                },
                {
                    "func_id": 783,
                    "func_name": "_usage",
                    "func_desc": "_usage",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _usage(self):\\n        return \"%(prog)s [options] [input files]\"",
                    "func_fullName": "mrjob.job._usage( self )"
                },
                {
                    "func_id": 784,
                    "func_name": "_print_help",
                    "func_desc": "_print_help",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _print_help(self, options):\\n        \"\"\"Print help for this job. This will either print runner\\n        or basic help. Override to allow other kinds of help.\"\"\"\\n        if options.runner:\\n            _print_help_for_runner(\\n                self._runner_opt_names_for_help(), options.deprecated)\\n        else:\\n            _print_basic_help(self.arg_parser,\\n                              self._usage(),\\n                              options.deprecated,\\n                              options.verbose)",
                    "func_fullName": "mrjob.job._print_help( self, options )"
                },
                {
                    "func_id": 785,
                    "func_name": "_runner_opt_names_for_help",
                    "func_desc": "_runner_opt_names_for_help",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _runner_opt_names_for_help(self):\\n        opts = set(self._runner_class().OPT_NAMES)\\n\\n        if self.options.runner == 'spark':\\n            # specific to Spark runner, but command-line only, so it doesn't\\n            # appear in SparkMRJobRunner.OPT_NAMES (see #2040)\\n            opts.add('max_output_files')\\n\\n        return opts",
                    "func_fullName": "mrjob.job._runner_opt_names_for_help( self )"
                },
                {
                    "func_id": 786,
                    "func_name": "_non_option_kwargs",
                    "func_desc": "_non_option_kwargs",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _non_option_kwargs(self):\\n        \"\"\"Keyword arguments to runner constructor that can't be set\\n        in mrjob.conf.\\n\\n        These should match the (named) arguments to\\n        :py:meth:`~mrjob.runner.MRJobRunner.__init__`.\\n        \"\"\"\\n        # build extra_args\\n        raw_args = _parse_raw_args(self.arg_parser, self._cl_args)\\n\\n        extra_args = []\\n\\n        for dest, option_string, args in raw_args:\\n            if dest in self._file_arg_dests:\\n                extra_args.append(option_string)\\n                extra_args.append(parse_legacy_hash_path('file', args[0]))\\n            elif dest in self._passthru_arg_dests:\\n                # special case for --hadoop-args=-verbose etc.\\n                if (option_string and len(args) == 1 and\\n                        args[0].startswith('-')):\\n                    extra_args.append('%s=%s' % (option_string, args[0]))\\n                else:\\n                    if option_string:\\n                        extra_args.append(option_string)\\n                    extra_args.extend(args)\\n\\n        # max_output_files is added by _add_runner_args() but can only\\n        # be set from the command line, so we add it here (see #2040)\\n        return dict(\\n            conf_paths=self.options.conf_paths,\\n            extra_args=extra_args,\\n            hadoop_input_format=self.hadoop_input_format(),\\n            hadoop_output_format=self.hadoop_output_format(),\\n            input_paths=self.options.args,\\n            max_output_files=self.options.max_output_files,\\n            mr_job_script=self.mr_job_script(),\\n            output_dir=self.options.output_dir,\\n            partitioner=self.partitioner(),\\n            stdin=self.stdin,\\n            step_output_dir=self.options.step_output_dir,\\n        )",
                    "func_fullName": "mrjob.job._non_option_kwargs( self )"
                },
                {
                    "func_id": 787,
                    "func_name": "_kwargs_from_switches",
                    "func_desc": "_kwargs_from_switches",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _kwargs_from_switches(self, keys):\\n        return dict(\\n            (key, getattr(self.options, key))\\n            for key in keys if hasattr(self.options, key)\\n        )",
                    "func_fullName": "mrjob.job._kwargs_from_switches( self, keys )"
                },
                {
                    "func_id": 788,
                    "func_name": "_job_kwargs",
                    "func_desc": "_job_kwargs",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _job_kwargs(self):\\n        \"\"\"Keyword arguments to the runner class that can be specified\\n        by the job/launcher itself.\"\"\"\\n        # use the most basic combiners; leave magic like resolving paths\\n        # and blanking out jobconf values to the runner\\n        return dict(\\n            # command-line has the final say on jobconf and libjars\\n            jobconf=combine_dicts(\\n                self.jobconf(), self.options.jobconf),\\n            libjars=combine_lists(\\n                self.libjars(), self.options.libjars),\\n            partitioner=self.partitioner(),\\n            sort_values=self.sort_values(),\\n            # TODO: should probably put self.options last below for consistency\\n            upload_archives=combine_lists(\\n                self.options.upload_archives, self.archives()),\\n            upload_dirs=combine_lists(\\n                self.options.upload_dirs, self.dirs()),\\n            upload_files=combine_lists(\\n                self.options.upload_files, self.files()),\\n        )",
                    "func_fullName": "mrjob.job._job_kwargs( self )"
                },
                {
                    "func_id": 805,
                    "func_name": "spark",
                    "func_desc": "spark",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def spark(self, input_path, output_path):\\n        \"\"\"Re-define this with Spark code to run. You can read input\\n        with *input_path* and output with *output_path*.\\n\\n        .. warning::\\n\\n           Prior to v0.6.8, to pass job methods into Spark\\n           (``rdd.flatMap(self.some_method)``), you first had to call\\n           :py:meth:`self.sandbox() <mrjob.job.MRJob.sandbox>`; otherwise\\n           Spark would error because *self* was not serializable.\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.job.spark( self, input_path, output_path )"
                },
                {
                    "func_id": 806,
                    "func_name": "spark_args",
                    "func_desc": "spark_args",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def spark_args(self):\\n        \"\"\"Redefine this to pass custom arguments to Spark.\"\"\"\\n        return []",
                    "func_fullName": "mrjob.job.spark_args( self )"
                },
                {
                    "func_id": 807,
                    "func_name": "steps",
                    "func_desc": "steps",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def steps(self):\\n        \"\"\"Re-define this to make a multi-step job.\\n\\n        If you don't re-define this, we'll automatically create a one-step\\n        job using any of :py:meth:`mapper`, :py:meth:`mapper_init`,\\n        :py:meth:`mapper_final`, :py:meth:`reducer_init`,\\n        :py:meth:`reducer_final`, and :py:meth:`reducer` that you've\\n        re-defined. For example::\\n\\n            def steps(self):\\n                return [MRStep(mapper=self.transform_input,\\n                               reducer=self.consolidate_1),\\n                        MRStep(reducer_init=self.log_mapper_init,\\n                               reducer=self.consolidate_2)]\\n\\n        :return: a list of steps constructed with\\n                 :py:class:`~mrjob.step.MRStep` or other classes in\\n                 :py:mod:`mrjob.step`.\\n        \"\"\"\\n        # only include methods that have been redefined\\n        from mrjob.step import _JOB_STEP_FUNC_PARAMS\\n        kwargs = dict(\\n            (func_name, getattr(self, func_name))\\n            for func_name in _JOB_STEP_FUNC_PARAMS + ('spark',)\\n            if (_im_func(getattr(self, func_name)) is not\\n                _im_func(getattr(MRJob, func_name))))\\n\\n        # special case for spark()\\n        # TODO: support jobconf as well\\n        if 'spark' in kwargs:\\n            if sorted(kwargs) != ['spark']:\\n                raise ValueError(\\n                    \"Can't mix spark() and streaming functions\")\\n            return [SparkStep(\\n                spark=kwargs['spark'],\\n                spark_args=self.spark_args())]\\n\\n        # MRStep takes commands as strings, but the user defines them in the\\n        # class as functions that return strings, so call the functions.\\n        updates = {}\\n        for k, v in kwargs.items():\\n            if k.endswith('_cmd') or k.endswith('_pre_filter'):\\n                updates[k] = v()\\n\\n        kwargs.update(updates)\\n\\n        if kwargs:\\n            return [MRStep(**kwargs)]\\n        else:\\n            return []",
                    "func_fullName": "mrjob.job.steps( self )"
                },
                {
                    "func_id": 808,
                    "func_name": "increment_counter",
                    "func_desc": "increment_counter",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def increment_counter(self, group, counter, amount=1):\\n        \"\"\"Increment a counter in Hadoop streaming by printing to stderr.\\n\\n        :type group: str\\n        :param group: counter group\\n        :type counter: str\\n        :param counter: description of the counter\\n        :type amount: int\\n        :param amount: how much to increment the counter by\\n\\n        Commas in ``counter`` or ``group`` will be automatically replaced\\n        with semicolons (commas confuse Hadoop streaming).\\n        \"\"\"\\n        # don't allow people to pass in floats\\n        from mrjob.py2 import integer_types\\n        if not isinstance(amount, integer_types):\\n            raise TypeError('amount must be an integer, not %r' % (amount,))\\n\\n        # cast non-strings to strings (if people pass in exceptions, etc)\\n        if not isinstance(group, string_types):\\n            group = str(group)\\n        if not isinstance(counter, string_types):\\n            counter = str(counter)\\n\\n        # Extra commas screw up hadoop and there's no way to escape them. So\\n        # replace them with the next best thing: semicolons!\\n        #\\n        # The relevant Hadoop code is incrCounter(), here:\\n        # http://svn.apache.org/viewvc/hadoop/mapreduce/trunk/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java?view=markup  # noqa\\n        group = group.replace(',', ';')\\n        counter = counter.replace(',', ';')\\n\\n        line = 'reporter:counter:%s,%s,%d\\n' % (group, counter, amount)\\n        if not isinstance(line, bytes):\\n            line = line.encode('utf_8')\\n\\n        self.stderr.write(line)\\n        self.stderr.flush()",
                    "func_fullName": "mrjob.job.increment_counter( self, group, counter, amount )"
                },
                {
                    "func_id": 809,
                    "func_name": "set_status",
                    "func_desc": "set_status",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def set_status(self, msg):\\n        \"\"\"Set the job status in hadoop streaming by printing to stderr.\\n\\n        This is also a good way of doing a keepalive for a job that goes a\\n        long time between outputs; Hadoop streaming usually times out jobs\\n        that give no output for longer than 10 minutes.\\n        \"\"\"\\n        line = 'reporter:status:%s\\n' % (msg,)\\n        if not isinstance(line, bytes):\\n            line = line.encode('utf_8')\\n\\n        self.stderr.write(line)\\n        self.stderr.flush()",
                    "func_fullName": "mrjob.job.set_status( self, msg )"
                },
                {
                    "func_id": 810,
                    "func_name": "run",
                    "func_desc": "run",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def run(cls):\\n        \"\"\"Entry point for running job from the command-line.\\n\\n        This is also the entry point when a mapper or reducer is run\\n        by Hadoop Streaming.\\n\\n        Does one of:\\n\\n        * Run a mapper (:option:`--mapper`). See :py:meth:`run_mapper`\\n        * Run a combiner (:option:`--combiner`). See :py:meth:`run_combiner`\\n        * Run a reducer (:option:`--reducer`). See :py:meth:`run_reducer`\\n        * Run the entire job. See :py:meth:`run_job`\\n        \"\"\"\\n        # load options from the command line\\n        cls().execute()",
                    "func_fullName": "mrjob.job.run( cls )"
                },
                {
                    "func_id": 811,
                    "func_name": "run_job",
                    "func_desc": "run_job",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def run_job(self):\\n        \"\"\"Run the all steps of the job, logging errors (and debugging output\\n        if :option:`--verbose` is specified) to STDERR and streaming the\\n        output to STDOUT.\\n\\n        Called from :py:meth:`run`. You'd probably only want to call this\\n        directly from automated tests.\\n        \"\"\"\\n        # self.stderr is strictly binary, need to wrap it so it's possible\\n        # to log to it in Python 3\\n        from mrjob.step import StepFailedException\\n        log_stream = codecs.getwriter('utf_8')(self.stderr)\\n\\n        self.set_up_logging(quiet=self.options.quiet,\\n                            verbose=self.options.verbose,\\n                            stream=log_stream)\\n\\n        with self.make_runner() as runner:\\n            try:\\n                runner.run()\\n            except StepFailedException as e:\\n                # no need for a runner stacktrace if step failed; runners will\\n                # log more useful information anyway\\n                log.error(str(e))\\n                sys.exit(1)\\n\\n            if self._should_cat_output():\\n                for chunk in runner.cat_output():\\n                    self.stdout.write(chunk)\\n                self.stdout.flush()",
                    "func_fullName": "mrjob.job.run_job( self )"
                },
                {
                    "func_id": 812,
                    "func_name": "set_up_logging",
                    "func_desc": "set_up_logging",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def set_up_logging(cls, quiet=False, verbose=False, stream=None):\\n        \"\"\"Set up logging when running from the command line. This is also\\n        used by the various command-line utilities.\\n\\n        :param bool quiet: If true, don't log. Overrides *verbose*.\\n        :param bool verbose: If true, set log level to ``DEBUG`` (default is\\n                             ``INFO``)\\n        :param bool stream: Stream to log to (default is ``sys.stderr``)\\n        \"\"\"\\n        from mrjob.util import log_to_stream\\n        from mrjob.util import log_to_null\\n        if quiet:\\n            log_to_null(name='mrjob')\\n            log_to_null(name='__main__')\\n        else:\\n            log_to_stream(name='mrjob', debug=verbose, stream=stream)\\n            log_to_stream(name='__main__', debug=verbose, stream=stream)",
                    "func_fullName": "mrjob.job.set_up_logging( cls, quiet, verbose, stream )"
                },
                {
                    "func_id": 813,
                    "func_name": "_should_cat_output",
                    "func_desc": "_should_cat_output",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _should_cat_output(self):\\n        if self.options.cat_output is None:\\n            return not self.options.output_dir\\n        else:\\n            return self.options.cat_output",
                    "func_fullName": "mrjob.job._should_cat_output( self )"
                },
                {
                    "func_id": 814,
                    "func_name": "execute",
                    "func_desc": "execute",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def execute(self):\\n        # MRJob does Hadoop Streaming stuff, or defers to its superclass\\n        # (MRJobLauncher) if not otherwise instructed\\n        if self.options.run_mapper:\\n            self.run_mapper(self.options.step_num)\\n\\n        elif self.options.run_combiner:\\n            self.run_combiner(self.options.step_num)\\n\\n        elif self.options.run_reducer:\\n            self.run_reducer(self.options.step_num)\\n\\n        elif self.options.run_spark:\\n            self.run_spark(self.options.step_num)\\n\\n        else:\\n            self.run_job()",
                    "func_fullName": "mrjob.job.execute( self )"
                },
                {
                    "func_id": 815,
                    "func_name": "make_runner",
                    "func_desc": "make_runner",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def make_runner(self):\\n        \"\"\"Make a runner based on command-line arguments, so we can\\n        launch this job on EMR, on Hadoop, or locally.\\n\\n        :rtype: :py:class:`mrjob.runner.MRJobRunner`\\n        \"\"\"\\n        bad_words = (\\n            '--mapper', '--reducer', '--combiner', '--step-num', '--spark')\\n        for w in bad_words:\\n            if w in sys.argv:\\n                raise UsageError(\"make_runner() was called with %s. This\"\\n                                 \" probably means you tried to use it from\"\\n                                 \" __main__, which doesn't work.\" % w)\\n\\n        runner_class = self._runner_class()\\n        kwargs = self._runner_kwargs()\\n\\n        # screen out most false-ish args so that it's readable\\n        log.debug('making runner: %s(%s, ...)' % (\\n            runner_class.__name__,\\n            ', '.join('%s=%s' % (k, v)\\n                      for k, v in sorted(kwargs.items())\\n                      if v not in (None, [], {}))))\\n\\n        return self._runner_class()(**self._runner_kwargs())",
                    "func_fullName": "mrjob.job.make_runner( self )"
                },
                {
                    "func_id": 816,
                    "func_name": "_runner_class",
                    "func_desc": "_runner_class",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _runner_class(self):\\n        \"\"\"Runner class as indicated by ``--runner``. Defaults to ``'inline'``.\\n        \"\"\"\\n        return _runner_class(self.options.runner or 'inline')",
                    "func_fullName": "mrjob.job._runner_class( self )"
                },
                {
                    "func_id": 817,
                    "func_name": "_runner_kwargs",
                    "func_desc": "_runner_kwargs",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _runner_kwargs(self):\\n        \"\"\"If we're building an inline or Spark runner,\\n        include mrjob_cls in kwargs.\"\"\"\\n        from mrjob.options import _RUNNER_OPTS\\n        kwargs = combine_dicts(\\n            self._non_option_kwargs(),\\n            # don't screen out irrelevant opts (see #1898)\\n            self._kwargs_from_switches(set(_RUNNER_OPTS)),\\n            self._job_kwargs(),\\n        )\\n\\n        if self._runner_class().alias in ('inline', 'spark'):\\n            kwargs = dict(mrjob_cls=self.__class__, **kwargs)\\n\\n        # pass steps to runner (see #1845)\\n        kwargs = dict(steps=self._steps_desc(), **kwargs)\\n\\n        return kwargs",
                    "func_fullName": "mrjob.job._runner_kwargs( self )"
                },
                {
                    "func_id": 818,
                    "func_name": "_get_step",
                    "func_desc": "_get_step",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_step(self, step_num, expected_type):\\n        \"\"\"Helper for run_* methods\"\"\"\\n        steps = self.steps()\\n        if not 0 <= step_num < len(steps):\\n            raise ValueError('Out-of-range step: %d' % step_num)\\n        step = steps[step_num]\\n        if not isinstance(step, expected_type):\\n            raise TypeError('Step %d is not a %s', expected_type.__name__)\\n        return step",
                    "func_fullName": "mrjob.job._get_step( self, step_num, expected_type )"
                },
                {
                    "func_id": 826,
                    "func_name": "run_spark",
                    "func_desc": "run_spark",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def run_spark(self, step_num):\\n        \"\"\"Run the Spark code for the given step.\\n\\n        :type step_num: int\\n        :param step_num: which step to run (0-indexed)\\n\\n        Called from :py:meth:`run`. You'd probably only want to call this\\n        directly from automated tests.\\n        \"\"\"\\n        step = self._get_step(step_num, SparkStep)\\n\\n        if len(self.options.args) != 2:\\n            raise ValueError('Wrong number of args')\\n        input_path, output_path = self.options.args\\n\\n        spark_method = step.spark\\n        spark_method(input_path, output_path)",
                    "func_fullName": "mrjob.job.run_spark( self, step_num )"
                },
                {
                    "func_id": 827,
                    "func_name": "_steps_desc",
                    "func_desc": "_steps_desc",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _steps_desc(self):\\n        step_descs = []\\n        for step_num, step in enumerate(self.steps()):\\n            step_descs.append(step.description(step_num))\\n        return step_descs",
                    "func_fullName": "mrjob.job._steps_desc( self )"
                },
                {
                    "func_id": 828,
                    "func_name": "mr_job_script",
                    "func_desc": "mr_job_script",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mr_job_script(cls):\\n        \"\"\"Path of this script. This returns the file containing\\n        this class, or ``None`` if there isn't any (e.g. it was\\n        defined from the command line interface.)\"\"\"\\n        try:\\n            return inspect.getsourcefile(cls)\\n        except TypeError:\\n            return None",
                    "func_fullName": "mrjob.job.mr_job_script( cls )"
                },
                {
                    "func_id": 829,
                    "func_name": "_read_input",
                    "func_desc": "_read_input",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _read_input(self):\\n        \"\"\"Read from stdin, or one more files, or directories.\\n        Yield one line at time.\\n\\n        - Resolve globs (``foo_*.gz``).\\n        - Decompress ``.gz`` and ``.bz2`` files.\\n        - If path is ``-``, read from STDIN.\\n        - Recursively read all files in a directory\\n        \"\"\"\\n        paths = self.options.args or ['-']\\n\\n        for path in paths:\\n            if path == '-':\\n                for line in self.stdin:\\n                    yield line\\n            else:\\n                with open(path, 'rb') as f:\\n                    for line in to_lines(decompress(f, path)):\\n                        yield line",
                    "func_fullName": "mrjob.job._read_input( self )"
                },
                {
                    "func_id": 830,
                    "func_name": "_wrap_protocols",
                    "func_desc": "_wrap_protocols",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _wrap_protocols(self, step_num, step_type):\\n        \"\"\"Pick the protocol classes to use for reading and writing\\n        for the given step.\\n\\n        Returns a tuple of ``(read_lines, write_line)``\\n\\n        ``read_lines()`` is a function that reads lines from input, decodes\\n            them, and yields key, value pairs.\\n        ``write_line()`` is a function that takes key and value as args,\\n            encodes them, and writes a line to output.\\n\\n        :param step_num: which step to run (e.g. 0)\\n        :param step_type: ``'mapper'``, ``'reducer'``, or ``'combiner'`` from\\n                          :py:mod:`mrjob.step`\\n        \"\"\"\\n        read, write = self.pick_protocols(step_num, step_type)\\n\\n        def read_lines():\\n            for line in self._read_input():\\n                key, value = read(line.rstrip(b'\\r\\n'))\\n                yield key, value\\n\\n        def write_line(key, value):\\n            self.stdout.write(write(key, value))\\n            self.stdout.write(b'\\n')\\n\\n        return read_lines, write_line",
                    "func_fullName": "mrjob.job._wrap_protocols( self, step_num, step_type )"
                },
                {
                    "func_id": 831,
                    "func_name": "_step_key",
                    "func_desc": "_step_key",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _step_key(self, step_num, step_type):\\n        return '%d-%s' % (step_num, step_type)",
                    "func_fullName": "mrjob.job._step_key( self, step_num, step_type )"
                },
                {
                    "func_id": 832,
                    "func_name": "_script_step_mapping",
                    "func_desc": "_script_step_mapping",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _script_step_mapping(self, steps_desc):\\n        \"\"\"Return a mapping of ``self._step_key(step_num, step_type)`` ->\\n        (place in sort order of all *script* steps), for the purposes of\\n        choosing which protocols to use for input and output.\\n\\n        Non-script steps do not appear in the mapping.\\n        \"\"\"\\n        mapping = {}\\n        script_step_num = 0\\n        for i, step in enumerate(steps_desc):\\n\\n            if 'mapper' in step and step['mapper']['type'] == 'script':\\n                k = self._step_key(i, 'mapper')\\n                mapping[k] = script_step_num\\n                script_step_num += 1\\n\\n            if 'reducer' in step and step['reducer']['type'] == 'script':\\n                k = self._step_key(i, 'reducer')\\n                mapping[k] = script_step_num\\n                script_step_num += 1\\n\\n        return mapping",
                    "func_fullName": "mrjob.job._script_step_mapping( self, steps_desc )"
                },
                {
                    "func_id": 834,
                    "func_name": "_pick_protocol_instances",
                    "func_desc": "_pick_protocol_instances",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _pick_protocol_instances(self, step_num, step_type):\\n        steps_desc = self._steps_desc()\\n\\n        step_map = self._script_step_mapping(steps_desc)\\n\\n        # pick input protocol\\n\\n        if step_type == 'combiner':\\n            # Combiners read and write the mapper's output protocol because\\n            # they have to be able to run 0-inf times without changing the\\n            # format of the data.\\n            # Combiners for non-script substeps can't use protocols, so this\\n            # function will just give us RawValueProtocol() in that case.\\n            previous_mapper_output = self._mapper_output_protocol(\\n                step_num, step_map)\\n            return previous_mapper_output, previous_mapper_output\\n        else:\\n            step_key = self._step_key(step_num, step_type)\\n\\n            if step_key not in step_map:\\n                raise ValueError(\\n                    \"Can't pick a protocol for a non-script step\")\\n\\n            real_num = step_map[step_key]\\n            if real_num == (len(step_map) - 1):\\n                write = self.output_protocol()\\n            else:\\n                write = self.internal_protocol()\\n\\n            if real_num == 0:\\n                read = self.input_protocol()\\n            else:\\n                read = self.internal_protocol()\\n            return read, write",
                    "func_fullName": "mrjob.job._pick_protocol_instances( self, step_num, step_type )"
                },
                {
                    "func_id": 835,
                    "func_name": "pick_protocols",
                    "func_desc": "pick_protocols",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def pick_protocols(self, step_num, step_type):\\n        \"\"\"Pick the protocol classes to use for reading and writing for the\\n        given step.\\n\\n        :type step_num: int\\n        :param step_num: which step to run (e.g. ``0`` for the first step)\\n        :type step_type: str\\n        :param step_type: one of `'mapper'`, `'combiner'`, or `'reducer'`\\n        :return: (read_function, write_function)\\n\\n        By default, we use one protocol for reading input, one\\n        internal protocol for communication between steps, and one\\n        protocol for final output (which is usually the same as the\\n        internal protocol). Protocols can be controlled by setting\\n        :py:attr:`INPUT_PROTOCOL`, :py:attr:`INTERNAL_PROTOCOL`, and\\n        :py:attr:`OUTPUT_PROTOCOL`.\\n\\n        Re-define this if you need fine control over which protocols\\n        are used by which steps.\\n        \"\"\"\\n\\n        # wrapping functionality like this makes testing much simpler\\n        p_read, p_write = self._pick_protocol_instances(step_num, step_type)\\n\\n        return p_read.read, p_write.write",
                    "func_fullName": "mrjob.job.pick_protocols( self, step_num, step_type )"
                },
                {
                    "func_id": 836,
                    "func_name": "configure_args",
                    "func_desc": "configure_args",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def configure_args(self):\\n        \"\"\"Define arguments for this script. Called from :py:meth:`__init__()`.\\n\\n        Re-define to define custom command-line arguments or pass\\n        through existing ones::\\n\\n            def configure_args(self):\\n                super(MRYourJob, self).configure_args()\\n\\n                self.add_passthru_arg(...)\\n                self.add_file_arg(...)\\n                self.pass_arg_through(...)\\n                ...\\n        \"\"\"\\n        self.arg_parser.add_argument(\\n            dest='args', nargs='*',\\n            help=('input paths to read (or stdin if not set). If --spark'\\n                  ' is set, the input and output path for the spark job.'))\\n\\n        _add_basic_args(self.arg_parser)\\n        _add_job_args(self.arg_parser)\\n        _add_runner_args(self.arg_parser)\\n        _add_step_args(self.arg_parser, include_deprecated=True)",
                    "func_fullName": "mrjob.job.configure_args( self )"
                },
                {
                    "func_id": 837,
                    "func_name": "load_args",
                    "func_desc": "load_args",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def load_args(self, args):\\n        \"\"\"Load command-line options into ``self.options``.\\n\\n        Called from :py:meth:`__init__()` after :py:meth:`configure_args`.\\n\\n        :type args: list of str\\n        :param args: a list of command line arguments. ``None`` will be\\n                     treated the same as ``[]``.\\n\\n        Re-define if you want to post-process command-line arguments::\\n\\n            def load_args(self, args):\\n                super(MRYourJob, self).load_args(args)\\n\\n                self.stop_words = self.options.stop_words.split(',')\\n                ...\\n        \"\"\"\\n        if hasattr(self.arg_parser, 'parse_intermixed_args'):\\n            # restore old optparse behavior on Python 3.7+. See #1701\\n            self.options = self.arg_parser.parse_intermixed_args(args)\\n        else:\\n            self.options = self.arg_parser.parse_args(args)\\n\\n        if self.options.help:\\n            self._print_help(self.options)\\n            sys.exit(0)",
                    "func_fullName": "mrjob.job.load_args( self, args )"
                },
                {
                    "func_id": 838,
                    "func_name": "add_file_arg",
                    "func_desc": "add_file_arg",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def add_file_arg(self, *args, **kwargs):\\n        \"\"\"Add a command-line option that sends an external file\\n        (e.g. a SQLite DB) to Hadoop::\\n\\n             def configure_args(self):\\n                super(MRYourJob, self).configure_args()\\n                self.add_file_arg('--scoring-db', help=...)\\n\\n        This does the right thing: the file will be uploaded to the working\\n        dir of the script on Hadoop, and the script will be passed the same\\n        option, but with the local name of the file in the script's working\\n        directory.\\n\\n        .. note::\\n\\n           If you pass a file to a job, best practice is to lazy-load its\\n           contents (e.g. make a method that opens the file the first time\\n           you call it) rather than loading it in your job's constructor or\\n           :py:meth:`load_args`. Not only is this more efficient, it's\\n           necessary if you want to run your job in a Spark executor\\n           (because the file may not be in the same place in a Spark driver).\\n\\n        .. note::\\n\\n           We suggest against sending Berkeley DBs to your job, as\\n           Berkeley DB is not forwards-compatible (so a Berkeley DB that you\\n           construct on your computer may not be readable from within\\n           Hadoop). Use SQLite databases instead. If all you need is an on-disk\\n           hash table, try out the :py:mod:`sqlite3dbm` module.\\n\\n        .. versionchanged:: 0.6.6\\n\\n           now accepts explicit ``type=str``\\n\\n        .. versionchanged:: 0.6.8\\n\\n           fully supported on Spark, including ``local[*]`` master\\n        \"\"\"\\n        if kwargs.get('type') not in (None, str):\\n            raise ArgumentTypeError(\\n                'file options must take strings')\\n\\n        if kwargs.get('action') not in (None, 'append', 'store'):\\n            raise ArgumentTypeError(\\n                \"file options must use the actions 'store' or 'append'\")\\n\\n        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\\n\\n        self._file_arg_dests.add(pass_opt.dest)",
                    "func_fullName": "mrjob.job.add_file_arg( self, *args, **kwargs )"
                },
                {
                    "func_id": 839,
                    "func_name": "add_passthru_arg",
                    "func_desc": "add_passthru_arg",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def add_passthru_arg(self, *args, **kwargs):\\n        \"\"\"Function to create options which both the job runner\\n        and the job itself respect (we use this for protocols, for example).\\n\\n        Use it like you would use\\n        :py:func:`argparse.ArgumentParser.add_argument`::\\n\\n            def configure_args(self):\\n                super(MRYourJob, self).configure_args()\\n                self.add_passthru_arg(\\n                    '--max-ngram-size', type=int, default=4, help='...')\\n\\n        If you want to pass files through to the mapper/reducer, use\\n        :py:meth:`add_file_arg` instead.\\n\\n        If you want to pass through a built-in option (e.g. ``--runner``, use\\n        :py:meth:`pass_arg_through` instead.\\n        \"\"\"\\n        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\\n\\n        self._passthru_arg_dests.add(pass_opt.dest)",
                    "func_fullName": "mrjob.job.add_passthru_arg( self, *args, **kwargs )"
                },
                {
                    "func_id": 840,
                    "func_name": "pass_arg_through",
                    "func_desc": "pass_arg_through",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def pass_arg_through(self, opt_str):\\n        \"\"\"Pass the given argument through to the job.\"\"\"\\n\\n        # _actions is hidden but the interface appears to be stable,\\n        # and there's no non-hidden interface we can use\\n        for action in self.arg_parser._actions:\\n            if opt_str in action.option_strings or opt_str == action.dest:\\n                self._passthru_arg_dests.add(action.dest)\\n                break\\n        else:\\n            raise ValueError('unknown arg: %s', opt_str)",
                    "func_fullName": "mrjob.job.pass_arg_through( self, opt_str )"
                },
                {
                    "func_id": 841,
                    "func_name": "is_task",
                    "func_desc": "is_task",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def is_task(self):\\n        \"\"\"True if this is a mapper, combiner, reducer, or Spark script.\\n\\n        This is mostly useful inside :py:meth:`load_args`, to disable\\n        loading args when we aren't running inside Hadoop.\\n        \"\"\"\\n        return (self.options.run_mapper or\\n                self.options.run_combiner or\\n                self.options.run_reducer or\\n                self.options.run_spark)",
                    "func_fullName": "mrjob.job.is_task( self )"
                },
                {
                    "func_id": 842,
                    "func_name": "input_protocol",
                    "func_desc": "input_protocol",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def input_protocol(self):\\n        \"\"\"Instance of the protocol to use to convert input lines to Python\\n        objects. Default behavior is to return an instance of\\n        :py:attr:`INPUT_PROTOCOL`.\\n        \"\"\"\\n        if not isinstance(self.INPUT_PROTOCOL, type):\\n            log.warning('INPUT_PROTOCOL should be a class, not %s' %\\n                        self.INPUT_PROTOCOL)\\n        return self.INPUT_PROTOCOL()",
                    "func_fullName": "mrjob.job.input_protocol( self )"
                },
                {
                    "func_id": 843,
                    "func_name": "internal_protocol",
                    "func_desc": "internal_protocol",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def internal_protocol(self):\\n        \"\"\"Instance of the protocol to use to communicate between steps.\\n        Default behavior is to return an instance of\\n        :py:attr:`INTERNAL_PROTOCOL`.\\n        \"\"\"\\n        if not isinstance(self.INTERNAL_PROTOCOL, type):\\n            log.warning('INTERNAL_PROTOCOL should be a class, not %s' %\\n                        self.INTERNAL_PROTOCOL)\\n        return self.INTERNAL_PROTOCOL()",
                    "func_fullName": "mrjob.job.internal_protocol( self )"
                },
                {
                    "func_id": 844,
                    "func_name": "output_protocol",
                    "func_desc": "output_protocol",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def output_protocol(self):\\n        \"\"\"Instance of the protocol to use to convert Python objects to output\\n        lines. Default behavior is to return an instance of\\n        :py:attr:`OUTPUT_PROTOCOL`.\\n        \"\"\"\\n        if not isinstance(self.OUTPUT_PROTOCOL, type):\\n            log.warning('OUTPUT_PROTOCOL should be a class, not %s' %\\n                        self.OUTPUT_PROTOCOL)\\n        return self.OUTPUT_PROTOCOL()",
                    "func_fullName": "mrjob.job.output_protocol( self )"
                },
                {
                    "func_id": 845,
                    "func_name": "parse_output",
                    "func_desc": "parse_output",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def parse_output(self, chunks):\\n        \"\"\"Parse the final output of this MRJob (as a stream of byte chunks)\\n        into a stream of ``(key, value)``.\\n        \"\"\"\\n        read = self.output_protocol().read\\n\\n        for line in to_lines(chunks):\\n            yield read(line)",
                    "func_fullName": "mrjob.job.parse_output( self, chunks )"
                },
                {
                    "func_id": 848,
                    "func_name": "libjars",
                    "func_desc": "libjars",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def libjars(self):\\n        \"\"\"Optional list of paths of jar files to run our job with using\\n        Hadoop's ``-libjars`` option. Normally setting :py:attr:`LIBJARS`\\n        is sufficient. Paths from :py:attr:`LIBJARS` are interpreted as\\n        relative to the the directory containing the script (paths from the\\n        command-line are relative to the current working directory).\\n\\n        Note that ``~`` and environment variables in paths will always be\\n        expanded by the job runner (see :mrjob-opt:`libjars`).\\n\\n        .. versionchanged:: 0.6.6\\n\\n           re-defining this no longer clobbers the command-line\\n           ``--libjars`` option\\n        \"\"\"\\n        script_dir = os.path.dirname(self.mr_job_script())\\n\\n        paths = []\\n\\n        # libjar paths will eventually be combined with combine_path_lists,\\n        # which will expand environment variables. We don't want to assume\\n        # a path like $MY_DIR/some.jar is always relative ($MY_DIR could start\\n        # with /), but we also don't want to expand environment variables\\n        # prematurely.\\n        for path in self.LIBJARS or []:\\n            if os.path.isabs(expand_path(path)):\\n                paths.append(path)\\n            else:\\n                paths.append(os.path.join(script_dir, path))\\n\\n        return paths",
                    "func_fullName": "mrjob.job.libjars( self )"
                },
                {
                    "func_id": 850,
                    "func_name": "archives",
                    "func_desc": "archives",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def archives(self):\\n        \"\"\"Like :py:attr:`ARCHIVES`, except that it can return a dynamically\\n        generated list of archives to upload and unpack. Overriding\\n        this method disables :py:attr:`ARCHIVES`.\\n\\n        Paths returned by this method are relative to the working directory\\n        (not the script). Note that the job runner will *always* expand\\n        environment variables and ``~`` in paths returned by this method.\\n\\n        You do not have to worry about inadvertently disabling ``--archives``;\\n        this switch is handled separately.\\n\\n        .. versionadded:: 0.6.4\\n        \"\"\"\\n        return self._upload_attr('ARCHIVES')",
                    "func_fullName": "mrjob.job.archives( self )"
                },
                {
                    "func_id": 851,
                    "func_name": "dirs",
                    "func_desc": "dirs",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def dirs(self):\\n        \"\"\"Like :py:attr:`DIRS`, except that it can return a dynamically\\n        generated list of directories to upload. Overriding\\n        this method disables :py:attr:`DIRS`.\\n\\n        Paths returned by this method are relative to the working directory\\n        (not the script). Note that the job runner will *always* expand\\n        environment variables and ``~`` in paths returned by this method.\\n\\n        You do not have to worry about inadvertently disabling ``--dirs``;\\n        this switch is handled separately.\\n\\n        .. versionadded:: 0.6.4\\n        \"\"\"\\n        return self._upload_attr('DIRS')",
                    "func_fullName": "mrjob.job.dirs( self )"
                },
                {
                    "func_id": 852,
                    "func_name": "files",
                    "func_desc": "files",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def files(self):\\n        \"\"\"Like :py:attr:`FILES`, except that it can return a dynamically\\n        generated list of files to upload. Overriding\\n        this method disables :py:attr:`FILES`.\\n\\n        Paths returned by this method are relative to the working directory\\n        (not the script). Note that the job runner will *always* expand\\n        environment variables and ``~`` in paths returned by this method.\\n\\n        You do not have to worry about inadvertently disabling ``--files``;\\n        this switch is handled separately.\\n\\n        .. versionadded:: 0.6.4\\n        \"\"\"\\n        return self._upload_attr('FILES')",
                    "func_fullName": "mrjob.job.files( self )"
                },
                {
                    "func_id": 853,
                    "func_name": "_upload_attr",
                    "func_desc": "_upload_attr",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _upload_attr(self, attr_name):\\n        \"\"\"Helper for :py:meth:`archives`, :py:meth:`dirs`, and\\n        :py:meth:`files`\"\"\"\\n        attr_value = getattr(self, attr_name)\\n\\n        # catch path instead of a list of paths\\n        if isinstance(attr_value, string_types):\\n            raise TypeError('%s must be a list or other sequence.' % attr_name)\\n\\n        script_dir = os.path.dirname(self.mr_job_script())\\n        paths = []\\n\\n        for path in attr_value:\\n            expanded_path = expand_path(path)\\n\\n            if os.path.isabs(expanded_path):\\n                paths.append(path)\\n            else:\\n                # relative subdirs are confusing; people will expect them\\n                # to appear in a subdir, not the same directory as the script,\\n                # but Hadoop doesn't work that way\\n                if os.sep in path.rstrip(os.sep) and '#' not in path:\\n                    log.warning(\\n                        '%s: %s will appear in same directory as job script,'\\n                        ' not a subdirectory' % (attr_name, path))\\n\\n                paths.append(os.path.join(script_dir, path))\\n\\n        return paths",
                    "func_fullName": "mrjob.job._upload_attr( self, attr_name )"
                },
                {
                    "func_id": 854,
                    "func_name": "jobconf",
                    "func_desc": "jobconf",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def jobconf(self):\\n        \"\"\"``-D`` args to pass to hadoop streaming. This should be a map\\n        from property name to value. By default, returns :py:attr:`JOBCONF`.\\n\\n        .. versionchanged:: 0.6.6\\n\\n           re-defining longer clobbers command-line\\n           ``--jobconf`` options.\\n        \"\"\"\\n        return dict(self.JOBCONF)",
                    "func_fullName": "mrjob.job.jobconf( self )"
                },
                {
                    "func_id": 856,
                    "func_name": "sandbox",
                    "func_desc": "sandbox",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def sandbox(self, stdin=None, stdout=None, stderr=None):\\n        \"\"\"Redirect stdin, stdout, and stderr for automated testing.\\n\\n        You can set stdin, stdout, and stderr to file objects. By\\n        default, they'll be set to empty ``BytesIO`` objects.\\n        You can then access the job's file handles through ``self.stdin``,\\n        ``self.stdout``, and ``self.stderr``. See :ref:`testing` for more\\n        information about testing.\\n\\n        You may call sandbox multiple times (this will essentially clear\\n        the file handles).\\n\\n        ``stdin`` is empty by default. You can set it to anything that yields\\n        lines::\\n\\n            mr_job.sandbox(stdin=BytesIO(b'some_data\\\\n'))\\n\\n        or, equivalently::\\n\\n            mr_job.sandbox(stdin=[b'some_data\\\\n'])\\n\\n        For convenience, this sandbox() returns self, so you can do::\\n\\n            mr_job = MRJobClassToTest().sandbox()\\n\\n        Simple testing example::\\n\\n            mr_job = MRYourJob.sandbox()\\n            self.assertEqual(list(mr_job.reducer('foo', ['a', 'b'])), [...])\\n\\n        More complex testing example::\\n\\n            from BytesIO import BytesIO\\n\\n            from mrjob.parse import parse_mr_job_stderr\\n            from mrjob.protocol import JSONProtocol\\n\\n            mr_job = MRYourJob(args=[...])\\n\\n            fake_input = '\"foo\"\\\\t\"bar\"\\\\n\"foo\"\\\\t\"baz\"\\\\n'\\n            mr_job.sandbox(stdin=BytesIO(fake_input))\\n\\n            mr_job.run_reducer(link_num=0)\\n\\n            self.assertEqual(mrjob.stdout.getvalue(), ...)\\n            self.assertEqual(parse_mr_job_stderr(mr_job.stderr), ...)\\n\\n        .. note::\\n\\n           If you are using Spark, it's recommended you only pass in\\n           :py:class:`io.BytesIO` or other serializable alternatives to file\\n           objects. *stdin*, *stdout*, and *stderr* get stored as job\\n           attributes, which means if they aren't serializable, neither\\n           is the job instance or its methods.\\n        \"\"\"\\n        self._stdin = stdin or BytesIO()\\n        self._stdout = stdout or BytesIO()\\n        self._stderr = stderr or BytesIO()\\n\\n        return self",
                    "func_fullName": "mrjob.job.sandbox( self, stdin, stdout, stderr )"
                },
                {
                    "func_id": 857,
                    "func_name": "read_lines",
                    "func_desc": "read_lines",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def read_lines():\\n            for line in self._read_input():\\n                key, value = read(line.rstrip(b'\\r\\n'))\\n                yield key, value",
                    "func_fullName": "mrjob.job.read_lines(  )"
                },
                {
                    "func_id": 858,
                    "func_name": "write_line",
                    "func_desc": "write_line",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def write_line(key, value):\\n            self.stdout.write(write(key, value))\\n            self.stdout.write(b'\\n')",
                    "func_fullName": "mrjob.job.write_line( key, value )"
                },
                {
                    "func_id": 859,
                    "func_name": "error",
                    "func_desc": "error",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "            def error(msg):\\n                raise ValueError(msg)",
                    "func_fullName": "mrjob.job.error( msg )"
                }
            ]
        },
        {
            "cluster_id": 39,
            "feature_id": 28,
            "feature_desc": "gamma=0.0135; k=2; a=0.25; combined=0.438; stability(ARI)=1.000; sep=0.054",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 368,
                    "func_name": "_group_records_for_split",
                    "func_desc": "_group_records_for_split",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _group_records_for_split(record_gen, split_size, reducer_key=None):\\n    \"\"\"Helper for _split_records().\"\"\"\\n    split_num = 0\\n    bytes_in_split = 0\\n\\n    last_key_value = None\\n\\n    for record in record_gen:\\n        same_key = False\\n\\n        if reducer_key:\\n            key_value = reducer_key(record)\\n            same_key = (key_value == last_key_value)\\n            last_key_value = key_value\\n\\n        if bytes_in_split >= split_size and not same_key:\\n            split_num += 1\\n            bytes_in_split = 0\\n\\n        yield split_num, record\\n        bytes_in_split += len(record)",
                    "func_fullName": "mrjob.sim._group_records_for_split( record_gen, split_size, reducer_key )"
                },
                {
                    "func_id": 369,
                    "func_name": "_run_mapper_and_combiner",
                    "func_desc": "_run_mapper_and_combiner",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _run_mapper_and_combiner(\\n        run_mapper, sort_input, run_combiner,\\n        mapper_input_path, mapper_output_path, combiner_input_path):\\n    \"\"\"Helper for :py:meth:`SimMRJobRunner._run_mapper_and_combiner_func`.\"\"\"\\n    # we don't need *combiner_output_path* because *run_combiner* already\\n    # knows it\\n\\n    if run_mapper:\\n        run_mapper()\\n    else:\\n        _symlink_or_copy(mapper_input_path, mapper_output_path)\\n\\n    if run_combiner:\\n        sort_input([mapper_output_path], combiner_input_path)\\n        run_combiner()",
                    "func_fullName": "mrjob.sim._run_mapper_and_combiner( run_mapper, sort_input, run_combiner, mapper_input_path, mapper_output_path, combiner_input_path )"
                },
                {
                    "func_id": 371,
                    "func_name": "_sort_lines_in_memory",
                    "func_desc": "_sort_lines_in_memory",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _sort_lines_in_memory(input_paths, output_path, sort_values=False):\\n    \"\"\"Sort lines from *input_paths* and output them into *output_path*.\\n\\n    If *sort_values* is true, sort by the entire line; otherwise just sort\\n    by everything up to the first tab.\\n    \"\"\"\\n    log.debug('sorting in memory: %s -> %s' %\\n              (', '.join(input_paths), output_path))\\n    lines = []\\n\\n    for input_path in input_paths:\\n        with open(input_path, 'rb') as input:\\n            lines.extend(input)\\n\\n    if sort_values:\\n        lines.sort()\\n    else:\\n        lines.sort(key=lambda line: line.split(b'\\t')[0])\\n\\n    with open(output_path, 'wb') as output:\\n        for line in lines:\\n            output.write(line)",
                    "func_fullName": "mrjob.sim._sort_lines_in_memory( input_paths, output_path, sort_values )"
                },
                {
                    "func_id": 372,
                    "func_name": "_split_records",
                    "func_desc": "_split_records",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _split_records(record_gen, split_size, reducer_key=None):\\n    \"\"\"Given a stream of records (bytestrings, usually lines), yield groups of\\n    records (as generators such that the total number of bytes in each group\\n    only barely exceeds *split_size*, and, if *reducer_key* is set, consecutive\\n    records with the same key will be in the same split.\"\"\"\\n    grouped_record_gen = _group_records_for_split(\\n        record_gen, split_size, reducer_key)\\n\\n    num_records = 0\\n    for group_id, grouped_records in itertools.groupby(\\n            grouped_record_gen, key=lambda gr: gr[0]):\\n        yield (record for _, record in grouped_records)\\n        num_records += 1\\n\\n    if num_records == 0:\\n        # special case for empty files\\n        yield ()",
                    "func_fullName": "mrjob.sim._split_records( record_gen, split_size, reducer_key )"
                },
                {
                    "func_id": 375,
                    "func_name": "_opt_combiners",
                    "func_desc": "_opt_combiners",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _opt_combiners(self):\\n        \"\"\"Combine *cmdenv* with :py:func:`~mrjob.conf.combine_local_envs`\"\"\"\\n        return combine_dicts(\\n            super(SimMRJobRunner, self)._opt_combiners(),\\n            dict(cmdenv=combine_local_envs),\\n        )",
                    "func_fullName": "mrjob.sim._opt_combiners( self )"
                },
                {
                    "func_id": 384,
                    "func_name": "_run_mappers_and_combiners",
                    "func_desc": "_run_mappers_and_combiners",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run_mappers_and_combiners(self, step_num, map_splits):\\n        try:\\n            self._run_multiple(\\n                self._run_mapper_and_combiner_func(\\n                    step_num, task_num, map_split)\\n                for task_num, map_split in enumerate(map_splits)\\n            )\\n        finally:\\n            self._parse_task_counters('mapper', step_num)\\n            self._parse_task_counters('combiner', step_num)",
                    "func_fullName": "mrjob.sim._run_mappers_and_combiners( self, step_num, map_splits )"
                },
                {
                    "func_id": 387,
                    "func_name": "get_hadoop_version",
                    "func_desc": "get_hadoop_version",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_hadoop_version(self):\\n        return self._opts['hadoop_version']",
                    "func_fullName": "mrjob.sim.get_hadoop_version( self )"
                },
                {
                    "func_id": 389,
                    "func_name": "_run_mapper_and_combiner_func",
                    "func_desc": "_run_mapper_and_combiner_func",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run_mapper_and_combiner_func(self, step_num, task_num, map_split):\\n        \"\"\"Returns a no-args function that runs one mapper, plus the\\n        corresponding combiner if there is one.\"\"\"\\n        step = self._get_step(step_num)\\n\\n        mapper_input_path = self._task_input_path(\\n            'mapper', step_num, task_num)\\n        mapper_output_path = self._task_output_path(\\n            'mapper', step_num, task_num)\\n\\n        run_mapper = None\\n\\n        if 'mapper' in step:\\n            run_mapper = self._run_task_func(\\n                'mapper', step_num, task_num, map_split)\\n\\n        sort_input = self._sort_input_func()\\n\\n        combiner_input_path = None\\n        run_combiner = None\\n        # don't need combiner_output_path; *run_combiner* already knows it\\n\\n        if 'combiner' in step:\\n            # create combiner dir\\n            self.fs.mkdir(self._task_dir('combiner', step_num, task_num))\\n\\n            combiner_input_path = self._task_input_path(\\n                'combiner', step_num, task_num)\\n            run_combiner = self._run_task_func(\\n                'combiner', step_num, task_num, map_split)\\n\\n        return partial(\\n            _run_mapper_and_combiner,\\n            run_mapper, sort_input, run_combiner,\\n            mapper_input_path, mapper_output_path, combiner_input_path)",
                    "func_fullName": "mrjob.sim._run_mapper_and_combiner_func( self, step_num, task_num, map_split )"
                },
                {
                    "func_id": 390,
                    "func_name": "_run_reducers",
                    "func_desc": "_run_reducers",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run_reducers(self, step_num, num_reducer_tasks):\\n        try:\\n            self._run_multiple(\\n                self._run_task_func('reducer', step_num, task_num)\\n                for task_num in range(num_reducer_tasks)\\n            )\\n        finally:\\n            self._parse_task_counters('reducer', step_num)",
                    "func_fullName": "mrjob.sim._run_reducers( self, step_num, num_reducer_tasks )"
                },
                {
                    "func_id": 395,
                    "func_name": "_num_mappers",
                    "func_desc": "_num_mappers",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _num_mappers(self, step_num):\\n        # TODO: look up mapred.job.maps (convert to int) in _jobconf_for_step()\\n        return self._num_cores()",
                    "func_fullName": "mrjob.sim._num_mappers( self, step_num )"
                },
                {
                    "func_id": 396,
                    "func_name": "_num_reducers",
                    "func_desc": "_num_reducers",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _num_reducers(self, step_num):\\n        # TODO: look up mapred.job.reduces in _jobconf_for_step()\\n        return self._num_cores()",
                    "func_fullName": "mrjob.sim._num_reducers( self, step_num )"
                },
                {
                    "func_id": 397,
                    "func_name": "_split_mapper_input",
                    "func_desc": "_split_mapper_input",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _split_mapper_input(self, input_paths, step_num):\\n        \"\"\"Take one or more input paths (which may be compressed) and split\\n        it to create the input files for the map tasks.\\n\\n        Yields \"splits\", which are dictionaries with the following keys:\\n\\n        input: path of input for one mapper\\n        file: path of original file\\n        start, length: chunk of original file in *input*\\n\\n        Uncompressed files will not be split (even ``.bz2`` files);\\n        uncompressed files will be split as to to attempt to create\\n        twice as many input files as there are mappers.\\n        \"\"\"\\n        input_paths = list(input_paths)\\n        manifest = (step_num == 0 and self._uses_input_manifest())\\n\\n        # determine split size\\n        if manifest:\\n            split_size = 1  # one line per mapper\\n        else:\\n            split_size = self._pick_mapper_split_size(input_paths, step_num)\\n\\n        # yield output fileobjs as needed\\n        split_fileobj_gen = self._yield_split_fileobjs('mapper', step_num)\\n\\n        results = []\\n\\n        for path in input_paths:\\n            with open(path, 'rb') as src:\\n                if is_compressed(path):\\n                    if manifest:\\n                        raise Exception('input manifest %s should not be'\\n                                        ' compressed!' % path)\\n\\n                    # if file is compressed, uncompress it into a single split\\n\\n                    # Hadoop tracks the compressed file's size\\n                    size = os.stat(path)[stat.ST_SIZE]\\n\\n                    with next(split_fileobj_gen) as dest:\\n                        for chunk in decompress(src, path):\\n                            dest.write(chunk)\\n\\n                    results.append(dict(\\n                        file=path,\\n                        start=0,\\n                        length=size,\\n                    ))\\n                else:\\n                    # otherwise, split into one or more input files\\n                    start = 0\\n                    length = 0\\n\\n                    for lines in _split_records(src, split_size):\\n                        with next(split_fileobj_gen) as dest:\\n                            for line in lines:\\n                                # simulate NLinesInputFormat by prefixing\\n                                # each line with byte number\\n                                if manifest:\\n                                    i = start + length\\n                                    dest.write(('%d\\t' % i).encode('ascii'))\\n                                dest.write(line)\\n                                length += len(line)\\n\\n                        results.append(dict(\\n                            file=path,\\n                            start=start,\\n                            length=length,\\n                        ))\\n\\n                        start += length\\n                        length = 0\\n\\n        return results",
                    "func_fullName": "mrjob.sim._split_mapper_input( self, input_paths, step_num )"
                },
                {
                    "func_id": 398,
                    "func_name": "_pick_mapper_split_size",
                    "func_desc": "_pick_mapper_split_size",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _pick_mapper_split_size(self, input_paths, step_num):\\n        if not isinstance(input_paths, list):\\n            raise TypeError\\n\\n        target_num_splits = self._num_mappers(step_num) * 2\\n\\n        # decide on a split size to approximate target_num_splits\\n        num_compressed = 0\\n        uncompressed_bytes = 0\\n\\n        for path in input_paths:\\n            if path.endswith('.gz') or path.endswith('.bz'):\\n                num_compressed += 1\\n            else:\\n                uncompressed_bytes += os.stat(path)[stat.ST_SIZE]\\n\\n        return uncompressed_bytes // max(\\n            target_num_splits - num_compressed, 1)",
                    "func_fullName": "mrjob.sim._pick_mapper_split_size( self, input_paths, step_num )"
                },
                {
                    "func_id": 399,
                    "func_name": "_split_reducer_input",
                    "func_desc": "_split_reducer_input",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _split_reducer_input(self, step_num):\\n        \"\"\"Split a single, uncompressed file containing sorted input for the\\n        reducer into input files for each reducer task.\\n\\n        Yield the paths of the reducer input files.\"\"\"\\n        path = self._sorted_reducer_input_path(step_num)\\n\\n        log.debug('splitting reducer input: %s' % path)\\n\\n        size = os.stat(path)[stat.ST_SIZE]\\n        split_size = size // (self._num_reducers(step_num) * 2)\\n\\n        # yield output fileobjs as needed\\n        split_fileobj_gen = self._yield_split_fileobjs('reducer', step_num)\\n\\n        def reducer_key(line):\\n            return line.split(b'\\t')[0]\\n\\n        num_reducer_tasks = 0\\n\\n        with open(path, 'rb') as src:\\n            for records in _split_records(src, split_size, reducer_key):\\n                with next(split_fileobj_gen) as dest:\\n                    for record in records:\\n                        dest.write(record)\\n                    num_reducer_tasks += 1\\n\\n        return num_reducer_tasks",
                    "func_fullName": "mrjob.sim._split_reducer_input( self, step_num )"
                },
                {
                    "func_id": 400,
                    "func_name": "_yield_split_fileobjs",
                    "func_desc": "_yield_split_fileobjs",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _yield_split_fileobjs(self, task_type, step_num):\\n        \"\"\"Used to split input for the given mapper/reducer.\\n\\n        Yields writeable fileobjs for input splits (check their *name*\\n        attribute to get the path)\\n        \"\"\"\\n        for task_num in itertools.count():\\n            path = self._task_input_path(task_type, step_num, task_num)\\n            self.fs.mkdir(dirname(path))\\n            yield open(path, 'wb')",
                    "func_fullName": "mrjob.sim._yield_split_fileobjs( self, task_type, step_num )"
                },
                {
                    "func_id": 415,
                    "func_name": "_sorted_reducer_input_path",
                    "func_desc": "_sorted_reducer_input_path",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _sorted_reducer_input_path(self, step_num):\\n        return join(self._step_dir(step_num), 'reducer', 'sorted-input')",
                    "func_fullName": "mrjob.sim._sorted_reducer_input_path( self, step_num )"
                },
                {
                    "func_id": 416,
                    "func_name": "_sort_input_func",
                    "func_desc": "_sort_input_func",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _sort_input_func(self):\\n        \"\"\"Returns a function that sorts lines from one or more input paths\\n        into a new file. Takes the arguments *input_path* and *output_path*.\\n\\n        By default, sorts in memory, but you can override this to\\n        use the :command:`sort` binary, etc.\\n        \"\"\"\\n        return partial(_sort_lines_in_memory, sort_values=self._sort_values)",
                    "func_fullName": "mrjob.sim._sort_input_func( self )"
                },
                {
                    "func_id": 417,
                    "func_name": "_sort_reducer_input",
                    "func_desc": "_sort_reducer_input",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _sort_reducer_input(self, step_num, num_map_tasks):\\n        step = self._get_step(step_num)\\n\\n        output_path = self._sorted_reducer_input_path(step_num)\\n        self.fs.mkdir(dirname(output_path))\\n\\n        prev_task_type = 'combiner' if step.get('combiner') else 'mapper'\\n        input_paths = [\\n            self._task_output_path(prev_task_type, step_num, task_num)\\n            for task_num in range(num_map_tasks)\\n        ]\\n\\n        self._sort_input_func()(input_paths, output_path)",
                    "func_fullName": "mrjob.sim._sort_reducer_input( self, step_num, num_map_tasks )"
                },
                {
                    "func_id": 420,
                    "func_name": "reducer_key",
                    "func_desc": "reducer_key",
                    "func_file": "sim",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def reducer_key(line):\\n            return line.split(b'\\t')[0]",
                    "func_fullName": "mrjob.sim.reducer_key( line )"
                },
                {
                    "func_id": 789,
                    "func_name": "mapper",
                    "func_desc": "mapper",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper(self, key, value):\\n        \"\"\"Re-define this to define the mapper for a one-step job.\\n\\n        Yields zero or more tuples of ``(out_key, out_value)``.\\n\\n        :param key: A value parsed from input.\\n        :param value: A value parsed from input.\\n\\n        If you don't re-define this, your job will have a mapper that simply\\n        yields ``(key, value)`` as-is.\\n\\n        By default (if you don't mess with :ref:`job-protocols`):\\n         - ``key`` will be ``None``\\n         - ``value`` will be the raw input line, with newline stripped.\\n         - ``out_key`` and ``out_value`` must be JSON-encodable: numeric,\\n           unicode, boolean, ``None``, list, or dict whose keys are unicodes.\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.job.mapper( self, key, value )"
                },
                {
                    "func_id": 790,
                    "func_name": "reducer",
                    "func_desc": "reducer",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def reducer(self, key, values):\\n        \"\"\"Re-define this to define the reducer for a one-step job.\\n\\n        Yields one or more tuples of ``(out_key, out_value)``\\n\\n        :param key: A key which was yielded by the mapper\\n        :param value: A generator which yields all values yielded by the\\n                      mapper which correspond to ``key``.\\n\\n        By default (if you don't mess with :ref:`job-protocols`):\\n         - ``out_key`` and ``out_value`` must be JSON-encodable.\\n         - ``key`` and ``value`` will have been decoded from JSON (so tuples\\n           will become lists).\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.job.reducer( self, key, values )"
                },
                {
                    "func_id": 791,
                    "func_name": "combiner",
                    "func_desc": "combiner",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def combiner(self, key, values):\\n        \"\"\"Re-define this to define the combiner for a one-step job.\\n\\n        Yields one or more tuples of ``(out_key, out_value)``\\n\\n        :param key: A key which was yielded by the mapper\\n        :param value: A generator which yields all values yielded by one mapper\\n                      task/node which correspond to ``key``.\\n\\n        By default (if you don't mess with :ref:`job-protocols`):\\n         - ``out_key`` and ``out_value`` must be JSON-encodable.\\n         - ``key`` and ``value`` will have been decoded from JSON (so tuples\\n           will become lists).\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.job.combiner( self, key, values )"
                },
                {
                    "func_id": 792,
                    "func_name": "mapper_init",
                    "func_desc": "mapper_init",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper_init(self):\\n        \"\"\"Re-define this to define an action to run before the mapper\\n        processes any input.\\n\\n        One use for this function is to initialize mapper-specific helper\\n        structures.\\n\\n        Yields one or more tuples of ``(out_key, out_value)``.\\n\\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.job.mapper_init( self )"
                },
                {
                    "func_id": 793,
                    "func_name": "mapper_final",
                    "func_desc": "mapper_final",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper_final(self):\\n        \"\"\"Re-define this to define an action to run after the mapper reaches\\n        the end of input.\\n\\n        One way to use this is to store a total in an instance variable, and\\n        output it after reading all input data. See :py:mod:`mrjob.examples`\\n        for an example.\\n\\n        Yields one or more tuples of ``(out_key, out_value)``.\\n\\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.job.mapper_final( self )"
                },
                {
                    "func_id": 794,
                    "func_name": "mapper_cmd",
                    "func_desc": "mapper_cmd",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper_cmd(self):\\n        \"\"\"Re-define this to define the mapper for a one-step job **as a shell\\n        command.** If you define your mapper this way, the command will be\\n        passed unchanged to Hadoop Streaming, with some minor exceptions. For\\n        important specifics, see :ref:`cmd-steps`.\\n\\n        Basic example::\\n\\n            def mapper_cmd(self):\\n                return 'cat'\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.job.mapper_cmd( self )"
                },
                {
                    "func_id": 795,
                    "func_name": "mapper_pre_filter",
                    "func_desc": "mapper_pre_filter",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper_pre_filter(self):\\n        \"\"\"Re-define this to specify a shell command to filter the mapper's\\n        input before it gets to your job's mapper in a one-step job. For\\n        important specifics, see :ref:`cmd-filters`.\\n\\n        Basic example::\\n\\n            def mapper_pre_filter(self):\\n                return 'grep \"ponies\"'\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.job.mapper_pre_filter( self )"
                },
                {
                    "func_id": 796,
                    "func_name": "mapper_raw",
                    "func_desc": "mapper_raw",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper_raw(self, input_path, input_uri):\\n        \"\"\"Re-define this to make Hadoop pass one input file to each\\n        mapper.\\n\\n        :param input_path: a local path that the input file has been copied to\\n        :param input_uri: the URI of the input file on HDFS, S3, etc\\n\\n        .. versionadded:: 0.6.3\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.job.mapper_raw( self, input_path, input_uri )"
                },
                {
                    "func_id": 797,
                    "func_name": "reducer_init",
                    "func_desc": "reducer_init",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def reducer_init(self):\\n        \"\"\"Re-define this to define an action to run before the reducer\\n        processes any input.\\n\\n        One use for this function is to initialize reducer-specific helper\\n        structures.\\n\\n        Yields one or more tuples of ``(out_key, out_value)``.\\n\\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.job.reducer_init( self )"
                },
                {
                    "func_id": 798,
                    "func_name": "reducer_final",
                    "func_desc": "reducer_final",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def reducer_final(self):\\n        \"\"\"Re-define this to define an action to run after the reducer reaches\\n        the end of input.\\n\\n        Yields one or more tuples of ``(out_key, out_value)``.\\n\\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.job.reducer_final( self )"
                },
                {
                    "func_id": 799,
                    "func_name": "reducer_cmd",
                    "func_desc": "reducer_cmd",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def reducer_cmd(self):\\n        \"\"\"Re-define this to define the reducer for a one-step job **as a shell\\n        command.** If you define your mapper this way, the command will be\\n        passed unchanged to Hadoop Streaming, with some minor exceptions. For\\n        specifics, see :ref:`cmd-steps`.\\n\\n        Basic example::\\n\\n            def reducer_cmd(self):\\n                return 'cat'\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.job.reducer_cmd( self )"
                },
                {
                    "func_id": 800,
                    "func_name": "reducer_pre_filter",
                    "func_desc": "reducer_pre_filter",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def reducer_pre_filter(self):\\n        \"\"\"Re-define this to specify a shell command to filter the reducer's\\n        input before it gets to your job's reducer in a one-step job. For\\n        important specifics, see :ref:`cmd-filters`.\\n\\n        Basic example::\\n\\n            def reducer_pre_filter(self):\\n                return 'grep \"ponies\"'\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.job.reducer_pre_filter( self )"
                },
                {
                    "func_id": 801,
                    "func_name": "combiner_init",
                    "func_desc": "combiner_init",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def combiner_init(self):\\n        \"\"\"Re-define this to define an action to run before the combiner\\n        processes any input.\\n\\n        One use for this function is to initialize combiner-specific helper\\n        structures.\\n\\n        Yields one or more tuples of ``(out_key, out_value)``.\\n\\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.job.combiner_init( self )"
                },
                {
                    "func_id": 802,
                    "func_name": "combiner_final",
                    "func_desc": "combiner_final",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def combiner_final(self):\\n        \"\"\"Re-define this to define an action to run after the combiner reaches\\n        the end of input.\\n\\n        Yields one or more tuples of ``(out_key, out_value)``.\\n\\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.job.combiner_final( self )"
                },
                {
                    "func_id": 803,
                    "func_name": "combiner_cmd",
                    "func_desc": "combiner_cmd",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def combiner_cmd(self):\\n        \"\"\"Re-define this to define the combiner for a one-step job **as a\\n        shell command.** If you define your mapper this way, the command will\\n        be passed unchanged to Hadoop Streaming, with some minor exceptions.\\n        For specifics, see :ref:`cmd-steps`.\\n\\n        Basic example::\\n\\n            def combiner_cmd(self):\\n                return 'cat'\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.job.combiner_cmd( self )"
                },
                {
                    "func_id": 804,
                    "func_name": "combiner_pre_filter",
                    "func_desc": "combiner_pre_filter",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def combiner_pre_filter(self):\\n        \"\"\"Re-define this to specify a shell command to filter the combiner's\\n        input before it gets to your job's combiner in a one-step job. For\\n        important specifics, see :ref:`cmd-filters`.\\n\\n        Basic example::\\n\\n            def combiner_pre_filter(self):\\n                return 'grep \"ponies\"'\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.job.combiner_pre_filter( self )"
                },
                {
                    "func_id": 819,
                    "func_name": "run_mapper",
                    "func_desc": "run_mapper",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def run_mapper(self, step_num=0):\\n        \"\"\"Run the mapper and final mapper action for the given step.\\n\\n        :type step_num: int\\n        :param step_num: which step to run (0-indexed)\\n\\n        Called from :py:meth:`run`. You'd probably only want to call this\\n        directly from automated tests.\\n        \"\"\"\\n        # pick input and output protocol\\n        read_lines, write_line = self._wrap_protocols(step_num, 'mapper')\\n\\n        for k, v in self.map_pairs(read_lines(), step_num=step_num):\\n            write_line(k, v)",
                    "func_fullName": "mrjob.job.run_mapper( self, step_num )"
                },
                {
                    "func_id": 820,
                    "func_name": "run_combiner",
                    "func_desc": "run_combiner",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def run_combiner(self, step_num=0):\\n        \"\"\"Run the combiner for the given step.\\n\\n        :type step_num: int\\n        :param step_num: which step to run (0-indexed)\\n\\n        If we encounter a line that can't be decoded by our input protocol,\\n        or a tuple that can't be encoded by our output protocol, we'll\\n        increment a counter rather than raising an exception. If\\n        --strict-protocols is set, then an exception is raised\\n\\n        Called from :py:meth:`run`. You'd probably only want to call this\\n        directly from automated tests.\\n        \"\"\"\\n        # pick input and output protocol\\n        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\\n\\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\\n            write_line(k, v)",
                    "func_fullName": "mrjob.job.run_combiner( self, step_num )"
                },
                {
                    "func_id": 821,
                    "func_name": "run_reducer",
                    "func_desc": "run_reducer",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def run_reducer(self, step_num=0):\\n        \"\"\"Run the reducer for the given step.\\n\\n        :type step_num: int\\n        :param step_num: which step to run (0-indexed)\\n\\n        Called from :py:meth:`run`. You'd probably only want to call this\\n        directly from automated tests.\\n        \"\"\"\\n        # pick input and output protocol\\n        read_lines, write_line = self._wrap_protocols(step_num, 'reducer')\\n\\n        for k, v in self.reduce_pairs(read_lines(), step_num=step_num):\\n            write_line(k, v)",
                    "func_fullName": "mrjob.job.run_reducer( self, step_num )"
                },
                {
                    "func_id": 822,
                    "func_name": "map_pairs",
                    "func_desc": "map_pairs",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def map_pairs(self, pairs, step_num=0):\\n        \"\"\"Runs :py:meth:`mapper_init`,\\n        :py:meth:`mapper`/:py:meth:`mapper_raw`, and :py:meth:`mapper_final`\\n        for one map task in one step.\\n\\n        Takes in a sequence of (key, value) pairs as input, and yields\\n        (key, value) pairs as output.\\n\\n        :py:meth:`run_mapper` essentially wraps this method with code to handle\\n        reading/decoding input and writing/encoding output.\\n\\n        .. versionadded:: 0.6.7\\n        \"\"\"\\n        step = self._get_step(step_num, MRStep)\\n\\n        mapper = step['mapper']\\n        mapper_raw = step['mapper_raw']\\n        mapper_init = step['mapper_init']\\n        mapper_final = step['mapper_final']\\n\\n        if mapper_init:\\n            for k, v in mapper_init() or ():\\n                yield k, v\\n\\n        if mapper_raw:\\n            if len(self.options.args) != 2:\\n                raise ValueError('Wrong number of args')\\n            input_path, input_uri = self.options.args\\n            for k, v in mapper_raw(input_path, input_uri) or ():\\n                yield k, v\\n        else:\\n            for key, value in pairs:\\n                for k, v in mapper(key, value) or ():\\n                    yield k, v\\n\\n        if mapper_final:\\n            for k, v in mapper_final() or ():\\n                yield k, v",
                    "func_fullName": "mrjob.job.map_pairs( self, pairs, step_num )"
                },
                {
                    "func_id": 823,
                    "func_name": "combine_pairs",
                    "func_desc": "combine_pairs",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def combine_pairs(self, pairs, step_num=0):\\n        \"\"\"Runs :py:meth:`combiner_init`,\\n        :py:meth:`combiner`, and :py:meth:`combiner_final`\\n        for one reduce task in one step.\\n\\n        Takes in a sequence of (key, value) pairs as input, and yields\\n        (key, value) pairs as output.\\n\\n        :py:meth:`run_combiner` essentially wraps this method with code to\\n        handle reading/decoding input and writing/encoding output.\\n\\n        .. versionadded:: 0.6.7\\n        \"\"\"\\n        for k, v in self._combine_or_reduce_pairs(pairs, 'combiner', step_num):\\n            yield k, v",
                    "func_fullName": "mrjob.job.combine_pairs( self, pairs, step_num )"
                },
                {
                    "func_id": 824,
                    "func_name": "reduce_pairs",
                    "func_desc": "reduce_pairs",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def reduce_pairs(self, pairs, step_num=0):\\n        \"\"\"Runs :py:meth:`reducer_init`,\\n        :py:meth:`reducer`, and :py:meth:`reducer_final`\\n        for one reduce task in one step.\\n\\n        Takes in a sequence of (key, value) pairs as input, and yields\\n        (key, value) pairs as output.\\n\\n        :py:meth:`run_reducer` essentially wraps this method with code to\\n        handle reading/decoding input and writing/encoding output.\\n\\n        .. versionadded:: 0.6.7\\n        \"\"\"\\n        for k, v in self._combine_or_reduce_pairs(pairs, 'reducer', step_num):\\n            yield k, v",
                    "func_fullName": "mrjob.job.reduce_pairs( self, pairs, step_num )"
                },
                {
                    "func_id": 825,
                    "func_name": "_combine_or_reduce_pairs",
                    "func_desc": "_combine_or_reduce_pairs",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _combine_or_reduce_pairs(self, pairs, mrc, step_num=0):\\n        \"\"\"Helper for :py:meth:`combine_pairs` and :py:meth:`reduce_pairs`.\"\"\"\\n        step = self._get_step(step_num, MRStep)\\n\\n        task = step[mrc]\\n        task_init = step[mrc + '_init']\\n        task_final = step[mrc + '_final']\\n        if task is None:\\n            raise ValueError('No %s in step %d' % (mrc, step_num))\\n\\n        if task_init:\\n            for k, v in task_init() or ():\\n                yield k, v\\n\\n        # group all values of the same key together, and pass to the reducer\\n        #\\n        # be careful to use generators for everything, to allow for\\n        # very large groupings of values\\n        for key, pairs_for_key in itertools.groupby(pairs, lambda k_v: k_v[0]):\\n            values = (value for _, value in pairs_for_key)\\n            for k, v in task(key, values) or ():\\n                yield k, v\\n\\n        if task_final:\\n            for k, v in task_final() or ():\\n                yield k, v",
                    "func_fullName": "mrjob.job._combine_or_reduce_pairs( self, pairs, mrc, step_num )"
                },
                {
                    "func_id": 833,
                    "func_name": "_mapper_output_protocol",
                    "func_desc": "_mapper_output_protocol",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _mapper_output_protocol(self, step_num, step_map):\\n        map_key = self._step_key(step_num, 'mapper')\\n        if map_key in step_map:\\n            if step_map[map_key] >= (len(step_map) - 1):\\n                return self.output_protocol()\\n            else:\\n                return self.internal_protocol()\\n        else:\\n            # mapper is not a script substep, so protocols don't apply at all\\n            return RawValueProtocol()",
                    "func_fullName": "mrjob.job._mapper_output_protocol( self, step_num, step_map )"
                },
                {
                    "func_id": 846,
                    "func_name": "hadoop_input_format",
                    "func_desc": "hadoop_input_format",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def hadoop_input_format(self):\\n        \"\"\"Optional Hadoop ``InputFormat`` class to parse input for\\n        the first step of the job.\\n\\n        Normally, setting :py:attr:`HADOOP_INPUT_FORMAT` is sufficient;\\n        redefining this method is only for when you want to get fancy.\\n        \"\"\"\\n        return self.HADOOP_INPUT_FORMAT",
                    "func_fullName": "mrjob.job.hadoop_input_format( self )"
                },
                {
                    "func_id": 847,
                    "func_name": "hadoop_output_format",
                    "func_desc": "hadoop_output_format",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def hadoop_output_format(self):\\n        \"\"\"Optional Hadoop ``OutputFormat`` class to write output for\\n        the last step of the job.\\n\\n        Normally, setting :py:attr:`HADOOP_OUTPUT_FORMAT` is sufficient;\\n        redefining this method is only for when you want to get fancy.\\n        \"\"\"\\n        return self.HADOOP_OUTPUT_FORMAT",
                    "func_fullName": "mrjob.job.hadoop_output_format( self )"
                },
                {
                    "func_id": 849,
                    "func_name": "partitioner",
                    "func_desc": "partitioner",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def partitioner(self):\\n        \"\"\"Optional Hadoop partitioner class to use to determine how mapper\\n        output should be sorted and distributed to reducers.\\n\\n        By default, returns :py:attr:`PARTITIONER`.\\n\\n        You probably don't need to re-define this; it's just here for\\n        completeness.\\n        \"\"\"\\n        return self.PARTITIONER",
                    "func_fullName": "mrjob.job.partitioner( self )"
                },
                {
                    "func_id": 855,
                    "func_name": "sort_values",
                    "func_desc": "sort_values",
                    "func_file": "job",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def sort_values(self):\\n        \"\"\"A method that by default, just returns the value of\\n        :py:attr:`SORT_VALUES`. Mostly exists for the sake\\n        of consistency, but you could override it if you wanted to make\\n        secondary sort configurable.\"\"\"\\n        return self.SORT_VALUES",
                    "func_fullName": "mrjob.job.sort_values( self )"
                }
            ]
        },
        {
            "cluster_id": 13,
            "feature_id": 29,
            "feature_desc": "gamma=0.0100; k=1; a=0.25; combined=0.764; stability(ARI)=1.000; sep=0.108",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 429,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "inline",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, mrjob_cls=None, **kwargs):\\n        \"\"\":py:class:`~mrjob.inline.InlineMRJobRunner` takes the same keyword\\n        args as :py:class:`~mrjob.runner.MRJobRunner`. However, please note\\n        that\\n        *hadoop_input_format*, *hadoop_output_format*, and *partitioner*\\n        are ignored\\n        because they require Java. If you need to test these, consider\\n        starting up a standalone Hadoop instance and running your job with\\n        ``-r hadoop``.\"\"\"\\n        super(InlineMRJobRunner, self).__init__(**kwargs)\\n        # if we run python -m mrjob.job, mrjob_cls is __main__.MRJob\\n        # which is identical to (but not a subclass of) mrjob.job.MRJob\\n        #\\n        # the base MRJob still isn't runnable, but this yields a more\\n        # useful error about the step having no mappers or reducers\\n        if not (mrjob_cls is None or issubclass(mrjob_cls, MRJob) or\\n                mrjob_cls.__module__ == '__main__'):\\n            raise TypeError\\n\\n        self._mrjob_cls = mrjob_cls\\n\\n        # used to explain exceptions\\n        self._error_while_reading_from = None\\n\\n        if self._opts['py_files']:\\n            log.warning(\"inline runner doesn't import py_files\")\\n\\n        if self._opts['setup']:\\n            log.warning(\"inline runner can't run setup commands\")",
                    "func_fullName": "mrjob.inline.__init__( self, mrjob_cls, **kwargs )"
                },
                {
                    "func_id": 430,
                    "func_name": "_check_step",
                    "func_desc": "_check_step",
                    "func_file": "inline",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _check_step(self, step, step_num):\\n        \"\"\"Don't try to run steps that include commands.\"\"\"\\n        super(InlineMRJobRunner, self)._check_step(step, step_num)\\n\\n        if step['type'] == 'streaming':\\n            for mrc in ('mapper', 'combiner', 'reducer'):\\n                if step.get(mrc):\\n                    if 'command' in step[mrc] or 'pre_filter' in step[mrc]:\\n                        raise NotImplementedError(\\n                            \"step %d's %s runs a command, but inline\"\\n                            \" runner does not support subprocesses (try\"\\n                            \" -r local)\" % (step_num, mrc))",
                    "func_fullName": "mrjob.inline._check_step( self, step, step_num )"
                },
                {
                    "func_id": 431,
                    "func_name": "_invoke_task_func",
                    "func_desc": "_invoke_task_func",
                    "func_file": "inline",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _invoke_task_func(self, task_type, step_num, task_num):\\n        \"\"\"Just run tasks in the same process.\"\"\"\\n        manifest = (step_num == 0 and task_type == 'mapper' and\\n                    self._uses_input_manifest())\\n\\n        # Don't care about pickleability since this runs in the same process\\n        def invoke_task(stdin, stdout, stderr, wd, env):\\n            with save_current_environment(), save_cwd(), save_sys_path(), \\\\n                    save_sys_std():\\n                # pretend we're running the script in the working dir\\n                os.environ.update(env)\\n                os.chdir(wd)\\n                sys.path = [os.getcwd()] + sys.path\\n\\n                # pretend we've redirected stdin/stdout/stderr\\n                sys.stdin = stdin\\n                sys.stdout = stdout\\n                sys.stderr = stderr\\n\\n                input_uri = None\\n                try:\\n                    args = self._args_for_task(step_num, task_type)\\n\\n                    if manifest:\\n                        # read input path from stdin, add to args\\n                        line = stdin.readline().decode('utf_8')\\n                        input_uri = line.split('\\t')[-1].rstrip()\\n                        # input_uri is an absolute path, can serve\\n                        # as path and uri both\\n                        args = list(args) + [input_uri, input_uri]\\n\\n                    task = self._mrjob_cls(args)\\n                    task.execute()\\n                except:\\n                    # so users can figure out where the exception came from;\\n                    # see _log_cause_of_error(). we can't wrap the exception\\n                    # because then we lose the stacktrace (which is the whole\\n                    # point of the inline runner)\\n\\n                    if input_uri:  # from manifest\\n                        self._error_while_reading_from = input_uri\\n                    else:\\n                        self._error_while_reading_from = self._task_input_path(\\n                            task_type, step_num, task_num)\\n\\n                    raise\\n\\n        return invoke_task",
                    "func_fullName": "mrjob.inline._invoke_task_func( self, task_type, step_num, task_num )"
                },
                {
                    "func_id": 432,
                    "func_name": "_run_step_on_spark",
                    "func_desc": "_run_step_on_spark",
                    "func_file": "inline",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run_step_on_spark(self, step, step_num):\\n        \"\"\"Set up a fake working directory and environment, and call the Spark\\n        method.\"\"\"\\n        # this is kind of a Spark-specific mash-up of _run_streaming_step()\\n        # (in sim.py) and _invoke_task_func(), above\\n\\n        # don't create the output dir for the step; that's Spark's job\\n\\n        # breaking the Spark step down into tasks is pyspark's job, so\\n        # we just have a single dummy task\\n\\n        self.fs.mkdir(self._task_dir('spark', step_num, task_num=0))\\n        # could potentially parse this for cause of error\\n        stderr_path = self._task_stderr_path('spark', step_num, task_num=0)\\n        stdout_path = self._task_output_path('spark', step_num, task_num=0)\\n\\n        self._create_dist_cache_dir(step_num)\\n        wd = self._setup_working_dir('spark', step_num, task_num=0)\\n\\n        # use abspath() on input URIs before changing working dir\\n        task_args = self._spark_script_args(step_num)\\n\\n        with open(stdout_path, 'wb') as stdout, \\\\n                open(stderr_path, 'wb') as stderr:\\n            with save_current_environment(), save_cwd(), save_sys_path(), \\\\n                    save_sys_std():\\n                os.environ.update(_fix_env(self._cmdenv()))\\n                os.chdir(wd)\\n                sys.path = [os.getcwd()] + sys.path\\n\\n                # pretend we redirected stdout and stderr\\n                sys.stdout, sys.stderr = stdout, stderr\\n\\n                task = self._mrjob_cls(task_args)\\n                task.execute()",
                    "func_fullName": "mrjob.inline._run_step_on_spark( self, step, step_num )"
                },
                {
                    "func_id": 433,
                    "func_name": "_log_cause_of_error",
                    "func_desc": "_log_cause_of_error",
                    "func_file": "inline",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _log_cause_of_error(self, ex):\\n        \"\"\"Just tell what file we were reading from (since they'll see\\n        the stacktrace from the actual exception)\"\"\"\\n        if self._error_while_reading_from:\\n            log.error('\\nError while reading from %s:\\n' %\\n                      self._error_while_reading_from)",
                    "func_fullName": "mrjob.inline._log_cause_of_error( self, ex )"
                },
                {
                    "func_id": 434,
                    "func_name": "_spark_executors_have_own_wd",
                    "func_desc": "_spark_executors_have_own_wd",
                    "func_file": "inline",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_executors_have_own_wd(self):\\n        return True  # because we fake it",
                    "func_fullName": "mrjob.inline._spark_executors_have_own_wd( self )"
                },
                {
                    "func_id": 435,
                    "func_name": "_spark_driver_has_own_wd",
                    "func_desc": "_spark_driver_has_own_wd",
                    "func_file": "inline",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_driver_has_own_wd(self):\\n        return True  # because we fake it",
                    "func_fullName": "mrjob.inline._spark_driver_has_own_wd( self )"
                },
                {
                    "func_id": 436,
                    "func_name": "_wd_mirror",
                    "func_desc": "_wd_mirror",
                    "func_file": "inline",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _wd_mirror(self):\\n        return None  # no need for this, we set up the working dir (Spark too)",
                    "func_fullName": "mrjob.inline._wd_mirror( self )"
                },
                {
                    "func_id": 437,
                    "func_name": "invoke_task",
                    "func_desc": "invoke_task",
                    "func_file": "inline",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def invoke_task(stdin, stdout, stderr, wd, env):\\n            with save_current_environment(), save_cwd(), save_sys_path(), \\\\n                    save_sys_std():\\n                # pretend we're running the script in the working dir\\n                os.environ.update(env)\\n                os.chdir(wd)\\n                sys.path = [os.getcwd()] + sys.path\\n\\n                # pretend we've redirected stdin/stdout/stderr\\n                sys.stdin = stdin\\n                sys.stdout = stdout\\n                sys.stderr = stderr\\n\\n                input_uri = None\\n                try:\\n                    args = self._args_for_task(step_num, task_type)\\n\\n                    if manifest:\\n                        # read input path from stdin, add to args\\n                        line = stdin.readline().decode('utf_8')\\n                        input_uri = line.split('\\t')[-1].rstrip()\\n                        # input_uri is an absolute path, can serve\\n                        # as path and uri both\\n                        args = list(args) + [input_uri, input_uri]\\n\\n                    task = self._mrjob_cls(args)\\n                    task.execute()\\n                except:\\n                    # so users can figure out where the exception came from;\\n                    # see _log_cause_of_error(). we can't wrap the exception\\n                    # because then we lose the stacktrace (which is the whole\\n                    # point of the inline runner)\\n\\n                    if input_uri:  # from manifest\\n                        self._error_while_reading_from = input_uri\\n                    else:\\n                        self._error_while_reading_from = self._task_input_path(\\n                            task_type, step_num, task_num)\\n\\n                    raise",
                    "func_fullName": "mrjob.inline.invoke_task( stdin, stdout, stderr, wd, env )"
                },
                {
                    "func_id": 1007,
                    "func_name": "_to_num_bytes",
                    "func_desc": "_to_num_bytes",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _to_num_bytes(java_mem_str):\\n    if isinstance(java_mem_str, string_types):\\n        for i, magnitude in enumerate(('k', 'm', 'g', 't'), start=1):\\n            if java_mem_str.lower().endswith(magnitude):\\n                return int(java_mem_str[:-1]) * 1024 ** i\\n\\n    return int(java_mem_str)",
                    "func_fullName": "mrjob.local._to_num_bytes( java_mem_str )"
                },
                {
                    "func_id": 1008,
                    "func_name": "_invoke_task_in_subprocess",
                    "func_desc": "_invoke_task_in_subprocess",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _invoke_task_in_subprocess(\\n        task_type, step_num, task_num,\\n        args, num_steps,\\n        stdin, stdout, stderr, wd, env):\\n    \"\"\"A pickleable function that invokes a task in a subprocess.\"\"\"\\n    log.debug('> %s' % cmd_line(args))\\n\\n    try:\\n        check_call(args, stdin=stdin, stdout=stdout, stderr=stderr,\\n                   cwd=wd, env=env)\\n    except Exception as ex:\\n        raise _TaskFailedException(\\n            reason=str(ex),\\n            step_num=step_num,\\n            num_steps=num_steps,\\n            task_type=task_type,\\n            task_num=task_num,\\n        )",
                    "func_fullName": "mrjob.local._invoke_task_in_subprocess( task_type, step_num, task_num, args, num_steps, stdin, stdout, stderr, wd, env )"
                },
                {
                    "func_id": 1009,
                    "func_name": "_pickle_safe",
                    "func_desc": "_pickle_safe",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _pickle_safe(func):\\n    \"\"\"Call no-args function *func*, returning *None* and ensuring\\n    that any exception raised is pickleable.\"\"\"\\n    try:\\n        func()  # always return None\\n    except _TaskFailedException:\\n        raise  # we know these are pickleable\\n    except Exception as ex:\\n        raise Exception(repr(ex))  # we know this is pickleable",
                    "func_fullName": "mrjob.local._pickle_safe( func )"
                },
                {
                    "func_id": 1010,
                    "func_name": "_sort_lines_with_sort_bin",
                    "func_desc": "_sort_lines_with_sort_bin",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _sort_lines_with_sort_bin(input_paths, output_path, sort_bin,\\n                              sort_values=False, tmp_dir=None):\\n    \"\"\"Sort lines the given *input_paths* into *output_path*,\\n    using *sort_bin*. If there is a problem, fall back to in-memory sort.\\n\\n    This is a helper for :py:meth:`LocalMRJobRunner._sort_input_func`.\\n\\n    *tmp_dir* determines the value of :envvar:`$TMP` and :envvar:`$TMPDIR`\\n    that *sort_bin* sees.\\n    \"\"\"\\n    if input_paths:\\n        env = os.environ.copy()\\n\\n        # ignore locale when sorting\\n        env['LC_ALL'] = 'C'\\n\\n        # Make sure that the tmp dir environment variables are changed if\\n        # the default is changed.\\n        env['TMP'] = tmp_dir\\n        env['TMPDIR'] = tmp_dir\\n\\n        with open(output_path, 'wb') as output:\\n            args = sort_bin + list(input_paths)\\n            log.debug('> %s' % cmd_line(args))\\n\\n            try:\\n                check_call(args, stdout=output, env=env)\\n                return\\n            except CalledProcessError:\\n                log.error(\\n                    '`%s` failed, falling back to in-memory sort' %\\n                    cmd_line(sort_bin))\\n            except OSError:\\n                log.error(\\n                    'no sort binary, falling back to in-memory sort')\\n\\n    _sort_lines_in_memory(input_paths, output_path, sort_values=sort_values)",
                    "func_fullName": "mrjob.local._sort_lines_with_sort_bin( input_paths, output_path, sort_bin, sort_values, tmp_dir )"
                },
                {
                    "func_id": 1011,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(\\n            self, reason=None, step_num=None, num_steps=None, step_desc=None,\\n            task_type=None, task_num=None):\\n        super(_TaskFailedException, self).__init__(\\n            reason=reason, step_num=step_num,\\n            num_steps=num_steps, step_desc=step_desc)\\n\\n        self.task_type = task_type\\n        self.task_num = task_num",
                    "func_fullName": "mrjob.local.__init__( self, reason, step_num, num_steps, step_desc, task_type, task_num )"
                },
                {
                    "func_id": 1012,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, **kwargs):\\n        \"\"\"Arguments to this constructor may also appear in :file:`mrjob.conf`\\n        under ``runners/local``.\\n\\n        :py:class:`~mrjob.local.LocalMRJobRunner`'s constructor takes the\\n        same keyword args as\\n        :py:class:`~mrjob.runner.MRJobRunner`. However, please note:\\n\\n        * *cmdenv* is combined with :py:func:`~mrjob.conf.combine_local_envs`\\n        * *python_bin* defaults to ``sys.executable`` (the current python\\n          interpreter)\\n        * *hadoop_input_format*, *hadoop_output_format*,\\n          and *partitioner* are ignored because they\\n          require Java. If you need to test these, consider starting up a\\n          standalone Hadoop instance and running your job with ``-r hadoop``.\\n        \"\"\"\\n        super(LocalMRJobRunner, self).__init__(**kwargs)",
                    "func_fullName": "mrjob.local.__init__( self, **kwargs )"
                },
                {
                    "func_id": 1013,
                    "func_name": "_invoke_task_func",
                    "func_desc": "_invoke_task_func",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _invoke_task_func(self, task_type, step_num, task_num):\\n        args = self._substep_args(step_num, task_type)\\n        num_steps = self._num_steps()\\n\\n        # stdin, stdout, stderr, wd, and env will be passed in later\\n        return partial(\\n            _invoke_task_in_subprocess,\\n            task_type, step_num, task_num,\\n            args, num_steps)",
                    "func_fullName": "mrjob.local._invoke_task_func( self, task_type, step_num, task_num )"
                },
                {
                    "func_id": 1014,
                    "func_name": "_run_step_on_spark",
                    "func_desc": "_run_step_on_spark",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run_step_on_spark(self, step, step_num):\\n        if self._opts['upload_archives']:\\n            log.warning('Spark master %r will probably ignore archives' %\\n                        self._spark_master())\\n\\n        spark_submit_args = self._args_for_spark_step(step_num)\\n\\n        env = dict(os.environ)\\n        env.update(self._spark_cmdenv(step_num))\\n\\n        returncode, step_interpretation = self._run_spark_submit(\\n            spark_submit_args, env, record_callback=_log_log4j_record)\\n\\n        if returncode:\\n            error = _pick_error(dict(step=step_interpretation))\\n            if error:\\n                _log_probable_cause_of_failure(log, error)\\n\\n            reason = str(CalledProcessError(returncode, spark_submit_args))\\n            raise StepFailedException(\\n                reason=reason, step_num=step_num,\\n                num_steps=self._num_steps())",
                    "func_fullName": "mrjob.local._run_step_on_spark( self, step, step_num )"
                },
                {
                    "func_id": 1015,
                    "func_name": "_run_multiple",
                    "func_desc": "_run_multiple",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run_multiple(self, funcs, num_processes=None):\\n        \"\"\"Use multiprocessing to run in parallel.\"\"\"\\n        pool = Pool(processes=self._opts['num_cores'])\\n\\n        try:\\n            results = [\\n                pool.apply_async(partial(_pickle_safe, func))\\n                for func in funcs\\n            ]\\n\\n            for result in results:\\n                result.get()\\n\\n        # make sure that the pool (and its file descriptors, etc.)\\n        # don't stay open. This doesn't matter much for individual jobs,\\n        # but it makes our automated tasks run out of file descriptors.\\n\\n            pool.close()\\n        except:\\n            # if there's an error in one task, terminate all others\\n            pool.terminate()\\n            raise\\n        finally:\\n            pool.join()",
                    "func_fullName": "mrjob.local._run_multiple( self, funcs, num_processes )"
                },
                {
                    "func_id": 1016,
                    "func_name": "_log_cause_of_error",
                    "func_desc": "_log_cause_of_error",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _log_cause_of_error(self, ex):\\n        if not isinstance(ex, _TaskFailedException):\\n            # if something went wrong inside mrjob, the stacktrace\\n            # will bubble up to the top level\\n            return\\n\\n        # not using LogInterpretationMixin because it would be overkill\\n\\n        if not self._opts['read_logs']:\\n            return\\n\\n        input_path = self._task_input_path(\\n            ex.task_type, ex.step_num, ex.task_num)\\n        stderr_path = self._task_stderr_path(\\n            ex.task_type, ex.step_num, ex.task_num)\\n\\n        if self.fs.exists(stderr_path):  # it should, but just to be safe\\n            # log-parsing code expects \"str\", not bytes; open in text mode\\n            with open(stderr_path) as stderr:\\n                task_error = _parse_task_stderr(stderr)\\n                if task_error:\\n                    task_error['path'] = stderr_path\\n                    error = dict(\\n                        split=dict(path=input_path),\\n                        task_error=task_error)\\n                    _log_probable_cause_of_failure(log, error)\\n                    return\\n\\n        # fallback if we can't find the error (e.g. the job does something\\n        # weird to stderr or stack traces)\\n        log.error('Error while reading from %s:\\n' % input_path)",
                    "func_fullName": "mrjob.local._log_cause_of_error( self, ex )"
                },
                {
                    "func_id": 1017,
                    "func_name": "_default_python_bin",
                    "func_desc": "_default_python_bin",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _default_python_bin(self, local=False):\\n        \"\"\"Always return *sys.executable*, if defined\"\"\"\\n        return super(LocalMRJobRunner, self)._default_python_bin(\\n            local=True)",
                    "func_fullName": "mrjob.local._default_python_bin( self, local )"
                },
                {
                    "func_id": 1018,
                    "func_name": "_sort_input_func",
                    "func_desc": "_sort_input_func",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _sort_input_func(self):\\n        \"\"\"Try sorting with the :command:`sort` binary before falling\\n        back to in-memory sort.\"\"\"\\n        if platform.system() == 'Windows':  # we assume Unix sort\\n            return super(LocalMRJobRunner, self)._sort_input_func()\\n        else:\\n            return partial(\\n                _sort_lines_with_sort_bin,\\n                sort_bin=self._sort_bin(),\\n                tmp_dir=self._get_local_tmp_dir())",
                    "func_fullName": "mrjob.local._sort_input_func( self )"
                },
                {
                    "func_id": 1019,
                    "func_name": "_sort_bin",
                    "func_desc": "_sort_bin",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _sort_bin(self):\\n        \"\"\"The binary to use to sort input.\\n\\n        (On Windows, we go straight to sorting in memory.)\\n        \"\"\"\\n        if self._opts['sort_bin']:\\n            return self._opts['sort_bin']\\n        elif self._sort_values:\\n            return ['sort']\\n        else:\\n            # only sort on the reducer key (see #660)\\n            return ['sort', '-t', '\\t', '-k', '1,1', '-s']",
                    "func_fullName": "mrjob.local._sort_bin( self )"
                },
                {
                    "func_id": 1020,
                    "func_name": "_spark_master",
                    "func_desc": "_spark_master",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_master(self):\\n        \"\"\"Use the local-cluster master, which simulates a Spark cluster.\"\"\"\\n        # figure out the required parameters to local-cluster\\n        num_executors = self._num_cores()\\n\\n        # for now assigning one core per executor, so we don't have to worry\\n        # about a number of cores that's not evenly divisible\\n        cores_per_executor = 1\\n\\n        executor_mem_bytes = _to_num_bytes(\\n            self._opts['jobconf'].get('spark.executor.memory') or\\n            _DEFAULT_EXECUTOR_MEMORY)\\n        executor_mem_mb = math.ceil(executor_mem_bytes / 1024.0 / 1024.0)\\n\\n        return 'local-cluster[%d,%d,%d]' % (\\n            num_executors, cores_per_executor, executor_mem_mb)",
                    "func_fullName": "mrjob.local._spark_master( self )"
                },
                {
                    "func_id": 1710,
                    "func_name": "_from_file_uri",
                    "func_desc": "_from_file_uri",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _from_file_uri(path_or_uri):\\n    if is_uri(path_or_uri):\\n        if path_or_uri.startswith('file:///'):\\n            return path_or_uri[7:]  # keep last /\\n        else:\\n            raise ValueError('Not a file:/// URI')\\n    else:\\n        return path_or_uri",
                    "func_fullName": "mrjob.fs.local._from_file_uri( path_or_uri )"
                },
                {
                    "func_id": 1711,
                    "func_name": "can_handle_path",
                    "func_desc": "can_handle_path",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def can_handle_path(self, path):\\n        return path.startswith('file:///') or not is_uri(path)",
                    "func_fullName": "mrjob.fs.local.can_handle_path( self, path )"
                },
                {
                    "func_id": 1712,
                    "func_name": "du",
                    "func_desc": "du",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def du(self, path_glob):\\n        path_glob = _from_file_uri(path_glob)\\n        return sum(os.path.getsize(path) for path in self.ls(path_glob))",
                    "func_fullName": "mrjob.fs.local.du( self, path_glob )"
                },
                {
                    "func_id": 1713,
                    "func_name": "ls",
                    "func_desc": "ls",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def ls(self, path_glob):\\n        bare_path_glob = _from_file_uri(path_glob)\\n        uri_scheme = path_glob[0:-len(bare_path_glob)]  # 'file:///' or ''\\n\\n        for path in glob.glob(bare_path_glob):\\n            if os.path.isdir(path):\\n                for dirname, _, filenames in os.walk(path, followlinks=True):\\n                    for filename in filenames:\\n                        yield uri_scheme + os.path.join(dirname, filename)\\n            else:\\n                yield uri_scheme + path",
                    "func_fullName": "mrjob.fs.local.ls( self, path_glob )"
                },
                {
                    "func_id": 1714,
                    "func_name": "_cat_file",
                    "func_desc": "_cat_file",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cat_file(self, path):\\n        from mrjob.cat import decompress\\n        path = _from_file_uri(path)\\n        with open(path, 'rb') as f:\\n            for chunk in decompress(f, path):\\n                yield chunk",
                    "func_fullName": "mrjob.fs.local._cat_file( self, path )"
                },
                {
                    "func_id": 1715,
                    "func_name": "exists",
                    "func_desc": "exists",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def exists(self, path_glob):\\n        path_glob = _from_file_uri(path_glob)\\n        return bool(glob.glob(path_glob))",
                    "func_fullName": "mrjob.fs.local.exists( self, path_glob )"
                },
                {
                    "func_id": 1716,
                    "func_name": "mkdir",
                    "func_desc": "mkdir",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mkdir(self, path):\\n        path = _from_file_uri(path)\\n        if not os.path.isdir(path):\\n            os.makedirs(path)",
                    "func_fullName": "mrjob.fs.local.mkdir( self, path )"
                },
                {
                    "func_id": 1717,
                    "func_name": "put",
                    "func_desc": "put",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def put(self, src, path):\\n        \"\"\"Copy a file from *src* to *path*\"\"\"\\n        path = _from_file_uri(path)\\n        shutil.copyfile(src, path)",
                    "func_fullName": "mrjob.fs.local.put( self, src, path )"
                },
                {
                    "func_id": 1718,
                    "func_name": "rm",
                    "func_desc": "rm",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def rm(self, path_glob):\\n        path_glob = _from_file_uri(path_glob)\\n        for path in glob.glob(path_glob):\\n            if os.path.isdir(path):\\n                log.debug('Recursively deleting %s' % path)\\n                shutil.rmtree(path)\\n            else:\\n                log.debug('Deleting %s' % path)\\n                os.remove(path)",
                    "func_fullName": "mrjob.fs.local.rm( self, path_glob )"
                },
                {
                    "func_id": 1719,
                    "func_name": "touchz",
                    "func_desc": "touchz",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def touchz(self, path):\\n        path = _from_file_uri(path)\\n        if os.path.isfile(path) and os.path.getsize(path) != 0:\\n            raise OSError('Non-empty file %r already exists!' % (path,))\\n\\n        # zero out the file\\n        with open(path, 'w'):\\n            pass",
                    "func_fullName": "mrjob.fs.local.touchz( self, path )"
                },
                {
                    "func_id": 1720,
                    "func_name": "_md5sum_file",
                    "func_desc": "_md5sum_file",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _md5sum_file(self, fileobj, block_size=(512 ** 2)):  # 256K default\\n        md5 = hashlib.md5()\\n        while True:\\n            data = fileobj.read(block_size)\\n            if not data:\\n                break\\n            md5.update(data)\\n        return md5.hexdigest()",
                    "func_fullName": "mrjob.fs.local._md5sum_file( self, fileobj, block_size )"
                },
                {
                    "func_id": 1721,
                    "func_name": "md5sum",
                    "func_desc": "md5sum",
                    "func_file": "local",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def md5sum(self, path):\\n        path = _from_file_uri(path)\\n        with open(path, 'rb') as f:\\n            return self._md5sum_file(f)",
                    "func_fullName": "mrjob.fs.local.md5sum( self, path )"
                }
            ]
        },
        {
            "cluster_id": 21,
            "feature_id": 30,
            "feature_desc": "gamma=0.0498; k=2; a=0.25; combined=0.445; stability(ARI)=1.000; sep=0.078",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 459,
                    "func_name": "_patch_params",
                    "func_desc": "_patch_params",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _patch_params(params, name, value):\\n    \"\"\"Helper method for _add_extra_cluster_params().\\n\\n    Set *name* in *params* to *value*\\n\\n    If *name* has one or more dots in it, recursively set the value\\n    in successive nested dictionaries, creating them if necessary.\\n    For example, if *name* is ``Instances.EmrManagedMasterSecurityGroup``,\\n    set ``params['Instances']['EmrManagedMasterSecurityGroup']``\\n\\n    If *value* is ``None``, delete the value (if it exists), rather than\\n    setting it to ``None``.\\n    \"\"\"\\n    if not isinstance(params, dict):\\n        raise TypeError('must be a dictionary')\\n\\n    if '.' in name:\\n        head, rest = name.split('.', 1)\\n        _patch_params(params.setdefault(head, {}), rest, value)\\n    elif value is None:\\n        if name in params:\\n            del params[name]\\n    elif isinstance(value, dict) and isinstance(params.get(name), dict):\\n        # recursively patch dicts rather than clobbering them (see #2154)\\n        for k, v in value.items():\\n            _patch_params(params[name], k, v)\\n    else:\\n        params[name] = value",
                    "func_fullName": "mrjob.cloud._patch_params( params, name, value )"
                },
                {
                    "func_id": 460,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, **kwargs):\\n        super(HadoopInTheCloudJobRunner, self).__init__(**kwargs)\\n\\n        # if *cluster_id* is not set, ``self._cluster_id`` will be\\n        # set when we create or join a cluster\\n        self._cluster_id = self._opts['cluster_id']\\n\\n        # bootstrapping\\n        self._bootstrap = self._bootstrap_python() + self._parse_bootstrap()\\n\\n        # add files to manager\\n        self._bootstrap_dir_mgr = WorkingDirManager()\\n\\n        for cmd in self._bootstrap:\\n            for token in cmd:\\n                if isinstance(token, dict):\\n                    # convert dir archive tokens to archives\\n                    if token['type'] == 'dir':\\n                        token['path'] = self._dir_archive_path(token['path'])\\n                        token['type'] = 'archive'\\n\\n                    self._bootstrap_dir_mgr.add(**token)\\n\\n        # we'll create this script later, as needed\\n        self._master_bootstrap_script_path = None\\n\\n        # ssh state\\n\\n        # the process for the SSH tunnel\\n        self._ssh_proc = None\\n\\n        # if this is true, stop trying to launch the SSH tunnel\\n        self._give_up_on_ssh_tunnel = False\\n\\n        # store the (tunneled) URL of the job tracker/resource manager\\n        self._ssh_tunnel_url = None",
                    "func_fullName": "mrjob.cloud.__init__( self, **kwargs )"
                },
                {
                    "func_id": 461,
                    "func_name": "_default_opts",
                    "func_desc": "_default_opts",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _default_opts(cls):\\n        return combine_dicts(\\n            super(HadoopInTheCloudJobRunner, cls)._default_opts(),\\n            dict(\\n                cloud_part_size_mb=_DEFAULT_CLOUD_PART_SIZE_MB,\\n                max_mins_idle=_DEFAULT_MAX_MINS_IDLE,\\n                # don't use a list because it makes it hard to read option\\n                # values when running in verbose mode. See #1284\\n                ssh_bind_ports=xrange(40001, 40841),\\n                ssh_tunnel=False,\\n                ssh_tunnel_is_open=False,\\n                # ssh_bin isn't included here. For example, the Dataproc\\n                # runner launches ssh through the gcloud util\\n            ),\\n        )",
                    "func_fullName": "mrjob.cloud._default_opts( cls )"
                },
                {
                    "func_id": 462,
                    "func_name": "_fix_opts",
                    "func_desc": "_fix_opts",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _fix_opts(self, opts, source=None):\\n        opts = super(HadoopInTheCloudJobRunner, self)._fix_opts(\\n            opts, source=source)\\n\\n        # cloud_part_size_mb should be a number\\n        if opts.get('cloud_part_size_mb') is not None:\\n            if not isinstance(opts['cloud_part_size_mb'],\\n                              (integer_types, float)):\\n                raise TypeError('cloud_part_size_mb must be a number')\\n\\n        return opts",
                    "func_fullName": "mrjob.cloud._fix_opts( self, opts, source )"
                },
                {
                    "func_id": 463,
                    "func_name": "_combine_opts",
                    "func_desc": "_combine_opts",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _combine_opts(self, opt_list):\\n        \"\"\"Propagate *instance_type* to other instance type opts, if not\\n        already set.\\n\\n        Also propagate core instance type to task instance type, if it's\\n        not already set.\\n        \"\"\"\\n        opts = super(HadoopInTheCloudJobRunner, self)._combine_opts(opt_list)\\n\\n        if opts['instance_type']:\\n            # figure out how late in the configs opt was set (setting\\n            # --instance_type on the command line overrides core_instance_type\\n            # set in configs)\\n            opt_priority = {k: -1 for k in opts}\\n\\n            for i, sub_opts in enumerate(opt_list):\\n                for k, v in sub_opts.items():\\n                    if v == opts[k]:\\n                        opt_priority[k] = i\\n\\n            # instance_type only affects master_instance_type if there are\\n            # no other instances\\n            if opts['num_core_instances'] or opts['num_task_instances']:\\n                propagate_to = ['core_instance_type', 'task_instance_type']\\n            else:\\n                propagate_to = ['master_instance_type']\\n\\n            for k in propagate_to:\\n                if opts[k] is None or (\\n                        opt_priority[k] < opt_priority['instance_type']):\\n                    opts[k] = opts['instance_type']\\n\\n        if not opts['task_instance_type']:\\n            opts['task_instance_type'] = opts['core_instance_type']\\n\\n        return opts",
                    "func_fullName": "mrjob.cloud._combine_opts( self, opt_list )"
                },
                {
                    "func_id": 464,
                    "func_name": "_bootstrap_python",
                    "func_desc": "_bootstrap_python",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _bootstrap_python(self):\\n        \"\"\"Redefine this to return a (possibly empty) list of parsed commands\\n        (in the same format as returned by parse_setup_cmd())' to make sure a\\n        compatible version of Python is installed\\n\\n        If the *bootstrap_python* option is false, should always return ``[]``.\\n        \"\"\"\\n        return []",
                    "func_fullName": "mrjob.cloud._bootstrap_python( self )"
                },
                {
                    "func_id": 465,
                    "func_name": "_cp_to_local_cmd",
                    "func_desc": "_cp_to_local_cmd",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cp_to_local_cmd(self):\\n        \"\"\"Command to copy files from the cloud to the local directory\\n        (usually via Hadoop). Redefine this as needed; for example, on EMR,\\n        we sometimes have to use ``aws s3 cp`` because ``hadoop`` isn't\\n        installed at bootstrap time.\"\"\"\\n        return 'hadoop fs -copyToLocal'",
                    "func_fullName": "mrjob.cloud._cp_to_local_cmd( self )"
                },
                {
                    "func_id": 466,
                    "func_name": "_parse_bootstrap",
                    "func_desc": "_parse_bootstrap",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _parse_bootstrap(self):\\n        \"\"\"Parse the *bootstrap* option with\\n        :py:func:`mrjob.setup.parse_setup_cmd()`.\\n        \"\"\"\\n        return [parse_setup_cmd(cmd) for cmd in self._opts['bootstrap']]",
                    "func_fullName": "mrjob.cloud._parse_bootstrap( self )"
                },
                {
                    "func_id": 467,
                    "func_name": "_create_master_bootstrap_script_if_needed",
                    "func_desc": "_create_master_bootstrap_script_if_needed",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _create_master_bootstrap_script_if_needed(self):\\n        \"\"\"Helper for :py:meth:`_add_bootstrap_files_for_upload`.\\n\\n        Create the master bootstrap script and write it into our local\\n        temp directory. Set self._master_bootstrap_script_path.\\n\\n        This will do nothing if there are no bootstrap scripts or commands,\\n        or if it has already been called.\"\"\"\\n        if self._master_bootstrap_script_path:\\n            return\\n\\n        # don't bother if we're not starting a cluster\\n        if self._cluster_id:\\n            return\\n\\n        # Also don't bother if we're not bootstrapping\\n        if not self._bootstrap:\\n            return\\n\\n        path = os.path.join(self._get_local_tmp_dir(), 'b.sh')\\n        log.info('writing master bootstrap script to %s' % path)\\n\\n        contents = self._master_bootstrap_script_content(\\n            self._bootstrap)\\n\\n        self._write_script(contents, path, 'master bootstrap script')\\n\\n        self._master_bootstrap_script_path = path",
                    "func_fullName": "mrjob.cloud._create_master_bootstrap_script_if_needed( self )"
                },
                {
                    "func_id": 468,
                    "func_name": "_master_bootstrap_script_content",
                    "func_desc": "_master_bootstrap_script_content",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _master_bootstrap_script_content(self, bootstrap):\\n        \"\"\"Return a list containing the lines of the master bootstrap script.\\n        (without trailing newlines)\\n        \"\"\"\\n        out = []\\n\\n        # shebang, precommands\\n        out.extend(self._start_of_sh_script())\\n        out.append('')\\n\\n        # for example, create a tmp dir and cd to it\\n        if self._bootstrap_pre_commands():\\n            out.extend(self._bootstrap_pre_commands())\\n            out.append('')\\n\\n        # store $PWD\\n        out.append('# store $PWD')\\n        out.append('__mrjob_PWD=$PWD')\\n        out.append('')\\n\\n        # special case for PWD being in /, which happens on Dataproc\\n        # (really we should cd to tmp or something)\\n        out.append('if [ $__mrjob_PWD = \"/\" ]; then')\\n        out.append('  __mrjob_PWD=\"\"')\\n        out.append('fi')\\n        out.append('')\\n\\n        # run commands in a block so we can redirect stdout to stderr\\n        # (e.g. to catch errors from compileall). See #370\\n        out.append('{')\\n\\n        # download files\\n        out.append('  # download files and mark them executable')\\n\\n        cp_to_local = self._cp_to_local_cmd()\\n\\n        # TODO: why bother with $__mrjob_PWD here, since we're already in it?\\n        for name, path in sorted(\\n                self._bootstrap_dir_mgr.name_to_path('file').items()):\\n            uri = self._upload_mgr.uri(path)\\n            out.append('  %s %s $__mrjob_PWD/%s' %\\n                       (cp_to_local, pipes.quote(uri), pipes.quote(name)))\\n            # imitate Hadoop Distributed Cache (see #1602)\\n            out.append('  chmod u+rx $__mrjob_PWD/%s' % pipes.quote(name))\\n            out.append('')\\n\\n        # download and unarchive archives\\n        archive_names_and_paths = sorted(\\n            self._bootstrap_dir_mgr.name_to_path('archive').items())\\n        if archive_names_and_paths:\\n            # make tmp dir if needed\\n            out.append('  # download and unpack archives')\\n            out.append('  __mrjob_TMP=$(mktemp -d)')\\n            out.append('')\\n\\n            for name, path in archive_names_and_paths:\\n                uri = self._upload_mgr.uri(path)\\n\\n                archive_file_name = self._bootstrap_dir_mgr.name(\\n                    'archive_file', path)\\n\\n                # copy file to tmp dir\\n                quoted_archive_path = '$__mrjob_TMP/%s' % pipes.quote(\\n                    archive_file_name)\\n\\n                out.append('  %s %s %s' % (\\n                    cp_to_local, pipes.quote(uri), quoted_archive_path))\\n\\n                out.append('  ' + _unarchive_cmd(path) % dict(\\n                    file=quoted_archive_path,\\n                    dir='$__mrjob_PWD/' + pipes.quote(name)))\\n\\n                # imitate Hadoop Distributed Cache (see #1602)\\n                out.append(\\n                    '  chmod u+rx -R $__mrjob_PWD/%s' % pipes.quote(name))\\n\\n                out.append('')\\n\\n        # run bootstrap commands\\n        out.append('  # bootstrap commands')\\n        for cmd in bootstrap:\\n            # reconstruct the command line, substituting $__mrjob_PWD/<name>\\n            # for path dicts\\n            line = '  '\\n            for token in cmd:\\n                if isinstance(token, dict):\\n                    # it's a path dictionary\\n                    line += '$__mrjob_PWD/'\\n                    line += pipes.quote(self._bootstrap_dir_mgr.name(**token))\\n                else:\\n                    # it's raw script\\n                    line += token\\n            out.append(line)\\n\\n        out.append('} 1>&2')  # stdout -> stderr for ease of error log parsing\\n\\n        return out",
                    "func_fullName": "mrjob.cloud._master_bootstrap_script_content( self, bootstrap )"
                },
                {
                    "func_id": 469,
                    "func_name": "_bootstrap_pre_commands",
                    "func_desc": "_bootstrap_pre_commands",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _bootstrap_pre_commands(self):\\n        \"\"\"A list of hard-coded commands to run at the beginning of the\\n        bootstrap script. Currently used by dataproc to cd into a tmp dir.\"\"\"\\n        return []",
                    "func_fullName": "mrjob.cloud._bootstrap_pre_commands( self )"
                },
                {
                    "func_id": 470,
                    "func_name": "_start_of_sh_script",
                    "func_desc": "_start_of_sh_script",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _start_of_sh_script(self):\\n        \"\"\"Return a list of lines (without trailing newlines) containing the\\n        shell script shebang and pre-commands.\"\"\"\\n        out = []\\n\\n        # shebang\\n        sh_bin = self._sh_bin()\\n        if not sh_bin[0].startswith('/'):\\n            sh_bin = ['/usr/bin/env'] + sh_bin\\n        out.append('#!' + cmd_line(sh_bin))\\n\\n        # hook for 'set -e', etc. (see #1549)\\n        out.extend(self._sh_pre_commands())\\n\\n        return out",
                    "func_fullName": "mrjob.cloud._start_of_sh_script( self )"
                },
                {
                    "func_id": 471,
                    "func_name": "_add_extra_cluster_params",
                    "func_desc": "_add_extra_cluster_params",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _add_extra_cluster_params(self, params):\\n        \"\"\"Return a dict with the *extra_cluster_params* opt patched into\\n        *params*, and ``None`` values removed.\"\"\"\\n        params = deepcopy(params)\\n\\n        for k, v in sorted(self._opts['extra_cluster_params'].items()):\\n            _patch_params(params, k, v)\\n\\n        return params",
                    "func_fullName": "mrjob.cloud._add_extra_cluster_params( self, params )"
                },
                {
                    "func_id": 472,
                    "func_name": "_ssh_tunnel_args",
                    "func_desc": "_ssh_tunnel_args",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ssh_tunnel_args(self, bind_port):\\n        \"\"\"Redefine this in your subclass. You will probably want to call\\n        :py:meth:`_ssh_tunnel_opts` somewhere in here.\\n\\n        Should return the list of args used to run the command\\n        to open the SSH tunnel, bound to *bind_port* on your computer,\\n        or ``None`` if it isn't possible to set up an SSH tunnel.\\n        \"\"\"\\n        return None",
                    "func_fullName": "mrjob.cloud._ssh_tunnel_args( self, bind_port )"
                },
                {
                    "func_id": 473,
                    "func_name": "_ssh_tunnel_config",
                    "func_desc": "_ssh_tunnel_config",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ssh_tunnel_config(self):\\n        \"\"\"Redefine this in your subclass. Should return a dict with the\\n        following keys:\\n\\n        *localhost*: once we SSH in, is the web interface?\\n                     reachable at ``localhost``\\n        *name*: either ``'job tracker'`` or ``'resource manager'``\\n        *path*: path of main page on web interface (e.g. \"/cluster\")\\n        *port*: port number of the web interface\\n        \"\"\"\\n        raise NotImplementedError",
                    "func_fullName": "mrjob.cloud._ssh_tunnel_config( self )"
                },
                {
                    "func_id": 474,
                    "func_name": "_launch_ssh_proc",
                    "func_desc": "_launch_ssh_proc",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _launch_ssh_proc(self, args):\\n        \"\"\"The command used to create a :py:class:`subprocess.Popen` to\\n        run the SSH tunnel. You usually don't need to redefine this.\"\"\"\\n        log.debug('> %s' % cmd_line(args))\\n        return Popen(args, stdin=PIPE, stdout=PIPE, stderr=PIPE)",
                    "func_fullName": "mrjob.cloud._launch_ssh_proc( self, args )"
                },
                {
                    "func_id": 475,
                    "func_name": "_ssh_launch_wait_secs",
                    "func_desc": "_ssh_launch_wait_secs",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ssh_launch_wait_secs(self):\\n        \"\"\"Wait this long after launching the SSH process before checking\\n        for failure (default 1 second). You may redefine this.\"\"\"\\n        return 1.0",
                    "func_fullName": "mrjob.cloud._ssh_launch_wait_secs( self )"
                },
                {
                    "func_id": 476,
                    "func_name": "_set_up_ssh_tunnel",
                    "func_desc": "_set_up_ssh_tunnel",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _set_up_ssh_tunnel(self):\\n        \"\"\"Call this whenever you think it is possible to SSH to your cluster.\\n        This sets :py:attr:`_ssh_proc`. Does nothing if :mrjob-opt:`ssh_tunnel`\\n        is not set, or there is already a tunnel process running.\\n        \"\"\"\\n        # did the user request an SSH tunnel?\\n        if not self._opts['ssh_tunnel']:\\n            return\\n\\n        # no point in trying to launch a nonexistent command twice\\n        if self._give_up_on_ssh_tunnel:\\n            return\\n\\n        # did we already launch the SSH tunnel process? is it still running?\\n        if self._ssh_proc:\\n            self._ssh_proc.poll()\\n            if self._ssh_proc.returncode is None:\\n                return\\n            else:\\n                log.warning('  Oops, ssh subprocess exited with return code'\\n                            ' %d, restarting...' % self._ssh_proc.returncode)\\n                self._ssh_proc = None\\n\\n        tunnel_config = self._ssh_tunnel_config()\\n\\n        bind_port = None\\n        popen_exception = None\\n        ssh_tunnel_args = []\\n\\n        for bind_port in self._pick_ssh_bind_ports():\\n            ssh_proc = None\\n            ssh_tunnel_args = self._ssh_tunnel_args(bind_port)\\n\\n            # can't launch SSH tunnel right now\\n            if not ssh_tunnel_args:\\n                return\\n\\n            try:\\n                ssh_proc = self._launch_ssh_proc(ssh_tunnel_args)\\n            except OSError as ex:\\n                # e.g. OSError(2, 'File not found')\\n                popen_exception = ex   # warning handled below\\n                break\\n\\n            if ssh_proc:\\n                time.sleep(self._ssh_launch_wait_secs())\\n\\n                ssh_proc.poll()\\n                # still running. We are golden\\n                if ssh_proc.returncode is None:\\n                    self._ssh_proc = ssh_proc\\n                    break\\n                else:\\n                    ssh_proc.stdin.close()\\n                    ssh_proc.stdout.close()\\n                    ssh_proc.stderr.close()\\n\\n        if self._ssh_proc:\\n            if self._opts['ssh_tunnel_is_open']:\\n                bind_host = socket.getfqdn()\\n            else:\\n                bind_host = 'localhost'\\n            self._ssh_tunnel_url = 'http://%s:%d%s' % (\\n                bind_host, bind_port, tunnel_config['path'])\\n            log.info('  Connect to %s at: %s' % (\\n                tunnel_config['name'], self._ssh_tunnel_url))\\n\\n        else:\\n            if popen_exception:\\n                # this only happens if the ssh binary is not present\\n                # or not executable (so tunnel_config and the args to the\\n                # ssh binary don't matter)\\n                log.warning(\\n                    \"    Couldn't open SSH tunnel: %s\" % popen_exception)\\n                self._give_up_on_ssh_tunnel = True\\n                return\\n            else:\\n                log.warning(\\n                    '    Failed to open ssh tunnel to %s' %\\n                    tunnel_config['name'])",
                    "func_fullName": "mrjob.cloud._set_up_ssh_tunnel( self )"
                },
                {
                    "func_id": 477,
                    "func_name": "_kill_ssh_tunnel",
                    "func_desc": "_kill_ssh_tunnel",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _kill_ssh_tunnel(self):\\n        \"\"\"Send SIGKILL to SSH tunnel, if it's running.\"\"\"\\n        if not self._ssh_proc:\\n            return\\n\\n        self._ssh_proc.poll()\\n        if self._ssh_proc.returncode is None:\\n            log.info('Killing our SSH tunnel (pid %d)' %\\n                     self._ssh_proc.pid)\\n\\n            self._ssh_proc.stdin.close()\\n            self._ssh_proc.stdout.close()\\n            self._ssh_proc.stderr.close()\\n\\n            try:\\n                if hasattr(signal, 'SIGKILL'):\\n                    os.kill(self._ssh_proc.pid, signal.SIGKILL)\\n                else:\\n                    # Windows doesn't have SIGKILL, see #1892\\n                    os.kill(self._ssh_proc.pid, signal.SIGABRT)\\n            except Exception as e:\\n                log.exception(e)\\n\\n        self._ssh_proc = None\\n        self._ssh_tunnel_url = None",
                    "func_fullName": "mrjob.cloud._kill_ssh_tunnel( self )"
                },
                {
                    "func_id": 478,
                    "func_name": "_ssh_tunnel_opts",
                    "func_desc": "_ssh_tunnel_opts",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ssh_tunnel_opts(self, bind_port):\\n        \"\"\"Options to SSH related to setting up a tunnel (rather than\\n        SSHing in). Helper for :py:meth:`_ssh_tunnel_args`.\\n        \"\"\"\\n        args = self._ssh_local_tunnel_opt(bind_port) + [\\n            '-N', '-n', '-q',\\n        ]\\n        if self._opts['ssh_tunnel_is_open']:\\n            args.extend(['-g', '-4'])  # -4: listen on IPv4 only\\n\\n        return args",
                    "func_fullName": "mrjob.cloud._ssh_tunnel_opts( self, bind_port )"
                },
                {
                    "func_id": 479,
                    "func_name": "_ssh_local_tunnel_opt",
                    "func_desc": "_ssh_local_tunnel_opt",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ssh_local_tunnel_opt(self, bind_port):\\n        \"\"\"Helper for :py:meth:`_ssh_tunnel_opts`.\"\"\"\\n        tunnel_config = self._ssh_tunnel_config()\\n\\n        return [\\n            '-L', '%d:%s:%d' % (\\n                bind_port,\\n                self._job_tracker_host(),\\n                tunnel_config['port'],\\n            ),\\n        ]",
                    "func_fullName": "mrjob.cloud._ssh_local_tunnel_opt( self, bind_port )"
                },
                {
                    "func_id": 480,
                    "func_name": "_pick_ssh_bind_ports",
                    "func_desc": "_pick_ssh_bind_ports",
                    "func_file": "cloud",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _pick_ssh_bind_ports(self):\\n        \"\"\"Pick a list of ports to try binding our SSH tunnel to.\\n\\n        We will try to bind the same port for any given cluster (Issue #67)\\n        \"\"\"\\n        # don't perturb the random number generator\\n        random_state = random.getstate()\\n        try:\\n            # seed random port selection on cluster ID\\n            random.seed(self._cluster_id)\\n            num_picks = min(_MAX_SSH_RETRIES,\\n                            len(self._opts['ssh_bind_ports']))\\n            return random.sample(self._opts['ssh_bind_ports'], num_picks)\\n        finally:\\n            random.setstate(random_state)",
                    "func_fullName": "mrjob.cloud._pick_ssh_bind_ports( self )"
                },
                {
                    "func_id": 1586,
                    "func_name": "_path_glob_to_parsed_gcs_uri",
                    "func_desc": "_path_glob_to_parsed_gcs_uri",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _path_glob_to_parsed_gcs_uri(path_glob):\\n    # support globs\\n    glob_match = GLOB_RE.match(path_glob)\\n\\n    # we're going to search for all keys starting with base_uri\\n    if glob_match:\\n        # cut it off at first wildcard\\n        base_uri = glob_match.group(1)\\n    else:\\n        base_uri = path_glob\\n\\n    bucket_name, base_name = parse_gcs_uri(base_uri)\\n    return bucket_name, base_name",
                    "func_fullName": "mrjob.fs.gcs._path_glob_to_parsed_gcs_uri( path_glob )"
                },
                {
                    "func_id": 1609,
                    "func_name": "is_gcs_uri",
                    "func_desc": "is_gcs_uri",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def is_gcs_uri(uri):\\n    \"\"\"Return True if *uri* can be parsed into an S3 URI, False otherwise.\\n    \"\"\"\\n    try:\\n        parse_gcs_uri(uri)\\n        return True\\n    except ValueError:\\n        return False",
                    "func_fullName": "mrjob.fs.gcs.is_gcs_uri( uri )"
                },
                {
                    "func_id": 1610,
                    "func_name": "parse_gcs_uri",
                    "func_desc": "parse_gcs_uri",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def parse_gcs_uri(uri):\\n    \"\"\"Parse a GCS URI into (bucket, key)\\n\\n    >>> parse_gcs_uri(\"gs://walrus/tmp/\")\\n    ('walrus', 'tmp/')\\n\\n    If ``uri`` is not a GCS URI, raise a ValueError\\n    \"\"\"\\n    components = urlparse(uri)\\n    if components.scheme != \"gs\" or '/' not in components.path:\\n        raise ValueError('Invalid GCS URI: %s' % uri)\\n\\n    return components.netloc, components.path[1:]",
                    "func_fullName": "mrjob.fs.gcs.parse_gcs_uri( uri )"
                },
                {
                    "func_id": 1611,
                    "func_name": "_is_permanent_google_error",
                    "func_desc": "_is_permanent_google_error",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _is_permanent_google_error(self, ex):\\n    return isinstance(ex, google.auth.exceptions.DefaultCredentialsError)",
                    "func_fullName": "mrjob.fs.gcs._is_permanent_google_error( self, ex )"
                },
                {
                    "func_id": 1612,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, credentials=None, project_id=None,\\n                 part_size=None, location=None, object_ttl_days=None):\\n        self._credentials = credentials\\n        self._project_id = project_id\\n        self._part_size = part_size\\n        self._location = location\\n        self._object_ttl_days = object_ttl_days",
                    "func_fullName": "mrjob.fs.gcs.__init__( self, credentials, project_id, part_size, location, object_ttl_days )"
                },
                {
                    "func_id": 1614,
                    "func_name": "api_client",
                    "func_desc": "api_client",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def api_client(self):\\n        raise NotImplementedError(\\n            '\"api_client\" was disabled in v0.6.2. use \"client\" instead')",
                    "func_fullName": "mrjob.fs.gcs.api_client( self )"
                },
                {
                    "func_id": 1615,
                    "func_name": "can_handle_path",
                    "func_desc": "can_handle_path",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def can_handle_path(self, path):\\n        return is_gcs_uri(path)",
                    "func_fullName": "mrjob.fs.gcs.can_handle_path( self, path )"
                }
            ]
        },
        {
            "cluster_id": 21,
            "feature_id": 31,
            "feature_desc": "gamma=0.0498; k=2; a=0.25; combined=0.445; stability(ARI)=1.000; sep=0.078",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1613,
                    "func_name": "client",
                    "func_desc": "client",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def client(self):\\n        return google.cloud.storage.client.Client(\\n            project=self._project_id, credentials=self._credentials)",
                    "func_fullName": "mrjob.fs.gcs.client( self )"
                },
                {
                    "func_id": 1616,
                    "func_name": "du",
                    "func_desc": "du",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def du(self, path_glob):\\n        \"\"\"Get the size of all files matching path_glob.\"\"\"\\n        return sum(blob.size for uri, blob in self._ls(path_glob))",
                    "func_fullName": "mrjob.fs.gcs.du( self, path_glob )"
                },
                {
                    "func_id": 1617,
                    "func_name": "ls",
                    "func_desc": "ls",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def ls(self, path_glob):\\n        for uri, blob in self._ls(path_glob):\\n            # don't return directory \"blobs\"\\n            if uri.endswith('/'):\\n                continue\\n\\n            yield uri",
                    "func_fullName": "mrjob.fs.gcs.ls( self, path_glob )"
                },
                {
                    "func_id": 1618,
                    "func_name": "_ls",
                    "func_desc": "_ls",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ls(self, path_glob):\\n        \"\"\"Helper method for :py:meth:`ls`; yields tuples of\\n        ``(uri, blob)`` where *blob* is the corresponding\\n        :py:class:`google.cloud.storage.blob.Blob`.\\n\\n        This *will* return empty \"directory\" globs.\\n        \"\"\"\\n        # support globs\\n        glob_match = GLOB_RE.match(path_glob)\\n\\n        # we're going to search for all keys starting with base_uri\\n        if glob_match:\\n            # cut it off at first wildcard\\n            base_uri = glob_match.group(1)\\n        else:\\n            base_uri = path_glob\\n\\n        bucket_name, base_name = parse_gcs_uri(base_uri)\\n\\n        # allow subdirectories of the path/glob\\n        if path_glob and not path_glob.endswith('/'):\\n            dir_glob = path_glob + '/*'\\n        else:\\n            dir_glob = path_glob + '*'\\n\\n        try:\\n            bucket = self.get_bucket(bucket_name)\\n        except google.api_core.exceptions.NotFound:\\n            return  # treat nonexistent buckets as empty\\n\\n        for blob in bucket.list_blobs(prefix=base_name):\\n            uri = \"gs://%s/%s\" % (bucket_name, blob.name)\\n\\n            # enforce globbing\\n            if not (fnmatch.fnmatchcase(uri, path_glob) or\\n                    fnmatch.fnmatchcase(uri, dir_glob)):\\n                continue\\n\\n            yield uri, blob",
                    "func_fullName": "mrjob.fs.gcs._ls( self, path_glob )"
                },
                {
                    "func_id": 1619,
                    "func_name": "md5sum",
                    "func_desc": "md5sum",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def md5sum(self, path):\\n        blob = self._get_blob(path)\\n        if not blob:\\n            raise IOError('Object %r does not exist' % (path,))\\n        return binascii.hexlify(b64decode(blob.md5_hash)).decode('ascii')",
                    "func_fullName": "mrjob.fs.gcs.md5sum( self, path )"
                },
                {
                    "func_id": 1620,
                    "func_name": "_cat_file",
                    "func_desc": "_cat_file",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cat_file(self, path):\\n        return decompress(self._cat_blob(path), path)",
                    "func_fullName": "mrjob.fs.gcs._cat_file( self, path )"
                },
                {
                    "func_id": 1621,
                    "func_name": "_cat_blob",
                    "func_desc": "_cat_blob",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cat_blob(self, gcs_uri):\\n        \"\"\":py:meth:`cat_file`, minus decompression.\"\"\"\\n        blob = self._get_blob(gcs_uri)\\n\\n        if not blob:\\n            return  # don't cat nonexistent files\\n\\n        start = 0\\n\\n        while True:\\n            end = start + _CAT_CHUNK_SIZE\\n            try:\\n                chunk = blob.download_as_string(start=start, end=end)\\n            except google.api_core.exceptions.RequestRangeNotSatisfiable:\\n                return\\n\\n            yield chunk\\n\\n            if len(chunk) < _CAT_CHUNK_SIZE:\\n                return\\n\\n            start = end",
                    "func_fullName": "mrjob.fs.gcs._cat_blob( self, gcs_uri )"
                },
                {
                    "func_id": 1622,
                    "func_name": "mkdir",
                    "func_desc": "mkdir",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mkdir(self, path):\\n        \"\"\"Does not actually create a directory on GCS (because GCS doesn't\\n        have directories), but creates the underlying bucket if it does not\\n        exist already.\\n        \"\"\"\\n        bucket_name, base_name = parse_gcs_uri(path)\\n\\n        try:\\n            self.get_bucket(bucket_name)\\n        except google.api_core.exceptions.NotFound:\\n            self.create_bucket(bucket_name)",
                    "func_fullName": "mrjob.fs.gcs.mkdir( self, path )"
                },
                {
                    "func_id": 1623,
                    "func_name": "exists",
                    "func_desc": "exists",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def exists(self, path_glob):\\n        \"\"\"Does the given path exist?\\n\\n        If dest is a directory (ends with a \"/\"), we check if there are\\n        any files starting with that path.\\n        \"\"\"\\n        try:\\n            paths = self.ls(path_glob)\\n        except:\\n            paths = []\\n        return any(paths)",
                    "func_fullName": "mrjob.fs.gcs.exists( self, path_glob )"
                },
                {
                    "func_id": 1624,
                    "func_name": "rm",
                    "func_desc": "rm",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def rm(self, path_glob):\\n        \"\"\"Remove all files matching the given glob.\"\"\"\\n        for uri, blob in self._ls(path_glob):\\n            blob.delete()",
                    "func_fullName": "mrjob.fs.gcs.rm( self, path_glob )"
                },
                {
                    "func_id": 1625,
                    "func_name": "touchz",
                    "func_desc": "touchz",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def touchz(self, path):\\n        # check if already exists\\n        old_blob = self._get_blob(path)\\n        if old_blob:\\n            raise IOError('Non-empty file %r already exists!' % (path,))\\n\\n        self._blob(path).upload_from_string(b'')",
                    "func_fullName": "mrjob.fs.gcs.touchz( self, path )"
                },
                {
                    "func_id": 1626,
                    "func_name": "put",
                    "func_desc": "put",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def put(self, src, path):\\n        \"\"\"Uploads a local file to a specific destination.\\n\\n        .. versionchanged::\\n\\n           0.7.0 removed *chunk_size* arg (use *part_size*\\n           in the constructor)\\n\\n        .. versionchanged:: 0.6.8 deprecated *chunk_size*\\n        \"\"\"\\n        part_size = self._part_size\\n\\n        old_blob = self._get_blob(path)\\n        if old_blob:\\n            raise IOError('File already exists: %s' % path)\\n\\n        self._blob(path, chunk_size=part_size).upload_from_filename(src)",
                    "func_fullName": "mrjob.fs.gcs.put( self, src, path )"
                },
                {
                    "func_id": 1627,
                    "func_name": "get_all_bucket_names",
                    "func_desc": "get_all_bucket_names",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_all_bucket_names(self, prefix=None):\\n        \"\"\"Yield the names of all buckets associated with this client.\\n\\n        :param prefix: optional prefix to search under (e.g. ``'mrjob-'``)\\n\\n        .. versionadded:: 0.6.2\\n        \"\"\"\\n        for b in self.client.list_buckets(prefix=prefix):\\n            yield b.name",
                    "func_fullName": "mrjob.fs.gcs.get_all_bucket_names( self, prefix )"
                },
                {
                    "func_id": 1628,
                    "func_name": "list_buckets",
                    "func_desc": "list_buckets",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def list_buckets(self, project, prefix=None):\\n        \"\"\"List buckets on GCS.\"\"\"\\n        raise NotImplementedError(\\n            'list_buckets() was disabled in v0.6.2. Use'\\n            'get_all_bucket_names() and get_bucket()')",
                    "func_fullName": "mrjob.fs.gcs.list_buckets( self, project, prefix )"
                },
                {
                    "func_id": 1629,
                    "func_name": "get_bucket",
                    "func_desc": "get_bucket",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_bucket(self, bucket_name):\\n        \"\"\"Return a :py:class:`google.cloud.storage.bucket.Bucket`\\n        Raises an exception if the bucket does not exist.\"\"\"\\n        return self.client.get_bucket(bucket_name)",
                    "func_fullName": "mrjob.fs.gcs.get_bucket( self, bucket_name )"
                },
                {
                    "func_id": 1630,
                    "func_name": "create_bucket",
                    "func_desc": "create_bucket",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def create_bucket(self, name,\\n                      location=None, object_ttl_days=None):\\n        \"\"\"Create a bucket on GCS, optionally setting location constraint.\\n        and time-to-live.\"\"\"\\n        bucket = self.client.bucket(name)\\n\\n        if location is None:\\n            location = self._location\\n        elif not location:\\n            location = None  # leave a way to use the API default\\n\\n        bucket.create(location=location)\\n\\n        if object_ttl_days is None:\\n            object_ttl_days = self._object_ttl_days\\n\\n        if object_ttl_days:\\n            bucket.lifecycle_rules = [\\n                dict(\\n                    action=dict(type='Delete'),\\n                    condition=dict(age=object_ttl_days)\\n                )\\n            ]",
                    "func_fullName": "mrjob.fs.gcs.create_bucket( self, name, location, object_ttl_days )"
                },
                {
                    "func_id": 1631,
                    "func_name": "delete_bucket",
                    "func_desc": "delete_bucket",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def delete_bucket(self, bucket):\\n        raise NotImplementedError(\\n            'delete_bucket() was disabled in v0.6.2. Use'\\n            'fs.bucket(name).delete()')",
                    "func_fullName": "mrjob.fs.gcs.delete_bucket( self, bucket )"
                },
                {
                    "func_id": 1632,
                    "func_name": "_get_blob",
                    "func_desc": "_get_blob",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_blob(self, uri, chunk_size=None):\\n        # NOTE: chunk_size seems not to work well with downloading\\n        bucket_name, blob_name = parse_gcs_uri(uri)\\n        bucket = self.client.get_bucket(bucket_name)\\n        return bucket.get_blob(blob_name, chunk_size=chunk_size)",
                    "func_fullName": "mrjob.fs.gcs._get_blob( self, uri, chunk_size )"
                },
                {
                    "func_id": 1633,
                    "func_name": "_blob",
                    "func_desc": "_blob",
                    "func_file": "gcs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _blob(self, uri, chunk_size=None):\\n        # NOTE: chunk_size seems not to work well with downloading\\n        bucket_name, blob_name = parse_gcs_uri(uri)\\n        bucket = self.client.get_bucket(bucket_name)\\n        return bucket.blob(blob_name, chunk_size=chunk_size)",
                    "func_fullName": "mrjob.fs.gcs._blob( self, uri, chunk_size )"
                }
            ]
        },
        {
            "cluster_id": 6,
            "feature_id": 32,
            "feature_desc": "gamma=0.0100; k=1; a=0.25; combined=0.427; stability(ARI)=1.000; sep=0.112",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 655,
                    "func_name": "_dict_list_to_compat_map",
                    "func_desc": "_dict_list_to_compat_map",
                    "func_file": "compat",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _dict_list_to_compat_map(dict_list):\\n    # compat_map = {\\n    #   ...\\n    #   a: {'1.0': a, '2.0': b}\\n    #   ..\\n    # }\\n    compat_map = {}\\n    for version_dict in dict_list:\\n        for value in version_dict.values():\\n            compat_map[value] = version_dict\\n    return compat_map",
                    "func_fullName": "mrjob.compat._dict_list_to_compat_map( dict_list )"
                },
                {
                    "func_id": 656,
                    "func_name": "jobconf_from_env",
                    "func_desc": "jobconf_from_env",
                    "func_file": "compat",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def jobconf_from_env(variable, default=None):\\n    \"\"\"Get the value of a jobconf variable from the runtime environment.\\n\\n    For example, a :py:class:`~mrjob.job.MRJob` could use\\n    ``jobconf_from_env('map.input.file')`` to get the name of the file a\\n    mapper is reading input from.\\n\\n    If the name of the jobconf variable is different in different versions of\\n    Hadoop (e.g. in Hadoop 2.0, ``map.input.file`` is\\n    ``mapreduce.map.input.file``), we'll automatically try all variants before\\n    giving up.\\n\\n    Return *default* if that jobconf variable isn't set.\\n    \"\"\"\\n    # try variable verbatim first\\n    name = variable.replace('.', '_')\\n    if name in os.environ:\\n        return os.environ[name]\\n\\n    # try alternatives (arbitrary order)\\n    for var in _JOBCONF_MAP.get(variable, {}).values():\\n        name = var.replace('.', '_')\\n        if name in os.environ:\\n            return os.environ[name]\\n\\n    return default",
                    "func_fullName": "mrjob.compat.jobconf_from_env( variable, default )"
                },
                {
                    "func_id": 657,
                    "func_name": "jobconf_from_dict",
                    "func_desc": "jobconf_from_dict",
                    "func_file": "compat",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def jobconf_from_dict(jobconf, name, default=None):\\n    \"\"\"Get the value of a jobconf variable from the given dictionary.\\n\\n    :param dict jobconf: jobconf dictionary\\n    :param string name: name of the jobconf variable (e.g. ``'user.name'``)\\n    :param default: fallback value\\n\\n    If the name of the jobconf variable is different in different versions of\\n    Hadoop (e.g. in Hadoop 2, ``map.input.file`` is\\n    ``mapreduce.map.input.file``), we'll automatically try all variants before\\n    giving up.\\n\\n    Return *default* if that jobconf variable isn't set    \"\"\"\\n    if name in jobconf:\\n        return jobconf[name]\\n\\n    # try alternatives (arbitrary order)\\n    for alternative in _JOBCONF_MAP.get(name, {}).values():\\n        if alternative in jobconf:\\n            return jobconf[alternative]\\n\\n    return default",
                    "func_fullName": "mrjob.compat.jobconf_from_dict( jobconf, name, default )"
                },
                {
                    "func_id": 658,
                    "func_name": "map_version",
                    "func_desc": "map_version",
                    "func_file": "compat",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def map_version(version, version_map):\\n    \"\"\"Allows you to look up something by version (e.g. which jobconf variable\\n    to use, specifying only the versions where that value changed.\\n\\n    *version* is a string\\n\\n    *version_map* is a map from version (as a string) that a value changed\\n    to the new value.\\n\\n    For efficiency, *version_map* can also be a list of tuples of\\n    ``(LooseVersion(version_as_string), value)``, with oldest versions first.\\n\\n    If *version* is less than any version in *version_map*, use the value for\\n    the earliest version in *version_map*.\\n    \"\"\"\\n    if version is None:\\n        raise TypeError\\n\\n    if not version_map:\\n        raise ValueError\\n\\n    if isinstance(version_map, dict):\\n        version_map = sorted((LooseVersion(k), v)\\n                             for k, v in version_map.items())\\n\\n    req_version = LooseVersion(version)\\n\\n    for min_version, value in reversed(version_map):\\n        if req_version >= min_version:\\n            return value\\n    else:\\n        return version_map[0][1]",
                    "func_fullName": "mrjob.compat.map_version( version, version_map )"
                },
                {
                    "func_id": 659,
                    "func_name": "translate_jobconf",
                    "func_desc": "translate_jobconf",
                    "func_file": "compat",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def translate_jobconf(variable, version):\\n    \"\"\"Translate *variable* to Hadoop version *version*. If it's not\\n    a variable we recognize, leave as-is.\\n    \"\"\"\\n    if version is None:\\n        raise TypeError\\n\\n    if variable in _JOBCONF_MAP:\\n        return map_version(version, _JOBCONF_MAP[variable])\\n    else:\\n        return variable",
                    "func_fullName": "mrjob.compat.translate_jobconf( variable, version )"
                },
                {
                    "func_id": 660,
                    "func_name": "translate_jobconf_for_all_versions",
                    "func_desc": "translate_jobconf_for_all_versions",
                    "func_file": "compat",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def translate_jobconf_for_all_versions(variable):\\n    \"\"\"Get all known variants of the given jobconf variable.\\n    Unlike :py:func:`translate_jobconf`, returns a list.\"\"\"\\n    return sorted(\\n        set([variable] + list(_JOBCONF_MAP.get(variable, {}).values())))",
                    "func_fullName": "mrjob.compat.translate_jobconf_for_all_versions( variable )"
                },
                {
                    "func_id": 661,
                    "func_name": "translate_jobconf_dict",
                    "func_desc": "translate_jobconf_dict",
                    "func_file": "compat",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def translate_jobconf_dict(jobconf, hadoop_version=None):\\n    \"\"\"Translates the configuration property name to match those that\\n    are accepted in hadoop_version. Prints a warning message if any\\n    configuration property name does not match the name in the hadoop\\n    version. Combines the original jobconf with the translated jobconf.\\n\\n    :return: a map consisting of the original and translated configuration\\n             property names and values.\\n    \"\"\"\\n    translated_jobconf = jobconf.copy()\\n    translation_warnings = {}\\n\\n    for variable, value in jobconf.items():\\n        if hadoop_version:\\n            variants = [translate_jobconf(variable, hadoop_version)]\\n        else:\\n            variants = translate_jobconf_for_all_versions(variable)\\n\\n        for variant in variants:\\n            if variant in jobconf:\\n                # this happens if variant == variable or\\n                # if the variant was in jobconf to start with\\n                continue\\n\\n            translated_jobconf[variant] = value\\n\\n            if hadoop_version:\\n                translation_warnings[variable] = variant\\n\\n    if translation_warnings:\\n        log.warning(\"Detected hadoop configuration property names that\"\\n                    \" do not match hadoop version %s:\"\\n                    \"\\nThe have been translated as follows\\n %s\",\\n                    hadoop_version,\\n                    '\\n'.join([\\n                        \"%s: %s\" % (variable, variant) for variable, variant\\n                        in sorted(translation_warnings.items())]))\\n\\n    return translated_jobconf",
                    "func_fullName": "mrjob.compat.translate_jobconf_dict( jobconf, hadoop_version )"
                },
                {
                    "func_id": 662,
                    "func_name": "uses_yarn",
                    "func_desc": "uses_yarn",
                    "func_file": "compat",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def uses_yarn(version):\\n    \"\"\"Basically, is this Hadoop 2? This also handles versions in the\\n    zero series (0.23+) where YARN originated.\"\"\"\\n    return (version_gte(version, '2') or\\n            version_gte(version, '0.23') and not version_gte(version, '1'))",
                    "func_fullName": "mrjob.compat.uses_yarn( version )"
                },
                {
                    "func_id": 663,
                    "func_name": "version_gte",
                    "func_desc": "version_gte",
                    "func_file": "compat",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def version_gte(version, cmp_version_str):\\n    \"\"\"Return ``True`` if version >= *cmp_version_str*.\"\"\"\\n\\n    if not isinstance(version, string_types):\\n        raise TypeError('%r is not a string' % version)\\n\\n    if not isinstance(cmp_version_str, string_types):\\n        raise TypeError('%r is not a string' % cmp_version_str)\\n\\n    return LooseVersion(version) >= LooseVersion(cmp_version_str)",
                    "func_fullName": "mrjob.compat.version_gte( version, cmp_version_str )"
                },
                {
                    "func_id": 2466,
                    "func_name": "b",
                    "func_desc": "b",
                    "func_file": "compat",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def b(s):\\n        return s",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.compat.b( s )"
                },
                {
                    "func_id": 2467,
                    "func_name": "b",
                    "func_desc": "b",
                    "func_file": "compat",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def b(s):\\n        return bytes(s, 'latin1')",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.compat.b( s )"
                },
                {
                    "func_id": 2426,
                    "func_name": "_import_speedups",
                    "func_desc": "_import_speedups",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _import_speedups():\\n    try:\\n        from . import _speedups\\n        return _speedups.encode_basestring_ascii, _speedups.make_encoder\\n    except ImportError:\\n        return None, None",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder._import_speedups(  )"
                },
                {
                    "func_id": 2427,
                    "func_name": "encode_basestring",
                    "func_desc": "encode_basestring",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def encode_basestring(s, _PY3=PY3, _q=u'\"'):\\n    \"\"\"Return a JSON representation of a Python string\\n\\n    \"\"\"\\n    if _PY3:\\n        if isinstance(s, bytes):\\n            s = str(s, 'utf-8')\\n        elif type(s) is not str:\\n            # convert an str subclass instance to exact str\\n            # raise a TypeError otherwise\\n            s = str.__str__(s)\\n    else:\\n        if isinstance(s, str) and HAS_UTF8.search(s) is not None:\\n            s = unicode(s, 'utf-8')\\n        elif type(s) not in (str, unicode):\\n            # convert an str subclass instance to exact str\\n            # convert a unicode subclass instance to exact unicode\\n            # raise a TypeError otherwise\\n            if isinstance(s, str):\\n                s = str.__str__(s)\\n            else:\\n                s = unicode.__getnewargs__(s)[0]\\n    def replace(match):\\n        return ESCAPE_DCT[match.group(0)]\\n    return _q + ESCAPE.sub(replace, s) + _q",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder.encode_basestring( s, _PY3, _q )"
                },
                {
                    "func_id": 2428,
                    "func_name": "py_encode_basestring_ascii",
                    "func_desc": "py_encode_basestring_ascii",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def py_encode_basestring_ascii(s, _PY3=PY3):\\n    \"\"\"Return an ASCII-only JSON representation of a Python string\\n\\n    \"\"\"\\n    if _PY3:\\n        if isinstance(s, bytes):\\n            s = str(s, 'utf-8')\\n        elif type(s) is not str:\\n            # convert an str subclass instance to exact str\\n            # raise a TypeError otherwise\\n            s = str.__str__(s)\\n    else:\\n        if isinstance(s, str) and HAS_UTF8.search(s) is not None:\\n            s = unicode(s, 'utf-8')\\n        elif type(s) not in (str, unicode):\\n            # convert an str subclass instance to exact str\\n            # convert a unicode subclass instance to exact unicode\\n            # raise a TypeError otherwise\\n            if isinstance(s, str):\\n                s = str.__str__(s)\\n            else:\\n                s = unicode.__getnewargs__(s)[0]\\n    def replace(match):\\n        s = match.group(0)\\n        try:\\n            return ESCAPE_DCT[s]\\n        except KeyError:\\n            n = ord(s)\\n            if n < 0x10000:\\n                #return '\\\\u{0:04x}'.format(n)\\n                return '\\\\u%04x' % (n,)\\n            else:\\n                # surrogate pair\\n                n -= 0x10000\\n                s1 = 0xd800 | ((n >> 10) & 0x3ff)\\n                s2 = 0xdc00 | (n & 0x3ff)\\n                #return '\\\\u{0:04x}\\\\u{1:04x}'.format(s1, s2)\\n                return '\\\\u%04x\\\\u%04x' % (s1, s2)\\n    return '\"' + str(ESCAPE_ASCII.sub(replace, s)) + '\"'",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder.py_encode_basestring_ascii( s, _PY3 )"
                },
                {
                    "func_id": 2435,
                    "func_name": "_make_iterencode",
                    "func_desc": "_make_iterencode",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\\n        _key_separator, _item_separator, _sort_keys, _skipkeys,\\n        _use_decimal, _namedtuple_as_object, _tuple_as_array,\\n        _int_as_string_bitcount, _item_sort_key,\\n        _encoding,_for_json,\\n        _iterable_as_array,\\n        ## HACK: hand-optimized bytecode; turn globals into locals\\n        _PY3=PY3,\\n        ValueError=ValueError,\\n        string_types=string_types,\\n        Decimal=None,\\n        dict=dict,\\n        float=float,\\n        id=id,\\n        integer_types=integer_types,\\n        isinstance=isinstance,\\n        list=list,\\n        str=str,\\n        tuple=tuple,\\n        iter=iter,\\n    ):\\n    if _use_decimal and Decimal is None:\\n        Decimal = decimal.Decimal\\n    if _item_sort_key and not callable(_item_sort_key):\\n        raise TypeError(\"item_sort_key must be None or callable\")\\n    elif _sort_keys and not _item_sort_key:\\n        _item_sort_key = itemgetter(0)\\n\\n    if (_int_as_string_bitcount is not None and\\n        (_int_as_string_bitcount <= 0 or\\n         not isinstance(_int_as_string_bitcount, integer_types))):\\n        raise TypeError(\"int_as_string_bitcount must be a positive integer\")\\n\\n    def call_method(obj, method_name):\\n        method = getattr(obj, method_name, None)\\n        if callable(method):\\n            try:\\n                return (method(),)\\n            except TypeError:\\n                pass\\n        return None\\n\\n    def _encode_int(value):\\n        skip_quoting = (\\n            _int_as_string_bitcount is None\\n            or\\n            _int_as_string_bitcount < 1\\n        )\\n        if type(value) not in integer_types:\\n            # See #118, do not trust custom str/repr\\n            value = int(value)\\n        if (\\n            skip_quoting or\\n            (-1 << _int_as_string_bitcount)\\n            < value <\\n            (1 << _int_as_string_bitcount)\\n        ):\\n            return str(value)\\n        return '\"' + str(value) + '\"'\\n\\n    def _iterencode_list(lst, _current_indent_level):\\n        if not lst:\\n            yield '[]'\\n            return\\n        if markers is not None:\\n            markerid = id(lst)\\n            if markerid in markers:\\n                raise ValueError(\"Circular reference detected\")\\n            markers[markerid] = lst\\n        buf = '['\\n        if _indent is not None:\\n            _current_indent_level += 1\\n            newline_indent = '\\n' + (_indent * _current_indent_level)\\n            separator = _item_separator + newline_indent\\n            buf += newline_indent\\n        else:\\n            newline_indent = None\\n            separator = _item_separator\\n        first = True\\n        for value in lst:\\n            if first:\\n                first = False\\n            else:\\n                buf = separator\\n            if isinstance(value, string_types):\\n                yield buf + _encoder(value)\\n            elif _PY3 and isinstance(value, bytes) and _encoding is not None:\\n                yield buf + _encoder(value)\\n            elif isinstance(value, RawJSON):\\n                yield buf + value.encoded_json\\n            elif value is None:\\n                yield buf + 'null'\\n            elif value is True:\\n                yield buf + 'true'\\n            elif value is False:\\n                yield buf + 'false'\\n            elif isinstance(value, integer_types):\\n                yield buf + _encode_int(value)\\n            elif isinstance(value, float):\\n                yield buf + _floatstr(value)\\n            elif _use_decimal and isinstance(value, Decimal):\\n                yield buf + str(value)\\n            else:\\n                yield buf\\n                for_json = _for_json and call_method(value, 'for_json')\\n                if for_json:\\n                    chunks = _iterencode(for_json[0], _current_indent_level)\\n                elif isinstance(value, list):\\n                    chunks = _iterencode_list(value, _current_indent_level)\\n                else:\\n                    _asdict = _namedtuple_as_object and call_method(value, '_asdict')\\n                    if _asdict:\\n                        dct = _asdict[0]\\n                        if not isinstance(dct, dict):\\n                            raise TypeError(\"_asdict() must return a dict, not %s\" % (type(dct).__name__,))\\n                        chunks = _iterencode_dict(dct,\\n                                                  _current_indent_level)\\n                    elif _tuple_as_array and isinstance(value, tuple):\\n                        chunks = _iterencode_list(value, _current_indent_level)\\n                    elif isinstance(value, dict):\\n                        chunks = _iterencode_dict(value, _current_indent_level)\\n                    else:\\n                        chunks = _iterencode(value, _current_indent_level)\\n                for chunk in chunks:\\n                    yield chunk\\n        if first:\\n            # iterable_as_array misses the fast path at the top\\n            yield '[]'\\n        else:\\n            if newline_indent is not None:\\n                _current_indent_level -= 1\\n                yield '\\n' + (_indent * _current_indent_level)\\n            yield ']'\\n        if markers is not None:\\n            del markers[markerid]\\n\\n    def _stringify_key(key):\\n        if isinstance(key, string_types): # pragma: no cover\\n            pass\\n        elif _PY3 and isinstance(key, bytes) and _encoding is not None:\\n            key = str(key, _encoding)\\n        elif isinstance(key, float):\\n            key = _floatstr(key)\\n        elif key is True:\\n            key = 'true'\\n        elif key is False:\\n            key = 'false'\\n        elif key is None:\\n            key = 'null'\\n        elif isinstance(key, integer_types):\\n            if type(key) not in integer_types:\\n                # See #118, do not trust custom str/repr\\n                key = int(key)\\n            key = str(key)\\n        elif _use_decimal and isinstance(key, Decimal):\\n            key = str(key)\\n        elif _skipkeys:\\n            key = None\\n        else:\\n            raise TypeError('keys must be str, int, float, bool or None, '\\n                            'not %s' % key.__class__.__name__)\\n        return key\\n\\n    def _iterencode_dict(dct, _current_indent_level):\\n        if not dct:\\n            yield '{}'\\n            return\\n        if markers is not None:\\n            markerid = id(dct)\\n            if markerid in markers:\\n                raise ValueError(\"Circular reference detected\")\\n            markers[markerid] = dct\\n        yield '{'\\n        if _indent is not None:\\n            _current_indent_level += 1\\n            newline_indent = '\\n' + (_indent * _current_indent_level)\\n            item_separator = _item_separator + newline_indent\\n            yield newline_indent\\n        else:\\n            newline_indent = None\\n            item_separator = _item_separator\\n        first = True\\n        if _PY3:\\n            iteritems = dct.items()\\n        else:\\n            iteritems = dct.iteritems()\\n        if _item_sort_key:\\n            items = []\\n            for k, v in dct.items():\\n                if not isinstance(k, string_types):\\n                    k = _stringify_key(k)\\n                    if k is None:\\n                        continue\\n                items.append((k, v))\\n            items.sort(key=_item_sort_key)\\n        else:\\n            items = iteritems\\n        for key, value in items:\\n            if not (_item_sort_key or isinstance(key, string_types)):\\n                key = _stringify_key(key)\\n                if key is None:\\n                    # _skipkeys must be True\\n                    continue\\n            if first:\\n                first = False\\n            else:\\n                yield item_separator\\n            yield _encoder(key)\\n            yield _key_separator\\n            if isinstance(value, string_types):\\n                yield _encoder(value)\\n            elif _PY3 and isinstance(value, bytes) and _encoding is not None:\\n                yield _encoder(value)\\n            elif isinstance(value, RawJSON):\\n                yield value.encoded_json\\n            elif value is None:\\n                yield 'null'\\n            elif value is True:\\n                yield 'true'\\n            elif value is False:\\n                yield 'false'\\n            elif isinstance(value, integer_types):\\n                yield _encode_int(value)\\n            elif isinstance(value, float):\\n                yield _floatstr(value)\\n            elif _use_decimal and isinstance(value, Decimal):\\n                yield str(value)\\n            else:\\n                for_json = _for_json and call_method(value, 'for_json')\\n                if for_json:\\n                    chunks = _iterencode(for_json[0], _current_indent_level)\\n                elif isinstance(value, list):\\n                    chunks = _iterencode_list(value, _current_indent_level)\\n                else:\\n                    _asdict = _namedtuple_as_object and call_method(value, '_asdict')\\n                    if _asdict:\\n                        dct = _asdict[0]\\n                        if not isinstance(dct, dict):\\n                            raise TypeError(\"_asdict() must return a dict, not %s\" % (type(dct).__name__,))\\n                        chunks = _iterencode_dict(dct,\\n                                                  _current_indent_level)\\n                    elif _tuple_as_array and isinstance(value, tuple):\\n                        chunks = _iterencode_list(value, _current_indent_level)\\n                    elif isinstance(value, dict):\\n                        chunks = _iterencode_dict(value, _current_indent_level)\\n                    else:\\n                        chunks = _iterencode(value, _current_indent_level)\\n                for chunk in chunks:\\n                    yield chunk\\n        if newline_indent is not None:\\n            _current_indent_level -= 1\\n            yield '\\n' + (_indent * _current_indent_level)\\n        yield '}'\\n        if markers is not None:\\n            del markers[markerid]\\n\\n    def _iterencode(o, _current_indent_level):\\n        if isinstance(o, string_types):\\n            yield _encoder(o)\\n        elif _PY3 and isinstance(o, bytes) and _encoding is not None:\\n            yield _encoder(o)\\n        elif isinstance(o, RawJSON):\\n            yield o.encoded_json\\n        elif o is None:\\n            yield 'null'\\n        elif o is True:\\n            yield 'true'\\n        elif o is False:\\n            yield 'false'\\n        elif isinstance(o, integer_types):\\n            yield _encode_int(o)\\n        elif isinstance(o, float):\\n            yield _floatstr(o)\\n        else:\\n            for_json = _for_json and call_method(o, 'for_json')\\n            if for_json:\\n                for chunk in _iterencode(for_json[0], _current_indent_level):\\n                    yield chunk\\n            elif isinstance(o, list):\\n                for chunk in _iterencode_list(o, _current_indent_level):\\n                    yield chunk\\n            else:\\n                _asdict = _namedtuple_as_object and call_method(o, '_asdict')\\n                if _asdict:\\n                    dct = _asdict[0]\\n                    if not isinstance(dct, dict):\\n                        raise TypeError(\"_asdict() must return a dict, not %s\" % (type(dct).__name__,))\\n                    for chunk in _iterencode_dict(dct, _current_indent_level):\\n                        yield chunk\\n                elif (_tuple_as_array and isinstance(o, tuple)):\\n                    for chunk in _iterencode_list(o, _current_indent_level):\\n                        yield chunk\\n                elif isinstance(o, dict):\\n                    for chunk in _iterencode_dict(o, _current_indent_level):\\n                        yield chunk\\n                elif _use_decimal and isinstance(o, Decimal):\\n                    yield str(o)\\n                else:\\n                    while _iterable_as_array:\\n                        # Markers are not checked here because it is valid for\\n                        # an iterable to return self.\\n                        try:\\n                            o = iter(o)\\n                        except TypeError:\\n                            break\\n                        for chunk in _iterencode_list(o, _current_indent_level):\\n                            yield chunk\\n                        return\\n                    if markers is not None:\\n                        markerid = id(o)\\n                        if markerid in markers:\\n                            raise ValueError(\"Circular reference detected\")\\n                        markers[markerid] = o\\n                    o = _default(o)\\n                    for chunk in _iterencode(o, _current_indent_level):\\n                        yield chunk\\n                    if markers is not None:\\n                        del markers[markerid]\\n\\n    return _iterencode",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder._make_iterencode( markers, _default, _encoder, _indent, _floatstr, _key_separator, _item_separator, _sort_keys, _skipkeys, _use_decimal, _namedtuple_as_object, _tuple_as_array, _int_as_string_bitcount, _item_sort_key, _encoding, _for_json, _iterable_as_array, _PY3, ValueError, string_types, Decimal, dict, float, id, integer_types, isinstance, list, str, tuple, iter )"
                },
                {
                    "func_id": 2436,
                    "func_name": "replace",
                    "func_desc": "replace",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def replace(match):\\n        return ESCAPE_DCT[match.group(0)]",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder.replace( match )"
                },
                {
                    "func_id": 2437,
                    "func_name": "replace",
                    "func_desc": "replace",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def replace(match):\\n        s = match.group(0)\\n        try:\\n            return ESCAPE_DCT[s]\\n        except KeyError:\\n            n = ord(s)\\n            if n < 0x10000:\\n                #return '\\\\u{0:04x}'.format(n)\\n                return '\\\\u%04x' % (n,)\\n            else:\\n                # surrogate pair\\n                n -= 0x10000\\n                s1 = 0xd800 | ((n >> 10) & 0x3ff)\\n                s2 = 0xdc00 | (n & 0x3ff)\\n                #return '\\\\u{0:04x}\\\\u{1:04x}'.format(s1, s2)\\n                return '\\\\u%04x\\\\u%04x' % (s1, s2)",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder.replace( match )"
                },
                {
                    "func_id": 2438,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, skipkeys=False, ensure_ascii=True,\\n                 check_circular=True, allow_nan=False, sort_keys=False,\\n                 indent=None, separators=None, encoding='utf-8', default=None,\\n                 use_decimal=True, namedtuple_as_object=True,\\n                 tuple_as_array=True, bigint_as_string=False,\\n                 item_sort_key=None, for_json=False, ignore_nan=False,\\n                 int_as_string_bitcount=None, iterable_as_array=False):\\n        \"\"\"Constructor for JSONEncoder, with sensible defaults.\\n\\n        If skipkeys is false, then it is a TypeError to attempt\\n        encoding of keys that are not str, int, long, float or None.  If\\n        skipkeys is True, such items are simply skipped.\\n\\n        If ensure_ascii is true, the output is guaranteed to be str\\n        objects with all incoming unicode characters escaped.  If\\n        ensure_ascii is false, the output will be unicode object.\\n\\n        If check_circular is true, then lists, dicts, and custom encoded\\n        objects will be checked for circular references during encoding to\\n        prevent an infinite recursion (which would cause an OverflowError).\\n        Otherwise, no such check takes place.\\n\\n        If allow_nan is true (default: False), then out of range float\\n        values (nan, inf, -inf) will be serialized to\\n        their JavaScript equivalents (NaN, Infinity, -Infinity)\\n        instead of raising a ValueError. See\\n        ignore_nan for ECMA-262 compliant behavior.\\n\\n        If sort_keys is true, then the output of dictionaries will be\\n        sorted by key; this is useful for regression tests to ensure\\n        that JSON serializations can be compared on a day-to-day basis.\\n\\n        If indent is a string, then JSON array elements and object members\\n        will be pretty-printed with a newline followed by that string repeated\\n        for each level of nesting. ``None`` (the default) selects the most compact\\n        representation without any newlines. For backwards compatibility with\\n        versions of simplejson earlier than 2.1.0, an integer is also accepted\\n        and is converted to a string with that many spaces.\\n\\n        If specified, separators should be an (item_separator, key_separator)\\n        tuple.  The default is (', ', ': ') if *indent* is ``None`` and\\n        (',', ': ') otherwise.  To get the most compact JSON representation,\\n        you should specify (',', ':') to eliminate whitespace.\\n\\n        If specified, default is a function that gets called for objects\\n        that can't otherwise be serialized.  It should return a JSON encodable\\n        version of the object or raise a ``TypeError``.\\n\\n        If encoding is not None, then all input strings will be\\n        transformed into unicode using that encoding prior to JSON-encoding.\\n        The default is UTF-8.\\n\\n        If use_decimal is true (default: ``True``), ``decimal.Decimal`` will\\n        be supported directly by the encoder. For the inverse, decode JSON\\n        with ``parse_float=decimal.Decimal``.\\n\\n        If namedtuple_as_object is true (the default), objects with\\n        ``_asdict()`` methods will be encoded as JSON objects.\\n\\n        If tuple_as_array is true (the default), tuple (and subclasses) will\\n        be encoded as JSON arrays.\\n\\n        If *iterable_as_array* is true (default: ``False``),\\n        any object not in the above table that implements ``__iter__()``\\n        will be encoded as a JSON array.\\n\\n        If bigint_as_string is true (not the default), ints 2**53 and higher\\n        or lower than -2**53 will be encoded as strings. This is to avoid the\\n        rounding that happens in Javascript otherwise.\\n\\n        If int_as_string_bitcount is a positive number (n), then int of size\\n        greater than or equal to 2**n or lower than or equal to -2**n will be\\n        encoded as strings.\\n\\n        If specified, item_sort_key is a callable used to sort the items in\\n        each dictionary. This is useful if you want to sort items other than\\n        in alphabetical order by key.\\n\\n        If for_json is true (not the default), objects with a ``for_json()``\\n        method will use the return value of that method for encoding as JSON\\n        instead of the object.\\n\\n        If *ignore_nan* is true (default: ``False``), then out of range\\n        :class:`float` values (``nan``, ``inf``, ``-inf``) will be serialized\\n        as ``null`` in compliance with the ECMA-262 specification. If true,\\n        this will override *allow_nan*.\\n\\n        \"\"\"\\n\\n        self.skipkeys = skipkeys\\n        self.ensure_ascii = ensure_ascii\\n        self.check_circular = check_circular\\n        self.allow_nan = allow_nan\\n        self.sort_keys = sort_keys\\n        self.use_decimal = use_decimal\\n        self.namedtuple_as_object = namedtuple_as_object\\n        self.tuple_as_array = tuple_as_array\\n        self.iterable_as_array = iterable_as_array\\n        self.bigint_as_string = bigint_as_string\\n        self.item_sort_key = item_sort_key\\n        self.for_json = for_json\\n        self.ignore_nan = ignore_nan\\n        self.int_as_string_bitcount = int_as_string_bitcount\\n        if indent is not None and not isinstance(indent, string_types):\\n            indent = indent * ' '\\n        self.indent = indent\\n        if separators is not None:\\n            self.item_separator, self.key_separator = separators\\n        elif indent is not None:\\n            self.item_separator = ','\\n        if default is not None:\\n            self.default = default\\n        self.encoding = encoding",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder.__init__( self, skipkeys, ensure_ascii, check_circular, allow_nan, sort_keys, indent, separators, encoding, default, use_decimal, namedtuple_as_object, tuple_as_array, bigint_as_string, item_sort_key, for_json, ignore_nan, int_as_string_bitcount, iterable_as_array )"
                },
                {
                    "func_id": 2439,
                    "func_name": "default",
                    "func_desc": "default",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def default(self, o):\\n        \"\"\"Implement this method in a subclass such that it returns\\n        a serializable object for ``o``, or calls the base implementation\\n        (to raise a ``TypeError``).\\n\\n        For example, to support arbitrary iterators, you could\\n        implement default like this::\\n\\n            def default(self, o):\\n                try:\\n                    iterable = iter(o)\\n                except TypeError:\\n                    pass\\n                else:\\n                    return list(iterable)\\n                return JSONEncoder.default(self, o)\\n\\n        \"\"\"\\n        raise TypeError('Object of type %s is not JSON serializable' %\\n                        o.__class__.__name__)",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder.default( self, o )"
                },
                {
                    "func_id": 2440,
                    "func_name": "encode",
                    "func_desc": "encode",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def encode(self, o):\\n        \"\"\"Return a JSON string representation of a Python data structure.\\n\\n        >>> from simplejson import JSONEncoder\\n        >>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\\n        '{\"foo\": [\"bar\", \"baz\"]}'\\n\\n        \"\"\"\\n        # This is for extremely simple cases and benchmarks.\\n        if isinstance(o, binary_type):\\n            _encoding = self.encoding\\n            if (_encoding is not None and not (_encoding == 'utf-8')):\\n                o = text_type(o, _encoding)\\n        if isinstance(o, string_types):\\n            if self.ensure_ascii:\\n                return encode_basestring_ascii(o)\\n            else:\\n                return encode_basestring(o)\\n        # This doesn't pass the iterator directly to ''.join() because the\\n        # exceptions aren't as detailed.  The list call should be roughly\\n        # equivalent to the PySequence_Fast that ''.join() would do.\\n        chunks = self.iterencode(o)\\n        if not isinstance(chunks, (list, tuple)):\\n            chunks = list(chunks)\\n        if self.ensure_ascii:\\n            return ''.join(chunks)\\n        else:\\n            return u''.join(chunks)",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder.encode( self, o )"
                },
                {
                    "func_id": 2441,
                    "func_name": "iterencode",
                    "func_desc": "iterencode",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def iterencode(self, o):\\n        \"\"\"Encode the given object and yield each string\\n        representation as available.\\n\\n        For example::\\n\\n            for chunk in JSONEncoder().iterencode(bigobject):\\n                mysocket.write(chunk)\\n\\n        \"\"\"\\n        if self.check_circular:\\n            markers = {}\\n        else:\\n            markers = None\\n        if self.ensure_ascii:\\n            _encoder = encode_basestring_ascii\\n        else:\\n            _encoder = encode_basestring\\n        if self.encoding != 'utf-8' and self.encoding is not None:\\n            def _encoder(o, _orig_encoder=_encoder, _encoding=self.encoding):\\n                if isinstance(o, binary_type):\\n                    o = text_type(o, _encoding)\\n                return _orig_encoder(o)\\n\\n        def floatstr(o, allow_nan=self.allow_nan, ignore_nan=self.ignore_nan,\\n                _repr=FLOAT_REPR, _inf=PosInf, _neginf=-PosInf):\\n            # Check for specials. Note that this type of test is processor\\n            # and/or platform-specific, so do tests which don't depend on\\n            # the internals.\\n\\n            if o != o:\\n                text = 'NaN'\\n            elif o == _inf:\\n                text = 'Infinity'\\n            elif o == _neginf:\\n                text = '-Infinity'\\n            else:\\n                if type(o) != float:\\n                    # See #118, do not trust custom str/repr\\n                    o = float(o)\\n                return _repr(o)\\n\\n            if ignore_nan:\\n                text = 'null'\\n            elif not allow_nan:\\n                raise ValueError(\\n                    \"Out of range float values are not JSON compliant: \" +\\n                    repr(o))\\n\\n            return text\\n\\n        key_memo = {}\\n        int_as_string_bitcount = (\\n            53 if self.bigint_as_string else self.int_as_string_bitcount)\\n        if (c_make_encoder is not None and self.indent is None):\\n            _iterencode = c_make_encoder(\\n                markers, self.default, _encoder, self.indent,\\n                self.key_separator, self.item_separator, self.sort_keys,\\n                self.skipkeys, self.allow_nan, key_memo, self.use_decimal,\\n                self.namedtuple_as_object, self.tuple_as_array,\\n                int_as_string_bitcount,\\n                self.item_sort_key, self.encoding, self.for_json,\\n                self.ignore_nan, decimal.Decimal, self.iterable_as_array)\\n        else:\\n            _iterencode = _make_iterencode(\\n                markers, self.default, _encoder, self.indent, floatstr,\\n                self.key_separator, self.item_separator, self.sort_keys,\\n                self.skipkeys, self.use_decimal,\\n                self.namedtuple_as_object, self.tuple_as_array,\\n                int_as_string_bitcount,\\n                self.item_sort_key, self.encoding, self.for_json,\\n                self.iterable_as_array, Decimal=decimal.Decimal)\\n        try:\\n            return _iterencode(o, 0)\\n        finally:\\n            key_memo.clear()",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder.iterencode( self, o )"
                },
                {
                    "func_id": 2442,
                    "func_name": "encode",
                    "func_desc": "encode",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def encode(self, o):\\n        # Override JSONEncoder.encode because it has hacks for\\n        # performance that make things more complicated.\\n        chunks = self.iterencode(o)\\n        if self.ensure_ascii:\\n            return ''.join(chunks)\\n        else:\\n            return u''.join(chunks)",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder.encode( self, o )"
                },
                {
                    "func_id": 2443,
                    "func_name": "iterencode",
                    "func_desc": "iterencode",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def iterencode(self, o):\\n        chunks = super(JSONEncoderForHTML, self).iterencode(o)\\n        for chunk in chunks:\\n            chunk = chunk.replace('&', '\\\\u0026')\\n            chunk = chunk.replace('<', '\\\\u003c')\\n            chunk = chunk.replace('>', '\\\\u003e')\\n\\n            if not self.ensure_ascii:\\n                chunk = chunk.replace(u'\\u2028', '\\\\u2028')\\n                chunk = chunk.replace(u'\\u2029', '\\\\u2029')\\n\\n            yield chunk",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder.iterencode( self, o )"
                },
                {
                    "func_id": 2444,
                    "func_name": "call_method",
                    "func_desc": "call_method",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def call_method(obj, method_name):\\n        method = getattr(obj, method_name, None)\\n        if callable(method):\\n            try:\\n                return (method(),)\\n            except TypeError:\\n                pass\\n        return None",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder.call_method( obj, method_name )"
                },
                {
                    "func_id": 2445,
                    "func_name": "_encode_int",
                    "func_desc": "_encode_int",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _encode_int(value):\\n        skip_quoting = (\\n            _int_as_string_bitcount is None\\n            or\\n            _int_as_string_bitcount < 1\\n        )\\n        if type(value) not in integer_types:\\n            # See #118, do not trust custom str/repr\\n            value = int(value)\\n        if (\\n            skip_quoting or\\n            (-1 << _int_as_string_bitcount)\\n            < value <\\n            (1 << _int_as_string_bitcount)\\n        ):\\n            return str(value)\\n        return '\"' + str(value) + '\"'",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder._encode_int( value )"
                },
                {
                    "func_id": 2446,
                    "func_name": "_iterencode_list",
                    "func_desc": "_iterencode_list",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _iterencode_list(lst, _current_indent_level):\\n        if not lst:\\n            yield '[]'\\n            return\\n        if markers is not None:\\n            markerid = id(lst)\\n            if markerid in markers:\\n                raise ValueError(\"Circular reference detected\")\\n            markers[markerid] = lst\\n        buf = '['\\n        if _indent is not None:\\n            _current_indent_level += 1\\n            newline_indent = '\\n' + (_indent * _current_indent_level)\\n            separator = _item_separator + newline_indent\\n            buf += newline_indent\\n        else:\\n            newline_indent = None\\n            separator = _item_separator\\n        first = True\\n        for value in lst:\\n            if first:\\n                first = False\\n            else:\\n                buf = separator\\n            if isinstance(value, string_types):\\n                yield buf + _encoder(value)\\n            elif _PY3 and isinstance(value, bytes) and _encoding is not None:\\n                yield buf + _encoder(value)\\n            elif isinstance(value, RawJSON):\\n                yield buf + value.encoded_json\\n            elif value is None:\\n                yield buf + 'null'\\n            elif value is True:\\n                yield buf + 'true'\\n            elif value is False:\\n                yield buf + 'false'\\n            elif isinstance(value, integer_types):\\n                yield buf + _encode_int(value)\\n            elif isinstance(value, float):\\n                yield buf + _floatstr(value)\\n            elif _use_decimal and isinstance(value, Decimal):\\n                yield buf + str(value)\\n            else:\\n                yield buf\\n                for_json = _for_json and call_method(value, 'for_json')\\n                if for_json:\\n                    chunks = _iterencode(for_json[0], _current_indent_level)\\n                elif isinstance(value, list):\\n                    chunks = _iterencode_list(value, _current_indent_level)\\n                else:\\n                    _asdict = _namedtuple_as_object and call_method(value, '_asdict')\\n                    if _asdict:\\n                        dct = _asdict[0]\\n                        if not isinstance(dct, dict):\\n                            raise TypeError(\"_asdict() must return a dict, not %s\" % (type(dct).__name__,))\\n                        chunks = _iterencode_dict(dct,\\n                                                  _current_indent_level)\\n                    elif _tuple_as_array and isinstance(value, tuple):\\n                        chunks = _iterencode_list(value, _current_indent_level)\\n                    elif isinstance(value, dict):\\n                        chunks = _iterencode_dict(value, _current_indent_level)\\n                    else:\\n                        chunks = _iterencode(value, _current_indent_level)\\n                for chunk in chunks:\\n                    yield chunk\\n        if first:\\n            # iterable_as_array misses the fast path at the top\\n            yield '[]'\\n        else:\\n            if newline_indent is not None:\\n                _current_indent_level -= 1\\n                yield '\\n' + (_indent * _current_indent_level)\\n            yield ']'\\n        if markers is not None:\\n            del markers[markerid]",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder._iterencode_list( lst, _current_indent_level )"
                },
                {
                    "func_id": 2447,
                    "func_name": "_stringify_key",
                    "func_desc": "_stringify_key",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _stringify_key(key):\\n        if isinstance(key, string_types): # pragma: no cover\\n            pass\\n        elif _PY3 and isinstance(key, bytes) and _encoding is not None:\\n            key = str(key, _encoding)\\n        elif isinstance(key, float):\\n            key = _floatstr(key)\\n        elif key is True:\\n            key = 'true'\\n        elif key is False:\\n            key = 'false'\\n        elif key is None:\\n            key = 'null'\\n        elif isinstance(key, integer_types):\\n            if type(key) not in integer_types:\\n                # See #118, do not trust custom str/repr\\n                key = int(key)\\n            key = str(key)\\n        elif _use_decimal and isinstance(key, Decimal):\\n            key = str(key)\\n        elif _skipkeys:\\n            key = None\\n        else:\\n            raise TypeError('keys must be str, int, float, bool or None, '\\n                            'not %s' % key.__class__.__name__)\\n        return key",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder._stringify_key( key )"
                },
                {
                    "func_id": 2448,
                    "func_name": "_iterencode_dict",
                    "func_desc": "_iterencode_dict",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _iterencode_dict(dct, _current_indent_level):\\n        if not dct:\\n            yield '{}'\\n            return\\n        if markers is not None:\\n            markerid = id(dct)\\n            if markerid in markers:\\n                raise ValueError(\"Circular reference detected\")\\n            markers[markerid] = dct\\n        yield '{'\\n        if _indent is not None:\\n            _current_indent_level += 1\\n            newline_indent = '\\n' + (_indent * _current_indent_level)\\n            item_separator = _item_separator + newline_indent\\n            yield newline_indent\\n        else:\\n            newline_indent = None\\n            item_separator = _item_separator\\n        first = True\\n        if _PY3:\\n            iteritems = dct.items()\\n        else:\\n            iteritems = dct.iteritems()\\n        if _item_sort_key:\\n            items = []\\n            for k, v in dct.items():\\n                if not isinstance(k, string_types):\\n                    k = _stringify_key(k)\\n                    if k is None:\\n                        continue\\n                items.append((k, v))\\n            items.sort(key=_item_sort_key)\\n        else:\\n            items = iteritems\\n        for key, value in items:\\n            if not (_item_sort_key or isinstance(key, string_types)):\\n                key = _stringify_key(key)\\n                if key is None:\\n                    # _skipkeys must be True\\n                    continue\\n            if first:\\n                first = False\\n            else:\\n                yield item_separator\\n            yield _encoder(key)\\n            yield _key_separator\\n            if isinstance(value, string_types):\\n                yield _encoder(value)\\n            elif _PY3 and isinstance(value, bytes) and _encoding is not None:\\n                yield _encoder(value)\\n            elif isinstance(value, RawJSON):\\n                yield value.encoded_json\\n            elif value is None:\\n                yield 'null'\\n            elif value is True:\\n                yield 'true'\\n            elif value is False:\\n                yield 'false'\\n            elif isinstance(value, integer_types):\\n                yield _encode_int(value)\\n            elif isinstance(value, float):\\n                yield _floatstr(value)\\n            elif _use_decimal and isinstance(value, Decimal):\\n                yield str(value)\\n            else:\\n                for_json = _for_json and call_method(value, 'for_json')\\n                if for_json:\\n                    chunks = _iterencode(for_json[0], _current_indent_level)\\n                elif isinstance(value, list):\\n                    chunks = _iterencode_list(value, _current_indent_level)\\n                else:\\n                    _asdict = _namedtuple_as_object and call_method(value, '_asdict')\\n                    if _asdict:\\n                        dct = _asdict[0]\\n                        if not isinstance(dct, dict):\\n                            raise TypeError(\"_asdict() must return a dict, not %s\" % (type(dct).__name__,))\\n                        chunks = _iterencode_dict(dct,\\n                                                  _current_indent_level)\\n                    elif _tuple_as_array and isinstance(value, tuple):\\n                        chunks = _iterencode_list(value, _current_indent_level)\\n                    elif isinstance(value, dict):\\n                        chunks = _iterencode_dict(value, _current_indent_level)\\n                    else:\\n                        chunks = _iterencode(value, _current_indent_level)\\n                for chunk in chunks:\\n                    yield chunk\\n        if newline_indent is not None:\\n            _current_indent_level -= 1\\n            yield '\\n' + (_indent * _current_indent_level)\\n        yield '}'\\n        if markers is not None:\\n            del markers[markerid]",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder._iterencode_dict( dct, _current_indent_level )"
                },
                {
                    "func_id": 2449,
                    "func_name": "_iterencode",
                    "func_desc": "_iterencode",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _iterencode(o, _current_indent_level):\\n        if isinstance(o, string_types):\\n            yield _encoder(o)\\n        elif _PY3 and isinstance(o, bytes) and _encoding is not None:\\n            yield _encoder(o)\\n        elif isinstance(o, RawJSON):\\n            yield o.encoded_json\\n        elif o is None:\\n            yield 'null'\\n        elif o is True:\\n            yield 'true'\\n        elif o is False:\\n            yield 'false'\\n        elif isinstance(o, integer_types):\\n            yield _encode_int(o)\\n        elif isinstance(o, float):\\n            yield _floatstr(o)\\n        else:\\n            for_json = _for_json and call_method(o, 'for_json')\\n            if for_json:\\n                for chunk in _iterencode(for_json[0], _current_indent_level):\\n                    yield chunk\\n            elif isinstance(o, list):\\n                for chunk in _iterencode_list(o, _current_indent_level):\\n                    yield chunk\\n            else:\\n                _asdict = _namedtuple_as_object and call_method(o, '_asdict')\\n                if _asdict:\\n                    dct = _asdict[0]\\n                    if not isinstance(dct, dict):\\n                        raise TypeError(\"_asdict() must return a dict, not %s\" % (type(dct).__name__,))\\n                    for chunk in _iterencode_dict(dct, _current_indent_level):\\n                        yield chunk\\n                elif (_tuple_as_array and isinstance(o, tuple)):\\n                    for chunk in _iterencode_list(o, _current_indent_level):\\n                        yield chunk\\n                elif isinstance(o, dict):\\n                    for chunk in _iterencode_dict(o, _current_indent_level):\\n                        yield chunk\\n                elif _use_decimal and isinstance(o, Decimal):\\n                    yield str(o)\\n                else:\\n                    while _iterable_as_array:\\n                        # Markers are not checked here because it is valid for\\n                        # an iterable to return self.\\n                        try:\\n                            o = iter(o)\\n                        except TypeError:\\n                            break\\n                        for chunk in _iterencode_list(o, _current_indent_level):\\n                            yield chunk\\n                        return\\n                    if markers is not None:\\n                        markerid = id(o)\\n                        if markerid in markers:\\n                            raise ValueError(\"Circular reference detected\")\\n                        markers[markerid] = o\\n                    o = _default(o)\\n                    for chunk in _iterencode(o, _current_indent_level):\\n                        yield chunk\\n                    if markers is not None:\\n                        del markers[markerid]",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder._iterencode( o, _current_indent_level )"
                },
                {
                    "func_id": 2450,
                    "func_name": "floatstr",
                    "func_desc": "floatstr",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def floatstr(o, allow_nan=self.allow_nan, ignore_nan=self.ignore_nan,\\n                _repr=FLOAT_REPR, _inf=PosInf, _neginf=-PosInf):\\n            # Check for specials. Note that this type of test is processor\\n            # and/or platform-specific, so do tests which don't depend on\\n            # the internals.\\n\\n            if o != o:\\n                text = 'NaN'\\n            elif o == _inf:\\n                text = 'Infinity'\\n            elif o == _neginf:\\n                text = '-Infinity'\\n            else:\\n                if type(o) != float:\\n                    # See #118, do not trust custom str/repr\\n                    o = float(o)\\n                return _repr(o)\\n\\n            if ignore_nan:\\n                text = 'null'\\n            elif not allow_nan:\\n                raise ValueError(\\n                    \"Out of range float values are not JSON compliant: \" +\\n                    repr(o))\\n\\n            return text",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder.floatstr( o, allow_nan, ignore_nan, _repr, _inf, _neginf )"
                },
                {
                    "func_id": 2451,
                    "func_name": "_encoder",
                    "func_desc": "_encoder",
                    "func_file": "encoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "            def _encoder(o, _orig_encoder=_encoder, _encoding=self.encoding):\\n                if isinstance(o, binary_type):\\n                    o = text_type(o, _encoding)\\n                return _orig_encoder(o)",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.encoder._encoder( o, _orig_encoder, _encoding )"
                },
                {
                    "func_id": 2452,
                    "func_name": "_import_c_scanstring",
                    "func_desc": "_import_c_scanstring",
                    "func_file": "decoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _import_c_scanstring():\\n    try:\\n        from ._speedups import scanstring\\n        return scanstring\\n    except ImportError:\\n        return None",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.decoder._import_c_scanstring(  )"
                },
                {
                    "func_id": 2453,
                    "func_name": "_floatconstants",
                    "func_desc": "_floatconstants",
                    "func_file": "decoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _floatconstants():\\n    if sys.version_info < (2, 6):\\n        _BYTES = '7FF80000000000007FF0000000000000'.decode('hex')\\n        nan, inf = struct.unpack('>dd', _BYTES)\\n    else:\\n        nan = float('nan')\\n        inf = float('inf')\\n    return nan, inf, -inf",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.decoder._floatconstants(  )"
                },
                {
                    "func_id": 2454,
                    "func_name": "scan_four_digit_hex",
                    "func_desc": "scan_four_digit_hex",
                    "func_file": "decoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def scan_four_digit_hex(s, end, _m=re.compile(r'^[0-9a-fA-F]{4}$').match):\\n    \"\"\"Scan a four digit hex number from s[end:end + 4]\\n    \"\"\"\\n    msg = \"Invalid \\\\uXXXX escape sequence\"\\n    esc = s[end:end + 4]\\n    if not _m(esc):\\n        raise JSONDecodeError(msg, s, end - 2)\\n    try:\\n        return int(esc, 16), end + 4\\n    except ValueError:\\n        raise JSONDecodeError(msg, s, end - 2)",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.decoder.scan_four_digit_hex( s, end, _m )"
                },
                {
                    "func_id": 2455,
                    "func_name": "py_scanstring",
                    "func_desc": "py_scanstring",
                    "func_file": "decoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def py_scanstring(s, end, encoding=None, strict=True,\\n        _b=BACKSLASH, _m=STRINGCHUNK.match, _join=u''.join,\\n        _PY3=PY3, _maxunicode=sys.maxunicode,\\n        _scan_four_digit_hex=scan_four_digit_hex):\\n    \"\"\"Scan the string s for a JSON string. End is the index of the\\n    character in s after the quote that started the JSON string.\\n    Unescapes all valid JSON string escape sequences and raises ValueError\\n    on attempt to decode an invalid string. If strict is False then literal\\n    control characters are allowed in the string.\\n\\n    Returns a tuple of the decoded string and the index of the character in s\\n    after the end quote.\"\"\"\\n    if encoding is None:\\n        encoding = DEFAULT_ENCODING\\n    chunks = []\\n    _append = chunks.append\\n    begin = end - 1\\n    while 1:\\n        chunk = _m(s, end)\\n        if chunk is None:\\n            raise JSONDecodeError(\\n                \"Unterminated string starting at\", s, begin)\\n        prev_end = end\\n        end = chunk.end()\\n        content, terminator = chunk.groups()\\n        # Content is contains zero or more unescaped string characters\\n        if content:\\n            if not _PY3 and not isinstance(content, unicode):\\n                content = unicode(content, encoding)\\n            _append(content)\\n        # Terminator is the end of string, a literal control character,\\n        # or a backslash denoting that an escape sequence follows\\n        if terminator == '\"':\\n            break\\n        elif terminator != '\\\\':\\n            if strict:\\n                msg = \"Invalid control character %r at\"\\n                raise JSONDecodeError(msg, s, prev_end)\\n            else:\\n                _append(terminator)\\n                continue\\n        try:\\n            esc = s[end]\\n        except IndexError:\\n            raise JSONDecodeError(\\n                \"Unterminated string starting at\", s, begin)\\n        # If not a unicode escape sequence, must be in the lookup table\\n        if esc != 'u':\\n            try:\\n                char = _b[esc]\\n            except KeyError:\\n                msg = \"Invalid \\\\X escape sequence %r\"\\n                raise JSONDecodeError(msg, s, end)\\n            end += 1\\n        else:\\n            # Unicode escape sequence\\n            uni, end = _scan_four_digit_hex(s, end + 1)\\n            # Check for surrogate pair on UCS-4 systems\\n            # Note that this will join high/low surrogate pairs\\n            # but will also pass unpaired surrogates through\\n            if (_maxunicode > 65535 and\\n                uni & 0xfc00 == 0xd800 and\\n                s[end:end + 2] == '\\\\u'):\\n                uni2, end2 = _scan_four_digit_hex(s, end + 2)\\n                if uni2 & 0xfc00 == 0xdc00:\\n                    uni = 0x10000 + (((uni - 0xd800) << 10) |\\n                                        (uni2 - 0xdc00))\\n                    end = end2\\n            char = unichr(uni)\\n        # Append the unescaped character\\n        _append(char)\\n    return _join(chunks), end",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.decoder.py_scanstring( s, end, encoding, strict, _b, _m, _join, _PY3, _maxunicode, _scan_four_digit_hex )"
                },
                {
                    "func_id": 2456,
                    "func_name": "JSONObject",
                    "func_desc": "JSONObject",
                    "func_file": "decoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def JSONObject(state, encoding, strict, scan_once, object_hook,\\n        object_pairs_hook, memo=None,\\n        _w=WHITESPACE.match, _ws=WHITESPACE_STR):\\n    (s, end) = state\\n    # Backwards compatibility\\n    if memo is None:\\n        memo = {}\\n    memo_get = memo.setdefault\\n    pairs = []\\n    # Use a slice to prevent IndexError from being raised, the following\\n    # check will raise a more specific ValueError if the string is empty\\n    nextchar = s[end:end + 1]\\n    # Normally we expect nextchar == '\"'\\n    if nextchar != '\"':\\n        if nextchar in _ws:\\n            end = _w(s, end).end()\\n            nextchar = s[end:end + 1]\\n        # Trivial empty object\\n        if nextchar == '}':\\n            if object_pairs_hook is not None:\\n                result = object_pairs_hook(pairs)\\n                return result, end + 1\\n            pairs = {}\\n            if object_hook is not None:\\n                pairs = object_hook(pairs)\\n            return pairs, end + 1\\n        elif nextchar != '\"':\\n            raise JSONDecodeError(\\n                \"Expecting property name enclosed in double quotes or '}'\",\\n                s, end)\\n    end += 1\\n    while True:\\n        key, end = scanstring(s, end, encoding, strict)\\n        key = memo_get(key, key)\\n\\n        # To skip some function call overhead we optimize the fast paths where\\n        # the JSON key separator is \": \" or just \":\".\\n        if s[end:end + 1] != ':':\\n            end = _w(s, end).end()\\n            if s[end:end + 1] != ':':\\n                raise JSONDecodeError(\"Expecting ':' delimiter\", s, end)\\n\\n        end += 1\\n\\n        try:\\n            if s[end] in _ws:\\n                end += 1\\n                if s[end] in _ws:\\n                    end = _w(s, end + 1).end()\\n        except IndexError:\\n            pass\\n\\n        value, end = scan_once(s, end)\\n        pairs.append((key, value))\\n\\n        try:\\n            nextchar = s[end]\\n            if nextchar in _ws:\\n                end = _w(s, end + 1).end()\\n                nextchar = s[end]\\n        except IndexError:\\n            nextchar = ''\\n        end += 1\\n\\n        if nextchar == '}':\\n            break\\n        elif nextchar != ',':\\n            raise JSONDecodeError(\"Expecting ',' delimiter or '}'\", s, end - 1)\\n\\n        try:\\n            nextchar = s[end]\\n            if nextchar in _ws:\\n                end += 1\\n                nextchar = s[end]\\n                if nextchar in _ws:\\n                    end = _w(s, end + 1).end()\\n                    nextchar = s[end]\\n        except IndexError:\\n            nextchar = ''\\n\\n        end += 1\\n        if nextchar != '\"':\\n            raise JSONDecodeError(\\n                \"Expecting property name enclosed in double quotes\",\\n                s, end - 1)\\n\\n    if object_pairs_hook is not None:\\n        result = object_pairs_hook(pairs)\\n        return result, end\\n    pairs = dict(pairs)\\n    if object_hook is not None:\\n        pairs = object_hook(pairs)\\n    return pairs, end",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.decoder.JSONObject( state, encoding, strict, scan_once, object_hook, object_pairs_hook, memo, _w, _ws )"
                },
                {
                    "func_id": 2457,
                    "func_name": "JSONArray",
                    "func_desc": "JSONArray",
                    "func_file": "decoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def JSONArray(state, scan_once, _w=WHITESPACE.match, _ws=WHITESPACE_STR):\\n    (s, end) = state\\n    values = []\\n    nextchar = s[end:end + 1]\\n    if nextchar in _ws:\\n        end = _w(s, end + 1).end()\\n        nextchar = s[end:end + 1]\\n    # Look-ahead for trivial empty array\\n    if nextchar == ']':\\n        return values, end + 1\\n    elif nextchar == '':\\n        raise JSONDecodeError(\"Expecting value or ']'\", s, end)\\n    _append = values.append\\n    while True:\\n        value, end = scan_once(s, end)\\n        _append(value)\\n        nextchar = s[end:end + 1]\\n        if nextchar in _ws:\\n            end = _w(s, end + 1).end()\\n            nextchar = s[end:end + 1]\\n        end += 1\\n        if nextchar == ']':\\n            break\\n        elif nextchar != ',':\\n            raise JSONDecodeError(\"Expecting ',' delimiter or ']'\", s, end - 1)\\n\\n        try:\\n            if s[end] in _ws:\\n                end += 1\\n                if s[end] in _ws:\\n                    end = _w(s, end + 1).end()\\n        except IndexError:\\n            pass\\n\\n    return values, end",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.decoder.JSONArray( state, scan_once, _w, _ws )"
                },
                {
                    "func_id": 2461,
                    "func_name": "bounded_int",
                    "func_desc": "bounded_int",
                    "func_file": "decoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def bounded_int(s, INT_MAX_STR_DIGITS=4300):\\n        \"\"\"Backport of the integer string length conversion limitation\\n\\n        https://docs.python.org/3/library/stdtypes.html#int-max-str-digits\\n        \"\"\"\\n        if len(s) > INT_MAX_STR_DIGITS:\\n            raise ValueError(\"Exceeds the limit (%s) for integer string conversion: value has %s digits\" % (INT_MAX_STR_DIGITS, len(s)))\\n        return int(s)",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.decoder.bounded_int( s, INT_MAX_STR_DIGITS )"
                },
                {
                    "func_id": 2462,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "decoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, encoding=None, object_hook=None, parse_float=None,\\n            parse_int=None, parse_constant=None, strict=True,\\n            object_pairs_hook=None, allow_nan=False):\\n        \"\"\"\\n        *encoding* determines the encoding used to interpret any\\n        :class:`str` objects decoded by this instance (``'utf-8'`` by\\n        default).  It has no effect when decoding :class:`unicode` objects.\\n\\n        Note that currently only encodings that are a superset of ASCII work,\\n        strings of other encodings should be passed in as :class:`unicode`.\\n\\n        *object_hook*, if specified, will be called with the result of every\\n        JSON object decoded and its return value will be used in place of the\\n        given :class:`dict`.  This can be used to provide custom\\n        deserializations (e.g. to support JSON-RPC class hinting).\\n\\n        *object_pairs_hook* is an optional function that will be called with\\n        the result of any object literal decode with an ordered list of pairs.\\n        The return value of *object_pairs_hook* will be used instead of the\\n        :class:`dict`.  This feature can be used to implement custom decoders\\n        that rely on the order that the key and value pairs are decoded (for\\n        example, :func:`collections.OrderedDict` will remember the order of\\n        insertion). If *object_hook* is also defined, the *object_pairs_hook*\\n        takes priority.\\n\\n        *parse_float*, if specified, will be called with the string of every\\n        JSON float to be decoded.  By default, this is equivalent to\\n        ``float(num_str)``. This can be used to use another datatype or parser\\n        for JSON floats (e.g. :class:`decimal.Decimal`).\\n\\n        *parse_int*, if specified, will be called with the string of every\\n        JSON int to be decoded.  By default, this is equivalent to\\n        ``int(num_str)``.  This can be used to use another datatype or parser\\n        for JSON integers (e.g. :class:`float`).\\n\\n        *allow_nan*, if True (default false), will allow the parser to\\n        accept the non-standard floats ``NaN``, ``Infinity``, and ``-Infinity``.\\n\\n        *parse_constant*, if specified, will be\\n        called with one of the following strings: ``'-Infinity'``,\\n        ``'Infinity'``, ``'NaN'``. It is not recommended to use this feature,\\n        as it is rare to parse non-compliant JSON containing these values.\\n\\n        *strict* controls the parser's behavior when it encounters an\\n        invalid control character in a string. The default setting of\\n        ``True`` means that unescaped control characters are parse errors, if\\n        ``False`` then control characters will be allowed in strings.\\n\\n        \"\"\"\\n        if encoding is None:\\n            encoding = DEFAULT_ENCODING\\n        self.encoding = encoding\\n        self.object_hook = object_hook\\n        self.object_pairs_hook = object_pairs_hook\\n        self.parse_float = parse_float or float\\n        self.parse_int = parse_int or bounded_int\\n        self.parse_constant = parse_constant or (allow_nan and _CONSTANTS.__getitem__ or None)\\n        self.strict = strict\\n        self.parse_object = JSONObject\\n        self.parse_array = JSONArray\\n        self.parse_string = scanstring\\n        self.memo = {}\\n        self.scan_once = make_scanner(self)",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.decoder.__init__( self, encoding, object_hook, parse_float, parse_int, parse_constant, strict, object_pairs_hook, allow_nan )"
                },
                {
                    "func_id": 2463,
                    "func_name": "decode",
                    "func_desc": "decode",
                    "func_file": "decoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def decode(self, s, _w=WHITESPACE.match, _PY3=PY3):\\n        \"\"\"Return the Python representation of ``s`` (a ``str`` or ``unicode``\\n        instance containing a JSON document)\\n\\n        \"\"\"\\n        if _PY3 and isinstance(s, bytes):\\n            s = str(s, self.encoding)\\n        obj, end = self.raw_decode(s)\\n        end = _w(s, end).end()\\n        if end != len(s):\\n            raise JSONDecodeError(\"Extra data\", s, end, len(s))\\n        return obj",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.decoder.decode( self, s, _w, _PY3 )"
                },
                {
                    "func_id": 2464,
                    "func_name": "raw_decode",
                    "func_desc": "raw_decode",
                    "func_file": "decoder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def raw_decode(self, s, idx=0, _w=WHITESPACE.match, _PY3=PY3):\\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` or ``unicode``\\n        beginning with a JSON document) and return a 2-tuple of the Python\\n        representation and the index in ``s`` where the document ended.\\n        Optionally, ``idx`` can be used to specify an offset in ``s`` where\\n        the JSON document begins.\\n\\n        This can be used to decode a JSON document from a string that may\\n        have extraneous data at the end.\\n\\n        \"\"\"\\n        if idx < 0:\\n            # Ensure that raw_decode bails on negative indexes, the regex\\n            # would otherwise mask this behavior. #98\\n            raise JSONDecodeError('Expecting value', s, idx)\\n        if _PY3 and not isinstance(s, str):\\n            raise TypeError(\"Input string must be text, not bytes\")\\n        # strip UTF-8 bom\\n        if len(s) > idx:\\n            ord0 = ord(s[idx])\\n            if ord0 == 0xfeff:\\n                idx += 1\\n            elif ord0 == 0xef and s[idx:idx + 3] == '\\xef\\xbb\\xbf':\\n                idx += 3\\n        return self.scan_once(s, idx=_w(s, idx).end())",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.decoder.raw_decode( self, s, idx, _w, _PY3 )"
                }
            ]
        },
        {
            "cluster_id": 8,
            "feature_id": 33,
            "feature_desc": "gamma=0.0000; k=1; a=0.25; combined=1.000; stability(ARI)=1.000; sep=1.000",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 664,
                    "func_name": "to_unicode",
                    "func_desc": "to_unicode",
                    "func_file": "py2",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def to_unicode(s):\\n    \"\"\"Convert ``bytes`` to unicode.\\n\\n    Use this if you need to ``print()`` or log bytes of an unknown encoding,\\n    or to parse strings out of bytes of unknown encoding (e.g. a log file).\\n\\n    This hopes that your bytes are UTF-8 decodable, but if not, falls back\\n    to latin-1, which always works.\\n    \"\"\"\\n    if isinstance(s, bytes):\\n        try:\\n            return s.decode('utf_8')\\n        except UnicodeDecodeError:\\n            return s.decode('latin_1')\\n    elif isinstance(s, string_types):  # e.g. is unicode\\n        return s\\n    else:\\n        raise TypeError",
                    "func_fullName": "mrjob.py2.to_unicode( s )"
                }
            ]
        },
        {
            "cluster_id": 0,
            "feature_id": 34,
            "feature_desc": "gamma=0.0165; k=5; a=0.25; combined=0.396; stability(ARI)=0.973; sep=0.068",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 665,
                    "func_name": "find_mrjob_conf",
                    "func_desc": "find_mrjob_conf",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def find_mrjob_conf():\\n    \"\"\"Look for :file:`mrjob.conf`, and return its path. Places we look:\\n\\n    - The location specified by :envvar:`MRJOB_CONF`\\n    - :file:`~/.mrjob.conf`\\n    - :file:`/etc/mrjob.conf`\\n\\n    Return ``None`` if we can't find it.\\n    \"\"\"\\n    def candidates():\\n        if 'MRJOB_CONF' in os.environ:\\n            yield expand_path(os.environ['MRJOB_CONF'])\\n\\n        # $HOME isn't necessarily set on Windows, but ~ works\\n        # use os.path.join() so we don't end up mixing \\ and /\\n        yield expand_path(os.path.join('~', '.mrjob.conf'))\\n\\n        # this only really makes sense on Unix, so no os.path.join()\\n        yield '/etc/mrjob.conf'\\n\\n    for path in candidates():\\n        log.debug('Looking for configs in %s' % path)\\n        if os.path.exists(path):\\n            log.info('Using configs in %s' % path)\\n            return path\\n    else:\\n        log.info('No configs found; falling back on auto-configuration')\\n        return None",
                    "func_fullName": "mrjob.conf.find_mrjob_conf(  )"
                },
                {
                    "func_id": 666,
                    "func_name": "_expanded_mrjob_conf_path",
                    "func_desc": "_expanded_mrjob_conf_path",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _expanded_mrjob_conf_path(conf_path=None):\\n    \"\"\"Return the path of a single conf file. If *conf_path* is ``False``,\\n    return ``None``, and if it's ``None``, return :py:func:`find_mrjob_conf`.\\n    Otherwise, expand environment variables and ``~`` in *conf_path* and\\n    return it.\\n\\n    Confusingly, this function doesn't actually return a \"real\" path according\\n    to ``os.path.realpath()``; it just resolves environment variables and\\n    ``~``.\\n    \"\"\"\\n    if conf_path is False:\\n        return None\\n    elif conf_path is None:\\n        return find_mrjob_conf()\\n    else:\\n        return expand_path(conf_path)",
                    "func_fullName": "mrjob.conf._expanded_mrjob_conf_path( conf_path )"
                },
                {
                    "func_id": 672,
                    "func_name": "_load_yaml_with_clear_tag",
                    "func_desc": "_load_yaml_with_clear_tag",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _load_yaml_with_clear_tag(stream):\\n    \"\"\"Like yaml.safe_load(), but everything with a !clear tag before it\\n    will be wrapped in ClearedValue().\"\"\"\\n    loader = yaml.SafeLoader(stream)\\n    loader.add_constructor('!clear', _cleared_value_constructor)\\n    try:\\n        return loader.get_single_data()\\n    finally:\\n        if hasattr(loader, 'dispose'):  # it doesn't in PyYAML 3.09\\n            loader.dispose()",
                    "func_fullName": "mrjob.conf._load_yaml_with_clear_tag( stream )"
                },
                {
                    "func_id": 673,
                    "func_name": "_cleared_value_representer",
                    "func_desc": "_cleared_value_representer",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _cleared_value_representer(dumper, data):\\n    if not isinstance(data, ClearedValue):\\n        raise TypeError\\n    node = dumper.represent_data(data.value)\\n    node.tag = '!clear'\\n    return node",
                    "func_fullName": "mrjob.conf._cleared_value_representer( dumper, data )"
                },
                {
                    "func_id": 674,
                    "func_name": "_dump_yaml_with_clear_tags",
                    "func_desc": "_dump_yaml_with_clear_tags",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _dump_yaml_with_clear_tags(data, stream=None, **kwds):\\n    class ClearedValueSafeDumper(yaml.SafeDumper):\\n        pass\\n\\n    ClearedValueSafeDumper.add_representer(\\n        ClearedValue, _cleared_value_representer)\\n\\n    return yaml.dump_all([data], stream, Dumper=ClearedValueSafeDumper, **kwds)",
                    "func_fullName": "mrjob.conf._dump_yaml_with_clear_tags( data, stream, **kwds )"
                },
                {
                    "func_id": 675,
                    "func_name": "_fix_clear_tags",
                    "func_desc": "_fix_clear_tags",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _fix_clear_tags(x):\\n    \"\"\"Recursively resolve :py:class:`ClearedValue` wrappers so that\\n    ``ClearedValue(...)`` can only wrap values in dicts (and in the top-level\\n    value we return).\\n\\n    In dicts, we treat ``ClearedValue(k): v`` or\\n    ``ClearedValue(k): ClearedValue(v)`` as equivalent to\\n    ``k: ClearedValue(v)``. ``ClearedValue(k): v1`` overrides ``k: v2``.\\n\\n    In lists, any ClearedValue wrappers are simply stripped.\\n    \"\"\"\\n    _fix = _fix_clear_tags\\n\\n    if isinstance(x, list):\\n        return [_fix(_strip_clear_tag(item)) for item in x]\\n\\n    elif isinstance(x, dict):\\n        d = dict((_fix(k), _fix(v)) for k, v in x.items())\\n\\n        # handle cleared keys\\n        for k, v in list(d.items()):\\n            if isinstance(k, ClearedValue):\\n                del d[k]\\n                d[_strip_clear_tag(k)] = ClearedValue(_strip_clear_tag(v))\\n\\n        return d\\n\\n    elif isinstance(x, ClearedValue):\\n        return ClearedValue(_fix(x.value))\\n\\n    else:\\n        return x",
                    "func_fullName": "mrjob.conf._fix_clear_tags( x )"
                },
                {
                    "func_id": 676,
                    "func_name": "_resolve_clear_tags_in_list",
                    "func_desc": "_resolve_clear_tags_in_list",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _resolve_clear_tags_in_list(items):\\n    \"\"\"Create a list from *items*. If we encounter a :py:class:`ClearedValue`,\\n    unwrap it and ignore previous values. Used by ``combine_*()`` functions\\n    to combine lists of values.\\n    \"\"\"\\n    result = []\\n\\n    for item in items:\\n        if isinstance(item, ClearedValue):\\n            result = [item.value]\\n        else:\\n            result.append(item)\\n\\n    return result",
                    "func_fullName": "mrjob.conf._resolve_clear_tags_in_list( items )"
                },
                {
                    "func_id": 677,
                    "func_name": "_strip_clear_tag",
                    "func_desc": "_strip_clear_tag",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _strip_clear_tag(v):\\n    \"\"\"remove the clear tag from the given value.\"\"\"\\n    if isinstance(v, ClearedValue):\\n        return v.value\\n    else:\\n        return v",
                    "func_fullName": "mrjob.conf._strip_clear_tag( v )"
                },
                {
                    "func_id": 678,
                    "func_name": "_conf_object_at_path",
                    "func_desc": "_conf_object_at_path",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _conf_object_at_path(conf_path):\\n    if conf_path is None:\\n        return None\\n\\n    with open(conf_path) as f:\\n        if yaml:\\n            return _fix_clear_tags(_load_yaml_with_clear_tag(f))\\n        else:\\n            try:\\n                return json.load(f)\\n            except ValueError as e:\\n                raise ValueError(\\n                    'Could not read JSON from %s\\n  %s\\n\\n'\\n                    'If your conf file is in YAML, you need to'\\n                    ' `pip install PyYAML` to read it' % (conf_path, str(e)))",
                    "func_fullName": "mrjob.conf._conf_object_at_path( conf_path )"
                },
                {
                    "func_id": 679,
                    "func_name": "load_opts_from_mrjob_conf",
                    "func_desc": "load_opts_from_mrjob_conf",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def load_opts_from_mrjob_conf(runner_alias, conf_path=None,\\n                              already_loaded=None):\\n    \"\"\"Load a list of dictionaries representing the options in a given\\n    mrjob.conf for a specific runner, resolving includes. Returns\\n    ``[(path, values)]``. If *conf_path* is not found, return ``[(None, {})]``.\\n\\n    :type runner_alias: str\\n    :param runner_alias: String identifier of the runner type, e.g. ``emr``,\\n                         ``local``, etc.\\n    :type conf_path: str\\n    :param conf_path: location of the file to load\\n    :type already_loaded: list\\n    :param already_loaded: list of real (according to ``os.path.realpath()``)\\n                           conf paths that have already\\n                           been loaded (used by\\n                           :py:func:`load_opts_from_mrjob_confs`).\\n\\n    Relative ``include:`` paths are relative to the real (after resolving\\n    symlinks) path of the including conf file\\n\\n    This will only load each config file once, even if it's referenced\\n    from multiple paths due to symlinks.\\n    \"\"\"\\n    if already_loaded is None:\\n        already_loaded = []\\n\\n    conf_path = _expanded_mrjob_conf_path(conf_path)\\n    return _load_opts_from_mrjob_conf(runner_alias, conf_path, already_loaded)",
                    "func_fullName": "mrjob.conf.load_opts_from_mrjob_conf( runner_alias, conf_path, already_loaded )"
                },
                {
                    "func_id": 680,
                    "func_name": "_load_opts_from_mrjob_conf",
                    "func_desc": "_load_opts_from_mrjob_conf",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _load_opts_from_mrjob_conf(runner_alias, conf_path, already_loaded):\\n    \"\"\"Helper for :py:func:`load_opts_from_mrjob_conf` for recursive use.\\n    This doesn't expand or default *conf_path*.\\n    \"\"\"\\n    conf = _conf_object_at_path(conf_path)\\n\\n    if conf is None:\\n        return [(None, {})]\\n\\n    # don't load same conf file twice\\n    real_conf_path = os.path.realpath(conf_path)\\n\\n    if real_conf_path in already_loaded:\\n        return []\\n    else:\\n        already_loaded.append(real_conf_path)\\n\\n    # get configs for our runner out of conf file\\n    try:\\n        values = conf['runners'][runner_alias] or {}\\n    except (KeyError, TypeError, ValueError):\\n        values = {}\\n\\n    inherited = []\\n    if conf.get('include', None):\\n        includes = conf['include']\\n        if isinstance(includes, string_types):\\n            includes = [includes]\\n\\n        # handle includes in reverse order so that include order takes\\n        # precedence over inheritance\\n        for include in reversed(includes):\\n            # make include relative to (real) conf_path (see #1166)\\n            # expand ~ *before* joining to dir of including file (see #1308)\\n            include = os.path.join(os.path.dirname(real_conf_path),\\n                                   expand_path(include))\\n\\n            inherited = _load_opts_from_mrjob_conf(\\n                runner_alias, include, already_loaded) + inherited\\n\\n    return inherited + [(conf_path, values)]",
                    "func_fullName": "mrjob.conf._load_opts_from_mrjob_conf( runner_alias, conf_path, already_loaded )"
                },
                {
                    "func_id": 681,
                    "func_name": "load_opts_from_mrjob_confs",
                    "func_desc": "load_opts_from_mrjob_confs",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def load_opts_from_mrjob_confs(runner_alias, conf_paths=None):\\n    \"\"\"Load a list of dictionaries representing the options in a given\\n    list of mrjob config files for a specific runner. Returns\\n    ``[(path, values), ...]``. If a path is not found, use ``(None, {})`` as\\n    its value.\\n\\n    If *conf_paths* is ``None``, look for a config file in the default\\n    locations (see :py:func:`find_mrjob_conf`).\\n\\n    :type runner_alias: str\\n    :param runner_alias: String identifier of the runner type, e.g. ``emr``,\\n                         ``local``, etc.\\n    :type conf_paths: list or ``None``\\n    :param conf_path: locations of the files to load\\n\\n    This will only load each config file once, even if it's referenced\\n    from multiple paths due to symlinks.\\n    \"\"\"\\n    if conf_paths is None:\\n        results = load_opts_from_mrjob_conf(runner_alias)\\n    else:\\n        # don't include conf files that were loaded earlier in conf_paths\\n        already_loaded = []\\n\\n        # load configs in reversed order so that order of conf paths takes\\n        # precedence over inheritance\\n        results = []\\n\\n        for path in reversed(conf_paths):\\n            results = load_opts_from_mrjob_conf(\\n                runner_alias, path, already_loaded=already_loaded) + results\\n\\n    if runner_alias and not any(conf for path, conf in results):\\n        log.warning('No configs specified for %s runner' % runner_alias)\\n\\n    return results",
                    "func_fullName": "mrjob.conf.load_opts_from_mrjob_confs( runner_alias, conf_paths )"
                },
                {
                    "func_id": 682,
                    "func_name": "dump_mrjob_conf",
                    "func_desc": "dump_mrjob_conf",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def dump_mrjob_conf(conf, f):\\n    \"\"\"Write out configuration options to a file.\\n\\n    Useful if you don't want to bother to figure out YAML.\\n\\n    *conf* should look something like this:\\n\\n        {'runners':\\n            'local': {'OPTION': VALUE, ...}\\n            'emr': {'OPTION': VALUE, ...}\\n            'hadoop: {'OPTION': VALUE, ...}\\n        }\\n\\n    :param f: a file object to write to (e.g. ``open('mrjob.conf', 'w')``)\\n    \"\"\"\\n    if yaml:\\n        _dump_yaml_with_clear_tags(conf, f, default_flow_style=False)\\n    else:\\n        json.dump(conf, f, indent=2)\\n    f.flush()",
                    "func_fullName": "mrjob.conf.dump_mrjob_conf( conf, f )"
                },
                {
                    "func_id": 683,
                    "func_name": "combine_values",
                    "func_desc": "combine_values",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def combine_values(*values):\\n    \"\"\"Return the last value in *values* that is not ``None``.\\n\\n    The default combiner; good for simple values (booleans, strings, numbers).\\n    \"\"\"\\n    for v in reversed(values):\\n        if v is not None:\\n            return v\\n    else:\\n        return None",
                    "func_fullName": "mrjob.conf.combine_values( *values )"
                },
                {
                    "func_id": 684,
                    "func_name": "combine_lists",
                    "func_desc": "combine_lists",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def combine_lists(*seqs):\\n    \"\"\"Concatenate the given sequences into a list. Ignore ``None`` values.\\n\\n    Generally this is used for a list of commands we want to run; the\\n    \"default\" commands get run before any commands specific to your job.\\n\\n    Strings, bytes, and non-sequence objects (e.g. numbers) are treated as\\n    single-item lists.\\n    \"\"\"\\n    result = []\\n\\n    for seq in seqs:\\n        if seq is None:\\n            continue\\n\\n        if isinstance(seq, (bytes, string_types, dict)):\\n            result.append(seq)\\n        else:\\n            try:\\n                result.extend(seq)\\n            except:\\n                result.append(seq)\\n\\n    return result",
                    "func_fullName": "mrjob.conf.combine_lists( *seqs )"
                },
                {
                    "func_id": 685,
                    "func_name": "combine_cmds",
                    "func_desc": "combine_cmds",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def combine_cmds(*cmds):\\n    \"\"\"Take zero or more commands to run on the command line, and return\\n    the last one that is not ``None``. Each command should either be a list\\n    containing the command plus switches, or a string, which will be parsed\\n    with :py:func:`shlex.split`. The string must either be a byte string or a\\n    unicode string containing no non-ASCII characters.\\n\\n    Returns either ``None`` or a list containing the command plus arguments.\\n    \"\"\"\\n    cmd = combine_values(*cmds)\\n\\n    if cmd is None:\\n        return None\\n    elif isinstance(cmd, string_types):\\n        return shlex_split(cmd)\\n    else:\\n        return list(cmd)",
                    "func_fullName": "mrjob.conf.combine_cmds( *cmds )"
                },
                {
                    "func_id": 686,
                    "func_name": "combine_dicts",
                    "func_desc": "combine_dicts",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def combine_dicts(*dicts):\\n    \"\"\"Combine zero or more dictionaries. Values from dicts later in the list\\n    take precedence over values earlier in the list.\\n\\n    If you pass in ``None`` in place of a dictionary, it will be ignored.\\n    \"\"\"\\n    result = {}\\n\\n    for d in dicts:\\n        if d:\\n            for k, v in d.items():\\n                # delete cleared key\\n                if isinstance(v, ClearedValue) and v.value is None:\\n                    result.pop(k, None)\\n\\n                # just set the value\\n                else:\\n                    result[k] = _strip_clear_tag(v)\\n\\n    return result",
                    "func_fullName": "mrjob.conf.combine_dicts( *dicts )"
                },
                {
                    "func_id": 687,
                    "func_name": "combine_envs",
                    "func_desc": "combine_envs",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def combine_envs(*envs):\\n    \"\"\"Combine zero or more dictionaries containing environment variables.\\n    Environment variable values may be wrapped in :py:class:`ClearedValue`.\\n\\n    Environment variables later from dictionaries later in the list take\\n    priority over those earlier in the list.\\n\\n    For variables ending with ``PATH``, we prepend (and add a colon) rather\\n    than overwriting. Wrapping a path value in :py:class:`ClearedValue`\\n    disables this behavior.\\n\\n    Environment set to ``ClearedValue(None)`` will *delete* environment\\n    variables earlier in the list, rather than setting them to ``None``.\\n\\n    If you pass in ``None`` in place of a dictionary in **envs**, it will be\\n    ignored.\\n    \"\"\"\\n    return _combine_envs_helper(envs, local=False)",
                    "func_fullName": "mrjob.conf.combine_envs( *envs )"
                },
                {
                    "func_id": 688,
                    "func_name": "combine_local_envs",
                    "func_desc": "combine_local_envs",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def combine_local_envs(*envs):\\n    \"\"\"Same as :py:func:`combine_envs`, except that paths are combined\\n    using the local path separator (e.g ``;`` on Windows rather than ``:``).\\n    \"\"\"\\n    return _combine_envs_helper(envs, local=True)",
                    "func_fullName": "mrjob.conf.combine_local_envs( *envs )"
                },
                {
                    "func_id": 689,
                    "func_name": "_combine_envs_helper",
                    "func_desc": "_combine_envs_helper",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _combine_envs_helper(envs, local):\\n    if local:\\n        pathsep = os.pathsep\\n    else:\\n        pathsep = ':'\\n\\n    result = {}\\n    for env in envs:\\n        if env:\\n            for k, v in env.items():\\n                # delete cleared keys\\n                if isinstance(v, ClearedValue) and v.value is None:\\n                    result.pop(k, None)\\n\\n                # append paths\\n                elif (k.endswith('PATH') and result.get(k) and\\n                      not isinstance(v, ClearedValue)):\\n                    result[k] = v + pathsep + result[k]\\n\\n                # just set the value\\n                else:\\n                    result[k] = _strip_clear_tag(v)\\n\\n    return result",
                    "func_fullName": "mrjob.conf._combine_envs_helper( envs, local )"
                },
                {
                    "func_id": 690,
                    "func_name": "combine_jobconfs",
                    "func_desc": "combine_jobconfs",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def combine_jobconfs(*jobconfs):\\n    \"\"\"Like combine_dicts(), but non-string values are converted to\\n    Java-readable string (e.g. True becomes 'true'). Keys whose\\n    value is None are blanked out.\"\"\"\\n    j = combine_dicts(*jobconfs)\\n\\n    return {k: _to_java_str(v) for k, v in j.items() if v is not None}",
                    "func_fullName": "mrjob.conf.combine_jobconfs( *jobconfs )"
                },
                {
                    "func_id": 691,
                    "func_name": "combine_paths",
                    "func_desc": "combine_paths",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def combine_paths(*paths):\\n    \"\"\"Returns the last value in *paths* that is not ``None``.\\n    Resolve ``~`` (home dir) and environment variables.\"\"\"\\n    return expand_path(combine_values(*paths))",
                    "func_fullName": "mrjob.conf.combine_paths( *paths )"
                },
                {
                    "func_id": 692,
                    "func_name": "combine_path_lists",
                    "func_desc": "combine_path_lists",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def combine_path_lists(*path_seqs):\\n    \"\"\"Concatenate the given sequences into a list. Ignore None values.\\n    Resolve ``~`` (home dir) and environment variables, and expand globs\\n    that refer to the local filesystem.\\n\\n    Can take single strings as well as lists.\\n    \"\"\"\\n    results = []\\n\\n    for path in combine_lists(*path_seqs):\\n        expanded = expand_path(path)\\n        # if we can't expand a glob, leave as-is (maybe it refers to\\n        # S3 or HDFS)\\n        paths = sorted(glob.glob(expanded)) or [expanded]\\n\\n        results.extend(paths)\\n\\n    return results",
                    "func_fullName": "mrjob.conf.combine_path_lists( *path_seqs )"
                },
                {
                    "func_id": 693,
                    "func_name": "combine_opts",
                    "func_desc": "combine_opts",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def combine_opts(combiners, *opts_list):\\n    \"\"\"The master combiner, used to combine dictionaries of options with\\n    appropriate sub-combiners.\\n\\n    :param combiners: a map from option name to a combine_*() function to\\n                      combine options by that name. By default, we combine\\n                      options using :py:func:`combine_values`.\\n    :param opts_list: one or more dictionaries to combine\\n\\n    The dict in *opts_list* may not be wrapped in :py:class:`ClearedValue`,\\n    but their values may, in which case values of that key from previous\\n    opt dicts will be ignored.\\n    \"\"\"\\n    final_opts = {}\\n\\n    keys = set()\\n    for opts in opts_list:\\n        if isinstance(opts, ClearedValue):\\n            raise TypeError\\n        elif opts:\\n            keys.update(opts)\\n\\n    for key in keys:\\n        values = _resolve_clear_tags_in_list(\\n            opts[key] for opts in opts_list if opts and key in opts)\\n\\n        combine_func = combiners.get(key) or combine_values\\n        final_opts[key] = combine_func(*values)\\n\\n    return final_opts",
                    "func_fullName": "mrjob.conf.combine_opts( combiners, *opts_list )"
                },
                {
                    "func_id": 1933,
                    "func_name": "mapper_final",
                    "func_desc": "mapper_final",
                    "func_file": "mr_wc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper_final(self):\\n        yield('chars', self.chars)\\n        yield('words', self.words)\\n        yield('lines', self.lines)",
                    "func_fullName": "mrjob.examples.mr_wc.mapper_final( self )"
                },
                {
                    "func_id": 2036,
                    "func_name": "resolve_pending_xref",
                    "func_desc": "resolve_pending_xref",
                    "func_file": "options_extension",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def resolve_pending_xref(app, fromdocname, node):\\n    # Based on nodes.py in Sphinx. Resolves a subset of possible pending_xref\\n    # nodes that we see in practice in the config reference table. Uses only\\n    # public methods (afaict the proper API, zero hacks).\\n\\n    # This function had to be moved over because we generate the option list\\n    # markup pretty late, and Sphinx apparently doesn't expose a way to resolve\\n    # refs at that point, even though it knows where all the refs should point\\n    # to.\\n\\n    if 'refdomain' in node and node['refdomain']:\\n        domain = None\\n        contnode = node[0].deepcopy()\\n\\n        builder = app.builder\\n        env = app.builder.env\\n        try:\\n            domain = env.domains[node['refdomain']]\\n        except KeyError:\\n            raise MRJobOptError('could not resolve domain for %s' % node)\\n        newnode = domain.resolve_xref(\\n            app, fromdocname, builder, node['reftype'], node['reftarget'],\\n            node, contnode)\\n        if newnode:\\n            return [newnode]\\n        else:\\n            # this reference can't be resolved, but that's probably because\\n            # it's an 'optional link' like an :envvar: with no definition in\\n            # the docs.\\n            return node.children\\n    else:\\n        return node.children",
                    "func_fullName": "docs.options_extension.resolve_pending_xref( app, fromdocname, node )"
                },
                {
                    "func_id": 2037,
                    "func_name": "resolve_possible_pending_xrefs",
                    "func_desc": "resolve_possible_pending_xrefs",
                    "func_file": "options_extension",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def resolve_possible_pending_xrefs(app, fromdocname, maybe_xrefs):\\n    \"\"\"If any node is a pending_xref, attempt to resolve it. If it cannot be\\n    resolved, replace it with its children.\\n    \"\"\"\\n    result = []\\n    for node in maybe_xrefs:\\n        if isinstance(node, addnodes.pending_xref):\\n            result.extend(resolve_pending_xref(\\n                app, fromdocname, node.deepcopy()))\\n        else:\\n            result.append(node)\\n    return result",
                    "func_fullName": "docs.options_extension.resolve_possible_pending_xrefs( app, fromdocname, maybe_xrefs )"
                },
                {
                    "func_id": 2041,
                    "func_name": "mrjob_opt_role",
                    "func_desc": "mrjob_opt_role",
                    "func_file": "options_extension",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def mrjob_opt_role(role, rawtext, text, lineno, inliner,\\n        options={}, content=[]):\\n    return [optionlink(text=text, option_info_key=text)], []",
                    "func_fullName": "docs.options_extension.mrjob_opt_role( role, rawtext, text, lineno, inliner, options, content )"
                },
                {
                    "func_id": 2044,
                    "func_name": "purge_options",
                    "func_desc": "purge_options",
                    "func_file": "options_extension",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def purge_options(app, env, docname):\\n    \"\"\"Clear our data from the environment when necessary\"\"\"\\n    if not hasattr(env, 'optionlist_all_options'):\\n        return\\n    env.optionlist_all_options = [\\n        option for option in env.optionlist_all_options\\n        if option['docname'] != docname]\\n    env.optionlist_indexed_options = dict([\\n        (option['options']['config'], option)\\n        for option in env.optionlist_all_options\\n    ])",
                    "func_fullName": "docs.options_extension.purge_options( app, env, docname )"
                },
                {
                    "func_id": 2045,
                    "func_name": "populate_option_lists",
                    "func_desc": "populate_option_lists",
                    "func_file": "options_extension",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def populate_option_lists(app, doctree, fromdocname):\\n    env = app.builder.env\\n\\n    for node in doctree.traverse(optionlist):\\n        # see parsers/rst/states.py, build_table()\\n        # it's a mess and so is this\\n        table = nodes.table()\\n\\n        # make the header block, I swear it's not my fault it's so convoluted\\n        tgroup = nodes.tgroup(cols=4)\\n        table += tgroup\\n\\n        for i in range(4):\\n            tgroup += nodes.colspec(colwidth=1)\\n\\n        thead = nodes.thead()\\n        row = nodes.row()\\n        for label in ['Config', 'Command line', 'Default', 'Type']:\\n            attributes = {\\n                'morerows': 0,\\n                'morecols': 0,\\n                'stub': False,\\n            }\\n            par = nodes.paragraph()\\n            par.append(nodes.Text(label, label))\\n            entry = nodes.entry(**attributes)\\n            entry += par\\n            row += entry\\n        thead.append(row)\\n        tgroup += thead\\n\\n        tbody = nodes.tbody()\\n        tgroup += tbody\\n        # end of header block; whew\\n\\n        # filter and sort options for this table\\n        my_options = [oi for oi in env.optionlist_all_options\\n                      if oi['options']['set'] == node['option_set']]\\n\\n        # automagically alphabetical\\n        # probably can assume we always have a config option, but who knows\\n        # what the future holds?\\n        def sort_key(oi):\\n            if 'config' in oi['options']:\\n                return oi['options']['config']\\n            else:\\n                return oi['options']['switch'].lstrip('-')\\n\\n        my_options.sort(key=sort_key)\\n\\n        # table body\\n\\n        for option_info in my_options:\\n            row = nodes.row()\\n\\n            config_column = nodes.entry()\\n            switches_column = nodes.entry()\\n            default_column = nodes.entry()\\n            type_column = nodes.entry()\\n\\n            # make a stub node for us to replace after links have been\\n            # resolved. one of these for each config key and switch.\\n            def make_refnode(text):\\n                par = nodes.paragraph()\\n                ol = optionlink(\\n                    text, option_info_key=option_info['options']['config'])\\n                par.append(ol)\\n                return par\\n\\n            config_column.append(\\n                make_refnode(option_info['options']['config']))\\n            switches_column.append(\\n                make_refnode(option_info['options'].get('switch', '')))\\n\\n            par = nodes.paragraph()\\n            par.extend(resolve_possible_pending_xrefs(\\n                app, fromdocname, option_info['default_nodes']))\\n            default_column.append(par)\\n\\n            par = nodes.paragraph()\\n            par.extend(resolve_possible_pending_xrefs(\\n                app, fromdocname, option_info['type_nodes']))\\n            type_column.append(par)\\n\\n            row.extend([\\n                config_column,\\n                switches_column,\\n                default_column,\\n                type_column,\\n            ])\\n\\n            tbody.append(row)\\n\\n        node.replace_self([table])",
                    "func_fullName": "docs.options_extension.populate_option_lists( app, doctree, fromdocname )"
                },
                {
                    "func_id": 2046,
                    "func_name": "replace_optionlinks_with_links",
                    "func_desc": "replace_optionlinks_with_links",
                    "func_file": "options_extension",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def replace_optionlinks_with_links(app, doctree, fromdocname):\\n    # optionlink has attrs text, docname, target,\\n\\n    for node in doctree.traverse(optionlink):\\n        env = app.builder.env\\n\\n        k = node['option_info_key']\\n        try:\\n            option_info = env.optionlist_indexed_options[k]\\n        except KeyError:\\n            raise MRJobOptError(\"Unknown mrjob-opt %s\" % k)\\n\\n        refnode = nodes.reference('', '')\\n        innernode = nodes.emphasis(node.text, node.text)\\n        refnode['refdocname'] = option_info['docname']\\n        refnode['refuri'] = app.builder.get_relative_uri(\\n            fromdocname, option_info['docname'])\\n        refnode['refuri'] += '#' + option_info['target']['refid']\\n        refnode.append(innernode)\\n\\n        node.replace_self([refnode])",
                    "func_fullName": "docs.options_extension.replace_optionlinks_with_links( app, doctree, fromdocname )"
                },
                {
                    "func_id": 2047,
                    "func_name": "doctree_resolved",
                    "func_desc": "doctree_resolved",
                    "func_file": "options_extension",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def doctree_resolved(app, doctree, fromdocname):\\n        populate_option_lists(app, doctree, fromdocname)\\n        replace_optionlinks_with_links(app, doctree, fromdocname)",
                    "func_fullName": "docs.options_extension.doctree_resolved( app, doctree, fromdocname )"
                },
                {
                    "func_id": 2052,
                    "func_name": "make_refnode",
                    "func_desc": "make_refnode",
                    "func_file": "options_extension",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "            def make_refnode(text):\\n                par = nodes.paragraph()\\n                ol = optionlink(\\n                    text, option_info_key=option_info['options']['config'])\\n                par.append(ol)\\n                return par",
                    "func_fullName": "docs.options_extension.make_refnode( text )"
                },
                {
                    "func_id": 2145,
                    "func_name": "flush",
                    "func_desc": "flush",
                    "func_file": "warcwriter",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def flush(self):\\n        buff = self.compressor.flush()\\n        self.out.write(buff)\\n        self.out.flush()",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.warcwriter.flush( self )"
                },
                {
                    "func_id": 2226,
                    "func_name": "process_all",
                    "func_desc": "process_all",
                    "func_file": "checker",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def process_all(self):\\n        for filename in self.inputs:\\n            try:\\n                self.process_one(filename)\\n            except ArchiveLoadFailed as e:\\n                print(filename)\\n                print('  saw exception ArchiveLoadFailed: '+str(e).rstrip())\\n                print('  skipping rest of file')\\n                self.exit_value = 1\\n        return self.exit_value",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.checker.process_all( self )"
                },
                {
                    "func_id": 2292,
                    "func_name": "split_prefix",
                    "func_desc": "split_prefix",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def split_prefix(key, prefixs):\\n        \"\"\"\\n        split key string into prefix and remainder\\n        for first matching prefix from a list\\n        \"\"\"\\n        key_upper = key.upper()\\n        for prefix in prefixs:\\n            if key_upper.startswith(prefix):\\n                plen = len(prefix)\\n                return (key_upper[:plen], key[plen:])",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.split_prefix( key, prefixs )"
                },
                {
                    "func_id": 2307,
                    "func_name": "process_all",
                    "func_desc": "process_all",
                    "func_file": "indexer",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def process_all(self):\\n        with open_or_default(self.output, 'wt', sys.stdout) as out:\\n            for filename in self.inputs:\\n                try:\\n                    stdin = sys.stdin.buffer\\n                except AttributeError:  # py2\\n                    stdin = sys.stdin\\n                with open_or_default(filename, 'rb', stdin) as fh:\\n                    self.process_one(fh, out, filename)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.indexer.process_all( self )"
                },
                {
                    "func_id": 2368,
                    "func_name": "flush",
                    "func_desc": "flush",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def flush(self):\\n        return self.fp.flush()",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.flush( self )"
                }
            ]
        },
        {
            "cluster_id": 0,
            "feature_id": 35,
            "feature_desc": "gamma=0.0165; k=5; a=0.25; combined=0.396; stability(ARI)=0.973; sep=0.068",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 671,
                    "func_name": "_cleared_value_constructor",
                    "func_desc": "_cleared_value_constructor",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _cleared_value_constructor(loader, node):\\n    # tried construct_object(), got an unconstructable recursive node warning\\n    if isinstance(node, yaml.MappingNode):\\n        value = loader.construct_mapping(node)\\n    elif isinstance(node, yaml.ScalarNode):\\n        # resolve null as None, not u'null'\\n        value = yaml.safe_load(node.value)\\n    elif isinstance(node, yaml.SequenceNode):\\n        value = loader.construct_sequence(node)\\n    else:\\n        raise TypeError\\n\\n    return ClearedValue(value)",
                    "func_fullName": "mrjob.conf._cleared_value_constructor( loader, node )"
                },
                {
                    "func_id": 696,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, value):\\n        self.value = value",
                    "func_fullName": "mrjob.conf.__init__( self, value )"
                },
                {
                    "func_id": 697,
                    "func_name": "__eq__",
                    "func_desc": "__eq__",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __eq__(self, other):\\n        if isinstance(other, ClearedValue):\\n            return self.value == other.value\\n        else:\\n            return False",
                    "func_fullName": "mrjob.conf.__eq__( self, other )"
                },
                {
                    "func_id": 698,
                    "func_name": "__hash__",
                    "func_desc": "__hash__",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __hash__(self):\\n        return hash(self.value)",
                    "func_fullName": "mrjob.conf.__hash__( self )"
                },
                {
                    "func_id": 699,
                    "func_name": "__repr__",
                    "func_desc": "__repr__",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __repr__(self):\\n        return '%s(%s)' % (self.__class__.__name__, repr(self.value))",
                    "func_fullName": "mrjob.conf.__repr__( self )"
                },
                {
                    "func_id": 1024,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "retry",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, wrapped, retry_if,\\n                 backoff=_DEFAULT_BACKOFF,\\n                 multiplier=_DEFAULT_MULTIPLIER,\\n                 max_tries=_DEFAULT_MAX_TRIES,\\n                 max_backoff=_DEFAULT_MAX_BACKOFF,\\n                 unwrap_methods=()):\\n        \"\"\"\\n        Wrap the given object\\n\\n        :param wrapped: the object to wrap\\n        :param retry_if: a method that takes an exception, and returns whether\\n                         we should retry\\n        :type backoff: float\\n        :param backoff: the number of seconds to wait the first time we get a\\n                        retriable error\\n        :type multiplier: float\\n        :param multiplier: if we retry multiple times, the amount to multiply\\n                           the backoff time by every time we get an error\\n        :type max_tries: int\\n        :param max_tries: how many tries we get. ``0`` means to keep trying\\n                          forever\\n        :type max_backoff: float\\n        :param max_backoff: cap the backoff at this number of seconds\\n        :type unwrap_methods: sequence\\n        :param unwrap_methods: names of methods to call with this object as\\n                               *self* rather than retrying on transient\\n                               errors (e.g. methods that return a paginator)\\n        \"\"\"\\n        self.__wrapped = wrapped\\n\\n        self.__retry_if = retry_if\\n\\n        self.__backoff = backoff\\n        if self.__backoff <= 0:\\n            raise ValueError('backoff must be positive')\\n\\n        self.__multiplier = multiplier\\n        if self.__multiplier < 1:\\n            raise ValueError('multiplier must be at least one!')\\n\\n        self.__max_tries = max_tries\\n\\n        self.__max_backoff = max_backoff\\n\\n        self.__unwrap_methods = set(unwrap_methods)",
                    "func_fullName": "mrjob.retry.__init__( self, wrapped, retry_if, backoff, multiplier, max_tries, max_backoff, unwrap_methods )"
                },
                {
                    "func_id": 1025,
                    "func_name": "__getattr__",
                    "func_desc": "__getattr__",
                    "func_file": "retry",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __getattr__(self, name):\\n        \"\"\"The glue that makes functions retriable, and returns other\\n        attributes from the wrapped object as-is.\"\"\"\\n        x = getattr(self.__wrapped, name)\\n\\n        if name in self.__unwrap_methods:\\n            return partial(x.__func__, self)\\n        elif hasattr(x, '__call__'):\\n            return self.__wrap_method_with_call_and_maybe_retry(x)\\n        else:\\n            return x",
                    "func_fullName": "mrjob.retry.__getattr__( self, name )"
                },
                {
                    "func_id": 1931,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "mr_wc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, *args, **kwargs):\\n        super(MRWordCountUtility, self).__init__(*args, **kwargs)\\n        self.chars = 0\\n        self.words = 0\\n        self.lines = 0",
                    "func_fullName": "mrjob.examples.mr_wc.__init__( self, *args, **kwargs )"
                },
                {
                    "func_id": 1932,
                    "func_name": "mapper",
                    "func_desc": "mapper",
                    "func_file": "mr_wc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper(self, _, line):\\n        # Don't actually yield anything for each line. Instead, collect them\\n        # and yield the sums when all lines have been processed. The results\\n        # will be collected by the reducer.\\n        self.chars += len(line) + 1  # +1 for newline\\n        self.words += sum(1 for word in line.split() if word.strip())\\n        self.lines += 1",
                    "func_fullName": "mrjob.examples.mr_wc.mapper( self, _, line )"
                },
                {
                    "func_id": 1934,
                    "func_name": "reducer",
                    "func_desc": "reducer",
                    "func_file": "mr_wc",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def reducer(self, key, values):\\n        yield(key, sum(values))",
                    "func_fullName": "mrjob.examples.mr_wc.reducer( self, key, values )"
                },
                {
                    "func_id": 2035,
                    "func_name": "setup",
                    "func_desc": "setup",
                    "func_file": "options_extension",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def setup(app):\\n    app.add_node(optionlist)\\n    app.add_node(optionlink,\\n                 html=(visit_noop, depart_noop),\\n                 latex=(visit_noop, depart_noop),\\n                 text=(visit_noop, depart_noop))\\n    app.add_node(option,\\n                 html=(visit_noop, depart_noop),\\n                 latex=(visit_noop, depart_noop),\\n                 text=(visit_noop, depart_noop))\\n\\n    def doctree_resolved(app, doctree, fromdocname):\\n        populate_option_lists(app, doctree, fromdocname)\\n        replace_optionlinks_with_links(app, doctree, fromdocname)\\n\\n    app.add_directive('mrjob-opt', OptionDirective)\\n    app.add_directive('mrjob-optlist', OptionlistDirective)\\n    app.add_role('mrjob-opt', mrjob_opt_role)\\n    app.connect('doctree-resolved', doctree_resolved)\\n    app.connect('env-purge-doc', purge_options)",
                    "func_fullName": "docs.options_extension.setup( app )"
                },
                {
                    "func_id": 2048,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "options_extension",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, text, *args, **kwargs):\\n        super(optionlink, self).__init__(text, *args, **kwargs)\\n        self.text = text",
                    "func_fullName": "docs.options_extension.__init__( self, text, *args, **kwargs )"
                },
                {
                    "func_id": 2056,
                    "func_name": "try_brotli_init",
                    "func_desc": "try_brotli_init",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def try_brotli_init():\\n    try:\\n        import brotli\\n\\n        def brotli_decompressor():\\n            decomp = brotli.Decompressor()\\n            decomp.unused_data = None\\n            return decomp\\n\\n        BufferedReader.DECOMPRESSORS['br'] = brotli_decompressor\\n    except ImportError:  #pragma: no cover\\n        pass",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders.try_brotli_init(  )"
                },
                {
                    "func_id": 2077,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, stream, block_size=BUFF_SIZE,\\n                 decomp_type=None,\\n                 starting_data=None,\\n                 read_all_members=False):\\n\\n        self.stream = stream\\n        self.block_size = block_size\\n\\n        self._init_decomp(decomp_type)\\n\\n        self.buff = None\\n        self.starting_data = starting_data\\n        self.num_read = 0\\n        self.buff_size = 0\\n\\n        self.read_all_members = read_all_members",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders.__init__( self, stream, block_size, decomp_type, starting_data, read_all_members )"
                },
                {
                    "func_id": 2080,
                    "func_name": "_fillbuff",
                    "func_desc": "_fillbuff",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _fillbuff(self, block_size=None):\\n        if not self.empty():\\n            return\\n\\n        # can't read past next member\\n        if self.rem_length() > 0:\\n            return\\n\\n        block_size = block_size or self.block_size\\n\\n        if self.starting_data:\\n            data = self.starting_data\\n            self.starting_data = None\\n        else:\\n            data = self.stream.read(block_size)\\n\\n        self._process_read(data)\\n\\n        # if raw data is not empty and decompressor set, but\\n        # decompressed buff is empty, keep reading --\\n        # decompressor likely needs more data to decompress\\n        while data and self.decompressor and not self.decompressor.unused_data and self.empty():\\n            data = self.stream.read(block_size)\\n            self._process_read(data)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders._fillbuff( self, block_size )"
                },
                {
                    "func_id": 2086,
                    "func_name": "empty",
                    "func_desc": "empty",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def empty(self):\\n        if not self.buff or self.buff.tell() >= self.buff_size:\\n            # if reading all members, attempt to get next member automatically\\n            if self.read_all_members:\\n                self.read_next_member()\\n\\n            return True\\n\\n        return False",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders.empty( self )"
                },
                {
                    "func_id": 2092,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, *args, **kwargs):\\n        if 'decomp_type' not in kwargs:\\n            kwargs['decomp_type'] = 'gzip'\\n        super(DecompressingBufferedReader, self).__init__(*args, **kwargs)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders.__init__( self, *args, **kwargs )"
                },
                {
                    "func_id": 2093,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, msg, data=b''):\\n        Exception.__init__(self, msg)\\n        self.data = data",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders.__init__( self, msg, data )"
                },
                {
                    "func_id": 2094,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, stream, raise_exceptions=False, **kwargs):\\n        super(ChunkedDataReader, self).__init__(stream, **kwargs)\\n        self.all_chunks_read = False\\n        self.not_chunked = False\\n\\n        # if False, we'll use best-guess fallback for parse errors\\n        self.raise_chunked_data_exceptions = raise_exceptions",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders.__init__( self, stream, raise_exceptions, **kwargs )"
                },
                {
                    "func_id": 2095,
                    "func_name": "_fillbuff",
                    "func_desc": "_fillbuff",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _fillbuff(self, block_size=None):\\n        if self.not_chunked:\\n            return super(ChunkedDataReader, self)._fillbuff(block_size)\\n\\n        # Loop over chunks until there is some data (not empty())\\n        # In particular, gzipped data may require multiple chunks to\\n        # return any decompressed result\\n        while (self.empty() and\\n               not self.all_chunks_read and\\n               not self.not_chunked):\\n\\n            try:\\n                length_header = self.stream.readline(64)\\n                self._try_decode(length_header)\\n            except ChunkedDataException as e:\\n                if self.raise_chunked_data_exceptions:\\n                    raise\\n\\n                # Can't parse the data as chunked.\\n                # It's possible that non-chunked data is served\\n                # with a Transfer-Encoding: chunked.\\n                # Treat this as non-chunk encoded from here on.\\n                self._process_read(length_header + e.data)\\n                self.not_chunked = True\\n\\n                # parse as block as non-chunked\\n                return super(ChunkedDataReader, self)._fillbuff(block_size)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders._fillbuff( self, block_size )"
                },
                {
                    "func_id": 2111,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "recordbuilder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, warc_version=None, header_filter=None):\\n        self.warc_version = self._parse_warc_version(warc_version)\\n\\n        self.header_filter = header_filter",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordbuilder.__init__( self, warc_version, header_filter )"
                },
                {
                    "func_id": 2121,
                    "func_name": "_iter_stream",
                    "func_desc": "_iter_stream",
                    "func_file": "recordbuilder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _iter_stream(stream):\\n        while True:\\n            buf = stream.read(BUFF_SIZE)\\n            if not buf:\\n                return\\n\\n            yield buf",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordbuilder._iter_stream( stream )"
                },
                {
                    "func_id": 2138,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "warcwriter",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, gzip=True, *args, **kwargs):\\n        super(BaseWARCWriter, self).__init__(warc_version=kwargs.get('warc_version'),\\n                                             header_filter=kwargs.get('header_filter'))\\n        self.gzip = gzip\\n        self.hostname = gethostname()\\n\\n        self.parser = StatusAndHeadersParser([], verify=False)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.warcwriter.__init__( self, gzip, *args, **kwargs )"
                },
                {
                    "func_id": 2143,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "warcwriter",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, out):\\n        self.compressor = zlib.compressobj(9, zlib.DEFLATED, zlib.MAX_WBITS + 16)\\n        self.out = out",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.warcwriter.__init__( self, out )"
                },
                {
                    "func_id": 2146,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "warcwriter",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, filebuf, *args, **kwargs):\\n        super(WARCWriter, self).__init__(*args, **kwargs)\\n        self.out = filebuf",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.warcwriter.__init__( self, filebuf, *args, **kwargs )"
                },
                {
                    "func_id": 2149,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "warcwriter",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, *args, **kwargs):\\n        out = self._create_temp_file()\\n        super(BufferWARCWriter, self).__init__(out, *args, **kwargs)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.warcwriter.__init__( self, *args, **kwargs )"
                },
                {
                    "func_id": 2176,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "recordloader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, *args, **kwargs):\\n        (self.format, self.rec_type, self.rec_headers, self.raw_stream,\\n         self.http_headers, self.content_type, self.length) = args\\n        self.payload_length = kwargs.get('payload_length', -1)\\n        self.digest_checker = kwargs.get('digest_checker')",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordloader.__init__( self, *args, **kwargs )"
                },
                {
                    "func_id": 2178,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "recordloader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, verify_http=True, arc2warc=True):\\n        if arc2warc:\\n            self.arc_parser = ARC2WARCHeadersParser()\\n        else:\\n            self.arc_parser = ARCHeadersParser()\\n\\n        self.warc_parser = StatusAndHeadersParser(self.WARC_TYPES)\\n        self.http_parser = StatusAndHeadersParser(self.HTTP_TYPES, verify_http)\\n\\n        self.http_req_parser = StatusAndHeadersParser(self.HTTP_VERBS, verify_http)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordloader.__init__( self, verify_http, arc2warc )"
                },
                {
                    "func_id": 2185,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "recordloader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self):\\n        self.headernames = self.get_header_names()",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordloader.__init__( self )"
                },
                {
                    "func_id": 2205,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "digestverifyingreader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, kind=None):\\n        self._problem = []\\n        self._passed = None\\n        self.kind = kind",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.digestverifyingreader.__init__( self, kind )"
                },
                {
                    "func_id": 2208,
                    "func_name": "problems",
                    "func_desc": "problems",
                    "func_file": "digestverifyingreader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def problems(self):\\n        return self._problem",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.digestverifyingreader.problems( self )"
                },
                {
                    "func_id": 2209,
                    "func_name": "problem",
                    "func_desc": "problem",
                    "func_file": "digestverifyingreader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def problem(self, value, passed=False):\\n        self._problem.append(value)\\n        if self.kind == 'raise':\\n            raise ArchiveLoadFailed(value)\\n        if self.kind == 'log':\\n            sys.stderr.write(value + '\\n')\\n        self._passed = passed",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.digestverifyingreader.problem( self, value, passed )"
                },
                {
                    "func_id": 2210,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "digestverifyingreader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, stream, limit, digest_checker, record_type=None,\\n                 payload_digest=None, block_digest=None, segment_number=None):\\n\\n        super(DigestVerifyingReader, self).__init__(stream, limit)\\n\\n        self.digest_checker = digest_checker\\n\\n        if record_type == 'revisit':\\n            block_digest = None\\n            payload_digest = None\\n        if segment_number is not None:  #pragma: no cover\\n            payload_digest = None\\n\\n        self.payload_digest = payload_digest\\n        self.block_digest = block_digest\\n\\n        self.payload_digester = None\\n        self.payload_digester_obj = None\\n        self.block_digester = None\\n\\n        if block_digest:\\n            try:\\n                algo, _ = _parse_digest(block_digest)\\n                self.block_digester = Digester(algo)\\n            except ValueError:\\n                self.digest_checker.problem('unknown hash algorithm name in block digest')\\n                self.block_digester = None\\n        if payload_digest:\\n            try:\\n                algo, _ = _parse_digest(self.payload_digest)\\n                self.payload_digester_obj = Digester(algo)\\n            except ValueError:\\n                self.digest_checker.problem('unknown hash algorithm name in payload digest')\\n                self.payload_digester_obj = None",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.digestverifyingreader.__init__( self, stream, limit, digest_checker, record_type, payload_digest, block_digest, segment_number )"
                },
                {
                    "func_id": 2212,
                    "func_name": "_update",
                    "func_desc": "_update",
                    "func_file": "digestverifyingreader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _update(self, buff):\\n        super(DigestVerifyingReader, self)._update(buff)\\n\\n        if self.payload_digester:\\n            self.payload_digester.update(buff)\\n        if self.block_digester:\\n            self.block_digester.update(buff)\\n\\n        if self.limit == 0:\\n            check = _compare_digest_rfc_3548(self.block_digester, self.block_digest)\\n            if check is False:\\n                self.digest_checker.problem('block digest failed: {}'.format(self.block_digest))\\n            elif check is True and self.digest_checker.passed is not False:\\n                self.digest_checker.passed = True\\n            check = _compare_digest_rfc_3548(self.payload_digester, self.payload_digest)\\n            if check is False:\\n                self.digest_checker.problem('payload digest failed {}'.format(self.payload_digest))\\n            elif check is True and self.digest_checker.passed is not False:\\n                self.digest_checker.passed = True\\n\\n        return buff",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.digestverifyingreader._update( self, buff )"
                },
                {
                    "func_id": 2217,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "recompressor",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, filename, output, verbose=False):\\n        self.filename = filename\\n        self.output = output\\n        self.verbose = verbose",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recompressor.__init__( self, filename, output, verbose )"
                },
                {
                    "func_id": 2225,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "checker",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, cmd):\\n        self.inputs = cmd.inputs\\n        self.verbose = cmd.verbose\\n        self.exit_value = 0",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.checker.__init__( self, cmd )"
                },
                {
                    "func_id": 2227,
                    "func_name": "process_one",
                    "func_desc": "process_one",
                    "func_file": "checker",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def process_one(self, filename):\\n        printed_filename = False\\n        with open(filename, 'rb') as stream:\\n            it = ArchiveIterator(stream, check_digests=True)\\n            for record in it:\\n                digest_present = (record.rec_headers.get_header('WARC-Payload-Digest') or\\n                                  record.rec_headers.get_header('WARC-Block-Digest'))\\n\\n                _read_entire_stream(record.content_stream())\\n\\n                d_msg = None\\n                output = []\\n\\n                rec_id = record.rec_headers.get_header('WARC-Record-ID')\\n                rec_type = record.rec_headers.get_header('WARC-Type')\\n                rec_offset = it.get_record_offset()\\n\\n                if record.digest_checker.passed is False:\\n                    self.exit_value = 1\\n                    output = list(record.digest_checker.problems) \\n                elif record.digest_checker.passed is True and self.verbose:\\n                    d_msg = 'digest pass'\\n                elif record.digest_checker.passed is None and self.verbose:\\n                    if digest_present and rec_type == 'revisit':\\n                        d_msg = 'digest present but not checked (revisit)'\\n                    elif digest_present:  # pragma: no cover\\n                        # should not happen\\n                        d_msg = 'digest present but not checked'\\n                    else:\\n                        d_msg = 'no digest to check'\\n\\n                if d_msg or output:\\n                    if not printed_filename:\\n                        print(filename)\\n                        printed_filename = True\\n                    print(' ', 'offset', rec_offset, 'WARC-Record-ID', rec_id, rec_type)\\n                    if d_msg:\\n                        print('   ', d_msg)\\n                    for o in output:\\n                        print('   ', o)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.checker.process_one( self, filename )"
                },
                {
                    "func_id": 2271,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, statusline, headers, protocol='', total_len=0, is_http_request=False):\\n        if is_http_request:\\n            protocol, statusline = statusline.split(' ', 1)\\n\\n        self.statusline = statusline\\n        self.headers = headers_to_str_headers(headers)\\n        self.protocol = protocol\\n        self.total_len = total_len\\n        self.headers_buff = None",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.__init__( self, statusline, headers, protocol, total_len, is_http_request )"
                },
                {
                    "func_id": 2280,
                    "func_name": "__repr__",
                    "func_desc": "__repr__",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __repr__(self):\\n        return \"StatusAndHeaders(protocol = '{0}', statusline = '{1}', \\\\nheaders = {2})\".format(self.protocol, self.statusline, self.headers)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.__repr__( self )"
                },
                {
                    "func_id": 2281,
                    "func_name": "__ne__",
                    "func_desc": "__ne__",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __ne__(self, other):\\n        return not (self == other)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.__ne__( self, other )"
                },
                {
                    "func_id": 2282,
                    "func_name": "__eq__",
                    "func_desc": "__eq__",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __eq__(self, other):\\n        if not other:\\n            return False\\n\\n        return (self.statusline == other.statusline and\\n                self.headers == other.headers and\\n                self.protocol == other.protocol)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.__eq__( self, other )"
                },
                {
                    "func_id": 2283,
                    "func_name": "__str__",
                    "func_desc": "__str__",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __str__(self, exclude_list=None):\\n        return self.to_str(exclude_list)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.__str__( self, exclude_list )"
                },
                {
                    "func_id": 2284,
                    "func_name": "__bool__",
                    "func_desc": "__bool__",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __bool__(self):\\n        return bool(self.statusline or self.headers)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.__bool__( self )"
                },
                {
                    "func_id": 2289,
                    "func_name": "__contains__",
                    "func_desc": "__contains__",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __contains__(self, key):\\n        return bool(self[key])",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.__contains__( self, key )"
                },
                {
                    "func_id": 2290,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, statuslist, verify=True):\\n        self.statuslist = statuslist\\n        self.verify = verify",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.__init__( self, statuslist, verify )"
                },
                {
                    "func_id": 2295,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, msg, statusline):\\n        super(StatusAndHeadersParserException, self).__init__(msg)\\n        self.statusline = statusline",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.__init__( self, msg, statusline )"
                },
                {
                    "func_id": 2306,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "indexer",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, fields, inputs, output, verify_http=False):\\n        if isinstance(fields, str):\\n            fields = fields.split(',')\\n        self.fields = fields\\n        self.record_parse = any(field.startswith('http:') for field in self.fields)\\n\\n        self.inputs = inputs\\n        self.output = output\\n        self.verify_http = verify_http",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.indexer.__init__( self, fields, inputs, output, verify_http )"
                },
                {
                    "func_id": 2308,
                    "func_name": "process_one",
                    "func_desc": "process_one",
                    "func_file": "indexer",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def process_one(self, input_, output, filename):\\n        it = self._create_record_iter(input_)\\n\\n        self._write_header(output, filename)\\n\\n        for record in it:\\n            self.process_index_entry(it, record, filename, output)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.indexer.process_one( self, input_, output, filename )"
                },
                {
                    "func_id": 2309,
                    "func_name": "process_index_entry",
                    "func_desc": "process_index_entry",
                    "func_file": "indexer",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def process_index_entry(self, it, record, filename, output):\\n        index = self._new_dict(record)\\n\\n        for field in self.fields:\\n            value = self.get_field(record, field, it, filename)\\n\\n            if value is not None:\\n                field = self.field_names.get(field, field)\\n                index[field] = value\\n\\n        self._write_line(output, index, record, filename)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.indexer.process_index_entry( self, it, record, filename, output )"
                },
                {
                    "func_id": 2311,
                    "func_name": "_new_dict",
                    "func_desc": "_new_dict",
                    "func_file": "indexer",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _new_dict(self, record):\\n        return OrderedDict()",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.indexer._new_dict( self, record )"
                },
                {
                    "func_id": 2322,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "utils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, type_='sha1'):\\n        self.type_ = type_\\n        self.digester = hashlib.new(type_)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.utils.__init__( self, type_ )"
                },
                {
                    "func_id": 2324,
                    "func_name": "__str__",
                    "func_desc": "__str__",
                    "func_file": "utils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __str__(self):\\n        return self.type_ + ':' + to_native_str(base64.b32encode(self.digester.digest()))",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.utils.__str__( self )"
                },
                {
                    "func_id": 2332,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "limitreader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, stream, limit):\\n        self.stream = stream\\n        self.limit = limit\\n        self._orig_limit = limit",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.limitreader.__init__( self, stream, limit )"
                },
                {
                    "func_id": 2333,
                    "func_name": "_update",
                    "func_desc": "_update",
                    "func_file": "limitreader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _update(self, buff):\\n        length = len(buff)\\n        self.limit -= length\\n        return buff",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.limitreader._update( self, buff )"
                },
                {
                    "func_id": 2362,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, fp, recorder):\\n        self.fp = fp\\n        self.recorder = recorder\\n\\n        self.recorder.set_remote_ip(self._get_remote_ip())",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.__init__( self, fp, recorder )"
                },
                {
                    "func_id": 2369,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, recorder, *args, **kwargs):\\n        httplib.HTTPResponse.__init__(self, *args, **kwargs)\\n        self.fp = RecordingStream(self.fp, recorder)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.__init__( self, recorder, *args, **kwargs )"
                },
                {
                    "func_id": 2370,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, *args, **kwargs):\\n        orig_connection.__init__(self, *args, **kwargs)\\n        if hasattr(self.local, 'recorder'):\\n            self.recorder = self.local.recorder\\n        else:\\n            self.recorder = None\\n\\n        def make_recording_response(*args, **kwargs):\\n            return RecordingHTTPResponse(self.recorder, *args, **kwargs)\\n\\n        if self.recorder:\\n            self.response_class = make_recording_response",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.__init__( self, *args, **kwargs )"
                },
                {
                    "func_id": 2372,
                    "func_name": "_tunnel",
                    "func_desc": "_tunnel",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _tunnel(self, *args, **kwargs):\\n        if self.recorder:\\n            self.recorder.start_tunnel()\\n\\n        return orig_connection._tunnel(self, *args, **kwargs)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http._tunnel( self, *args, **kwargs )"
                },
                {
                    "func_id": 2374,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, writer, filter_func=None, record_ip=True):\\n        self.writer = writer\\n        self.filter_func = filter_func\\n        self.request_out = None\\n        self.response_out = None\\n        self.url = None\\n        self.connect_host = self.connect_port = None\\n        self.started_req = False\\n        self.first_line_read = False\\n        self.lock = threading.Lock()\\n        self.warc_headers = {}\\n        self.record_ip = record_ip",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.__init__( self, writer, filter_func, record_ip )"
                },
                {
                    "func_id": 2375,
                    "func_name": "start_tunnel",
                    "func_desc": "start_tunnel",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def start_tunnel(self):\\n        self.connect_host = self.connect_port = None\\n        self.started_req = False\\n        self.first_line_read = False",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.start_tunnel( self )"
                },
                {
                    "func_id": 2376,
                    "func_name": "start",
                    "func_desc": "start",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def start(self):\\n        self.request_out = self._create_buffer()\\n        self.response_out = self._create_buffer()\\n        self.url = None\\n        self.started_req = True\\n        self.first_line_read = False",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.start( self )"
                },
                {
                    "func_id": 2390,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "extractor",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, filename, offset):\\n        self.filename = filename\\n        self.offset = offset",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.extractor.__init__( self, filename, offset )"
                },
                {
                    "func_id": 2408,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "archiveiterator",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, fh):\\n        self.fh = fh\\n        self.offset = 0",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.archiveiterator.__init__( self, fh )"
                },
                {
                    "func_id": 2411,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "archiveiterator",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, fileobj, no_record_parse=False,\\n                 verify_http=False, arc2warc=False,\\n                 ensure_http_headers=False, block_size=BUFF_SIZE,\\n                 check_digests=False):\\n\\n        self.fh = fileobj\\n\\n        self.loader = ArcWarcRecordLoader(verify_http=verify_http,\\n                                          arc2warc=arc2warc)\\n        self.known_format = None\\n\\n        self.mixed_arc_warc = arc2warc\\n\\n        self.member_info = None\\n        self.no_record_parse = no_record_parse\\n        self.ensure_http_headers = ensure_http_headers\\n\\n        try:\\n            self.offset = self.fh.tell()\\n        except:\\n            self.fh = UnseekableYetTellable(self.fh)\\n            self.offset = self.fh.tell()\\n\\n        self.reader = DecompressingBufferedReader(self.fh,\\n                                                  block_size=block_size)\\n\\n        self.next_line = None\\n\\n        self.check_digests = check_digests\\n        self.err_count = 0\\n        self.record = None\\n\\n        self.the_iter = self._iterate_records()",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.archiveiterator.__init__( self, fileobj, no_record_parse, verify_http, arc2warc, ensure_http_headers, block_size, check_digests )"
                },
                {
                    "func_id": 2412,
                    "func_name": "__iter__",
                    "func_desc": "__iter__",
                    "func_file": "archiveiterator",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __iter__(self):\\n        return self.the_iter",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.archiveiterator.__iter__( self )"
                },
                {
                    "func_id": 2413,
                    "func_name": "__next__",
                    "func_desc": "__next__",
                    "func_file": "archiveiterator",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __next__(self):\\n        return six.next(self.the_iter)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.archiveiterator.__next__( self )"
                },
                {
                    "func_id": 2422,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "archiveiterator",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, *args, **kwargs):\\n        super(WARCIterator, self).__init__(*args, **kwargs)\\n        self.known_format = 'warc'",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.archiveiterator.__init__( self, *args, **kwargs )"
                },
                {
                    "func_id": 2423,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "archiveiterator",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, *args, **kwargs):\\n        super(ARCIterator, self).__init__(*args, **kwargs)\\n        self.known_format = 'arc'",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.archiveiterator.__init__( self, *args, **kwargs )"
                },
                {
                    "func_id": 2425,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "raw_json",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, encoded_json):\\n        self.encoded_json = encoded_json",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.raw_json.__init__( self, encoded_json )"
                },
                {
                    "func_id": 2482,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "ordered_dict",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, *args, **kwds):\\n        if len(args) > 1:\\n            raise TypeError('expected at most 1 arguments, got %d' % len(args))\\n        try:\\n            self.__end\\n        except AttributeError:\\n            self.clear()\\n        self.update(*args, **kwds)",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.ordered_dict.__init__( self, *args, **kwds )"
                },
                {
                    "func_id": 2484,
                    "func_name": "__setitem__",
                    "func_desc": "__setitem__",
                    "func_file": "ordered_dict",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __setitem__(self, key, value):\\n        if key not in self:\\n            end = self.__end\\n            curr = end[1]\\n            curr[2] = end[1] = self.__map[key] = [key, curr, end]\\n        dict.__setitem__(self, key, value)",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.ordered_dict.__setitem__( self, key, value )"
                },
                {
                    "func_id": 2485,
                    "func_name": "__delitem__",
                    "func_desc": "__delitem__",
                    "func_file": "ordered_dict",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __delitem__(self, key):\\n        dict.__delitem__(self, key)\\n        key, prev, next = self.__map.pop(key)\\n        prev[2] = next\\n        next[1] = prev",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.ordered_dict.__delitem__( self, key )"
                },
                {
                    "func_id": 2486,
                    "func_name": "__iter__",
                    "func_desc": "__iter__",
                    "func_file": "ordered_dict",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __iter__(self):\\n        end = self.__end\\n        curr = end[2]\\n        while curr is not end:\\n            yield curr[0]\\n            curr = curr[2]",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.ordered_dict.__iter__( self )"
                },
                {
                    "func_id": 2487,
                    "func_name": "__reversed__",
                    "func_desc": "__reversed__",
                    "func_file": "ordered_dict",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __reversed__(self):\\n        end = self.__end\\n        curr = end[1]\\n        while curr is not end:\\n            yield curr[0]\\n            curr = curr[1]",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.ordered_dict.__reversed__( self )"
                },
                {
                    "func_id": 2488,
                    "func_name": "popitem",
                    "func_desc": "popitem",
                    "func_file": "ordered_dict",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def popitem(self, last=True):\\n        if not self:\\n            raise KeyError('dictionary is empty')\\n        key = reversed(self).next() if last else iter(self).next()\\n        value = self.pop(key)\\n        return key, value",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.ordered_dict.popitem( self, last )"
                },
                {
                    "func_id": 2489,
                    "func_name": "__reduce__",
                    "func_desc": "__reduce__",
                    "func_file": "ordered_dict",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __reduce__(self):\\n        items = [[k, self[k]] for k in self]\\n        tmp = self.__map, self.__end\\n        del self.__map, self.__end\\n        inst_dict = vars(self).copy()\\n        self.__map, self.__end = tmp\\n        if inst_dict:\\n            return (self.__class__, (items,), inst_dict)\\n        return self.__class__, (items,)",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.ordered_dict.__reduce__( self )"
                },
                {
                    "func_id": 2490,
                    "func_name": "keys",
                    "func_desc": "keys",
                    "func_file": "ordered_dict",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def keys(self):\\n        return list(self)",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.ordered_dict.keys( self )"
                },
                {
                    "func_id": 2491,
                    "func_name": "__repr__",
                    "func_desc": "__repr__",
                    "func_file": "ordered_dict",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __repr__(self):\\n        if not self:\\n            return '%s()' % (self.__class__.__name__,)\\n        return '%s(%r)' % (self.__class__.__name__, self.items())",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.ordered_dict.__repr__( self )"
                },
                {
                    "func_id": 2493,
                    "func_name": "fromkeys",
                    "func_desc": "fromkeys",
                    "func_file": "ordered_dict",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def fromkeys(cls, iterable, value=None):\\n        d = cls()\\n        for key in iterable:\\n            d[key] = value\\n        return d",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.ordered_dict.fromkeys( cls, iterable, value )"
                },
                {
                    "func_id": 2494,
                    "func_name": "__eq__",
                    "func_desc": "__eq__",
                    "func_file": "ordered_dict",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __eq__(self, other):\\n        if isinstance(other, OrderedDict):\\n            return len(self)==len(other) and \\\\n                   all(p==q for p, q in  zip(self.items(), other.items()))\\n        return dict.__eq__(self, other)",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.ordered_dict.__eq__( self, other )"
                },
                {
                    "func_id": 2495,
                    "func_name": "__ne__",
                    "func_desc": "__ne__",
                    "func_file": "ordered_dict",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __ne__(self, other):\\n        return not self == other",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.ordered_dict.__ne__( self, other )"
                }
            ]
        },
        {
            "cluster_id": 0,
            "feature_id": 36,
            "feature_desc": "gamma=0.0165; k=5; a=0.25; combined=0.396; stability(ARI)=0.973; sep=0.068",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 694,
                    "func_name": "_to_java_str",
                    "func_desc": "_to_java_str",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _to_java_str(x):\\n    \"\"\"Convert a value (usually for a configuration property) into its\\n    Java string representation, falling back to the Python representation\\n    if None is available.\"\"\"\\n    # e.g. True -> 'true', None -> 'null'. See #323\\n    if isinstance(x, string_types):\\n        return x\\n    elif x is None:\\n        # Note: combine_jobconfs() blanks out keys with None values\\n        return 'null'\\n    elif isinstance(x, bool):\\n        return 'true' if x else 'false'\\n    else:\\n        return str(x)",
                    "func_fullName": "mrjob.conf._to_java_str( x )"
                },
                {
                    "func_id": 2053,
                    "func_name": "gzip_decompressor",
                    "func_desc": "gzip_decompressor",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def gzip_decompressor():\\n    \"\"\"\\n    Decompressor which can handle decompress gzip stream\\n    \"\"\"\\n    return zlib.decompressobj(16 + zlib.MAX_WBITS)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders.gzip_decompressor(  )"
                },
                {
                    "func_id": 2054,
                    "func_name": "deflate_decompressor",
                    "func_desc": "deflate_decompressor",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def deflate_decompressor():\\n    return zlib.decompressobj()",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders.deflate_decompressor(  )"
                },
                {
                    "func_id": 2055,
                    "func_name": "deflate_decompressor_alt",
                    "func_desc": "deflate_decompressor_alt",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def deflate_decompressor_alt():\\n    return zlib.decompressobj(-zlib.MAX_WBITS)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders.deflate_decompressor_alt(  )"
                },
                {
                    "func_id": 2078,
                    "func_name": "set_decomp",
                    "func_desc": "set_decomp",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def set_decomp(self, decomp_type):\\n        self._init_decomp(decomp_type)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders.set_decomp( self, decomp_type )"
                },
                {
                    "func_id": 2079,
                    "func_name": "_init_decomp",
                    "func_desc": "_init_decomp",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _init_decomp(self, decomp_type):\\n        self.num_block_read = 0\\n        if decomp_type:\\n            try:\\n                self.decomp_type = decomp_type\\n                self.decompressor = self.DECOMPRESSORS[decomp_type.lower()]()\\n            except KeyError:\\n                raise Exception('Decompression type not supported: ' +\\n                                decomp_type)\\n        else:\\n            self.decomp_type = None\\n            self.decompressor = None",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders._init_decomp( self, decomp_type )"
                },
                {
                    "func_id": 2082,
                    "func_name": "_decompress",
                    "func_desc": "_decompress",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _decompress(self, data):\\n        if self.decompressor and data:\\n            try:\\n                data = self.decompressor.decompress(data)\\n            except Exception as e:\\n                # if first read attempt, assume non-gzipped stream\\n                if self.num_block_read == 0:\\n                    if self.decomp_type == 'deflate':\\n                        self._init_decomp('deflate_alt')\\n                        data = self._decompress(data)\\n                    else:\\n                        self.decompressor = None\\n                # otherwise (partly decompressed), something is wrong\\n                else:\\n                    sys.stderr.write(str(e) + '\\n')\\n                    return b''\\n        return data",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders._decompress( self, data )"
                },
                {
                    "func_id": 2090,
                    "func_name": "close_decompressor",
                    "func_desc": "close_decompressor",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def close_decompressor(self):\\n        if self.decompressor:\\n            self.decompressor.flush()\\n            self.decompressor = None",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders.close_decompressor( self )"
                },
                {
                    "func_id": 2091,
                    "func_name": "get_supported_decompressors",
                    "func_desc": "get_supported_decompressors",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_supported_decompressors(cls):\\n        return cls.DECOMPRESSORS.keys()",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders.get_supported_decompressors( cls )"
                },
                {
                    "func_id": 2096,
                    "func_name": "_try_decode",
                    "func_desc": "_try_decode",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _try_decode(self, length_header):\\n        # decode length header\\n        try:\\n            # ensure line ends with \\r\\n\\n            assert(length_header[-2:] == b'\\r\\n')\\n            chunk_size = length_header[:-2].split(b';')[0]\\n            chunk_size = int(chunk_size, 16)\\n            # sanity check chunk size\\n            assert(chunk_size <= 2**31)\\n        except (ValueError, AssertionError):\\n            raise ChunkedDataException(b\"Couldn't decode length header \" +\\n                                       length_header)\\n\\n        if not chunk_size:\\n            # chunk_size 0 indicates end of file. read final bytes to compute digest.\\n            final_data = self.stream.read(2)\\n            try:\\n                assert(final_data == b'\\r\\n')\\n            except AssertionError:\\n                raise ChunkedDataException(b\"Incorrect \\r\\n after length header of 0\")\\n            self.all_chunks_read = True\\n            self._process_read(b'')\\n            return\\n\\n        data_len = 0\\n        data = b''\\n\\n        # read chunk\\n        while data_len < chunk_size:\\n            new_data = self.stream.read(chunk_size - data_len)\\n\\n            # if we unexpectedly run out of data,\\n            # either raise an exception or just stop reading,\\n            # assuming file was cut off\\n            if not new_data:\\n                if self.raise_chunked_data_exceptions:\\n                    msg = 'Ran out of data before end of chunk'\\n                    raise ChunkedDataException(msg, data)\\n                else:\\n                    chunk_size = data_len\\n                    self.all_chunks_read = True\\n\\n            data += new_data\\n            data_len = len(data)\\n\\n        # if we successfully read a block without running out,\\n        # it should end in \\r\\n\\n        if not self.all_chunks_read:\\n            clrf = self.stream.read(2)\\n            if clrf != b'\\r\\n':\\n                raise ChunkedDataException(b\"Chunk terminator not found.\",\\n                                           data)\\n\\n        # hand to base class for further processing\\n        self._process_read(data)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders._try_decode( self, length_header )"
                },
                {
                    "func_id": 2097,
                    "func_name": "brotli_decompressor",
                    "func_desc": "brotli_decompressor",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def brotli_decompressor():\\n            decomp = brotli.Decompressor()\\n            decomp.unused_data = None\\n            return decomp",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders.brotli_decompressor(  )"
                },
                {
                    "func_id": 2115,
                    "func_name": "_init_warc_headers",
                    "func_desc": "_init_warc_headers",
                    "func_file": "recordbuilder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _init_warc_headers(self, uri, record_type, warc_headers_dict):\\n        warc_headers = StatusAndHeaders('', list(warc_headers_dict.items()), protocol=self.warc_version)\\n        warc_headers.replace_header('WARC-Type', record_type)\\n        if not warc_headers.get_header('WARC-Record-ID'):\\n            warc_headers.add_header('WARC-Record-ID', self._make_warc_id())\\n\\n        if uri:\\n            warc_headers.replace_header('WARC-Target-URI', uri)\\n\\n        if not warc_headers.get_header('WARC-Date'):\\n            warc_headers.add_header('WARC-Date', self.curr_warc_date())\\n\\n        return warc_headers",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordbuilder._init_warc_headers( self, uri, record_type, warc_headers_dict )"
                },
                {
                    "func_id": 2120,
                    "func_name": "ensure_digest",
                    "func_desc": "ensure_digest",
                    "func_file": "recordbuilder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def ensure_digest(self, record, block=True, payload=True):\\n        if block:\\n            if record.rec_headers.get_header('WARC-Block-Digest'):\\n                block = False\\n\\n        if payload:\\n            if (record.rec_headers.get_header('WARC-Payload-Digest') or\\n                (record.rec_type in self.NO_PAYLOAD_DIGEST_TYPES)):\\n                payload = False\\n\\n        block_digester = self._create_digester() if block else None\\n        payload_digester = self._create_digester() if payload else None\\n\\n        has_length = (record.length is not None)\\n\\n        if not block_digester and not payload_digester and has_length:\\n            return\\n\\n        temp_file = None\\n        try:\\n            # force buffering if no length is set\\n            assert(has_length)\\n            pos = record.raw_stream.tell()\\n            record.raw_stream.seek(pos)\\n        except:\\n            pos = 0\\n            temp_file = self._create_temp_file()\\n\\n        if block_digester and record.http_headers:\\n            if not record.http_headers.headers_buff:\\n                record.http_headers.compute_headers_buffer(self.header_filter)\\n            block_digester.update(record.http_headers.headers_buff)\\n\\n        for buf in self._iter_stream(record.raw_stream):\\n            if block_digester:\\n                block_digester.update(buf)\\n\\n            if payload_digester:\\n                payload_digester.update(buf)\\n\\n            if temp_file:\\n                temp_file.write(buf)\\n\\n        if temp_file:\\n            record.payload_length = temp_file.tell()\\n            temp_file.seek(0)\\n            record._orig_stream = record.raw_stream\\n            record.raw_stream = temp_file\\n        else:\\n            record.raw_stream.seek(pos)\\n\\n        if payload_digester:\\n            record.rec_headers.add_header('WARC-Payload-Digest', str(payload_digester))\\n\\n        if block_digester:\\n            record.rec_headers.add_header('WARC-Block-Digest', str(block_digester))",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordbuilder.ensure_digest( self, record, block, payload )"
                },
                {
                    "func_id": 2122,
                    "func_name": "_create_digester",
                    "func_desc": "_create_digester",
                    "func_file": "recordbuilder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _create_digester():\\n        return Digester('sha1')",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordbuilder._create_digester(  )"
                },
                {
                    "func_id": 2150,
                    "func_name": "get_contents",
                    "func_desc": "get_contents",
                    "func_file": "warcwriter",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_contents(self):\\n        pos = self.out.tell()\\n        self.out.seek(0)\\n        buff = self.out.read()\\n        self.out.seek(pos)\\n        return buff",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.warcwriter.get_contents( self )"
                },
                {
                    "func_id": 2151,
                    "func_name": "get_stream",
                    "func_desc": "get_stream",
                    "func_file": "warcwriter",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_stream(self):\\n        self.out.seek(0)\\n        return self.out",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.warcwriter.get_stream( self )"
                },
                {
                    "func_id": 2180,
                    "func_name": "wrap_digest_verifying_stream",
                    "func_desc": "wrap_digest_verifying_stream",
                    "func_file": "recordloader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def wrap_digest_verifying_stream(self, stream, rec_type, rec_headers, digest_checker, length=None):\\n        payload_digest = rec_headers.get_header('WARC-Payload-Digest')\\n        block_digest = rec_headers.get_header('WARC-Block-Digest')\\n        segment_number = rec_headers.get_header('WARC-Segment-Number')\\n\\n        if not payload_digest and not block_digest:\\n            return stream, False\\n\\n        stream = DigestVerifyingReader(stream, length, digest_checker,\\n                                       record_type=rec_type,\\n                                       payload_digest=payload_digest,\\n                                       block_digest=block_digest,\\n                                       segment_number=segment_number)\\n        return stream, True",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordloader.wrap_digest_verifying_stream( self, stream, rec_type, rec_headers, digest_checker, length )"
                },
                {
                    "func_id": 2181,
                    "func_name": "load_http_headers",
                    "func_desc": "load_http_headers",
                    "func_file": "recordloader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def load_http_headers(self, rec_type, uri, stream, length):\\n        # only if length == 0 don't parse\\n        # try parsing is length is unknown (length is None) or length > 0\\n        if length == 0:\\n            return None\\n\\n        # only certain record types can have http headers\\n        if rec_type not in self.HTTP_RECORDS:\\n            return None\\n\\n        # only http:/https: uris can have http headers\\n        if not uri.startswith(self.HTTP_SCHEMES):\\n            return None\\n\\n        # request record: parse request\\n        if rec_type == 'request':\\n            return self.http_req_parser.parse(stream)\\n\\n        elif rec_type == 'revisit':\\n            try:\\n                return self.http_parser.parse(stream)\\n            except EOFError:\\n                # empty revisit with no http headers, is ok!\\n                return None\\n\\n        # response record or non-empty revisit: parse HTTP status and headers!\\n        else:\\n            return self.http_parser.parse(stream)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordloader.load_http_headers( self, rec_type, uri, stream, length )"
                },
                {
                    "func_id": 2182,
                    "func_name": "default_http_headers",
                    "func_desc": "default_http_headers",
                    "func_file": "recordloader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def default_http_headers(self, length, content_type=None):\\n        headers = []\\n        if content_type:\\n            headers.append(('Content-Type', content_type))\\n\\n        if length is not None and length >= 0:\\n            headers.append(('Content-Length', str(length)))\\n\\n        return StatusAndHeaders('200 OK', headers=headers, protocol='HTTP/1.0')",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordloader.default_http_headers( self, length, content_type )"
                },
                {
                    "func_id": 2183,
                    "func_name": "_detect_type_load_headers",
                    "func_desc": "_detect_type_load_headers",
                    "func_file": "recordloader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _detect_type_load_headers(self, stream,\\n                                  statusline=None, known_format=None):\\n        \"\"\" If known_format is specified ('warc' or 'arc'),\\n        parse only as that format.\\n\\n        Otherwise, try parsing record as WARC, then try parsing as ARC.\\n        if neither one succeeds, we're out of luck.\\n        \"\"\"\\n\\n        if known_format != 'arc':\\n            # try as warc first\\n            try:\\n                rec_headers = self.warc_parser.parse(stream, statusline)\\n                return 'warc', rec_headers\\n            except StatusAndHeadersParserException as se:\\n                if known_format == 'warc':\\n                    msg = 'Invalid WARC record, first line: '\\n                    raise ArchiveLoadFailed(msg + str(se.statusline))\\n\\n                statusline = se.statusline\\n                pass\\n\\n        # now try as arc\\n        try:\\n            rec_headers = self.arc_parser.parse(stream, statusline)\\n            return self.arc_parser.get_rec_type(), rec_headers\\n        except StatusAndHeadersParserException as se:\\n            if known_format == 'arc':\\n                msg = 'Invalid ARC record, first line: '\\n            else:\\n                msg = 'Unknown archive format, first line: '\\n            raise ArchiveLoadFailed(msg + str(se.statusline))",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordloader._detect_type_load_headers( self, stream, statusline, known_format )"
                },
                {
                    "func_id": 2184,
                    "func_name": "_ensure_target_uri_format",
                    "func_desc": "_ensure_target_uri_format",
                    "func_file": "recordloader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ensure_target_uri_format(self, rec_headers):\\n        \"\"\"Checks the value for the WARC-Target-URI header field to see if it starts\\n        with '<' and ends with '>' (Wget 1.19 bug) and if '<' and '>' are present,\\n        corrects and updates the field returning the corrected value for the field\\n        otherwise just returns the fields value. Also checks for the presence of\\n        spaces and percent-encodes them if present, for more reliable parsing\\n        downstream.\\n\\n        :param StatusAndHeaders rec_headers: The parsed WARC headers\\n        :return: The value for the WARC-Target-URI field\\n        :rtype: str | None\\n        \"\"\"\\n        uri = rec_headers.get_header('WARC-Target-URI')\\n\\n        if uri is not None and uri.startswith('<') and uri.endswith('>'):\\n            uri = uri[1:-1]\\n            rec_headers.replace_header('WARC-Target-URI', uri)\\n\\n        if uri is not None and \" \" in uri:\\n            logger.warning(\"Replacing spaces in invalid WARC-Target-URI: {}\".format(uri))\\n            uri = uri.replace(\" \", \"%20\")\\n            rec_headers.replace_header('WARC-Target-URI', uri)\\n\\n        return uri",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordloader._ensure_target_uri_format( self, rec_headers )"
                },
                {
                    "func_id": 2186,
                    "func_name": "get_rec_type",
                    "func_desc": "get_rec_type",
                    "func_file": "recordloader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_rec_type(self):\\n        return 'arc'",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordloader.get_rec_type( self )"
                },
                {
                    "func_id": 2188,
                    "func_name": "get_header_names",
                    "func_desc": "get_header_names",
                    "func_file": "recordloader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_header_names(cls):\\n        return cls.ARC_HEADERS",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordloader.get_header_names( cls )"
                },
                {
                    "func_id": 2189,
                    "func_name": "_get_protocol_and_headers",
                    "func_desc": "_get_protocol_and_headers",
                    "func_file": "recordloader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_protocol_and_headers(self, headerline, parts):\\n        headers = []\\n\\n        for name, value in zip(self.headernames, parts):\\n            headers.append((name, value))\\n\\n        return ('ARC/1.0', headers)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordloader._get_protocol_and_headers( self, headerline, parts )"
                },
                {
                    "func_id": 2190,
                    "func_name": "get_rec_type",
                    "func_desc": "get_rec_type",
                    "func_file": "recordloader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_rec_type(self):\\n        return 'arc2warc'",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordloader.get_rec_type( self )"
                },
                {
                    "func_id": 2191,
                    "func_name": "get_header_names",
                    "func_desc": "get_header_names",
                    "func_file": "recordloader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_header_names(cls):\\n        return cls.ARC_TO_WARC_HEADERS",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordloader.get_header_names( cls )"
                },
                {
                    "func_id": 2192,
                    "func_name": "_get_protocol_and_headers",
                    "func_desc": "_get_protocol_and_headers",
                    "func_file": "recordloader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_protocol_and_headers(self, headerline, parts):\\n        headers = []\\n\\n        if headerline.startswith('filedesc://'):\\n            rec_type = 'warcinfo'\\n        else:\\n            rec_type = 'response'\\n            parts[3] = 'application/http;msgtype=response'\\n\\n        headers.append(('WARC-Type', rec_type))\\n        headers.append(('WARC-Record-ID', StatusAndHeadersParser.make_warc_id()))\\n\\n        for name, value in zip(self.headernames, parts):\\n            if name == 'WARC-Date':\\n                value = timestamp_to_iso_date(value)\\n\\n            if rec_type == 'warcinfo' and name == 'WARC-Target-URI':\\n                name = 'WARC-Filename'\\n                value = value[len('filedesc://'):]\\n\\n            headers.append((name, value))\\n\\n        return ('WARC/1.0', headers)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordloader._get_protocol_and_headers( self, headerline, parts )"
                },
                {
                    "func_id": 2201,
                    "func_name": "_compare_digest_rfc_3548",
                    "func_desc": "_compare_digest_rfc_3548",
                    "func_file": "digestverifyingreader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _compare_digest_rfc_3548(digester, digest):\\n    '''\\n    The WARC standard does not recommend a digest algorithm and appears to\\n    allow any encoding from RFC3548. The Python base64 module supports\\n    RFC3548 although the base64 alternate alphabet is not exactly a first\\n    class citizen. Hopefully digest algos are named with the same names\\n    used by OpenSSL.\\n    '''\\n    if not digester or not digest:\\n        return None\\n\\n    digester_b32 = str(digester)\\n\\n    our_algo, our_value = _parse_digest(digester_b32)\\n    warc_algo, warc_value = _parse_digest(digest)\\n\\n    warc_b32 = _to_b32(len(our_value), warc_value)\\n\\n    if our_value == warc_b32:\\n        return True\\n\\n    return False",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.digestverifyingreader._compare_digest_rfc_3548( digester, digest )"
                },
                {
                    "func_id": 2202,
                    "func_name": "_to_b32",
                    "func_desc": "_to_b32",
                    "func_file": "digestverifyingreader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _to_b32(length, value):\\n    '''\\n    Convert value to base 32, given that it's supposed to have the same\\n    length as the digest we're about to compare it to\\n    '''\\n    if len(value) == length:\\n        return value  # casefold needed here? -- rfc recommends not allowing\\n\\n    if len(value) > length:\\n        binary = base64.b16decode(value, casefold=True)\\n    else:\\n        binary = _b64_wrapper(value)\\n\\n    return to_native_str(base64.b32encode(binary), encoding='ascii')",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.digestverifyingreader._to_b32( length, value )"
                },
                {
                    "func_id": 2203,
                    "func_name": "_b64_wrapper",
                    "func_desc": "_b64_wrapper",
                    "func_file": "digestverifyingreader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _b64_wrapper(value):\\n    if '-' in value or '_' in value:\\n        return base64.b64decode(value, altchars=base64_url_filename_safe_alt)\\n    else:\\n        return base64.b64decode(value)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.digestverifyingreader._b64_wrapper( value )"
                },
                {
                    "func_id": 2204,
                    "func_name": "_parse_digest",
                    "func_desc": "_parse_digest",
                    "func_file": "digestverifyingreader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_digest(digest):\\n    algo, sep, value = digest.partition(':')\\n    if sep == ':':\\n        return algo, value\\n    else:\\n        raise ValueError('could not parse digest algorithm out of '+digest)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.digestverifyingreader._parse_digest( digest )"
                },
                {
                    "func_id": 2211,
                    "func_name": "begin_payload",
                    "func_desc": "begin_payload",
                    "func_file": "digestverifyingreader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def begin_payload(self):\\n        self.payload_digester = self.payload_digester_obj\\n        if self.limit == 0:\\n            check = _compare_digest_rfc_3548(self.payload_digester, self.payload_digest)\\n            if check is False:\\n                self.digest_checker.problem('payload digest failed: {}'.format(self.payload_digest))\\n                self.payload_digester = None  # prevent double-fire\\n            elif check is True and self.digest_checker.passed is not False:\\n                self.digest_checker.passed = True",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.digestverifyingreader.begin_payload( self )"
                },
                {
                    "func_id": 2218,
                    "func_name": "recompress",
                    "func_desc": "recompress",
                    "func_file": "recompressor",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def recompress(self):\\n        from warcio.cli import main\\n        try:\\n            count = 0\\n            msg = ''\\n            with open(self.filename, 'rb') as stream:\\n                try:\\n                    count = self.load_and_write(stream, self.output)\\n                    msg = 'No Errors Found!'\\n                except Exception as e:\\n                    if self.verbose:\\n                        print('Parsing Error(s) Found:')\\n                        print(str(e) if isinstance(e, ArchiveLoadFailed) else repr(e))\\n                        print()\\n\\n                    count = self.decompress_and_recompress(stream, self.output)\\n                    msg = 'Compression Errors Found and Fixed!'\\n\\n                if self.verbose:\\n                    print('Records successfully read and compressed:')\\n                    main(['index', self.output])\\n                    print('')\\n\\n                print('{0} records read and recompressed to file: {1}'.format(count, self.output))\\n                print(msg)\\n\\n        except:\\n            if self.verbose:\\n                print('Exception Details:')\\n                traceback.print_exc()\\n                print('')\\n\\n            print('Recompress Failed: {0} could not be read as a WARC or ARC'.format(self.filename))\\n            sys.exit(1)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recompressor.recompress( self )"
                },
                {
                    "func_id": 2220,
                    "func_name": "decompress_and_recompress",
                    "func_desc": "decompress_and_recompress",
                    "func_file": "recompressor",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def decompress_and_recompress(self, stream, output):\\n        with tempfile.TemporaryFile() as tout:\\n            decomp = DecompressingBufferedReader(stream, read_all_members=True)\\n\\n            # decompress entire file to temp file\\n            stream.seek(0)\\n            shutil.copyfileobj(decomp, tout)\\n\\n            # attempt to compress and write temp\\n            tout.seek(0)\\n            return self.load_and_write(tout, output)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recompressor.decompress_and_recompress( self, stream, output )"
                },
                {
                    "func_id": 2272,
                    "func_name": "get_header",
                    "func_desc": "get_header",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_header(self, name, default_value=None):\\n        \"\"\"\\n        return header (name, value)\\n        if found\\n        \"\"\"\\n        name_lower = name.lower()\\n        for value in self.headers:\\n            if value[0].lower() == name_lower:\\n                return value[1]\\n\\n        return default_value",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.get_header( self, name, default_value )"
                },
                {
                    "func_id": 2273,
                    "func_name": "add_header",
                    "func_desc": "add_header",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def add_header(self, name, value):\\n        self.headers.append((name, value))",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.add_header( self, name, value )"
                },
                {
                    "func_id": 2274,
                    "func_name": "replace_header",
                    "func_desc": "replace_header",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def replace_header(self, name, value):\\n        \"\"\"\\n        replace header with new value or add new header\\n        return old header value, if any\\n        \"\"\"\\n        name_lower = name.lower()\\n        for index in range(len(self.headers) - 1, -1, -1):\\n            curr_name, curr_value = self.headers[index]\\n            if curr_name.lower() == name_lower:\\n                self.headers[index] = (curr_name, value)\\n                return curr_value\\n\\n        self.headers.append((name, value))\\n        return None",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.replace_header( self, name, value )"
                },
                {
                    "func_id": 2275,
                    "func_name": "remove_header",
                    "func_desc": "remove_header",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def remove_header(self, name):\\n        \"\"\"\\n        Remove header (case-insensitive)\\n        return True if header removed, False otherwise\\n        \"\"\"\\n        name_lower = name.lower()\\n        for index in range(len(self.headers) - 1, -1, -1):\\n            if self.headers[index][0].lower() == name_lower:\\n                del self.headers[index]\\n                return True\\n\\n        return False",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.remove_header( self, name )"
                },
                {
                    "func_id": 2276,
                    "func_name": "get_statuscode",
                    "func_desc": "get_statuscode",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_statuscode(self):\\n        \"\"\"\\n        Return the statuscode part of the status response line\\n        (Assumes no protocol in the statusline)\\n        \"\"\"\\n        code = self.statusline.split(' ', 1)[0]\\n        return code",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.get_statuscode( self )"
                },
                {
                    "func_id": 2279,
                    "func_name": "compute_headers_buffer",
                    "func_desc": "compute_headers_buffer",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def compute_headers_buffer(self, header_filter=None):\\n        \"\"\"\\n        Set buffer representing headers\\n        \"\"\"\\n        # HTTP headers %-encoded as ascii (see to_ascii_bytes for more info)\\n        self.headers_buff = self.to_ascii_bytes(header_filter)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.compute_headers_buffer( self, header_filter )"
                },
                {
                    "func_id": 2285,
                    "func_name": "to_str",
                    "func_desc": "to_str",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def to_str(self, filter_func=None):\\n        string = self.protocol\\n\\n        if string and self.statusline:\\n            string += ' '\\n\\n        if self.statusline:\\n            string += self.statusline\\n\\n        if string:\\n            string += '\\r\\n'\\n\\n        for h in self.headers:\\n            if filter_func:\\n                h = filter_func(h)\\n                if not h:\\n                    continue\\n\\n            string += ': '.join(h) + '\\r\\n'\\n\\n        return string",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.to_str( self, filter_func )"
                },
                {
                    "func_id": 2286,
                    "func_name": "to_bytes",
                    "func_desc": "to_bytes",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def to_bytes(self, filter_func=None, encoding='utf-8'):\\n        return self.to_str(filter_func).encode(encoding) + b'\\r\\n'",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.to_bytes( self, filter_func, encoding )"
                },
                {
                    "func_id": 2287,
                    "func_name": "to_ascii_bytes",
                    "func_desc": "to_ascii_bytes",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def to_ascii_bytes(self, filter_func=None):\\n        \"\"\" Attempt to encode the headers block as ascii\\n            If encoding fails, call percent_encode_non_ascii_headers()\\n            to encode any headers per RFCs\\n        \"\"\"\\n        try:\\n            string = self.to_str(filter_func)\\n            string = string.encode('ascii')\\n        except (UnicodeEncodeError, UnicodeDecodeError):\\n            self.percent_encode_non_ascii_headers()\\n            string = self.to_str(filter_func)\\n            string = string.encode('ascii')\\n\\n        return string + b'\\r\\n'",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.to_ascii_bytes( self, filter_func )"
                },
                {
                    "func_id": 2288,
                    "func_name": "percent_encode_non_ascii_headers",
                    "func_desc": "percent_encode_non_ascii_headers",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def percent_encode_non_ascii_headers(self, encoding='UTF-8'):\\n        \"\"\" Encode any headers that are not plain ascii\\n            as UTF-8 as per:\\n            https://tools.ietf.org/html/rfc8187#section-3.2.3\\n            https://tools.ietf.org/html/rfc5987#section-3.2.2\\n        \"\"\"\\n        def do_encode(m):\\n            return \"*={0}''\".format(encoding) + quote(to_native_str(m.group(1)))\\n\\n        for index in range(len(self.headers) - 1, -1, -1):\\n            curr_name, curr_value = self.headers[index]\\n            try:\\n                # test if header is ascii encodable, no action needed\\n                curr_value.encode('ascii')\\n            except:\\n                # if single value header, (eg. no ';'), %-encode entire header\\n                if ';' not in curr_value:\\n                    new_value = quote(curr_value)\\n\\n                else:\\n                # %-encode value in ; name=\"value\"\\n                    new_value = self.ENCODE_HEADER_RX.sub(do_encode, curr_value)\\n                    if new_value == curr_value:\\n                        new_value = quote(curr_value)\\n\\n                self.headers[index] = (curr_name, new_value)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.percent_encode_non_ascii_headers( self, encoding )"
                },
                {
                    "func_id": 2294,
                    "func_name": "decode_header",
                    "func_desc": "decode_header",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def decode_header(line):\\n        try:\\n            # attempt to decode as utf-8 first\\n            return to_native_str(line, 'utf-8')\\n        except:\\n            # if fails, default to ISO-8859-1\\n            return to_native_str(line, 'iso-8859-1')",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.decode_header( line )"
                },
                {
                    "func_id": 2296,
                    "func_name": "do_encode",
                    "func_desc": "do_encode",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def do_encode(m):\\n            return \"*={0}''\".format(encoding) + quote(to_native_str(m.group(1)))",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.do_encode( m )"
                },
                {
                    "func_id": 2312,
                    "func_name": "get_field",
                    "func_desc": "get_field",
                    "func_file": "indexer",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_field(self, record, name, it, filename):\\n        value = None\\n        if name == 'offset':\\n            value = str(it.get_record_offset())\\n        elif name == 'length':\\n            value = str(it.get_record_length())\\n        elif name == 'filename':\\n            value = os.path.basename(filename)\\n        elif name == 'http:status':\\n            if record.rec_type in ('response', 'revisit') and record.http_headers:\\n                value = record.http_headers.get_statuscode()\\n        elif name.startswith('http:'):\\n            if record.http_headers:\\n                value = record.http_headers.get_header(name[5:])\\n        else:\\n            value = record.rec_headers.get_header(name)\\n\\n        return value",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.indexer.get_field( self, record, name, it, filename )"
                },
                {
                    "func_id": 2313,
                    "func_name": "_write_header",
                    "func_desc": "_write_header",
                    "func_file": "indexer",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _write_header(self, out, filename):\\n        pass",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.indexer._write_header( self, out, filename )"
                },
                {
                    "func_id": 2315,
                    "func_name": "to_native_str",
                    "func_desc": "to_native_str",
                    "func_file": "utils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def to_native_str(value, encoding='utf-8'):\\n    if isinstance(value, str):\\n        return value\\n\\n    if six.PY3 and isinstance(value, six.binary_type):  #pragma: no cover\\n        return value.decode(encoding)\\n    elif six.PY2 and isinstance(value, six.text_type):  #pragma: no cover\\n        return value.encode(encoding)\\n    else:\\n        return value",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.utils.to_native_str( value, encoding )"
                },
                {
                    "func_id": 2317,
                    "func_name": "headers_to_str_headers",
                    "func_desc": "headers_to_str_headers",
                    "func_file": "utils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def headers_to_str_headers(headers):\\n    '''\\n    Converts dict or tuple-based headers of bytes or str to\\n    tuple-based headers of str, which is the python norm (pep 3333)\\n    '''\\n    ret = []\\n\\n    if isinstance(headers, collections_abc.Mapping):\\n        h = headers.items()\\n    else:\\n        h = headers\\n\\n    if six.PY2:  #pragma: no cover\\n        return h\\n\\n    for tup in h:\\n        k, v = tup\\n        if isinstance(k, six.binary_type):\\n            k = k.decode('iso-8859-1')\\n        if isinstance(v, six.binary_type):\\n            v = v.decode('iso-8859-1')\\n        ret.append((k, v))\\n    return ret",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.utils.headers_to_str_headers( headers )"
                },
                {
                    "func_id": 2361,
                    "func_name": "capture_http",
                    "func_desc": "capture_http",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def capture_http(warc_writer=None, filter_func=None, append=True,\\n                record_ip=True, **kwargs):\\n    out = None\\n    if warc_writer == None:\\n        if 'gzip' not in kwargs:\\n            kwargs['gzip'] = False\\n\\n        warc_writer = BufferWARCWriter(**kwargs)\\n\\n    if isinstance(warc_writer, str):\\n        out = open(warc_writer, 'ab' if append else 'xb')\\n        warc_writer = WARCWriter(out, **kwargs)\\n\\n    try:\\n        recorder = RequestRecorder(warc_writer, filter_func, record_ip=record_ip)\\n        RecordingHTTPConnection.local.recorder = recorder\\n        yield warc_writer\\n\\n    finally:\\n        RecordingHTTPConnection.local.recorder = None\\n        if out:\\n            out.close()",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.capture_http( warc_writer, filter_func, append, record_ip, **kwargs )"
                },
                {
                    "func_id": 2363,
                    "func_name": "_get_remote_ip",
                    "func_desc": "_get_remote_ip",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_remote_ip(self):\\n        try:\\n            fp = self.fp\\n            # for python 3, need to get 'raw' fp\\n            if hasattr(fp, 'raw'):  #pragma: no cover\\n                fp = fp.raw\\n\\n            socket = fp._sock\\n\\n            # wrapped ssl socket\\n            if hasattr(socket, 'socket'):\\n                socket = socket.socket\\n\\n            return socket.getpeername()[0]\\n\\n        except Exception:  #pragma: no cover\\n            return None",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http._get_remote_ip( self )"
                },
                {
                    "func_id": 2383,
                    "func_name": "extract_url",
                    "func_desc": "extract_url",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def extract_url(self, data, host, port, default_port):\\n        if self.first_line_read:\\n            return\\n\\n        self.first_line_read = True\\n        buff = BytesIO(data)\\n        line = to_native_str(buff.readline(), 'latin-1')\\n\\n        parts = line.split(' ', 2)\\n        verb = parts[0]\\n        path = parts[1]\\n\\n        if verb == \"CONNECT\":\\n            parts = path.split(\":\", 1)\\n            self.connect_host = parts[0]\\n            self.connect_port = int(parts[1]) if len(parts) > 1 else default_port\\n            self.warc_headers['WARC-Proxy-Host'] = \"https://{0}:{1}\".format(host, port)\\n            return\\n\\n        if self.connect_host:\\n            host = self.connect_host\\n\\n        if self.connect_port:\\n            port = self.connect_port\\n\\n        if path.startswith(('http:', 'https:')):\\n            self.warc_headers['WARC-Proxy-Host'] = \"http://{0}:{1}\".format(host, port)\\n            self.url = path\\n            return\\n\\n        scheme = 'https' if default_port == 443 else 'http'\\n        self.url = scheme + '://' + host\\n        if port != default_port:\\n            self.url += ':' + str(port)\\n\\n        self.url += path",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.extract_url( self, data, host, port, default_port )"
                },
                {
                    "func_id": 2385,
                    "func_name": "send_request",
                    "func_desc": "send_request",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def send_request(buff):\\n            self.recorder.extract_url(buff, self.host, self.port, self.default_port)\\n\\n            orig_connection.send(self, buff)\\n            self.recorder.write_request(buff)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.send_request( buff )"
                },
                {
                    "func_id": 2416,
                    "func_name": "_raise_invalid_gzip_err",
                    "func_desc": "_raise_invalid_gzip_err",
                    "func_file": "archiveiterator",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _raise_invalid_gzip_err(self):\\n        \"\"\" A gzip file with multiple ARC/WARC records, non-chunked\\n        has been detected. This is not valid for replay, so notify user\\n        \"\"\"\\n        frmt = 'warc/arc'\\n        if self.known_format:\\n            frmt = self.known_format\\n\\n        frmt_up = frmt.upper()\\n\\n        msg = self.GZIP_ERR_MSG.format(frmt, frmt_up)\\n        raise ArchiveLoadFailed(msg)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.archiveiterator._raise_invalid_gzip_err( self )"
                }
            ]
        },
        {
            "cluster_id": 0,
            "feature_id": 37,
            "feature_desc": "gamma=0.0165; k=5; a=0.25; combined=0.396; stability(ARI)=0.973; sep=0.068",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 695,
                    "func_name": "candidates",
                    "func_desc": "candidates",
                    "func_file": "conf",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def candidates():\\n        if 'MRJOB_CONF' in os.environ:\\n            yield expand_path(os.environ['MRJOB_CONF'])\\n\\n        # $HOME isn't necessarily set on Windows, but ~ works\\n        # use os.path.join() so we don't end up mixing \\ and /\\n        yield expand_path(os.path.join('~', '.mrjob.conf'))\\n\\n        # this only really makes sense on Unix, so no os.path.join()\\n        yield '/etc/mrjob.conf'",
                    "func_fullName": "mrjob.conf.candidates(  )"
                },
                {
                    "func_id": 2049,
                    "func_name": "run",
                    "func_desc": "run",
                    "func_file": "options_extension",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def run(self):\\n        # all we have to do during parsing is make a node where the directive\\n        # is and remember which options it's supposed to have\\n        node = optionlist('', option_set=self.content[0])\\n        return [node]",
                    "func_fullName": "docs.options_extension.run( self )"
                },
                {
                    "func_id": 2050,
                    "func_name": "run",
                    "func_desc": "run",
                    "func_file": "options_extension",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def run(self):\\n        env = self.state.document.settings.env\\n\\n        # generate the linkback node for this option\\n        targetid = \"option-%s\" % self.options['config']\\n        targetnode = nodes.target('', '', ids=[targetid])\\n\\n        # Each option will be outputted as a single-item definition list\\n        # (just like it was doing before we used this extension)\\n        dl = nodes.definition_list()\\n        dl['classes'].append('mrjob-opt')\\n        dli = nodes.definition_list_item()\\n\\n        term = nodes.term()\\n\\n        # config option shall be bold\\n        if 'config' in self.options:\\n            cfg = self.options['config']\\n            term.append(nodes.strong(cfg, cfg))\\n            if 'switch' in self.options:\\n                term.append(nodes.Text(' (', ' ('))\\n\\n        # switch shall be comma-separated literals\\n        if 'switch' in self.options:\\n            switches = self.options['switch'].split(', ')\\n            for i, s in enumerate(switches):\\n                if i > 0:\\n                    term.append(nodes.Text(', ', ', '))\\n                term.append(nodes.literal(s, s))\\n            if 'config' in self.options:\\n                term.append(nodes.Text(')', ')'))\\n\\n        dli.append(term)\\n\\n        # classifier is either plain text or a link to some more docs, so parse\\n        # its contents\\n        classifier = nodes.classifier()\\n        type_nodes, messages = self.state.inline_text(\\n            self.options.get('type', ''), self.lineno)\\n\\n        classifier.extend(type_nodes)\\n        dli.append(classifier)\\n\\n        # definition holds the description\\n        defn = nodes.definition()\\n\\n        # add a default if any\\n        default_nodes = []\\n        if 'default' in self.options:\\n            default_par = nodes.paragraph()\\n            default_par.append(nodes.strong('Default: ', 'Default: '))\\n            textnodes, messages = self.state.inline_text(\\n                self.options['default'], self.lineno)\\n            default_nodes = textnodes\\n            default_par.extend(textnodes)\\n            defn.append(default_par)\\n\\n        # parse the description like a nested block (see\\n        # sphinx.compat.make_admonition)\\n        desc_par = nodes.paragraph()\\n        self.state.nested_parse(self.content, self.content_offset, desc_par)\\n        defn.append(desc_par)\\n\\n        dli.append(defn)\\n        dl.append(dli)\\n\\n        if not hasattr(env, 'optionlist_all_options'):\\n            env.optionlist_all_options = []\\n            env.optionlist_indexed_options = {}\\n\\n        # store info for the optionlist traversal to find\\n        info = {\\n            'docname': env.docname,\\n            'lineno': self.lineno,\\n            'options': self.options,\\n            'content': self.content,\\n            'target': targetnode,\\n            'type_nodes': type_nodes,\\n            'default_nodes': default_nodes,\\n        }\\n        env.optionlist_all_options.append(info)\\n        env.optionlist_indexed_options[self.options['config']] = info\\n\\n        return [targetnode, dl]",
                    "func_fullName": "docs.options_extension.run( self )"
                },
                {
                    "func_id": 2081,
                    "func_name": "_process_read",
                    "func_desc": "_process_read",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _process_read(self, data):\\n        # don't process if no raw data read\\n        if not data:\\n            self.buff = None\\n            return\\n\\n        data = self._decompress(data)\\n        self.buff_size = len(data)\\n        self.num_read += self.buff_size\\n        self.num_block_read += self.buff_size\\n        self.buff = BytesIO(data)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders._process_read( self, data )"
                },
                {
                    "func_id": 2083,
                    "func_name": "read",
                    "func_desc": "read",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def read(self, length=None):\\n        \"\"\"\\n        Fill bytes and read some number of bytes\\n        (up to length if specified)\\n        <= length bytes may be read if reached the end of input\\n        if at buffer boundary, will attempt to read again until\\n        specified length is read\\n        \"\"\"\\n        all_buffs = []\\n        while length is None or length > 0:\\n            self._fillbuff()\\n            if self.empty():\\n                break\\n\\n            buff = self.buff.read(length)\\n            all_buffs.append(buff)\\n            if length:\\n                length -= len(buff)\\n\\n        return b''.join(all_buffs)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders.read( self, length )"
                },
                {
                    "func_id": 2084,
                    "func_name": "readline",
                    "func_desc": "readline",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def readline(self, length=None):\\n        \"\"\"\\n        Fill buffer and read a full line from the buffer\\n        (up to specified length, if provided)\\n        If no newline found at end, try filling buffer again in case\\n        at buffer boundary.\\n        \"\"\"\\n        if length == 0:\\n            return b''\\n\\n        self._fillbuff()\\n\\n        if self.empty():\\n            return b''\\n\\n        linebuff = self.buff.readline(length)\\n\\n        # we may be at a boundary\\n        while not linebuff.endswith(b'\\n'):\\n            if length:\\n                length -= len(linebuff)\\n                if length <= 0:\\n                    break\\n\\n            self._fillbuff()\\n\\n            if self.empty():\\n                break\\n\\n            linebuff += self.buff.readline(length)\\n\\n        return linebuff",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders.readline( self, length )"
                },
                {
                    "func_id": 2085,
                    "func_name": "tell",
                    "func_desc": "tell",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def tell(self):\\n        return self.num_read",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders.tell( self )"
                },
                {
                    "func_id": 2087,
                    "func_name": "read_next_member",
                    "func_desc": "read_next_member",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def read_next_member(self):\\n        if not self.decompressor or not self.decompressor.unused_data:\\n            return False\\n\\n        self.starting_data = self.decompressor.unused_data\\n        self._init_decomp(self.decomp_type)\\n        return True",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders.read_next_member( self )"
                },
                {
                    "func_id": 2089,
                    "func_name": "close",
                    "func_desc": "close",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def close(self):\\n        if self.stream:\\n            self.stream.close()\\n            self.stream = None\\n\\n        self.buff = None\\n\\n        self.close_decompressor()",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders.close( self )"
                },
                {
                    "func_id": 2144,
                    "func_name": "write",
                    "func_desc": "write",
                    "func_file": "warcwriter",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def write(self, buff):\\n        #if isinstance(buff, str):\\n        #    buff = buff.encode('utf-8')\\n        buff = self.compressor.compress(buff)\\n        self.out.write(buff)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.warcwriter.write( self, buff )"
                },
                {
                    "func_id": 2187,
                    "func_name": "parse",
                    "func_desc": "parse",
                    "func_file": "recordloader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def parse(self, stream, headerline=None):\\n        total_read = 0\\n\\n        if headerline is None:\\n            headerline = stream.readline()\\n\\n        headerline = StatusAndHeadersParser.decode_header(headerline)\\n\\n        header_len = len(headerline)\\n\\n        if header_len == 0:\\n            raise EOFError()\\n\\n        headerline = headerline.rstrip()\\n\\n        headernames = self.headernames\\n\\n        # if arc header, consume next two lines\\n        if headerline.startswith('filedesc://'):\\n            version = StatusAndHeadersParser.decode_header(stream.readline())  # skip version\\n            spec = StatusAndHeadersParser.decode_header(stream.readline())  # skip header spec, use preset one\\n            total_read += len(version)\\n            total_read += len(spec)\\n\\n        parts = headerline.rsplit(' ', len(headernames)-1)\\n\\n        if len(parts) != len(headernames):\\n            msg = 'Wrong # of headers, expected arc headers {0}, Found {1}'\\n            msg = msg.format(headernames, parts)\\n            raise StatusAndHeadersParserException(msg, parts)\\n\\n\\n        protocol, headers = self._get_protocol_and_headers(headerline, parts)\\n\\n        return StatusAndHeaders(statusline='',\\n                                headers=headers,\\n                                protocol='WARC/1.0',\\n                                total_len=total_read)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordloader.parse( self, stream, headerline )"
                },
                {
                    "func_id": 2206,
                    "func_name": "passed",
                    "func_desc": "passed",
                    "func_file": "digestverifyingreader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def passed(self):\\n        return self._passed",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.digestverifyingreader.passed( self )"
                },
                {
                    "func_id": 2207,
                    "func_name": "passed",
                    "func_desc": "passed",
                    "func_file": "digestverifyingreader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def passed(self, value):\\n        self._passed = value",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.digestverifyingreader.passed( self, value )"
                },
                {
                    "func_id": 2221,
                    "func_name": "_read_entire_stream",
                    "func_desc": "_read_entire_stream",
                    "func_file": "checker",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _read_entire_stream(stream):\\n    while True:\\n        piece = stream.read(1024*1024)\\n        if len(piece) == 0:\\n            break",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.checker._read_entire_stream( stream )"
                },
                {
                    "func_id": 2291,
                    "func_name": "parse",
                    "func_desc": "parse",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def parse(self, stream, full_statusline=None):\\n        \"\"\"\\n        parse stream for status line and headers\\n        return a StatusAndHeaders object\\n\\n        support continuation headers starting with space or tab\\n        \"\"\"\\n\\n        # status line w newlines intact\\n        if full_statusline is None:\\n            full_statusline = stream.readline()\\n\\n        full_statusline = self.decode_header(full_statusline)\\n\\n        statusline, total_read = _strip_count(full_statusline, 0)\\n\\n        headers = []\\n\\n        # at end of stream\\n        if total_read == 0:\\n            raise EOFError()\\n        elif not statusline:\\n            return StatusAndHeaders(statusline=statusline,\\n                                    headers=headers,\\n                                    protocol='',\\n                                    total_len=total_read)\\n\\n        # validate only if verify is set\\n        if self.verify:\\n            protocol_status = self.split_prefix(statusline, self.statuslist)\\n\\n            if not protocol_status:\\n                msg = 'Expected Status Line starting with {0} - Found: {1}'\\n                msg = msg.format(self.statuslist, statusline)\\n                raise StatusAndHeadersParserException(msg, full_statusline)\\n        else:\\n            protocol_status = statusline.split(' ', 1)\\n\\n        line, total_read = _strip_count(self.decode_header(stream.readline()), total_read)\\n        while line:\\n            result = line.split(':', 1)\\n            if len(result) == 2:\\n                name = result[0].rstrip(' \\t')\\n                value = result[1].lstrip()\\n            else:\\n                name = result[0]\\n                value = None\\n\\n            next_line, total_read = _strip_count(self.decode_header(stream.readline()),\\n                                                 total_read)\\n\\n            # append continuation lines, if any\\n            while next_line and next_line.startswith((' ', '\\t')):\\n                if value is not None:\\n                    value += next_line\\n                next_line, total_read = _strip_count(self.decode_header(stream.readline()),\\n                                                     total_read)\\n\\n            if value is not None:\\n                header = (name, value)\\n                headers.append(header)\\n\\n            line = next_line\\n\\n        if len(protocol_status) > 1:\\n            statusline = protocol_status[1].strip()\\n        else:\\n            statusline = ''\\n\\n        return StatusAndHeaders(statusline=statusline,\\n                                headers=headers,\\n                                protocol=protocol_status[0],\\n                                total_len=total_read)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.parse( self, stream, full_statusline )"
                },
                {
                    "func_id": 2316,
                    "func_name": "open_or_default",
                    "func_desc": "open_or_default",
                    "func_file": "utils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def open_or_default(filename, mod, default_fh):\\n    if filename == '-' or filename == b'-':\\n        yield default_fh\\n    elif filename and isinstance(filename, str):\\n        res = open(filename, mod)\\n        yield res\\n        res.close()\\n    elif filename:\\n        yield filename\\n    else:\\n        yield default_fh",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.utils.open_or_default( filename, mod, default_fh )"
                },
                {
                    "func_id": 2321,
                    "func_name": "open",
                    "func_desc": "open",
                    "func_file": "utils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def open(filename, mode='r', **kwargs):  #pragma: no cover\\n    \"\"\"\\n    open() which supports exclusive mode 'x' in python < 3.3\\n    \"\"\"\\n    if six.PY3 or 'x' not in mode:\\n        return sys_open(filename, mode, **kwargs)\\n\\n    flags = os.O_EXCL | os.O_CREAT | os.O_WRONLY\\n    if 'b' in mode and hasattr(os, 'O_BINARY'):\\n        flags |= os.O_BINARY\\n\\n    fd = os.open(filename, flags)\\n    mode = mode.replace('x', 'w')\\n    return os.fdopen(fd, mode, 0x664)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.utils.open( filename, mode, **kwargs )"
                },
                {
                    "func_id": 2323,
                    "func_name": "update",
                    "func_desc": "update",
                    "func_file": "utils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def update(self, buff):\\n        self.digester.update(buff)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.utils.update( self, buff )"
                },
                {
                    "func_id": 2334,
                    "func_name": "read",
                    "func_desc": "read",
                    "func_file": "limitreader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def read(self, length=None):\\n        if length is not None:\\n            length = min(length, self.limit)\\n        else:\\n            length = self.limit\\n\\n        if length == 0:\\n            return b''\\n\\n        buff = self.stream.read(length)\\n        return self._update(buff)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.limitreader.read( self, length )"
                },
                {
                    "func_id": 2335,
                    "func_name": "readline",
                    "func_desc": "readline",
                    "func_file": "limitreader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def readline(self, length=None):\\n        if length is not None:\\n            length = min(length, self.limit)\\n        else:\\n            length = self.limit\\n\\n        if length == 0:\\n            return b''\\n\\n        buff = self.stream.readline(length)\\n        return self._update(buff)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.limitreader.readline( self, length )"
                },
                {
                    "func_id": 2336,
                    "func_name": "close",
                    "func_desc": "close",
                    "func_file": "limitreader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def close(self):\\n        self.stream.close()",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.limitreader.close( self )"
                },
                {
                    "func_id": 2337,
                    "func_name": "tell",
                    "func_desc": "tell",
                    "func_file": "limitreader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def tell(self):\\n        # implement our own tell\\n        return self._orig_limit - self.limit",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.limitreader.tell( self )"
                },
                {
                    "func_id": 2364,
                    "func_name": "read",
                    "func_desc": "read",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def read(self, amt=None):  #pragma: no cover\\n        buff = self.fp.read(amt)\\n        self.recorder.write_response(buff)\\n        return buff",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.read( self, amt )"
                },
                {
                    "func_id": 2365,
                    "func_name": "readinto",
                    "func_desc": "readinto",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def readinto(self, buff):  #pragma: no cover\\n        res = self.fp.readinto(buff)\\n        self.recorder.write_response(buff)\\n        return res",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.readinto( self, buff )"
                },
                {
                    "func_id": 2366,
                    "func_name": "readline",
                    "func_desc": "readline",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def readline(self, maxlen=-1):\\n        line = self.fp.readline(maxlen)\\n        self.recorder.write_response(line)\\n        return line",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.readline( self, maxlen )"
                },
                {
                    "func_id": 2367,
                    "func_name": "close",
                    "func_desc": "close",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def close(self):\\n        self.recorder.done()\\n        if self.fp:\\n            return self.fp.close()",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.close( self )"
                },
                {
                    "func_id": 2371,
                    "func_name": "send",
                    "func_desc": "send",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def send(self, data):\\n        if not self.recorder:\\n            orig_connection.send(self, data)\\n            return\\n\\n        def send_request(buff):\\n            self.recorder.extract_url(buff, self.host, self.port, self.default_port)\\n\\n            orig_connection.send(self, buff)\\n            self.recorder.write_request(buff)\\n\\n        # if sending request body as stream\\n        # (supported via httplib but seems unused via higher-level apis)\\n        if hasattr(data, 'read') and not isinstance(data, array):  #pragma: no cover\\n            while True:\\n                buff = data.read(BUFF_SIZE)\\n                if not buff:\\n                    break\\n\\n                send_request(buff)\\n        else:\\n            send_request(data)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.send( self, data )"
                },
                {
                    "func_id": 2382,
                    "func_name": "done",
                    "func_desc": "done",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def done(self):\\n        if not self.started_req:\\n            return\\n\\n        try:\\n            request = self._create_record(self.request_out, 'request')\\n            response = self._create_record(self.response_out, 'response')\\n\\n            if self.filter_func:\\n                request, response = self.filter_func(request, response, self)\\n                if not request or not response:\\n                    return\\n\\n            with self.lock:\\n                self.writer.write_request_response_pair(request, response)\\n        finally:\\n            self.request_out.close()\\n            self.response_out.close()",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.done( self )"
                },
                {
                    "func_id": 2391,
                    "func_name": "extract",
                    "func_desc": "extract",
                    "func_file": "extractor",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def extract(self, payload_only, headers_only):\\n        with open(self.filename, 'rb') as fh:\\n            fh.seek(int(self.offset))\\n            it = iter(ArchiveIterator(fh))\\n            record = next(it)\\n\\n            try:\\n                stdout_raw = sys.stdout.buffer\\n            except AttributeError:  #pragma: no cover\\n                stdout_raw = sys.stdout\\n\\n            if payload_only:\\n                stream = record.content_stream()\\n                buf = stream.read(self.READ_SIZE)\\n                while buf:\\n                    stdout_raw.write(buf)\\n                    buf = stream.read(self.READ_SIZE)\\n            else:\\n                stdout_raw.write(record.rec_headers.to_bytes())\\n                if record.http_headers:\\n                    stdout_raw.write(record.http_headers.to_bytes())\\n                if not headers_only:\\n                    buf = record.raw_stream.read(self.READ_SIZE)\\n                    while buf:\\n                        stdout_raw.write(buf)\\n                        buf = record.raw_stream.read(self.READ_SIZE)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.extractor.extract( self, payload_only, headers_only )"
                },
                {
                    "func_id": 2409,
                    "func_name": "tell",
                    "func_desc": "tell",
                    "func_file": "archiveiterator",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def tell(self):\\n        return self.offset",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.archiveiterator.tell( self )"
                },
                {
                    "func_id": 2410,
                    "func_name": "read",
                    "func_desc": "read",
                    "func_file": "archiveiterator",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def read(self, size=-1):\\n        result = self.fh.read(size)\\n        self.offset += len(result)\\n        return result",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.archiveiterator.read( self, size )"
                },
                {
                    "func_id": 2414,
                    "func_name": "close",
                    "func_desc": "close",
                    "func_file": "archiveiterator",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def close(self):\\n        self.record = None\\n        if self.reader:\\n            self.reader.close_decompressor()\\n            self.reader = None",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.archiveiterator.close( self )"
                },
                {
                    "func_id": 2417,
                    "func_name": "_consume_blanklines",
                    "func_desc": "_consume_blanklines",
                    "func_file": "archiveiterator",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _consume_blanklines(self):\\n        \"\"\" Consume blank lines that are between records\\n        - For warcs, there are usually 2\\n        - For arcs, may be 1 or 0\\n        - For block gzipped files, these are at end of each gzip envelope\\n          and are included in record length which is the full gzip envelope\\n        - For uncompressed, they are between records and so are NOT part of\\n          the record length\\n\\n          count empty_size so that it can be subtracted from\\n          the record length for uncompressed\\n\\n          if first line read is not blank, likely error in WARC/ARC,\\n          display a warning\\n        \"\"\"\\n        empty_size = 0\\n        first_line = True\\n\\n        while True:\\n            line = self.reader.readline()\\n            if len(line) == 0:\\n                return None, empty_size\\n\\n            stripped = line.rstrip()\\n\\n            if len(stripped) == 0 or first_line:\\n                empty_size += len(line)\\n\\n                if len(stripped) != 0:\\n                    # if first line is not blank,\\n                    # likely content-length was invalid, display warning\\n                    err_offset = self.fh.tell() - self.reader.rem_length() - empty_size\\n                    sys.stderr.write(self.INC_RECORD.format(err_offset, line))\\n                    self.err_count += 1\\n\\n                first_line = False\\n                continue\\n\\n            return line, empty_size",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.archiveiterator._consume_blanklines( self )"
                },
                {
                    "func_id": 2418,
                    "func_name": "read_to_end",
                    "func_desc": "read_to_end",
                    "func_file": "archiveiterator",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def read_to_end(self, record=None):\\n        \"\"\" Read remainder of the stream\\n        If a digester is included, update it\\n        with the data read\\n        \"\"\"\\n\\n        # no current record to read\\n        if not self.record:\\n            return None\\n\\n        # already at end of this record, don't read until it is consumed\\n        if self.member_info:\\n            return None\\n\\n        curr_offset = self.offset\\n\\n        while True:\\n            b = self.record.raw_stream.read(BUFF_SIZE)\\n            if not b:\\n                break\\n\\n        \"\"\"\\n        - For compressed files, blank lines are consumed\\n          since they are part of record length\\n        - For uncompressed files, blank lines are read later,\\n          and not included in the record length\\n        \"\"\"\\n        #if self.reader.decompressor:\\n        self.next_line, empty_size = self._consume_blanklines()\\n\\n        self.offset = self.fh.tell() - self.reader.rem_length()\\n        #if self.offset < 0:\\n        #    raise Exception('Not Gzipped Properly')\\n\\n        if self.next_line:\\n            self.offset -= len(self.next_line)\\n\\n        length = self.offset - curr_offset\\n\\n        if not self.reader.decompressor:\\n            length -= empty_size\\n\\n        self.member_info = (curr_offset, length)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.archiveiterator.read_to_end( self, record )"
                },
                {
                    "func_id": 2483,
                    "func_name": "clear",
                    "func_desc": "clear",
                    "func_file": "ordered_dict",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def clear(self):\\n        self.__end = end = []\\n        end += [None, end, end]         # sentinel node for doubly linked list\\n        self.__map = {}                 # key --> [key, prev, next]\\n        dict.clear(self)",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.ordered_dict.clear( self )"
                },
                {
                    "func_id": 2492,
                    "func_name": "copy",
                    "func_desc": "copy",
                    "func_file": "ordered_dict",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def copy(self):\\n        return self.__class__(self)",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.ordered_dict.copy( self )"
                }
            ]
        },
        {
            "cluster_id": 0,
            "feature_id": 38,
            "feature_desc": "gamma=0.0165; k=5; a=0.25; combined=0.396; stability(ARI)=0.973; sep=0.068",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1026,
                    "func_name": "__wrap_method_with_call_and_maybe_retry",
                    "func_desc": "__wrap_method_with_call_and_maybe_retry",
                    "func_file": "retry",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __wrap_method_with_call_and_maybe_retry(self, f):\\n        \"\"\"Wrap method f in a retry loop.\"\"\"\\n\\n        def call_and_maybe_retry(*args, **kwargs):\\n            backoff = self.__backoff\\n            tries = 0\\n\\n            while not self.__max_tries or tries < self.__max_tries:\\n                try:\\n                    return f(*args, **kwargs)\\n                except Exception as ex:\\n                    if (self.__retry_if(ex) and\\n                        (tries < self.__max_tries - 1 or\\n                         not self.__max_tries)):\\n                        log.info('Got retriable error: %r' % ex)\\n                        log.info('Backing off for %.1f seconds' % backoff)\\n                        time.sleep(backoff)\\n                        tries += 1\\n                        backoff *= self.__multiplier\\n                        backoff = min(backoff, self.__max_backoff)\\n                    else:\\n                        raise\\n\\n        # pretend to be the original function\\n        call_and_maybe_retry.__doc__ = f.__doc__\\n\\n        if hasattr(f, '__name__'):\\n            call_and_maybe_retry.__name__ = f.__name__\\n\\n        return call_and_maybe_retry",
                    "func_fullName": "mrjob.retry.__wrap_method_with_call_and_maybe_retry( self, f )"
                },
                {
                    "func_id": 1027,
                    "func_name": "call_and_maybe_retry",
                    "func_desc": "call_and_maybe_retry",
                    "func_file": "retry",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def call_and_maybe_retry(*args, **kwargs):\\n            backoff = self.__backoff\\n            tries = 0\\n\\n            while not self.__max_tries or tries < self.__max_tries:\\n                try:\\n                    return f(*args, **kwargs)\\n                except Exception as ex:\\n                    if (self.__retry_if(ex) and\\n                        (tries < self.__max_tries - 1 or\\n                         not self.__max_tries)):\\n                        log.info('Got retriable error: %r' % ex)\\n                        log.info('Backing off for %.1f seconds' % backoff)\\n                        time.sleep(backoff)\\n                        tries += 1\\n                        backoff *= self.__multiplier\\n                        backoff = min(backoff, self.__max_backoff)\\n                    else:\\n                        raise",
                    "func_fullName": "mrjob.retry.call_and_maybe_retry( *args, **kwargs )"
                },
                {
                    "func_id": 2039,
                    "func_name": "visit_noop",
                    "func_desc": "visit_noop",
                    "func_file": "options_extension",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def visit_noop(self, node):\\n    pass",
                    "func_fullName": "docs.options_extension.visit_noop( self, node )"
                },
                {
                    "func_id": 2040,
                    "func_name": "depart_noop",
                    "func_desc": "depart_noop",
                    "func_file": "options_extension",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def depart_noop(self, node):\\n    pass",
                    "func_fullName": "docs.options_extension.depart_noop( self, node )"
                },
                {
                    "func_id": 2051,
                    "func_name": "sort_key",
                    "func_desc": "sort_key",
                    "func_file": "options_extension",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def sort_key(oi):\\n            if 'config' in oi['options']:\\n                return oi['options']['config']\\n            else:\\n                return oi['options']['switch'].lstrip('-')",
                    "func_fullName": "docs.options_extension.sort_key( oi )"
                },
                {
                    "func_id": 2088,
                    "func_name": "rem_length",
                    "func_desc": "rem_length",
                    "func_file": "bufferedreaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def rem_length(self):\\n        rem = 0\\n        if self.buff:\\n            rem = self.buff_size - self.buff.tell()\\n\\n        if self.decompressor and self.decompressor.unused_data:\\n            rem += len(self.decompressor.unused_data)\\n        return rem",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.bufferedreaders.rem_length( self )"
                },
                {
                    "func_id": 2112,
                    "func_name": "create_warcinfo_record",
                    "func_desc": "create_warcinfo_record",
                    "func_file": "recordbuilder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def create_warcinfo_record(self, filename, info):\\n        warc_headers = StatusAndHeaders('', [], protocol=self.warc_version)\\n        warc_headers.add_header('WARC-Type', 'warcinfo')\\n        warc_headers.add_header('WARC-Record-ID', self._make_warc_id())\\n        if filename:\\n            warc_headers.add_header('WARC-Filename', filename)\\n        warc_headers.add_header('WARC-Date', self.curr_warc_date())\\n\\n        warcinfo = BytesIO()\\n        for name, value in six.iteritems(info):\\n            if not value:\\n                continue\\n\\n            line = name + ': ' + str(value) + '\\r\\n'\\n            warcinfo.write(line.encode('utf-8'))\\n\\n        length = warcinfo.tell()\\n        warcinfo.seek(0)\\n\\n        return self.create_warc_record('', 'warcinfo',\\n                                       warc_headers=warc_headers,\\n                                       payload=warcinfo,\\n                                       length=length)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordbuilder.create_warcinfo_record( self, filename, info )"
                },
                {
                    "func_id": 2113,
                    "func_name": "create_revisit_record",
                    "func_desc": "create_revisit_record",
                    "func_file": "recordbuilder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def create_revisit_record(self, uri, digest, refers_to_uri, refers_to_date,\\n                              http_headers=None, warc_headers_dict=None):\\n\\n        assert digest, 'Digest can not be empty'\\n        if warc_headers_dict is None:\\n            warc_headers_dict = dict()\\n\\n        record = self.create_warc_record(uri, 'revisit', http_headers=http_headers,\\n                                                         warc_headers_dict=warc_headers_dict)\\n\\n        revisit_profile = self.REVISIT_PROFILE_1_1 if self.warc_version == self.WARC_1_1 else self.REVISIT_PROFILE\\n        record.rec_headers.add_header('WARC-Profile', revisit_profile)\\n\\n        record.rec_headers.add_header('WARC-Refers-To-Target-URI', refers_to_uri)\\n        record.rec_headers.add_header('WARC-Refers-To-Date', refers_to_date)\\n\\n        record.rec_headers.add_header('WARC-Payload-Digest', digest)\\n\\n        return record",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordbuilder.create_revisit_record( self, uri, digest, refers_to_uri, refers_to_date, http_headers, warc_headers_dict )"
                },
                {
                    "func_id": 2114,
                    "func_name": "create_warc_record",
                    "func_desc": "create_warc_record",
                    "func_file": "recordbuilder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def create_warc_record(self, uri, record_type,\\n                           payload=None,\\n                           length=None,\\n                           warc_content_type='',\\n                           warc_headers_dict=None,\\n                           warc_headers=None,\\n                           http_headers=None):\\n        if warc_headers_dict is None:\\n            warc_headers_dict = dict()\\n\\n        if payload and not http_headers:\\n            loader = ArcWarcRecordLoader()\\n            http_headers = loader.load_http_headers(record_type, uri, payload, length)\\n            if http_headers and length is not None:\\n                length -= payload.tell()\\n\\n        if not payload:\\n            payload = BytesIO()\\n            length = 0\\n\\n        if not warc_headers:\\n            warc_headers = self._init_warc_headers(uri, record_type, warc_headers_dict)\\n\\n        # compute Content-Type\\n        if not warc_content_type:\\n            warc_content_type = warc_headers.get_header('Content-Type')\\n\\n            if not warc_content_type:\\n                warc_content_type = self.WARC_RECORDS.get(record_type,\\n                                                'application/warc-record')\\n\\n        record = ArcWarcRecord('warc', record_type, warc_headers, payload,\\n                               http_headers, warc_content_type, length)\\n\\n        record.payload_length = length\\n\\n        self.ensure_digest(record, block=False, payload=True)\\n\\n        return record",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordbuilder.create_warc_record( self, uri, record_type, payload, length, warc_content_type, warc_headers_dict, warc_headers, http_headers )"
                },
                {
                    "func_id": 2116,
                    "func_name": "curr_warc_date",
                    "func_desc": "curr_warc_date",
                    "func_file": "recordbuilder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def curr_warc_date(self):\\n        use_micros = (self.warc_version >= self.WARC_1_1)\\n        return self._make_warc_date(use_micros=use_micros)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordbuilder.curr_warc_date( self )"
                },
                {
                    "func_id": 2117,
                    "func_name": "_parse_warc_version",
                    "func_desc": "_parse_warc_version",
                    "func_file": "recordbuilder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _parse_warc_version(self, version):\\n        if not version:\\n            return self.WARC_VERSION\\n\\n        version = str(version)\\n        if version.startswith('WARC/'):\\n            return version\\n\\n        return 'WARC/' + version",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordbuilder._parse_warc_version( self, version )"
                },
                {
                    "func_id": 2118,
                    "func_name": "_make_warc_id",
                    "func_desc": "_make_warc_id",
                    "func_file": "recordbuilder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _make_warc_id(cls):\\n        return StatusAndHeadersParser.make_warc_id()",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordbuilder._make_warc_id( cls )"
                },
                {
                    "func_id": 2119,
                    "func_name": "_make_warc_date",
                    "func_desc": "_make_warc_date",
                    "func_file": "recordbuilder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _make_warc_date(cls, use_micros=False):\\n        return datetime_to_iso_date(datetime.now(timezone.utc).replace(tzinfo=None), use_micros=use_micros)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordbuilder._make_warc_date( cls, use_micros )"
                },
                {
                    "func_id": 2123,
                    "func_name": "_create_temp_file",
                    "func_desc": "_create_temp_file",
                    "func_file": "recordbuilder",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _create_temp_file():\\n        return tempfile.SpooledTemporaryFile(max_size=512*1024)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordbuilder._create_temp_file(  )"
                },
                {
                    "func_id": 2139,
                    "func_name": "write_request_response_pair",
                    "func_desc": "write_request_response_pair",
                    "func_file": "warcwriter",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def write_request_response_pair(self, req, resp, params=None):\\n        url = resp.rec_headers.get_header('WARC-Target-URI')\\n        dt = resp.rec_headers.get_header('WARC-Date')\\n\\n        req.rec_headers.replace_header('WARC-Target-URI', url)\\n        req.rec_headers.replace_header('WARC-Date', dt)\\n\\n        resp_id = resp.rec_headers.get_header('WARC-Record-ID')\\n        if resp_id:\\n            req.rec_headers.add_header('WARC-Concurrent-To', resp_id)\\n\\n        self._do_write_req_resp(req, resp, params)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.warcwriter.write_request_response_pair( self, req, resp, params )"
                },
                {
                    "func_id": 2140,
                    "func_name": "write_record",
                    "func_desc": "write_record",
                    "func_file": "warcwriter",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def write_record(self, record, params=None):  #pragma: no cover\\n        raise NotImplemented()",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.warcwriter.write_record( self, record, params )"
                },
                {
                    "func_id": 2141,
                    "func_name": "_do_write_req_resp",
                    "func_desc": "_do_write_req_resp",
                    "func_file": "warcwriter",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _do_write_req_resp(self, req, resp, params):  #pragma: no cover\\n        raise NotImplemented()",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.warcwriter._do_write_req_resp( self, req, resp, params )"
                },
                {
                    "func_id": 2142,
                    "func_name": "_write_warc_record",
                    "func_desc": "_write_warc_record",
                    "func_file": "warcwriter",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _write_warc_record(self, out, record):\\n        if self.gzip:\\n            out = GzippingWrapper(out)\\n\\n        if record.http_headers:\\n            record.http_headers.compute_headers_buffer(self.header_filter)\\n\\n        # Content-Length is None/unknown\\n        # Fix record by: buffering and recomputing all digests and length\\n        # (since no length, can't trust existing digests)\\n        # Also remove content-type for consistent header ordering\\n        if record.length is None:\\n            record.rec_headers.remove_header('WARC-Block-Digest')\\n            if record.rec_type != 'revisit':\\n                record.rec_headers.remove_header('WARC-Payload-Digest')\\n            record.rec_headers.remove_header('Content-Type')\\n\\n            self.ensure_digest(record, block=True, payload=True)\\n\\n            record.length = record.payload_length\\n\\n        # ensure digests are set\\n        else:\\n            self.ensure_digest(record, block=True, payload=True)\\n\\n        if record.content_type != None:\\n            # ensure proper content type\\n            record.rec_headers.replace_header('Content-Type', record.content_type)\\n\\n        if record.rec_type == 'revisit':\\n            http_headers_only = True\\n        else:\\n            http_headers_only = False\\n\\n        # compute Content-Length\\n        if record.http_headers and record.payload_length >= 0:\\n            actual_len = 0\\n\\n            if record.http_headers:\\n                actual_len = len(record.http_headers.headers_buff)\\n\\n            if not http_headers_only:\\n                actual_len += record.payload_length\\n\\n            record.length = actual_len\\n\\n        record.rec_headers.replace_header('Content-Length', str(record.length))\\n\\n        # write record headers -- encoded as utf-8\\n        # WARC headers can be utf-8 per spec\\n        out.write(record.rec_headers.to_bytes(encoding='utf-8'))\\n\\n        # write headers buffer, if any\\n        if record.http_headers:\\n            out.write(record.http_headers.headers_buff)\\n\\n        if not http_headers_only:\\n            try:\\n                for buf in self._iter_stream(record.raw_stream):\\n                    out.write(buf)\\n            finally:\\n                if hasattr(record, '_orig_stream'):\\n                    record.raw_stream.close()\\n                    record.raw_stream = record._orig_stream\\n\\n        # add two lines\\n        out.write(b'\\r\\n\\r\\n')\\n\\n        out.flush()",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.warcwriter._write_warc_record( self, out, record )"
                },
                {
                    "func_id": 2147,
                    "func_name": "write_record",
                    "func_desc": "write_record",
                    "func_file": "warcwriter",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def write_record(self, record, params=None):\\n        self._write_warc_record(self.out, record)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.warcwriter.write_record( self, record, params )"
                },
                {
                    "func_id": 2148,
                    "func_name": "_do_write_req_resp",
                    "func_desc": "_do_write_req_resp",
                    "func_file": "warcwriter",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _do_write_req_resp(self, req, resp, params):\\n        self._write_warc_record(self.out, resp)\\n        self._write_warc_record(self.out, req)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.warcwriter._do_write_req_resp( self, req, resp, params )"
                },
                {
                    "func_id": 2177,
                    "func_name": "content_stream",
                    "func_desc": "content_stream",
                    "func_file": "recordloader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def content_stream(self):\\n        if not self.http_headers:\\n            return self.raw_stream\\n\\n        encoding = self.http_headers.get_header('content-encoding')\\n\\n        if encoding:\\n            encoding = encoding.lower()\\n\\n            if encoding not in BufferedReader.get_supported_decompressors():\\n                encoding = None\\n\\n        if self.http_headers.get_header('transfer-encoding') == 'chunked':\\n            return ChunkedDataReader(self.raw_stream, decomp_type=encoding)\\n        elif encoding:\\n            return BufferedReader(self.raw_stream, decomp_type=encoding)\\n        else:\\n            return self.raw_stream",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordloader.content_stream( self )"
                },
                {
                    "func_id": 2179,
                    "func_name": "parse_record_stream",
                    "func_desc": "parse_record_stream",
                    "func_file": "recordloader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def parse_record_stream(self, stream,\\n                            statusline=None,\\n                            known_format=None,\\n                            no_record_parse=False,\\n                            ensure_http_headers=False,\\n                            check_digests=False):\\n        \"\"\" Parse file-like stream and return an ArcWarcRecord\\n        encapsulating the record headers, http headers (if any),\\n        and a stream limited to the remainder of the record.\\n\\n        Pass statusline and known_format to detect_type_loader_headers()\\n        to facilitate parsing.\\n        \"\"\"\\n        (the_format, rec_headers) = (self.\\n                                     _detect_type_load_headers(stream,\\n                                                               statusline,\\n                                                               known_format))\\n\\n        if the_format == 'arc':\\n            uri = rec_headers.get_header('uri')\\n            length = rec_headers.get_header('length')\\n            content_type = rec_headers.get_header('content-type')\\n            sub_len = rec_headers.total_len\\n            if uri and uri.startswith('filedesc://'):\\n                rec_type = 'arc_header'\\n            else:\\n                rec_type = 'response'\\n\\n        elif the_format in ('warc', 'arc2warc'):\\n            rec_type = rec_headers.get_header('WARC-Type')\\n            uri = self._ensure_target_uri_format(rec_headers)\\n            length = rec_headers.get_header('Content-Length')\\n            content_type = rec_headers.get_header('Content-Type')\\n            if the_format == 'warc':\\n                sub_len = 0\\n            else:\\n                sub_len = rec_headers.total_len\\n                the_format = 'warc'\\n\\n        is_err = False\\n\\n        try:\\n            if length is not None:\\n                length = int(length) - sub_len\\n                if length < 0:\\n                    is_err = True\\n\\n        except (ValueError, TypeError):\\n            is_err = True\\n\\n        # err condition\\n        if is_err:\\n            length = 0\\n\\n        is_verifying = False\\n        digest_checker = DigestChecker(check_digests)\\n\\n        # limit stream to the length for all valid records\\n        if length is not None and length >= 0:\\n            stream = LimitReader.wrap_stream(stream, length)\\n            if check_digests:\\n                stream, is_verifying = self.wrap_digest_verifying_stream(stream, rec_type,\\n                                                                         rec_headers, digest_checker,\\n                                                                         length=length)\\n\\n        http_headers = None\\n        payload_length = -1\\n\\n        # load http headers if parsing\\n        if not no_record_parse:\\n            start = stream.tell()\\n            http_headers = self.load_http_headers(rec_type, uri, stream, length)\\n            if length and http_headers:\\n                payload_length = length - (stream.tell() - start)\\n\\n        # generate validate http headers (eg. for replay)\\n        if not http_headers and ensure_http_headers:\\n            http_headers = self.default_http_headers(length, content_type)\\n\\n        if is_verifying:\\n            stream.begin_payload()\\n\\n        return ArcWarcRecord(the_format, rec_type,\\n                             rec_headers, stream, http_headers,\\n                             content_type, length, payload_length=payload_length, digest_checker=digest_checker)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recordloader.parse_record_stream( self, stream, statusline, known_format, no_record_parse, ensure_http_headers, check_digests )"
                },
                {
                    "func_id": 2219,
                    "func_name": "load_and_write",
                    "func_desc": "load_and_write",
                    "func_file": "recompressor",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def load_and_write(self, stream, output):\\n        count = 0\\n        with open(output, 'wb') as out:\\n            writer = WARCWriter(filebuf=out, gzip=True)\\n\\n            for record in ArchiveIterator(stream,\\n                                          no_record_parse=False,\\n                                          arc2warc=True,\\n                                          verify_http=False):\\n\\n                writer.write_record(record)\\n                count += 1\\n\\n            return count",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.recompressor.load_and_write( self, stream, output )"
                },
                {
                    "func_id": 2264,
                    "func_name": "_strip_count",
                    "func_desc": "_strip_count",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _strip_count(string, total_read):\\n    length = len(string)\\n    return string.rstrip(), total_read + length",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders._strip_count( string, total_read )"
                },
                {
                    "func_id": 2277,
                    "func_name": "validate_statusline",
                    "func_desc": "validate_statusline",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def validate_statusline(self, valid_statusline):\\n        \"\"\"\\n        Check that the statusline is valid, eg. starts with a numeric\\n        code. If not, replace with passed in valid_statusline\\n        \"\"\"\\n        code = self.get_statuscode()\\n        try:\\n            code = int(code)\\n            assert(code > 0)\\n            return True\\n        except(ValueError, AssertionError):\\n            self.statusline = valid_statusline\\n            return False",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.validate_statusline( self, valid_statusline )"
                },
                {
                    "func_id": 2278,
                    "func_name": "add_range",
                    "func_desc": "add_range",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def add_range(self, start, part_len, total_len):\\n        \"\"\"\\n        Add range headers indicating that this a partial response\\n        \"\"\"\\n        content_range = 'bytes {0}-{1}/{2}'.format(start,\\n                                                   start + part_len - 1,\\n                                                   total_len)\\n\\n        self.statusline = '206 Partial Content'\\n        self.replace_header('Content-Range', content_range)\\n        self.replace_header('Content-Length', str(part_len))\\n        self.replace_header('Accept-Ranges', 'bytes')\\n        return self",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.add_range( self, start, part_len, total_len )"
                },
                {
                    "func_id": 2293,
                    "func_name": "make_warc_id",
                    "func_desc": "make_warc_id",
                    "func_file": "statusandheaders",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def make_warc_id(id_=None):\\n        if not id_:\\n            id_ = uuid.uuid4()\\n        return '<urn:uuid:{0}>'.format(id_)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.statusandheaders.make_warc_id( id_ )"
                },
                {
                    "func_id": 2310,
                    "func_name": "_create_record_iter",
                    "func_desc": "_create_record_iter",
                    "func_file": "indexer",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _create_record_iter(self, input_):\\n        return ArchiveIterator(input_,\\n                               no_record_parse=not self.record_parse,\\n                               arc2warc=True,\\n                               verify_http=self.verify_http)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.indexer._create_record_iter( self, input_ )"
                },
                {
                    "func_id": 2314,
                    "func_name": "_write_line",
                    "func_desc": "_write_line",
                    "func_file": "indexer",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _write_line(self, out, index, record, filename):\\n        out.write(json.dumps(index) + '\\n')",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.indexer._write_line( self, out, index, record, filename )"
                },
                {
                    "func_id": 2338,
                    "func_name": "wrap_stream",
                    "func_desc": "wrap_stream",
                    "func_file": "limitreader",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def wrap_stream(stream, content_length):\\n        \"\"\"\\n        If given content_length is an int > 0, wrap the stream\\n        in a LimitReader. Otherwise, return the stream unaltered\\n        \"\"\"\\n        try:\\n            content_length = int(content_length)\\n            if content_length >= 0:\\n                # optimize: if already a LimitStream, set limit to\\n                # the smaller of the two limits\\n                if isinstance(stream, LimitReader):\\n                    stream.limit = min(stream.limit, content_length)\\n                else:\\n                    stream = LimitReader(stream, content_length)\\n\\n        except (ValueError, TypeError):\\n            pass\\n\\n        return stream",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.limitreader.wrap_stream( stream, content_length )"
                },
                {
                    "func_id": 2373,
                    "func_name": "putrequest",
                    "func_desc": "putrequest",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def putrequest(self, *args, **kwargs):\\n        if self.recorder:\\n            self.recorder.start()\\n        return orig_connection.putrequest(self, *args, **kwargs)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.putrequest( self, *args, **kwargs )"
                },
                {
                    "func_id": 2377,
                    "func_name": "_create_buffer",
                    "func_desc": "_create_buffer",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _create_buffer(self):\\n        return SpooledTemporaryFile(BUFF_SIZE)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http._create_buffer( self )"
                },
                {
                    "func_id": 2378,
                    "func_name": "set_remote_ip",
                    "func_desc": "set_remote_ip",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def set_remote_ip(self, remote_ip):\\n        if self.record_ip and remote_ip:  #pragma: no cover\\n            self.warc_headers['WARC-IP-Address'] = remote_ip",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.set_remote_ip( self, remote_ip )"
                },
                {
                    "func_id": 2379,
                    "func_name": "write_request",
                    "func_desc": "write_request",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def write_request(self, buff):\\n        if self.started_req:\\n            self.request_out.write(buff)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.write_request( self, buff )"
                },
                {
                    "func_id": 2380,
                    "func_name": "write_response",
                    "func_desc": "write_response",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def write_response(self, buff):\\n        if self.started_req:\\n            self.response_out.write(buff)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.write_response( self, buff )"
                },
                {
                    "func_id": 2381,
                    "func_name": "_create_record",
                    "func_desc": "_create_record",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _create_record(self, out, record_type):\\n        length = out.tell()\\n        out.seek(0)\\n        return self.writer.create_warc_record(\\n                warc_headers_dict=self.warc_headers,\\n                uri=self.url,\\n                record_type=record_type,\\n                payload=out,\\n                length=length)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http._create_record( self, out, record_type )"
                },
                {
                    "func_id": 2384,
                    "func_name": "make_recording_response",
                    "func_desc": "make_recording_response",
                    "func_file": "capture_http",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def make_recording_response(*args, **kwargs):\\n            return RecordingHTTPResponse(self.recorder, *args, **kwargs)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.capture_http.make_recording_response( *args, **kwargs )"
                },
                {
                    "func_id": 2415,
                    "func_name": "_iterate_records",
                    "func_desc": "_iterate_records",
                    "func_file": "archiveiterator",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _iterate_records(self):\\n        \"\"\" iterate over each record\\n        \"\"\"\\n        raise_invalid_gzip = False\\n        empty_record = False\\n\\n        while True:\\n            try:\\n                self.record = self._next_record(self.next_line)\\n                if raise_invalid_gzip:\\n                    self._raise_invalid_gzip_err()\\n\\n                yield self.record\\n\\n            except EOFError:\\n                empty_record = True\\n\\n            self.read_to_end()\\n\\n            if self.reader.decompressor:\\n                # if another gzip member, continue\\n                if self.reader.read_next_member():\\n                    continue\\n\\n                # if empty record, then we're done\\n                elif empty_record:\\n                    break\\n\\n                # otherwise, probably a gzip\\n                # containing multiple non-chunked records\\n                # raise this as an error\\n                else:\\n                    raise_invalid_gzip = True\\n\\n            # non-gzip, so we're done\\n            elif empty_record:\\n                break\\n\\n        self.close()",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.archiveiterator._iterate_records( self )"
                },
                {
                    "func_id": 2419,
                    "func_name": "get_record_offset",
                    "func_desc": "get_record_offset",
                    "func_file": "archiveiterator",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_record_offset(self):\\n        if not self.member_info:\\n            self.read_to_end()\\n\\n        return self.member_info[0]",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.archiveiterator.get_record_offset( self )"
                },
                {
                    "func_id": 2420,
                    "func_name": "get_record_length",
                    "func_desc": "get_record_length",
                    "func_file": "archiveiterator",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_record_length(self):\\n        if not self.member_info:\\n            self.read_to_end()\\n\\n        return self.member_info[1]",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.archiveiterator.get_record_length( self )"
                },
                {
                    "func_id": 2421,
                    "func_name": "_next_record",
                    "func_desc": "_next_record",
                    "func_file": "archiveiterator",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _next_record(self, next_line):\\n        \"\"\" Use loader to parse the record from the reader stream\\n        Supporting warc and arc records\\n        \"\"\"\\n        record = self.loader.parse_record_stream(self.reader,\\n                                                 next_line,\\n                                                 self.known_format,\\n                                                 self.no_record_parse,\\n                                                 self.ensure_http_headers,\\n                                                 self.check_digests)\\n\\n        self.member_info = None\\n\\n        # Track known format for faster parsing of other records\\n        if not self.mixed_arc_warc:\\n            self.known_format = record.format\\n\\n        return record",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.archiveiterator._next_record( self, next_line )"
                }
            ]
        },
        {
            "cluster_id": 35,
            "feature_id": 39,
            "feature_desc": "gamma=0.0368; k=2; a=0.25; combined=0.439; stability(ARI)=1.000; sep=0.072",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 860,
                    "func_name": "_unarchive_cmd",
                    "func_desc": "_unarchive_cmd",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _unarchive_cmd(path):\\n    \"\"\"Look up the unarchive command to use with the given file extension,\\n    or raise KeyError if there is no matching command.\"\"\"\\n    for ext, unarchive_cmd in sorted(_EXT_TO_UNARCHIVE_CMD.items()):\\n        # use this so we can match e.g. mrjob-0.7.0.tar.gz\\n        if path.endswith(ext):\\n            return unarchive_cmd\\n\\n    raise KeyError('unknown archive type: %s' % path)",
                    "func_fullName": "mrjob.bin._unarchive_cmd( path )"
                },
                {
                    "func_id": 903,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, **kwargs):\\n        super(MRJobBinRunner, self).__init__(**kwargs)\\n\\n        # where a zip file of the mrjob library is stored locally\\n        self._mrjob_zip_path = None\\n\\n        # we'll create the setup wrapper scripts later\\n        self._setup_wrapper_script_path = None\\n        self._manifest_setup_script_path = None\\n        self._spark_python_wrapper_path = None\\n\\n        # self._setup is a list of shell commands with path dicts\\n        # interleaved; see mrjob.setup.parse_setup_cmd() for details\\n        self._setup = [parse_setup_cmd(cmd) for cmd in self._opts['setup']]\\n\\n        for cmd in self._setup:\\n            for token in cmd:\\n                if isinstance(token, dict):\\n                    # convert dir archives tokens to archives\\n                    if token['type'] == 'dir':\\n                        # feed the archive's path to self._working_dir_mgr\\n                        token['path'] = self._dir_archive_path(token['path'])\\n                        token['type'] = 'archive'\\n\\n                    self._working_dir_mgr.add(**token)\\n\\n        # warning: no setup scripts on Spark when no working dir\\n        if self._setup and self._has_pyspark_steps() and not(\\n                self._spark_executors_have_own_wd()):\\n            log.warning(\"setup commands aren't supported on Spark master %r\" %\\n                        self._spark_master())\\n\\n        # --py-files on Spark doesn't allow '#' (see #1375)\\n        if any('#' in path for path in self._opts['py_files']):\\n            raise ValueError(\"py_files cannot contain '#'\")\\n\\n        # Keep track of where the spark-submit binary is\\n        self._spark_submit_bin = self._opts['spark_submit_bin']",
                    "func_fullName": "mrjob.bin.__init__( self, **kwargs )"
                },
                {
                    "func_id": 904,
                    "func_name": "_default_opts",
                    "func_desc": "_default_opts",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _default_opts(cls):\\n        return combine_dicts(\\n            super(MRJobBinRunner, cls)._default_opts(),\\n            dict(\\n                read_logs=True,\\n            )\\n        )",
                    "func_fullName": "mrjob.bin._default_opts( cls )"
                },
                {
                    "func_id": 905,
                    "func_name": "_fix_opt",
                    "func_desc": "_fix_opt",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _fix_opt(self, opt_key, opt_value, source):\\n        \"\"\"Check sh_bin\"\"\"\\n        opt_value = super(MRJobBinRunner, self)._fix_opt(\\n            opt_key, opt_value, source)\\n\\n        # check that sh_bin doesn't have too many args\\n        if opt_key == 'sh_bin':\\n            # opt_value is usually a string, combiner makes it a list of args\\n            sh_bin = combine_cmds(opt_value)\\n\\n            # empty sh_bin just means to use the default, see #1926\\n\\n            # make these hard requirements in v0.7.0?\\n            if len(sh_bin) > 1 and not os.path.isabs(sh_bin[0]):\\n                log.warning('sh_bin (from %s) should use an absolute path'\\n                            ' if you want it to take arguments' % source)\\n            elif len(sh_bin) > 2:\\n                log.warning('sh_bin (from %s) should not take more than one'\\n                            ' argument' % source)\\n\\n        return opt_value",
                    "func_fullName": "mrjob.bin._fix_opt( self, opt_key, opt_value, source )"
                },
                {
                    "func_id": 906,
                    "func_name": "_python_bin",
                    "func_desc": "_python_bin",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _python_bin(self):\\n        \"\"\"Python binary used for everything other than invoking the job.\\n\\n        For running job tasks (e.g. ``--mapper``, ``--spark``), we use\\n        :py:meth:`_task_python_bin`, which can be set to a different value\\n        by setting :mrjob-opt:`task_python_bin`.\\n\\n        Ways mrjob uses Python other than running tasks:\\n         * file locking in setup wrapper scripts\\n         * finding site-packages dir to bootstrap mrjob on clusters\\n         * invoking ``cat.py`` in local mode\\n         * the Python binary for Spark (``$PYSPARK_PYTHON``)\\n        \"\"\"\\n        # python_bin isn't an option for inline runners\\n        return self._opts['python_bin'] or self._default_python_bin()",
                    "func_fullName": "mrjob.bin._python_bin( self )"
                },
                {
                    "func_id": 907,
                    "func_name": "_task_python_bin",
                    "func_desc": "_task_python_bin",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _task_python_bin(self):\\n        \"\"\"Python binary used to invoke job with ``--mapper``,\\n        ``--reducer``, ``--spark``, etc.\"\"\"\\n        return (self._opts['task_python_bin'] or\\n                self._python_bin())",
                    "func_fullName": "mrjob.bin._task_python_bin( self )"
                },
                {
                    "func_id": 908,
                    "func_name": "_default_python_bin",
                    "func_desc": "_default_python_bin",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _default_python_bin(self, local=False):\\n        \"\"\"The default python command. If local is true, try to use\\n        sys.executable. Otherwise use 'python2.7' or 'python3' as appropriate.\\n\\n        This returns a single-item list (because it's a command).\\n        \"\"\"\\n        is_pypy = (python_implementation() == 'PyPy')\\n\\n        if local and sys.executable:\\n            return [sys.executable]\\n        else:\\n            if PY2:\\n                return ['pypy'] if is_pypy else ['python2.7']\\n            else:\\n                return ['pypy3'] if is_pypy else ['python3']",
                    "func_fullName": "mrjob.bin._default_python_bin( self, local )"
                },
                {
                    "func_id": 909,
                    "func_name": "_script_args_for_step",
                    "func_desc": "_script_args_for_step",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _script_args_for_step(self, step_num, mrc, input_manifest=False):\\n        args = (self._task_python_bin() +\\n                [self._working_dir_mgr.name('file', self._script_path)] +\\n                self._args_for_task(step_num, mrc))\\n\\n        if input_manifest and mrc == 'mapper':\\n            wrapper = self._manifest_setup_script_path\\n        elif self._setup_wrapper_script_path:\\n            wrapper = self._setup_wrapper_script_path\\n        else:\\n            return args\\n\\n        return (self._sh_bin() + [\\n            self._working_dir_mgr.name('file', wrapper)] + args)",
                    "func_fullName": "mrjob.bin._script_args_for_step( self, step_num, mrc, input_manifest )"
                },
                {
                    "func_id": 910,
                    "func_name": "_substep_args",
                    "func_desc": "_substep_args",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _substep_args(self, step_num, mrc):\\n        step = self._get_step(step_num)\\n\\n        if step[mrc]['type'] == 'command':\\n            cmd = step[mrc]['command']\\n\\n            # never wrap custom hadoop streaming commands in bash\\n            if isinstance(cmd, string_types):\\n                return shlex_split(cmd)\\n            else:\\n                return cmd\\n\\n        elif step[mrc]['type'] == 'script':\\n            script_args = self._script_args_for_step(\\n                step_num, mrc, input_manifest=step.get('input_manifest'))\\n\\n            if 'pre_filter' in step[mrc]:\\n                return self._sh_wrap(\\n                    '%s | %s' % (step[mrc]['pre_filter'],\\n                                 cmd_line(script_args)))\\n            else:\\n                return script_args\\n        else:\\n            raise ValueError(\"Invalid %s step %d: %r\" % (\\n                mrc, step_num, step[mrc]))",
                    "func_fullName": "mrjob.bin._substep_args( self, step_num, mrc )"
                },
                {
                    "func_id": 911,
                    "func_name": "_render_substep",
                    "func_desc": "_render_substep",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _render_substep(self, step_num, mrc):\\n        step = self._get_step(step_num)\\n\\n        if mrc in step:\\n            # cmd_line() does things that shell is fine with but\\n            # Hadoop Streaming finds confusing.\\n            return _hadoop_cmd_line(self._substep_args(step_num, mrc))\\n        else:\\n            if mrc == 'mapper':\\n                return 'cat'\\n            else:\\n                return None",
                    "func_fullName": "mrjob.bin._render_substep( self, step_num, mrc )"
                },
                {
                    "func_id": 918,
                    "func_name": "_py_files",
                    "func_desc": "_py_files",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _py_files(self):\\n        \"\"\"Everything in the *py_files* opt, plus a .zip of the mrjob\\n        library if needed.\\n        \"\"\"\\n        py_files = list(self._opts['py_files'])\\n\\n        if self._bootstrap_mrjob():\\n            py_files.append(self._create_mrjob_zip())\\n\\n        return py_files",
                    "func_fullName": "mrjob.bin._py_files( self )"
                },
                {
                    "func_id": 919,
                    "func_name": "_create_setup_wrapper_scripts",
                    "func_desc": "_create_setup_wrapper_scripts",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _create_setup_wrapper_scripts(self):\\n        \"\"\"Create the setup wrapper script, and write it into our local temp\\n        directory (by default, to a file named setup-wrapper.sh).\\n\\n        This will set ``self._setup_wrapper_script_path``, and add it to\\n        ``self._working_dir_mgr``\\n\\n        This will do nothing if ``self._setup`` is empty or\\n        this method has already been called.\\n\\n        If *local* is true, use local line endings (e.g. Windows). Otherwise,\\n        use UNIX line endings (see #1071).\\n        \"\"\"\\n        if self._has_hadoop_streaming_steps():\\n            streaming_setup = self._py_files_setup() + self._setup\\n\\n            if streaming_setup and not self._setup_wrapper_script_path:\\n\\n                self._setup_wrapper_script_path = self._write_setup_script(\\n                    streaming_setup, 'setup-wrapper.sh',\\n                    'streaming setup wrapper script')\\n\\n            if (self._uses_input_manifest() and not\\n                    self._manifest_setup_script_path):\\n\\n                self._manifest_setup_script_path = self._write_setup_script(\\n                    streaming_setup, 'manifest-setup.sh',\\n                    'manifest setup wrapper script',\\n                    manifest=True)\\n\\n        if (self._has_pyspark_steps() and\\n                self._spark_executors_have_own_wd() and\\n                not self._spark_python_wrapper_path):\\n\\n            pyspark_setup = self._pyspark_setup()\\n            if pyspark_setup:\\n                self._spark_python_wrapper_path = self._write_setup_script(\\n                    pyspark_setup,\\n                    'python-wrapper.sh', 'Spark Python wrapper script',\\n                    wrap_python=True)",
                    "func_fullName": "mrjob.bin._create_setup_wrapper_scripts( self )"
                },
                {
                    "func_id": 921,
                    "func_name": "_py_files_setup",
                    "func_desc": "_py_files_setup",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _py_files_setup(self):\\n        \"\"\"A list of additional setup commands to emulate Spark's\\n        --py-files option on Hadoop Streaming.\"\"\"\\n        result = []\\n\\n        for py_file in self._py_files():\\n            path_dict = {'type': 'file', 'name': None, 'path': py_file}\\n            self._working_dir_mgr.add(**path_dict)\\n            result.append(['export PYTHONPATH=', path_dict, ':$PYTHONPATH'])\\n\\n        return result",
                    "func_fullName": "mrjob.bin._py_files_setup( self )"
                },
                {
                    "func_id": 922,
                    "func_name": "_write_setup_script",
                    "func_desc": "_write_setup_script",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _write_setup_script(self, setup, filename, desc,\\n                            manifest=False, wrap_python=False):\\n        \"\"\"Write a setup script and return its path.\"\"\"\\n        contents = self._setup_wrapper_script_content(\\n            setup, manifest=manifest, wrap_python=wrap_python)\\n\\n        path = os.path.join(self._get_local_tmp_dir(), filename)\\n        self._write_script(contents, path, desc)\\n\\n        self._working_dir_mgr.add('file', path)\\n\\n        return path",
                    "func_fullName": "mrjob.bin._write_setup_script( self, setup, filename, desc, manifest, wrap_python )"
                },
                {
                    "func_id": 923,
                    "func_name": "_create_mrjob_zip",
                    "func_desc": "_create_mrjob_zip",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _create_mrjob_zip(self):\\n        \"\"\"Make a zip of the mrjob library, without .pyc or .pyo files,\\n        This will also set ``self._mrjob_zip_path`` and return it.\\n\\n        Typically called from\\n        :py:meth:`_create_setup_wrapper_scripts`.\\n\\n        It's safe to call this method multiple times (we'll only create\\n        the zip file once.)\\n        \"\"\"\\n        if not self._mrjob_zip_path:\\n            # find mrjob library\\n            import mrjob\\n\\n            if not os.path.basename(mrjob.__file__).startswith('__init__.'):\\n                raise Exception(\\n                    \"Bad path for mrjob library: %s; can't bootstrap mrjob\",\\n                    mrjob.__file__)\\n\\n            mrjob_dir = os.path.dirname(mrjob.__file__) or '.'\\n\\n            zip_path = os.path.join(self._get_local_tmp_dir(), 'mrjob.zip')\\n\\n            def filter_path(path):\\n                filename = os.path.basename(path)\\n                return not(filename.lower().endswith('.pyc') or\\n                           filename.lower().endswith('.pyo') or\\n                           # filter out emacs backup files\\n                           filename.endswith('~') or\\n                           # filter out emacs lock files\\n                           filename.startswith('.#') or\\n                           # filter out MacFuse resource forks\\n                           filename.startswith('._'))\\n\\n            log.debug('archiving %s -> %s as %s' % (\\n                mrjob_dir, zip_path, os.path.join('mrjob', '')))\\n            zip_dir(mrjob_dir, zip_path, filter=filter_path, prefix='mrjob')\\n\\n            self._mrjob_zip_path = zip_path\\n\\n        return self._mrjob_zip_path",
                    "func_fullName": "mrjob.bin._create_mrjob_zip( self )"
                },
                {
                    "func_id": 924,
                    "func_name": "_setup_wrapper_script_content",
                    "func_desc": "_setup_wrapper_script_content",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _setup_wrapper_script_content(\\n            self, setup, manifest=False, wrap_python=False):\\n        \"\"\"Return a (Bourne) shell script that runs the setup commands and then\\n        executes whatever is passed to it (this will be our mapper/reducer),\\n        as a list of strings (one for each line, including newlines).\\n\\n        We obtain a file lock so that two copies of the setup commands\\n        cannot run simultaneously on the same machine (this helps for running\\n        :command:`make` on a shared source code archive, for example).\\n        \"\"\"\\n        lines = []\\n\\n        # TODO: this is very similar to _start_of_sh_script() in cloud.py\\n\\n        if wrap_python:\\n            # start with shebang\\n            sh_bin = self._sh_bin()\\n\\n            if os.path.isabs(sh_bin[0]):\\n                shebang_bin = sh_bin\\n            else:\\n                shebang_bin = ['/usr/bin/env'] + list(sh_bin)\\n\\n            if len(shebang_bin) > 2:\\n                # Linux limits shebang to one binary and one arg\\n                shebang_bin = shebang_bin[:2]\\n                log.warning('Limiting shebang to two arguments:'\\n                            '#!%s' % cmd_line(shebang_bin))\\n\\n            lines.append('#!%s' % cmd_line(shebang_bin))\\n\\n        # hook for 'set -e', etc.\\n        pre_commands = self._sh_pre_commands()\\n        if pre_commands:\\n            for cmd in pre_commands:\\n                lines.append(cmd)\\n            lines.append('')\\n\\n        if setup:\\n            lines.extend(self._setup_cmd_content(setup))\\n\\n        # handle arguments to the script\\n        if wrap_python:\\n            # pretend to be python ($@ is arguments to the python binary)\\n            python_bin = self._task_python_bin()\\n            lines.append('%s \"$@\"' % cmd_line(python_bin))\\n        elif manifest:\\n            # arguments ($@) are a command\\n            # eventually runs: \"$@\" $INPUT_PATH $INPUT_URI\\n            lines.extend(self._manifest_download_content())\\n        else:\\n            # arguments ($@) are a command, just run it\\n            lines.append('\"$@\"')\\n\\n        return lines",
                    "func_fullName": "mrjob.bin._setup_wrapper_script_content( self, setup, manifest, wrap_python )"
                },
                {
                    "func_id": 925,
                    "func_name": "_setup_cmd_content",
                    "func_desc": "_setup_cmd_content",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _setup_cmd_content(self, setup):\\n        \"\"\"Write setup script content to obtain a file lock, run setup\\n        commands in a way that doesn't perturb the script, and then\\n        release the lock and return to the original working directory.\"\"\"\\n        lines = []\\n\\n        lines.append('# store $PWD')\\n        lines.append('__mrjob_PWD=$PWD')\\n        lines.append('')\\n\\n        lines.append('# obtain exclusive file lock')\\n        # Basically, we're going to tie file descriptor 9 to our lockfile,\\n        # use a subprocess to obtain a lock (which we somehow inherit too),\\n        # and then release the lock by closing the file descriptor.\\n        # File descriptors 10 and higher are used internally by the shell,\\n        # so 9 is as out-of-the-way as we can get.\\n        lines.append('exec 9>/tmp/wrapper.lock.%s' % self._job_key)\\n        # would use flock(1), but it's not always available\\n        lines.append(\"%s -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\" %\\n                     cmd_line(self._python_bin()))\\n        lines.append('')\\n\\n        lines.append('# setup commands')\\n        # group setup commands so we can redirect their input/output (see\\n        # below). Don't use parens; this would invoke a subshell, which would\\n        # keep us from exporting environment variables to the task.\\n        lines.append('{')\\n        for cmd in setup:\\n            # reconstruct the command line, substituting $__mrjob_PWD/<name>\\n            # for path dicts\\n            line = '  '  # indent, since these commands are in a group\\n            for token in cmd:\\n                if isinstance(token, dict):\\n                    # it's a path dictionary\\n                    line += '$__mrjob_PWD/'\\n                    line += pipes.quote(self._working_dir_mgr.name(**token))\\n                else:\\n                    # it's raw script\\n                    line += token\\n            lines.append(line)\\n        # redirect setup commands' input/output so they don't interfere\\n        # with the task (see Issue #803).\\n        lines.append('} 0</dev/null 1>&2')\\n        lines.append('')\\n\\n        lines.append('# release exclusive file lock')\\n        lines.append('exec 9>&-')\\n        lines.append('')\\n\\n        lines.append('# run task from the original working directory')\\n        lines.append('cd $__mrjob_PWD')\\n\\n        return lines",
                    "func_fullName": "mrjob.bin._setup_cmd_content( self, setup )"
                },
                {
                    "func_id": 926,
                    "func_name": "_manifest_download_content",
                    "func_desc": "_manifest_download_content",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _manifest_download_content(self):\\n        \"\"\"write the part of the manifest setup script after setup, that\\n        downloads the input file, runs the script, and then deletes\\n        the file.\"\"\"\\n        lines = []\\n\\n        lines.append('{')\\n\\n        # read URI from stdin\\n        lines.append('  # read URI of input file from stdin')\\n        lines.append('  INPUT_URI=$(cut -f 2)')\\n        lines.append('')\\n\\n        # pick file extension (e.g. \".warc.gz\")\\n        lines.append('  # pick file extension')\\n        lines.append(\"  FILE_EXT=$(basename $INPUT_URI | sed -e 's/^[^.]*//')\")\\n        lines.append('')\\n\\n        # pick a unique name in the current directory to download the file to\\n        lines.append('  # pick filename to download to')\\n        lines.append('  INPUT_PATH=$(mktemp ./input-XXXXXXXXXX$FILE_EXT)')\\n        lines.append('  rm $INPUT_PATH')\\n        lines.append('')\\n\\n        # download the file (using different commands depending on the path)\\n        lines.append('  # download the input file')\\n        lines.append('  case $INPUT_URI in')\\n        download_cmds = (\\n            list(self._manifest_download_commands()) + [('*', 'cp')])\\n        for glob, cmd in download_cmds:\\n            lines.append('    %s)' % glob)\\n            lines.append('      %s $INPUT_URI $INPUT_PATH' % cmd)\\n            lines.append('      ;;')\\n        lines.append('  esac')\\n        lines.append('')\\n\\n        # unpack .bz2 and .gz files\\n        lines.append('  # if input file is compressed, unpack it')\\n        lines.append('  case $INPUT_PATH in')\\n        for ext, cmd in self._manifest_uncompress_commands():\\n            lines.append('    *.%s)' % ext)\\n            lines.append('      %s $INPUT_PATH' % cmd)\\n            lines.append(\"      INPUT_PATH=\"\\n                         r\"$(echo $INPUT_PATH | sed -e 's/\\.%s$//')\" % ext)\\n            lines.append('      ;;')\\n        lines.append('  esac')\\n        lines.append('} 1>&2')\\n        lines.append('')\\n\\n        # don't exit if script fails\\n        lines.append('# run our mrjob script')\\n        lines.append('set +e')\\n        # pass input path and URI to script\\n        lines.append('\"$@\" $INPUT_PATH $INPUT_URI')\\n        lines.append('')\\n\\n        # save return code, turn off echo\\n        lines.append('# if script fails, print input URI before exiting')\\n        lines.append('{ RETURNCODE=$?; set +x; } 1>&2 2>/dev/null')\\n        lines.append('')\\n\\n        lines.append('{')\\n\\n        # handle errors\\n        lines.append('  if [ $RETURNCODE -ne 0 ]')\\n        lines.append('  then')\\n        lines.append('    echo')\\n        lines.append('    echo \"while reading input from $INPUT_URI\"')\\n        lines.append('  fi')\\n        lines.append('')\\n\\n        # clean up input\\n        lines.append('  rm $INPUT_PATH')\\n        lines.append('} 1>&2')\\n        lines.append('')\\n\\n        # exit with correct status\\n        lines.append('exit $RETURNCODE')\\n\\n        return lines",
                    "func_fullName": "mrjob.bin._manifest_download_content( self )"
                },
                {
                    "func_id": 927,
                    "func_name": "_manifest_download_commands",
                    "func_desc": "_manifest_download_commands",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _manifest_download_commands(self):\\n        \"\"\"Return a list of ``(glob, cmd)``, where *glob*\\n        matches a path or URI to download, and download command is a command\\n        to download it (e.g. ```hadoop fs -copyToLocal``), as a\\n        string.\\n\\n        Redefine this in your subclass. More specific blobs should come first.\\n        \"\"\"\\n        return []",
                    "func_fullName": "mrjob.bin._manifest_download_commands( self )"
                },
                {
                    "func_id": 928,
                    "func_name": "_manifest_uncompress_commands",
                    "func_desc": "_manifest_uncompress_commands",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _manifest_uncompress_commands(self):\\n        \"\"\"Return a list of ``(ext, cmd)`` where ``ext`` is a file extension\\n        (e.g. ``gz``) and ``cmd`` is a command to uncompress it (e.g.\\n        ``gunzip``).\"\"\"\\n        return [\\n            ('bz2', 'bunzip2'),\\n            ('gz', 'gunzip'),\\n        ]",
                    "func_fullName": "mrjob.bin._manifest_uncompress_commands( self )"
                },
                {
                    "func_id": 929,
                    "func_name": "_sh_bin",
                    "func_desc": "_sh_bin",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _sh_bin(self):\\n        \"\"\"The sh binary and any arguments, as a list. Override this\\n        if, for example, a runner needs different default values\\n        depending on circumstances (see :py:class:`~mrjob.emr.EMRJobRunner`).\\n        \"\"\"\\n        return self._opts['sh_bin'] or self._default_sh_bin()",
                    "func_fullName": "mrjob.bin._sh_bin( self )"
                },
                {
                    "func_id": 930,
                    "func_name": "_default_sh_bin",
                    "func_desc": "_default_sh_bin",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _default_sh_bin(self):\\n        \"\"\"The default sh binary, if :mrjob-opt:`sh_bin` isn't set.\"\"\"\\n        return ['/bin/sh', '-ex']",
                    "func_fullName": "mrjob.bin._default_sh_bin( self )"
                },
                {
                    "func_id": 931,
                    "func_name": "_sh_pre_commands",
                    "func_desc": "_sh_pre_commands",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _sh_pre_commands(self):\\n        \"\"\"A list of lines to put at the very start of any sh script\\n        (e.g. ``set -e`` when ``sh -e`` wont work, see #1549)\\n        \"\"\"\\n        return []",
                    "func_fullName": "mrjob.bin._sh_pre_commands( self )"
                },
                {
                    "func_id": 932,
                    "func_name": "_sh_wrap",
                    "func_desc": "_sh_wrap",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _sh_wrap(self, cmd_str):\\n        \"\"\"Helper for _substep_args()\\n\\n        Wrap command in sh -c '...' to allow for pipes, etc.\\n        Use *sh_bin* option.\"\"\"\\n        # prepend set -e etc.\\n        cmd_str = '; '.join(self._sh_pre_commands() + [cmd_str])\\n\\n        return self._sh_bin() + ['-c', cmd_str]",
                    "func_fullName": "mrjob.bin._sh_wrap( self, cmd_str )"
                },
                {
                    "func_id": 943,
                    "func_name": "filter_path",
                    "func_desc": "filter_path",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "            def filter_path(path):\\n                filename = os.path.basename(path)\\n                return not(filename.lower().endswith('.pyc') or\\n                           filename.lower().endswith('.pyo') or\\n                           # filter out emacs backup files\\n                           filename.endswith('~') or\\n                           # filter out emacs lock files\\n                           filename.startswith('.#') or\\n                           # filter out MacFuse resource forks\\n                           filename.startswith('._'))",
                    "func_fullName": "mrjob.bin.filter_path( path )"
                },
                {
                    "func_id": 1354,
                    "func_name": "cmd_line",
                    "func_desc": "cmd_line",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def cmd_line(args):\\n    \"\"\"build a command line that works in a shell.\\n    \"\"\"\\n    args = [str(x) for x in args]\\n    return ' '.join(pipes.quote(x) for x in args)",
                    "func_fullName": "mrjob.util.cmd_line( args )"
                },
                {
                    "func_id": 1355,
                    "func_name": "expand_path",
                    "func_desc": "expand_path",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def expand_path(path):\\n    \"\"\"Resolve ``~`` (home dir) and environment variables in *path*.\\n\\n    If *path* is ``None``, return ``None``.\\n    \"\"\"\\n    if path is None:\\n        return None\\n    else:\\n        return os.path.expanduser(os.path.expandvars(path))",
                    "func_fullName": "mrjob.util.expand_path( path )"
                },
                {
                    "func_id": 1356,
                    "func_name": "file_ext",
                    "func_desc": "file_ext",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def file_ext(filename):\\n    \"\"\"return the file extension, including the ``.``\\n\\n    >>> file_ext('foo.tar.gz')\\n    '.tar.gz'\\n\\n    >>> file_ext('.emacs')\\n    ''\\n\\n    >>> file_ext('.mrjob.conf')\\n    '.conf'\\n    \"\"\"\\n    stripped_name = filename.lstrip('.')\\n    dot_index = stripped_name.find('.')\\n\\n    if dot_index == -1:\\n        return ''\\n    return stripped_name[dot_index:]",
                    "func_fullName": "mrjob.util.file_ext( filename )"
                },
                {
                    "func_id": 1357,
                    "func_name": "log_to_null",
                    "func_desc": "log_to_null",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def log_to_null(name=None):\\n    \"\"\"Set up a null handler for the given stream, to suppress\\n    \"no handlers could be found\" warnings.\"\"\"\\n    logger = logging.getLogger(name)\\n    logger.addHandler(NullHandler())",
                    "func_fullName": "mrjob.util.log_to_null( name )"
                },
                {
                    "func_id": 1359,
                    "func_name": "random_identifier",
                    "func_desc": "random_identifier",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def random_identifier():\\n    \"\"\"A random 16-digit hex string.\"\"\"\\n    return '%016x' % random.randint(0, 2 ** 64 - 1)",
                    "func_fullName": "mrjob.util.random_identifier(  )"
                },
                {
                    "func_id": 1360,
                    "func_name": "safeeval",
                    "func_desc": "safeeval",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def safeeval(expr, globals=None, locals=None):\\n    \"\"\"Like eval, but with nearly everything in the environment\\n    blanked out, so that it's difficult to cause mischief.\\n\\n    *globals* and *locals* are optional dictionaries mapping names to\\n    values for those names (just like in :py:func:`eval`).\\n    \"\"\"\\n    # blank out builtins, but keep None, True, and False\\n    from mrjob.py2 import PY2\\n    safe_globals = {\\n        'False': False,\\n        'None': None,\\n        'True': True,\\n        '__builtin__': None,\\n        '__builtins__': None,\\n        'set': set\\n    }\\n\\n    # xrange is range in Python 3\\n    if PY2:\\n        safe_globals['xrange'] = xrange\\n    else:\\n        safe_globals['range'] = range\\n\\n    # PyPy needs special magic\\n    def open(*args, **kwargs):\\n        raise NameError(\"name 'open' is not defined\")\\n    safe_globals['open'] = open\\n\\n    # add the user-specified global variables\\n    if globals:\\n        safe_globals.update(globals)\\n\\n    return eval(expr, safe_globals, locals)",
                    "func_fullName": "mrjob.util.safeeval( expr, globals, locals )"
                },
                {
                    "func_id": 1361,
                    "func_name": "save_current_environment",
                    "func_desc": "save_current_environment",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def save_current_environment():\\n    \"\"\" Context manager that saves os.environ and loads\\n        it back again after execution\\n    \"\"\"\\n    original_environ = os.environ.copy()\\n\\n    try:\\n        yield\\n\\n    finally:\\n        os.environ.clear()\\n        os.environ.update(original_environ)",
                    "func_fullName": "mrjob.util.save_current_environment(  )"
                },
                {
                    "func_id": 1362,
                    "func_name": "save_cwd",
                    "func_desc": "save_cwd",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def save_cwd():\\n    \"\"\"Context manager that saves the current working directory,\\n    and chdir's back to it after execution.\"\"\"\\n    original_cwd = os.getcwd()\\n\\n    try:\\n        yield\\n\\n    finally:\\n        os.chdir(original_cwd)",
                    "func_fullName": "mrjob.util.save_cwd(  )"
                },
                {
                    "func_id": 1363,
                    "func_name": "save_sys_std",
                    "func_desc": "save_sys_std",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def save_sys_std():\\n    \"\"\"Context manager that saves the current values of `sys.stdin`,\\n    `sys.stdout`, and `sys.stderr`, and flushes these filehandles before\\n    and after switching them out.\"\"\"\\n\\n    stdin, stdout, stderr = sys.stdin, sys.stdout, sys.stderr\\n\\n    try:\\n        sys.stdout.flush()\\n        sys.stderr.flush()\\n\\n        yield\\n\\n        # at this point, sys.stdout/stderr may have been patched. Don't\\n        # raise an exception if flush() fails\\n        try:\\n            sys.stdout.flush()\\n        except:\\n            pass\\n\\n        try:\\n            sys.stderr.flush()\\n        except:\\n            pass\\n    finally:\\n        sys.stdin, sys.stdout, sys.stderr = stdin, stdout, stderr",
                    "func_fullName": "mrjob.util.save_sys_std(  )"
                },
                {
                    "func_id": 1364,
                    "func_name": "save_sys_path",
                    "func_desc": "save_sys_path",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def save_sys_path():\\n    \"\"\"Context manager that saves sys.path and restores it after execution.\"\"\"\\n    original_sys_path = list(sys.path)\\n\\n    try:\\n        yield\\n\\n    finally:\\n        sys.path = original_sys_path",
                    "func_fullName": "mrjob.util.save_sys_path(  )"
                },
                {
                    "func_id": 1365,
                    "func_name": "shlex_split",
                    "func_desc": "shlex_split",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def shlex_split(s):\\n    \"\"\"Wrapper around shlex.split(), but convert to str if Python version <\\n    2.7.3 when unicode support was added.\\n    \"\"\"\\n    if sys.version_info < (2, 7, 3):\\n        return shlex.split(str(s))\\n    else:\\n        return shlex.split(s)",
                    "func_fullName": "mrjob.util.shlex_split( s )"
                },
                {
                    "func_id": 1366,
                    "func_name": "strip_microseconds",
                    "func_desc": "strip_microseconds",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def strip_microseconds(delta):\\n    \"\"\"Return the given :py:class:`datetime.timedelta`, without microseconds.\\n\\n    Useful for printing :py:class:`datetime.timedelta` objects.\\n    \"\"\"\\n    return timedelta(delta.days, delta.seconds)",
                    "func_fullName": "mrjob.util.strip_microseconds( delta )"
                },
                {
                    "func_id": 1367,
                    "func_name": "to_lines",
                    "func_desc": "to_lines",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def to_lines(chunks):\\n    \"\"\"Take in data as a sequence of bytes, and yield it, one line at a time.\\n\\n    Only breaks lines on ``\\\\n`` (not ``\\\\r``), and does not add\\n    a trailing newline.\\n\\n    For efficiency, passes through anything with a ``readline()`` attribute.\\n    \"\"\"\\n    # hopefully this is good enough for anything mrjob will encounter\\n    if hasattr(chunks, 'readline'):\\n        return chunks\\n    else:\\n        return _to_lines(chunks)",
                    "func_fullName": "mrjob.util.to_lines( chunks )"
                },
                {
                    "func_id": 1368,
                    "func_name": "_to_lines",
                    "func_desc": "_to_lines",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _to_lines(chunks):\\n    \"\"\"Take in data as a sequence of bytes, and yield it, one line at a time.\\n\\n    Only breaks lines on ``\\\\n`` (not ``\\\\r``), and does not add\\n    a trailing newline.\\n\\n    Exception: if we encounter an empty bytestring ``b''``, immediately yield\\n    what we have so far rather than joining it to the next chunk. This allows\\n    us to handle bytes from multiple files without joining the end of one\\n    file to the beginning of the next one.\\n\\n    Optimizes for:\\n\\n    * chunks bigger than lines (e.g. reading test files)\\n    * chunks that are lines (idempotency)\\n    \"\"\"\\n    # list of chunks with no final newline\\n    leftovers = []\\n\\n    for chunk in chunks:\\n        # special case for b'' standing for EOF\\n        if chunk == b'':\\n            if leftovers:\\n                yield b''.join(leftovers)\\n                leftovers = []\\n\\n            continue\\n\\n        start = 0\\n\\n        while start < len(chunk):\\n            end = chunk.find(b'\\n', start) + 1\\n\\n            if end == 0:  # no newlines found\\n                leftovers.append(chunk[start:])\\n                break\\n\\n            if leftovers:\\n                leftovers.append(chunk[start:end])\\n                yield b''.join(leftovers)\\n                leftovers = []\\n            else:\\n                yield chunk[start:end]\\n\\n            start = end\\n\\n    if leftovers:\\n        yield b''.join(leftovers)",
                    "func_fullName": "mrjob.util._to_lines( chunks )"
                },
                {
                    "func_id": 1369,
                    "func_name": "unique",
                    "func_desc": "unique",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def unique(items):\\n    \"\"\"Yield items from *item* in order, skipping duplicates.\"\"\"\\n    seen = set()\\n\\n    for item in items:\\n        if item in seen:\\n            continue\\n        else:\\n            yield item\\n            seen.add(item)",
                    "func_fullName": "mrjob.util.unique( items )"
                },
                {
                    "func_id": 1370,
                    "func_name": "unarchive",
                    "func_desc": "unarchive",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def unarchive(archive_path, dest):\\n    \"\"\"Extract the contents of a tar or zip file at *archive_path* into the\\n    directory *dest*.\\n\\n    :type archive_path: str\\n    :param archive_path: path to archive file\\n    :type dest: str\\n    :param dest: path to directory where archive will be extracted\\n\\n    *dest* will be created if it doesn't already exist.\\n\\n    tar files can be gzip compressed, bzip2 compressed, or uncompressed. Files\\n    within zip files can be deflated or stored.\\n    \"\"\"\\n    if tarfile.is_tarfile(archive_path):\\n        with tarfile.open(archive_path, 'r') as archive:\\n            archive.extractall(dest)\\n    elif is_zipfile(archive_path):\\n        with ZipFile(archive_path, 'r') as archive:\\n            for name in archive.namelist():\\n                # the zip spec specifies that front slashes are always\\n                # used as directory separators\\n                dest_path = os.path.join(dest, *name.split('/'))\\n\\n                # now, split out any dirname and filename and create\\n                # one and/or the other\\n                dirname, filename = os.path.split(dest_path)\\n                if dirname and not os.path.exists(dirname):\\n                    os.makedirs(dirname)\\n                if filename:\\n                    with open(dest_path, 'wb') as dest_file:\\n                        dest_file.write(archive.read(name))\\n    else:\\n        raise IOError('Unknown archive type: %s' % (archive_path,))",
                    "func_fullName": "mrjob.util.unarchive( archive_path, dest )"
                },
                {
                    "func_id": 1371,
                    "func_name": "which",
                    "func_desc": "which",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def which(cmd, path=None):\\n    \"\"\"Like the UNIX which command: search in *path* for the executable named\\n    *cmd*. *path* defaults to :envvar:`PATH`. Returns ``None`` if no\\n    such executable found.\\n\\n    This is basically ``shutil.which()`` (which was introduced in Python 3.3)\\n    without the *mode* argument. Best practice is to always specify *path*\\n    as a keyword argument.\\n    \"\"\"\\n    if hasattr(shutil, 'which'):\\n        # added in Python 3.3\\n        return shutil.which(cmd, path=path)\\n    elif path is None and os.environ.get('PATH') is None:\\n        # find_executable() errors if neither path nor $PATH is set\\n        return None\\n    else:\\n        return find_executable(cmd, path=path)",
                    "func_fullName": "mrjob.util.which( cmd, path )"
                },
                {
                    "func_id": 1372,
                    "func_name": "zip_dir",
                    "func_desc": "zip_dir",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def zip_dir(dir, out_path, filter=None, prefix=''):\\n    \"\"\"Compress the given *dir* into a zip file at *out_path*.\\n\\n    If we encounter symlinks, include the actual file, not the symlink.\\n\\n    :type dir: str\\n    :param dir: dir to tar up\\n    :type out_path: str\\n    :param out_path: where to write the tarball too\\n    :param filter: if defined, a function that takes paths (relative to *dir*\\n                   and returns ``True`` if we should keep them\\n    :type prefix: str\\n    :param prefix: subdirectory inside the tarball to put everything into (e.g.\\n                   ``'mrjob'``)\\n    \"\"\"\\n    if not os.path.isdir(dir):\\n        raise IOError('Not a directory: %r' % (dir,))\\n\\n    if not filter:\\n        filter = lambda path: True\\n\\n    with _create_zip_file(out_path) as zip_file:\\n        for dirpath, dirnames, filenames in os.walk(dir, followlinks=True):\\n            for filename in filenames:\\n                path = os.path.join(dirpath, filename)\\n                rel_path = os.path.relpath(path, dir)\\n\\n                if filter(rel_path):\\n                    # copy over real files, not symlinks\\n                    real_path = os.path.realpath(path)\\n                    path_in_zip_file = os.path.join(prefix, rel_path)\\n                    zip_file.write(real_path, arcname=path_in_zip_file)",
                    "func_fullName": "mrjob.util.zip_dir( dir, out_path, filter, prefix )"
                },
                {
                    "func_id": 1373,
                    "func_name": "_create_zip_file",
                    "func_desc": "_create_zip_file",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _create_zip_file(path):\\n    try:\\n        return ZipFile(path, mode='w', compression=ZIP_DEFLATED)\\n    except RuntimeError:  # zlib not available\\n        return ZipFile(path, mode='w', compression=ZIP_STORED)",
                    "func_fullName": "mrjob.util._create_zip_file( path )"
                },
                {
                    "func_id": 1375,
                    "func_name": "open",
                    "func_desc": "open",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def open(*args, **kwargs):\\n        raise NameError(\"name 'open' is not defined\")",
                    "func_fullName": "mrjob.util.open( *args, **kwargs )"
                },
                {
                    "func_id": 1730,
                    "func_name": "_cat_log_lines",
                    "func_desc": "_cat_log_lines",
                    "func_file": "wrap",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _cat_log_lines(fs, path):\\n    \"\"\"Yield lines from the given log.\\n\\n    Log errors rather than raising them.\\n    \"\"\"\\n    try:\\n        if not fs.exists(path):\\n            return\\n        for line in to_lines(fs.cat(path)):\\n            yield to_unicode(line)\\n    except (IOError, OSError) as e:\\n        log.warning(\"couldn't cat() %s: %r\" % (path, e))",
                    "func_fullName": "mrjob.logs.wrap._cat_log_lines( fs, path )"
                },
                {
                    "func_id": 1731,
                    "func_name": "_ls_logs",
                    "func_desc": "_ls_logs",
                    "func_file": "wrap",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _ls_logs(fs, log_dir_stream, matcher, is_spark=False, **kwargs):\\n    \"\"\"Return a list matches against log files. Used to implement\\n    ``_ls_*_logs()`` functions.\\n\\n    This yields dictionaries with ``path`` set to matching log path, and\\n    other information (e.g. corresponding job_id) returned by *matcher*\\n\\n    *fs* is a :py:class:`mrjob.fs.Filesystem`\\n\\n    *log_dir_stream* is a sequence of lists of log dirs. The idea is that\\n    there may be copies of the same logs in multiple places (e.g.\\n    on S3 and by SSHing into nodes) and we want to list them all without\\n    finding duplicate copies. This function will go through the lists of\\n    log dirs in turn, stopping if it finds any matches from a list.\\n\\n    *matcher* is a function that takes (log_path, **kwargs)\\n    and returns either None (no match) or a dictionary with information\\n    about the path (e.g. the corresponding job_id). It's okay to return\\n    an empty dict.\\n    \"\"\"\\n    # wrapper for fs.ls() that turns IOErrors into warnings\\n    def _fs_ls(path):\\n        try:\\n            log.debug('    listing logs in %s' % log_dir)\\n            if fs.exists(log_dir):\\n                for path in fs.ls(log_dir):\\n                    yield path\\n        except (IOError, OSError) as e:\\n            log.warning(\"couldn't ls() %s: %r\" % (log_dir, e))\\n\\n    for log_dirs in log_dir_stream:\\n        if isinstance(log_dirs, str):\\n            raise TypeError\\n\\n        matched = False\\n\\n        for log_dir in log_dirs:\\n            matches = []\\n\\n            for path in _fs_ls(log_dir):\\n                match = matcher(path, **kwargs)\\n                if match is not None:\\n                    match['path'] = path\\n                    matches.append(match)\\n\\n            if matches:\\n                matched = True\\n\\n                if is_spark:\\n                    matches = _sort_for_spark(matches)\\n                else:\\n                    matches = _sort_by_recency(matches)\\n\\n                for match in matches:\\n                    yield match\\n\\n        if matched:\\n            return  # e.g. don't check S3 if we can get logs via SSH",
                    "func_fullName": "mrjob.logs.wrap._ls_logs( fs, log_dir_stream, matcher, is_spark, **kwargs )"
                },
                {
                    "func_id": 1732,
                    "func_name": "_logs_exist",
                    "func_desc": "_logs_exist",
                    "func_file": "wrap",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _logs_exist(fs, path):\\n    \"\"\"Do ``fs.exists(path)``, and return ``None`` if it raises ``IOError``\"\"\"\\n    try:\\n        return fs.exists(path)\\n    except IOError:\\n        return None",
                    "func_fullName": "mrjob.logs.wrap._logs_exist( fs, path )"
                },
                {
                    "func_id": 1733,
                    "func_name": "_fs_ls",
                    "func_desc": "_fs_ls",
                    "func_file": "wrap",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _fs_ls(path):\\n        try:\\n            log.debug('    listing logs in %s' % log_dir)\\n            if fs.exists(log_dir):\\n                for path in fs.ls(log_dir):\\n                    yield path\\n        except (IOError, OSError) as e:\\n            log.warning(\"couldn't ls() %s: %r\" % (log_dir, e))",
                    "func_fullName": "mrjob.logs.wrap._fs_ls( path )"
                },
                {
                    "func_id": 1866,
                    "func_name": "main",
                    "func_desc": "main",
                    "func_file": "mrboss",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def main(cl_args=None):\\n    usage = 'usage: %(prog)s boss CLUSTER_ID [options] \"command string\"'\\n    description = ('Run a command on the master and all worker nodes of an EMR'\\n                   ' cluster. Store stdout/stderr for results in OUTPUT_DIR.')\\n\\n    arg_parser = ArgumentParser(usage=usage, description=description)\\n    arg_parser.add_argument('-o', '--output-dir', dest='output_dir',\\n                            default=None,\\n                            help=\"Specify an output directory (default:\"\\n                            \" CLUSTER_ID)\")\\n\\n    arg_parser.add_argument(dest='cluster_id',\\n                            help='ID of cluster to run command on')\\n    arg_parser.add_argument(dest='cmd_string',\\n                            help='command to run, as a single string')\\n\\n    _add_basic_args(arg_parser)\\n    _add_runner_args(\\n        arg_parser,\\n        {'ec2_key_pair_file', 'ssh_bin'} | _filter_by_role(\\n            EMRJobRunner.OPT_NAMES, 'connect')\\n    )\\n\\n    _alphabetize_actions(arg_parser)\\n\\n    options = arg_parser.parse_args(cl_args)\\n\\n    MRJob.set_up_logging(quiet=options.quiet, verbose=options.verbose)\\n\\n    runner_kwargs = options.__dict__.copy()\\n    for unused_arg in ('cluster_id', 'cmd_string', 'output_dir',\\n                       'quiet', 'verbose'):\\n        del runner_kwargs[unused_arg]\\n\\n    cmd_args = shlex_split(options.cmd_string)\\n\\n    output_dir = os.path.abspath(options.output_dir or options.cluster_id)\\n\\n    with EMRJobRunner(\\n            cluster_id=options.cluster_id, **runner_kwargs) as runner:\\n        _run_on_all_nodes(runner, output_dir, cmd_args)",
                    "func_fullName": "mrjob.tools.emr.mrboss.main( cl_args )"
                },
                {
                    "func_id": 1867,
                    "func_name": "_run_on_all_nodes",
                    "func_desc": "_run_on_all_nodes",
                    "func_file": "mrboss",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _run_on_all_nodes(runner, output_dir, cmd_args, print_stderr=True):\\n    \"\"\"Given an :py:class:`EMRJobRunner`, run the command specified by\\n    *cmd_args* on all nodes in the cluster and save the stdout and stderr of\\n    each run to subdirectories of *output_dir*.\\n    \"\"\"\\n    master_addr = runner._address_of_master()\\n    addresses = [master_addr]\\n\\n    worker_addrs = runner._ssh_worker_hosts()\\n\\n    if worker_addrs:\\n        addresses += ['%s!%s' % (master_addr, worker_addr)\\n                      for worker_addr in worker_addrs]\\n\\n    for addr in addresses:\\n\\n        stdout, stderr = runner.fs.ssh._ssh_run(addr, cmd_args)\\n\\n        if print_stderr:\\n            print('---')\\n            print('Command completed on %s.' % addr)\\n            print(to_unicode(stderr), end=' ')\\n\\n        if '!' in addr:\\n            base_dir = os.path.join(output_dir, 'worker ' + addr.split('!')[1])\\n        else:\\n            base_dir = os.path.join(output_dir, 'master')\\n\\n        if not os.path.exists(base_dir):\\n            os.makedirs(base_dir)\\n\\n        with open(os.path.join(base_dir, 'stdout'), 'wb') as f:\\n            f.write(stdout)\\n\\n        with open(os.path.join(base_dir, 'stderr'), 'wb') as f:\\n            f.write(stderr)",
                    "func_fullName": "mrjob.tools.emr.mrboss._run_on_all_nodes( runner, output_dir, cmd_args, print_stderr )"
                }
            ]
        },
        {
            "cluster_id": 35,
            "feature_id": 40,
            "feature_desc": "gamma=0.0368; k=2; a=0.25; combined=0.439; stability(ARI)=1.000; sep=0.072",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 901,
                    "func_name": "_hadoop_cmd_line",
                    "func_desc": "_hadoop_cmd_line",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _hadoop_cmd_line(args):\\n    \"\"\"Escape args of a command line in a way that Hadoop can process\\n    them.\"\"\"\\n    return ' '.join(_hadoop_escape_arg(arg) for arg in args)",
                    "func_fullName": "mrjob.bin._hadoop_cmd_line( args )"
                },
                {
                    "func_id": 902,
                    "func_name": "_hadoop_escape_arg",
                    "func_desc": "_hadoop_escape_arg",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _hadoop_escape_arg(arg):\\n    \"\"\"Escape a single command argument in a way that Hadoop can process it.\"\"\"\\n    if _HADOOP_SAFE_ARG_RE.match(arg):\\n        return arg\\n    else:\\n        return \"'%s'\" % arg.replace(\"'\", r\"'\\''\")",
                    "func_fullName": "mrjob.bin._hadoop_escape_arg( arg )"
                },
                {
                    "func_id": 912,
                    "func_name": "_hadoop_args_for_step",
                    "func_desc": "_hadoop_args_for_step",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _hadoop_args_for_step(self, step_num):\\n        \"\"\"Build a list of extra arguments to the hadoop binary.\\n\\n        This handles *cmdenv*, *hadoop_extra_args*, *hadoop_input_format*,\\n        *hadoop_output_format*, *jobconf*, and *partitioner*.\\n\\n        This doesn't handle input, output, mappers, reducers, or uploading\\n        files.\\n        \"\"\"\\n        args = []\\n\\n        # -libjars, -D\\n        args.extend(self._hadoop_generic_args_for_step(step_num))\\n\\n        # hadoop_extra_args (if defined; it's not for sim runners)\\n        # this has to come after -D because it may include streaming-specific\\n        # args (see #1332).\\n        args.extend(self._opts.get('hadoop_extra_args', ()))\\n\\n        # partitioner\\n        partitioner = self._partitioner or self._sort_values_partitioner()\\n        if partitioner:\\n            args.extend(['-partitioner', partitioner])\\n\\n        # cmdenv\\n        for key, value in sorted(self._cmdenv().items()):\\n            args.append('-cmdenv')\\n            args.append('%s=%s' % (key, value))\\n\\n        # hadoop_input_format\\n        if step_num == 0:\\n            if self._uses_input_manifest():\\n                args.extend(['-inputformat', _MANIFEST_INPUT_FORMAT])\\n            elif self._hadoop_input_format:\\n                args.extend(['-inputformat', self._hadoop_input_format])\\n\\n        # hadoop_output_format\\n        if (step_num == self._num_steps() - 1 and self._hadoop_output_format):\\n            args.extend(['-outputformat', self._hadoop_output_format])\\n\\n        return args",
                    "func_fullName": "mrjob.bin._hadoop_args_for_step( self, step_num )"
                },
                {
                    "func_id": 913,
                    "func_name": "_hadoop_streaming_jar_args",
                    "func_desc": "_hadoop_streaming_jar_args",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _hadoop_streaming_jar_args(self, step_num):\\n        \"\"\"The arguments that come after ``hadoop jar <streaming jar path>``\\n        when running a Hadoop streaming job.\"\"\"\\n        args = []\\n\\n        # get command for each part of the job\\n        mapper, combiner, reducer = (\\n            self._hadoop_streaming_commands(step_num))\\n\\n        # set up uploading from HDFS/cloud storage to the working dir\\n        args.extend(self._upload_args())\\n\\n        # if no reducer, shut off reducer tasks. This has to come before\\n        # extra hadoop args, which could contain jar-specific args\\n        # (e.g. -outputformat). See #1331.\\n        #\\n        # might want to just integrate this into _hadoop_args_for_step?\\n        if not reducer:\\n            args.extend(['-D', ('%s=0' % translate_jobconf(\\n                'mapreduce.job.reduces', self.get_hadoop_version()))])\\n\\n        # Add extra hadoop args first as hadoop args could be a hadoop\\n        # specific argument which must come before job\\n        # specific args.\\n        args.extend(self._hadoop_args_for_step(step_num))\\n\\n        # set up input\\n        for input_uri in self._step_input_uris(step_num):\\n            args.extend(['-input', input_uri])\\n\\n        # set up output\\n        args.append('-output')\\n        args.append(self._step_output_uri(step_num))\\n\\n        args.append('-mapper')\\n        args.append(mapper)\\n\\n        if combiner:\\n            args.append('-combiner')\\n            args.append(combiner)\\n\\n        if reducer:\\n            args.append('-reducer')\\n            args.append(reducer)\\n\\n        return args",
                    "func_fullName": "mrjob.bin._hadoop_streaming_jar_args( self, step_num )"
                },
                {
                    "func_id": 914,
                    "func_name": "_hadoop_streaming_commands",
                    "func_desc": "_hadoop_streaming_commands",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _hadoop_streaming_commands(self, step_num):\\n        return (\\n            self._render_substep(step_num, 'mapper'),\\n            self._render_substep(step_num, 'combiner'),\\n            self._render_substep(step_num, 'reducer'),\\n        )",
                    "func_fullName": "mrjob.bin._hadoop_streaming_commands( self, step_num )"
                },
                {
                    "func_id": 915,
                    "func_name": "_hadoop_generic_args_for_step",
                    "func_desc": "_hadoop_generic_args_for_step",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _hadoop_generic_args_for_step(self, step_num):\\n        \"\"\"Arguments like -D and -libjars that apply to every Hadoop\\n        subcommand.\"\"\"\\n        args = []\\n\\n        # libjars (#198)\\n        libjar_paths = self._libjar_paths()\\n        if libjar_paths:\\n            args.extend(['-libjars', ','.join(libjar_paths)])\\n\\n        # jobconf (-D)\\n        jobconf = self._jobconf_for_step(step_num)\\n\\n        for key, value in sorted(jobconf.items()):\\n            args.extend(['-D', '%s=%s' % (key, value)])\\n\\n        return args",
                    "func_fullName": "mrjob.bin._hadoop_generic_args_for_step( self, step_num )"
                },
                {
                    "func_id": 916,
                    "func_name": "_libjar_paths",
                    "func_desc": "_libjar_paths",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _libjar_paths(self):\\n        \"\"\"Paths or URIs of libjars, from Hadoop/Spark's point of view.\\n\\n        Override this for non-local libjars (e.g. on EMR).\\n        \"\"\"\\n        return self._opts['libjars']",
                    "func_fullName": "mrjob.bin._libjar_paths( self )"
                },
                {
                    "func_id": 917,
                    "func_name": "_interpolate_jar_step_args",
                    "func_desc": "_interpolate_jar_step_args",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _interpolate_jar_step_args(self, args, step_num):\\n        \"\"\"Like :py:meth:`_interpolate_step_args` except it\\n        also replaces `~mrjob.step.GENERIC_ARGS` with\\n        :py:meth:`_hadoop_generic_args_for_step`. This only\\n        makes sense for jar steps; Spark should raise an error\\n        if `~mrjob.step.GENERIC_ARGS` is encountered.\\n        \"\"\"\\n        result = []\\n\\n        for arg in args:\\n            if arg == mrjob.step.GENERIC_ARGS:\\n                result.extend(\\n                    self._hadoop_generic_args_for_step(step_num))\\n            else:\\n                result.append(arg)\\n\\n        return self._interpolate_step_args(result, step_num)",
                    "func_fullName": "mrjob.bin._interpolate_jar_step_args( self, args, step_num )"
                },
                {
                    "func_id": 920,
                    "func_name": "_pyspark_setup",
                    "func_desc": "_pyspark_setup",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _pyspark_setup(self):\\n        \"\"\"Like ``self._setup``, but prepends commands for archive\\n        emulation if needed.\"\"\"\\n        setup = []\\n\\n        if self._emulate_archives_on_spark():\\n            for name, path in sorted(\\n                    self._working_dir_mgr.name_to_path('archive').items()):\\n\\n                archive_file_name = self._working_dir_mgr.name(\\n                    'archive_file', path)\\n\\n                setup.append(_unarchive_cmd(path) % dict(\\n                    file=pipes.quote(archive_file_name),\\n                    dir=pipes.quote(name)))\\n\\n        setup.extend(self._setup)\\n\\n        return setup",
                    "func_fullName": "mrjob.bin._pyspark_setup( self )"
                },
                {
                    "func_id": 933,
                    "func_name": "_args_for_spark_step",
                    "func_desc": "_args_for_spark_step",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _args_for_spark_step(self, step_num, last_step_num=None):\\n        \"\"\"The actual arguments used to run the spark-submit command.\\n\\n        This handles both all Spark step types (``spark``, ``spark_jar``,\\n        and ``spark_script``).\\n\\n        *last_step_num* is only used by the Spark runner, where multiple\\n        streaming steps are run in a single Spark job\\n        \"\"\"\\n        return (\\n            self.get_spark_submit_bin() +\\n            self._spark_submit_args(step_num) +\\n            [self._spark_script_path(step_num)] +\\n            self._spark_script_args(step_num, last_step_num)\\n        )",
                    "func_fullName": "mrjob.bin._args_for_spark_step( self, step_num, last_step_num )"
                },
                {
                    "func_id": 934,
                    "func_name": "_run_spark_submit",
                    "func_desc": "_run_spark_submit",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run_spark_submit(self, spark_submit_args, env, record_callback):\\n        \"\"\"Run the spark submit binary in a subprocess, using a PTY if possible\\n\\n        :param spark_submit_args: spark-submit binary and arguments, as as list\\n        :param env: environment variables, as a dict\\n        :param record_callback: a function that takes a single log4j record\\n                                as its argument (see\\n                                :py:func:`~mrjob.logs.log4j\\\\n                                ._parse_hadoop_log4j_records)\\n\\n        :return: tuple of the subprocess's return code and a\\n                 step interpretation dictionary\\n        \"\"\"\\n        log.debug('> %s' % cmd_line(spark_submit_args))\\n        log.debug('  with environment: %r' % sorted(env.items()))\\n\\n        # these should always be set, but just in case\\n        returncode = 0\\n        step_interpretation = {}\\n\\n        # try to use a PTY if it's available\\n        try:\\n            pid, master_fd = pty.fork()\\n        except (AttributeError, OSError):\\n            # no PTYs, just use Popen\\n\\n            # user won't get much feedback for a while, so tell them\\n            # spark-submit is running\\n            log.debug('No PTY available, using Popen() to invoke spark-submit')\\n\\n            step_proc = Popen(\\n                spark_submit_args, stdout=PIPE, stderr=PIPE, env=env)\\n\\n            # parse driver output\\n            step_interpretation = _parse_spark_log(\\n                step_proc.stderr, record_callback=record_callback)\\n\\n            # there shouldn't be much output on STDOUT, just echo it\\n            for record in _parse_hadoop_log4j_records(step_proc.stdout):\\n                record_callback(record)\\n\\n            step_proc.stdout.close()\\n            step_proc.stderr.close()\\n\\n            returncode = step_proc.wait()\\n        else:\\n            # we have PTYs\\n            if pid == 0:  # we are the child process\\n                try:\\n                    os.execvpe(spark_submit_args[0], spark_submit_args, env)\\n                    # now this process is no longer Python\\n                except OSError as ex:\\n                    # use _exit() so we don't do cleanup, etc. that's\\n                    # the parent process's job\\n                    os._exit(ex.errno)\\n                finally:\\n                    # if we get some other exception, still exit hard\\n                    os._exit(-1)\\n            else:\\n                log.debug('Invoking spark-submit via PTY')\\n\\n                with os.fdopen(master_fd, 'rb') as master:\\n                    step_interpretation = (\\n                        _parse_spark_log(\\n                            _eio_to_eof(master),\\n                            record_callback=record_callback))\\n\\n                    _, returncode = os.waitpid(pid, 0)\\n\\n        return (returncode, step_interpretation)",
                    "func_fullName": "mrjob.bin._run_spark_submit( self, spark_submit_args, env, record_callback )"
                },
                {
                    "func_id": 935,
                    "func_name": "get_spark_submit_bin",
                    "func_desc": "get_spark_submit_bin",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_spark_submit_bin(self):\\n        \"\"\"Return the location of the ``spark-submit`` binary, searching for it\\n        if necessary.\"\"\"\\n        if not self._spark_submit_bin:\\n            self._spark_submit_bin = self._find_spark_submit_bin()\\n        return self._spark_submit_bin",
                    "func_fullName": "mrjob.bin.get_spark_submit_bin( self )"
                },
                {
                    "func_id": 936,
                    "func_name": "_find_spark_submit_bin",
                    "func_desc": "_find_spark_submit_bin",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _find_spark_submit_bin(self):\\n        \"\"\"Attempt to find the spark binary. Returns a list of arguments.\\n        Defaults to ``['spark-submit']``.\\n\\n        Re-define this in your subclass if you already know where\\n        to find spark-submit (e.g. on cloud services).\\n        \"\"\"\\n        for path in unique(self._spark_submit_bin_dirs()):\\n            log.info('Looking for spark-submit binary in %s...' % (\\n                path or '$PATH'))\\n\\n            spark_submit_bin = which('spark-submit', path=path)\\n\\n            if spark_submit_bin:\\n                log.info('Found spark-submit binary: %s' % spark_submit_bin)\\n                return [spark_submit_bin]\\n        else:\\n            log.info(\"Falling back to 'spark-submit'\")\\n            return ['spark-submit']",
                    "func_fullName": "mrjob.bin._find_spark_submit_bin( self )"
                },
                {
                    "func_id": 937,
                    "func_name": "_spark_submit_bin_dirs",
                    "func_desc": "_spark_submit_bin_dirs",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_submit_bin_dirs(self):\\n        # $SPARK_HOME\\n        spark_home = os.environ.get('SPARK_HOME')\\n        if spark_home:\\n            yield os.path.join(spark_home, 'bin')\\n\\n        yield None  # use $PATH\\n\\n        # look for pyspark installation (see #1984)\\n        if pyspark:\\n            yield os.path.join(os.path.dirname(pyspark.__file__), 'bin')\\n\\n        # some other places recommended by install docs (see #1366)\\n        yield '/usr/lib/spark/bin'\\n        yield '/usr/local/spark/bin'\\n        yield '/usr/local/lib/spark/bin'",
                    "func_fullName": "mrjob.bin._spark_submit_bin_dirs( self )"
                },
                {
                    "func_id": 938,
                    "func_name": "_spark_submit_args",
                    "func_desc": "_spark_submit_args",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_submit_args(self, step_num):\\n        \"\"\"Build a list of extra args to the spark-submit binary for\\n        the given spark or spark_script step.\"\"\"\\n        step = self._get_step(step_num)\\n\\n        args = []\\n\\n        # --conf arguments include python bin, cmdenv, jobconf. Make sure\\n        # that we can always override these manually\\n        jobconf = {}\\n        for key, value in self._spark_cmdenv(step_num).items():\\n            jobconf['spark.executorEnv.%s' % key] = value\\n            if self._spark_master() == 'yarn':  # YARN only, see #1919\\n                jobconf['spark.yarn.appMasterEnv.%s' % key] = value\\n\\n        jobconf.update(self._jobconf_for_step(step_num))\\n\\n        for key, value in sorted(jobconf.items()):\\n            args.extend(['--conf', '%s=%s' % (key, value)])\\n\\n        # add --class (JAR steps)\\n        if step.get('main_class'):\\n            args.extend(['--class', step['main_class']])\\n\\n        # add --jars, if any\\n        libjar_paths = self._libjar_paths()\\n        if libjar_paths:\\n            args.extend(['--jars', ','.join(libjar_paths)])\\n\\n        # spark-submit treats --master and --deploy-mode as aliases for\\n        # --conf spark.master=... and --conf spark.deploy-mode=... (see #2032).\\n        #\\n        # we never want jobconf to override spark master or deploy mode, so put\\n        # these switches after --conf\\n\\n        # add --master\\n        if self._spark_master():\\n            args.extend(['--master', self._spark_master()])\\n\\n        # add --deploy-mode\\n        if self._spark_deploy_mode():\\n            args.extend(['--deploy-mode', self._spark_deploy_mode()])\\n\\n        # --files and --archives\\n        args.extend(self._spark_upload_args())\\n\\n        # --py-files (Python only)\\n        # spark runner can run 'streaming' steps, so just exclude\\n        # non-Python steps\\n        if 'jar' not in step['type']:\\n            py_file_uris = self._py_files()\\n\\n            if self._upload_mgr:\\n                # don't assume py_files are in _upload_mgr; for example,\\n                # spark-submit doesn't need to upload them\\n                path_to_uri = self._upload_mgr.path_to_uri()\\n                py_file_uris = [path_to_uri.get(p, p) for p in py_file_uris]\\n\\n            if py_file_uris:\\n                args.extend(['--py-files', ','.join(py_file_uris)])\\n\\n        # spark_args option\\n        args.extend(self._opts['spark_args'])\\n\\n        # step spark_args\\n        if step.get('spark_args'):\\n            args.extend(step['spark_args'])\\n\\n        return args",
                    "func_fullName": "mrjob.bin._spark_submit_args( self, step_num )"
                },
                {
                    "func_id": 939,
                    "func_name": "_spark_upload_args",
                    "func_desc": "_spark_upload_args",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_upload_args(self):\\n        if not self._spark_executors_have_own_wd():\\n            # don't bother, there's no working dir to upload to\\n            return []\\n\\n        return self._upload_args_helper(\\n            '--files', None,\\n            '--archives', None,\\n            always_use_hash=False,\\n            emulate_archives=self._emulate_archives_on_spark())",
                    "func_fullName": "mrjob.bin._spark_upload_args( self )"
                },
                {
                    "func_id": 940,
                    "func_name": "_spark_script_path",
                    "func_desc": "_spark_script_path",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_script_path(self, step_num):\\n        \"\"\"The path of the spark script or JAR, used by\\n        _args_for_spark_step().\"\"\"\\n        step = self._get_step(step_num)\\n\\n        if step['type'] == 'spark':\\n            path = self._script_path\\n        elif step['type'] == 'spark_jar':\\n            path = step['jar']\\n        elif step['type'] == 'spark_script':\\n            path = step['script']\\n        else:\\n            raise TypeError('Bad step type: %r' % step['type'])\\n\\n        return self._interpolate_spark_script_path(path)",
                    "func_fullName": "mrjob.bin._spark_script_path( self, step_num )"
                },
                {
                    "func_id": 941,
                    "func_name": "_interpolate_spark_script_path",
                    "func_desc": "_interpolate_spark_script_path",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _interpolate_spark_script_path(self, path):\\n        \"\"\"Redefine this in your subclass if the given path needs to be\\n        translated to a URI when running spark (e.g. on EMR).\"\"\"\\n        return path",
                    "func_fullName": "mrjob.bin._interpolate_spark_script_path( self, path )"
                },
                {
                    "func_id": 942,
                    "func_name": "_spark_cmdenv",
                    "func_desc": "_spark_cmdenv",
                    "func_file": "bin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_cmdenv(self, step_num):\\n        \"\"\"Returns a dictionary mapping environment variable to value,\\n        including mapping PYSPARK_PYTHON to self._python_bin()\\n        \"\"\"\\n        step = self._get_step(step_num)\\n\\n        cmdenv = {}\\n\\n        if self._step_type_uses_pyspark(step['type']):\\n            driver_python = cmd_line(self._python_bin())\\n\\n            if self._spark_python_wrapper_path:\\n                executor_python = './%s' % self._working_dir_mgr.name(\\n                    'file', self._spark_python_wrapper_path)\\n            else:\\n                executor_python = cmd_line(self._task_python_bin())\\n\\n            if self._spark_deploy_mode() == 'cluster':\\n                # treat driver like executors (they run in same environment)\\n                cmdenv['PYSPARK_PYTHON'] = executor_python\\n            elif driver_python == executor_python:\\n                # no difference, just set $PYSPARK_PYTHON\\n                cmdenv['PYSPARK_PYTHON'] = driver_python\\n            else:\\n                # set different pythons for driver and executor\\n                cmdenv['PYSPARK_PYTHON'] = executor_python\\n                cmdenv['PYSPARK_DRIVER_PYTHON'] = driver_python\\n\\n        cmdenv.update(self._cmdenv())\\n        return cmdenv",
                    "func_fullName": "mrjob.bin._spark_cmdenv( self, step_num )"
                },
                {
                    "func_id": 1358,
                    "func_name": "log_to_stream",
                    "func_desc": "log_to_stream",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def log_to_stream(name=None, stream=None, format=None, level=None,\\n                  debug=False):\\n    \"\"\"Set up logging.\\n\\n    :type name: str\\n    :param name: name of the logger, or ``None`` for the root logger\\n    :type stream: file object\\n    :param stream:  stream to log to (default is ``sys.stderr``)\\n    :type format: str\\n    :param format: log message format (default is '%(message)s')\\n    :param level: log level to use\\n    :type debug: bool\\n    :param debug: quick way of setting the log level: if true, use\\n                  ``logging.DEBUG``, otherwise use ``logging.INFO``\\n    \"\"\"\\n    if level is None:\\n        level = logging.DEBUG if debug else logging.INFO\\n\\n    if format is None:\\n        format = '%(message)s'\\n\\n    if stream is None:\\n        stream = sys.stderr\\n\\n    handler = logging.StreamHandler(stream)\\n    handler.setLevel(level)\\n    handler.setFormatter(logging.Formatter(format))\\n\\n    logger = logging.getLogger(name)\\n    logger.setLevel(level)\\n    logger.addHandler(handler)",
                    "func_fullName": "mrjob.util.log_to_stream( name, stream, format, level, debug )"
                },
                {
                    "func_id": 1374,
                    "func_name": "emit",
                    "func_desc": "emit",
                    "func_file": "util",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def emit(self, record):\\n        pass",
                    "func_fullName": "mrjob.util.emit( self, record )"
                }
            ]
        },
        {
            "cluster_id": 1,
            "feature_id": 41,
            "feature_desc": "gamma=0.0608; k=17; a=0.25; combined=0.388; stability(ARI)=0.911; sep=0.150",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 944,
                    "func_name": "_client_error_code",
                    "func_desc": "_client_error_code",
                    "func_file": "aws",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _client_error_code(ex):\\n    \"\"\"Get the error code for the given ClientError\"\"\"\\n    return ex.response.get('Error', {}).get('Code', '')",
                    "func_fullName": "mrjob.aws._client_error_code( ex )"
                },
                {
                    "func_id": 945,
                    "func_name": "_client_error_status",
                    "func_desc": "_client_error_status",
                    "func_file": "aws",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _client_error_status(ex):\\n    \"\"\"Get the HTTP status for the given ClientError\"\"\"\\n    resp = ex.response\\n    # sometimes status code is in ResponseMetadata, not Error\\n    return (resp.get('Error', {}).get('HTTPStatusCode') or\\n            resp.get('ResponseMetadata', {}).get('HTTPStatusCode'))",
                    "func_fullName": "mrjob.aws._client_error_status( ex )"
                },
                {
                    "func_id": 946,
                    "func_name": "_is_retriable_client_error",
                    "func_desc": "_is_retriable_client_error",
                    "func_file": "aws",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _is_retriable_client_error(ex):\\n    \"\"\"Is the exception from a boto3 client retriable?\"\"\"\\n    if isinstance(ex, botocore.exceptions.ClientError):\\n        # these rarely get through in boto3\\n        code = _client_error_code(ex)\\n        # \"Throttl\" catches \"Throttled\" and \"Throttling\"\\n        if any(c in code for c in ('Throttl', 'RequestExpired', 'Timeout')):\\n            return True\\n        # spurious 505s thought to be part of an AWS load balancer issue\\n        return _client_error_status(ex) == 505\\n    # in Python 2.7, SSLError is a subclass of socket.error, so catch\\n    # SSLError first\\n    elif isinstance(ex, SSLError):\\n        # catch ssl.SSLError: ('The read operation timed out',). See #1827.\\n        # also catches 'The write operation timed out'\\n        return any(isinstance(arg, str) and 'timed out' in arg\\n                   for arg in ex.args)\\n    elif isinstance(ex, socket.error):\\n        return ex.args in ((104, 'Connection reset by peer'),\\n                           (110, 'Connection timed out'))\\n    else:\\n        return False",
                    "func_fullName": "mrjob.aws._is_retriable_client_error( ex )"
                },
                {
                    "func_id": 1224,
                    "func_name": "_check_output_not_exists",
                    "func_desc": "_check_output_not_exists",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _check_output_not_exists(self):\\n        \"\"\"Verify the output path does not already exist. This avoids\\n        provisioning a cluster only to have Hadoop refuse to launch.\\n        \"\"\"\\n        try:\\n            if self.fs.exists(self._output_dir):\\n                raise IOError(\\n                    'Output path %s already exists!' % (self._output_dir,))\\n        except botocore.exceptions.ClientError:\\n            pass",
                    "func_fullName": "mrjob.emr._check_output_not_exists( self )"
                },
                {
                    "func_id": 1253,
                    "func_name": "_action_on_failure",
                    "func_desc": "_action_on_failure",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _action_on_failure(self):\\n        # don't terminate other people's clusters\\n        if (self._opts['emr_action_on_failure']):\\n            return self._opts['emr_action_on_failure']\\n        elif not self._add_steps_in_batch():\\n            # concurrent clusters don't allow CANCEL_ON_WAIT\\n            return 'CONTINUE'\\n        elif (self._opts['cluster_id'] or\\n                self._opts['pool_clusters']):\\n            return 'CANCEL_AND_WAIT'\\n        else:\\n            return 'TERMINATE_CLUSTER'",
                    "func_fullName": "mrjob.emr._action_on_failure( self )"
                },
                {
                    "func_id": 1281,
                    "func_name": "_check_for_missing_default_iam_roles",
                    "func_desc": "_check_for_missing_default_iam_roles",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _check_for_missing_default_iam_roles(self, cluster):\\n        \"\"\"If cluster couldn't start due to missing IAM roles, tell\\n        user what to do.\"\"\"\\n        if not cluster:\\n            cluster = self._describe_cluster()\\n\\n        reason = _get_reason(cluster)\\n        if any(reason.endswith('/%s is invalid' % role)\\n               for role in (_FALLBACK_INSTANCE_PROFILE,\\n                            _FALLBACK_SERVICE_ROLE)):\\n            log.warning(\\n                '\\n'\\n                'Ask your admin to create the default EMR roles'\\n                ' by following:\\n\\n'\\n                '    http://docs.aws.amazon.com/ElasticMapReduce/latest'\\n                '/DeveloperGuide/emr-iam-roles-creatingroles.html\\n')",
                    "func_fullName": "mrjob.emr._check_for_missing_default_iam_roles( self, cluster )"
                },
                {
                    "func_id": 1318,
                    "func_name": "make_emr_client",
                    "func_desc": "make_emr_client",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def make_emr_client(self):\\n        \"\"\"Create a :py:mod:`boto3` EMR client.\\n\\n        :return: a :py:class:`botocore.client.EMR` wrapped in a\\n                :py:class:`mrjob.retry.RetryWrapper`\\n        \"\"\"\\n        # ...which is then wrapped in bacon! Mmmmm!\\n        if boto3 is None:\\n            raise ImportError('You must install boto3 to connect to EMR')\\n\\n        raw_emr_client = boto3.client(\\n            'emr',\\n            aws_access_key_id=self._opts['aws_access_key_id'],\\n            aws_secret_access_key=self._opts['aws_secret_access_key'],\\n            aws_session_token=self._opts['aws_session_token'],\\n            endpoint_url=_endpoint_url(self._opts['emr_endpoint']),\\n            region_name=self._opts['region'],\\n        )\\n\\n        # #1799: don't retry faster than EMR checks the API\\n        return _wrap_aws_client(raw_emr_client,\\n                                min_backoff=self._opts['check_cluster_every'])",
                    "func_fullName": "mrjob.emr.make_emr_client( self )"
                },
                {
                    "func_id": 1330,
                    "func_name": "make_iam_client",
                    "func_desc": "make_iam_client",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def make_iam_client(self):\\n        \"\"\"Create a :py:mod:`boto3` IAM client.\\n\\n        :return: a :py:class:`botocore.client.IAM` wrapped in a\\n                :py:class:`mrjob.retry.RetryWrapper`\\n        \"\"\"\\n        if boto3 is None:\\n            raise ImportError('You must install boto3 to connect to IAM')\\n\\n        # special logic for setting IAM endpoint (which you don't usually\\n        # want to do, because IAM is regionless).\\n        endpoint_url = _endpoint_url(self._opts['iam_endpoint'])\\n        if endpoint_url:\\n            # keep boto3 from loading a nonsensical region name from configs\\n            # (see https://github.com/boto/boto3/issues/985)\\n            region_name = _DEFAULT_AWS_REGION\\n            log.debug('creating IAM client to %s' % endpoint_url)\\n        else:\\n            region_name = None\\n            log.debug('creating IAM client')\\n\\n        raw_iam_client = boto3.client(\\n            'iam',\\n            aws_access_key_id=self._opts['aws_access_key_id'],\\n            aws_secret_access_key=self._opts['aws_secret_access_key'],\\n            aws_session_token=self._opts['aws_session_token'],\\n            endpoint_url=endpoint_url,\\n            region_name=region_name,\\n        )\\n\\n        return _wrap_aws_client(raw_iam_client)",
                    "func_fullName": "mrjob.emr.make_iam_client( self )"
                },
                {
                    "func_id": 1896,
                    "func_name": "_terminate_and_notify",
                    "func_desc": "_terminate_and_notify",
                    "func_file": "terminate_idle_clusters",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _terminate_and_notify(runner, cluster_id, cluster_name, num_steps,\\n                          is_pending, time_idle,\\n                          dry_run=False, quiet=False):\\n    emr_client = runner.make_emr_client()\\n\\n    if not dry_run:\\n        emr_client.terminate_job_flows(JobFlowIds=[cluster_id])\\n\\n    if not quiet:\\n        fmt = ('Terminated cluster %s (%s); was %s for %s')\\n        msg = fmt % (\\n            cluster_id, cluster_name,\\n            'pending' if is_pending else 'idle',\\n            strip_microseconds(time_idle))\\n\\n        print(msg)",
                    "func_fullName": "mrjob.tools.emr.terminate_idle_clusters._terminate_and_notify( runner, cluster_id, cluster_name, num_steps, is_pending, time_idle, dry_run, quiet )"
                }
            ]
        },
        {
            "cluster_id": 1,
            "feature_id": 42,
            "feature_desc": "gamma=0.0608; k=17; a=0.25; combined=0.388; stability(ARI)=0.911; sep=0.150",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 947,
                    "func_name": "_wrap_aws_client",
                    "func_desc": "_wrap_aws_client",
                    "func_file": "aws",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _wrap_aws_client(raw_client, min_backoff=None):\\n    \"\"\"Wrap a given boto3 Client object so that it can retry when\\n    throttled.\"\"\"\\n    return RetryWrapper(\\n        raw_client,\\n        retry_if=_is_retriable_client_error,\\n        backoff=max(_AWS_BACKOFF, min_backoff or 0),\\n        multiplier=_AWS_BACKOFF_MULTIPLIER,\\n        max_tries=_AWS_MAX_TRIES,\\n        unwrap_methods={'get_paginator'})",
                    "func_fullName": "mrjob.aws._wrap_aws_client( raw_client, min_backoff )"
                },
                {
                    "func_id": 1202,
                    "func_name": "_build_instance_group",
                    "func_desc": "_build_instance_group",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _build_instance_group(role, instance_type, num_instances, bid_price):\\n    \"\"\"Helper method for creating instance groups. For use when\\n    creating a cluster using a list of InstanceGroups\\n\\n        - role is either 'MASTER', 'CORE', or 'TASK'.\\n        - instance_type is an EC2 instance type\\n        - count is an int\\n        - bid_price is a number, a string, or None. If None,\\n          this instance group will be use the ON-DEMAND market\\n          instead of the SPOT market.\\n    \"\"\"\\n    if role not in _INSTANCE_ROLES:\\n        raise ValueError\\n\\n    if not instance_type:\\n        raise ValueError\\n\\n    if not num_instances:\\n        raise ValueError\\n\\n    ig = dict(\\n        InstanceCount=num_instances,\\n        InstanceRole=role,\\n        InstanceType=instance_type,\\n        Market='ON_DEMAND',\\n        Name=role.lower(),  # just name the groups \"core\", \"master\", and \"task\"\\n    )\\n\\n    if bid_price:\\n        ig['Market'] = 'SPOT'\\n        ig['BidPrice'] = str(bid_price)  # must be a string\\n\\n    return ig",
                    "func_fullName": "mrjob.emr._build_instance_group( role, instance_type, num_instances, bid_price )"
                },
                {
                    "func_id": 1242,
                    "func_name": "_instance_type",
                    "func_desc": "_instance_type",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _instance_type(self, role):\\n        \"\"\"What instance type should we use for the given role?\\n        (one of 'MASTER', 'CORE', 'TASK')\"\"\"\\n        if role not in _INSTANCE_ROLES:\\n            raise ValueError\\n\\n        # explicitly set\\n        if self._opts[role.lower() + '_instance_type']:\\n            return self._opts[role.lower() + '_instance_type']\\n\\n        elif self._instance_is_worker(role):\\n            # using *instance_type* here is defensive programming;\\n            # if set, it should have already been popped into the worker\\n            # instance type option(s) by _fix_instance_opts() above\\n            return self._opts['instance_type'] or self._default_instance_type()\\n        else:\\n            return self._default_instance_type()",
                    "func_fullName": "mrjob.emr._instance_type( self, role )"
                },
                {
                    "func_id": 1243,
                    "func_name": "_default_instance_type",
                    "func_desc": "_default_instance_type",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _default_instance_type(self):\\n        \"\"\"Default instance type if not set by the user.\"\"\"\\n        # m5.xlarge is available on all regions, but only works in AMI 5.13.0\\n        # or later. See #2098.\\n        if self._image_version_gte('5.13.0'):\\n            return 'm5.xlarge'\\n        else:\\n            return 'm4.large'",
                    "func_fullName": "mrjob.emr._default_instance_type( self )"
                },
                {
                    "func_id": 1244,
                    "func_name": "_instance_is_worker",
                    "func_desc": "_instance_is_worker",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _instance_is_worker(self, role):\\n        \"\"\"Do instances of the given role run tasks? True for non-master\\n        instances and sole master instance.\"\"\"\\n        if role not in _INSTANCE_ROLES:\\n            raise ValueError\\n\\n        return (role != 'MASTER' or\\n                sum(self._num_instances(role)\\n                    for role in _INSTANCE_ROLES) == 1)",
                    "func_fullName": "mrjob.emr._instance_is_worker( self, role )"
                },
                {
                    "func_id": 1245,
                    "func_name": "_num_instances",
                    "func_desc": "_num_instances",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _num_instances(self, role):\\n        \"\"\"How many of the given instance type do we want?\"\"\"\\n        if role not in _INSTANCE_ROLES:\\n            raise ValueError\\n\\n        if role == 'MASTER':\\n            return 1  # there can be only one\\n        else:\\n            return self._opts['num_' + role.lower() + '_instances']",
                    "func_fullName": "mrjob.emr._num_instances( self, role )"
                },
                {
                    "func_id": 1246,
                    "func_name": "_instance_bid_price",
                    "func_desc": "_instance_bid_price",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _instance_bid_price(self, role):\\n        \"\"\"What's the bid price for the given role (if any)?\"\"\"\\n        if role not in _INSTANCE_ROLES:\\n            raise ValueError\\n\\n        return self._opts[role.lower() + '_instance_bid_price']",
                    "func_fullName": "mrjob.emr._instance_bid_price( self, role )"
                },
                {
                    "func_id": 1247,
                    "func_name": "_instance_groups",
                    "func_desc": "_instance_groups",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _instance_groups(self):\\n        \"\"\"Which instance groups do we want to request?\\n\\n        Returns the value of the ``InstanceGroups`` parameter\\n        passed to the EMR API.\\n        \"\"\"\\n        if self._opts['instance_groups']:\\n            return self._opts['instance_groups']\\n\\n        return [\\n            _build_instance_group(\\n                role=role,\\n                instance_type=self._instance_type(role),\\n                num_instances=self._num_instances(role),\\n                bid_price=self._instance_bid_price(role),\\n            )\\n            for role in _INSTANCE_ROLES\\n            if self._num_instances(role)\\n        ]",
                    "func_fullName": "mrjob.emr._instance_groups( self )"
                },
                {
                    "func_id": 1251,
                    "func_name": "_instance_profile",
                    "func_desc": "_instance_profile",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _instance_profile(self):\\n        try:\\n            return (self._opts['iam_instance_profile'] or\\n                    get_or_create_mrjob_instance_profile(\\n                        self.make_iam_client()))\\n        except botocore.exceptions.ClientError as ex:\\n            if _client_error_status(ex) != 403:\\n                raise\\n            log.warning(\\n                \"Can't access IAM API, trying default instance profile: %s\" %\\n                _FALLBACK_INSTANCE_PROFILE)\\n            return _FALLBACK_INSTANCE_PROFILE",
                    "func_fullName": "mrjob.emr._instance_profile( self )"
                },
                {
                    "func_id": 1325,
                    "func_name": "_get_collection_type",
                    "func_desc": "_get_collection_type",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_collection_type(self):\\n        \"\"\"Return the collection type of the cluster (either\\n        ``'INSTANCE_FLEET'`` or ``'INSTANCE_GROUP'``).\"\"\"\\n        return self._get_cluster_info('collection_type')",
                    "func_fullName": "mrjob.emr._get_collection_type( self )"
                }
            ]
        },
        {
            "cluster_id": 1,
            "feature_id": 43,
            "feature_desc": "gamma=0.0608; k=17; a=0.25; combined=0.388; stability(ARI)=0.911; sep=0.150",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 948,
                    "func_name": "_boto3_now",
                    "func_desc": "_boto3_now",
                    "func_file": "aws",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _boto3_now():\\n    \"\"\"Get a ``datetime`` that's compatible with :py:mod:`boto3`.\\n    These are always UTC time, with time zone ``dateutil.tz.tzutc()``.\\n    \"\"\"\\n    if tzutc is None:\\n        raise ImportError(\\n            'You must install dateutil to get boto3-compatible datetimes')\\n\\n    return datetime.now(tzutc())",
                    "func_fullName": "mrjob.aws._boto3_now(  )"
                },
                {
                    "func_id": 949,
                    "func_name": "_boto3_paginate",
                    "func_desc": "_boto3_paginate",
                    "func_file": "aws",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _boto3_paginate(what, boto3_client, api_call, **api_params):\\n    \"\"\"Yield results from a paginatable API client call.\\n\\n    *what* is the name of the field the holds the list of items\\n    in each page (e.g. ``'InstanceGroups'``).\\n\\n    This doesn't do anything magical; it just saves the trouble of creating\\n    variable names for your paginator and its pages.\\n\\n    You can add the keyword ``_delay`` to *api_params* to sleep that many\\n    seconds after each API call\\n    \"\"\"\\n    # added the _delay kwarg for the audit-emr-usage tool; see #1091\\n    _delay = api_params.pop('_delay', None)\\n\\n    paginator = boto3_client.get_paginator(api_call)\\n\\n    for page in paginator.paginate(**api_params):\\n        if _delay:\\n            time.sleep(_delay)\\n\\n        for item in page[what]:\\n            yield item",
                    "func_fullName": "mrjob.aws._boto3_paginate( what, boto3_client, api_call, **api_params )"
                },
                {
                    "func_id": 1295,
                    "func_name": "counters",
                    "func_desc": "counters",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def counters(self):\\n        # not using self._pick_counters() because we don't want to\\n        # initiate a log fetch\\n        return [_pick_counters(log_interpretation)\\n                for log_interpretation in self._log_interpretations]",
                    "func_fullName": "mrjob.emr.counters( self )"
                },
                {
                    "func_id": 1852,
                    "func_name": "main",
                    "func_desc": "main",
                    "func_file": "audit_usage",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def main(args=None):\\n    # parse command-line args\\n    arg_parser = _make_arg_parser()\\n\\n    options = arg_parser.parse_args(args)\\n\\n    MRJob.set_up_logging(quiet=options.quiet, verbose=options.verbose)\\n\\n    now = _boto3_now()\\n\\n    log.info('getting cluster history...')\\n    clusters = list(_yield_clusters(\\n        max_days_ago=options.max_days_ago, now=now, **_runner_kwargs(options)))\\n\\n    log.info('compiling cluster stats...')\\n    stats = _clusters_to_stats(clusters, now=now)\\n\\n    _print_report(stats, now=now)",
                    "func_fullName": "mrjob.tools.emr.audit_usage.main( args )"
                },
                {
                    "func_id": 1862,
                    "func_name": "_print_report",
                    "func_desc": "_print_report",
                    "func_file": "audit_usage",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _print_report(stats, now=None):\\n    \"\"\"Print final report.\\n\\n    :param stats: a dictionary returned by :py:func:`_clusters_to_stats`\\n    :param now: the current UTC time, as a :py:class:`datetime.datetime`.\\n                Defaults to the current time.\\n    \"\"\"\\n    if now is None:\\n        now = _boto3_now()\\n\\n    s = stats\\n\\n    if not s['clusters']:\\n        print('No clusters created in the past two months!')\\n        return\\n\\n    print('Total  # of Clusters: %d' % len(s['clusters']))\\n    print()\\n\\n    print('* All times are in UTC.')\\n    print()\\n\\n    print('Min create time: %s' % min(cs['created'] for cs in s['clusters']))\\n    print('Max create time: %s' % max(cs['created'] for cs in s['clusters']))\\n    print('   Current time: %s' % now.replace(microsecond=0))\\n    print()\\n\\n    print('* All usage is measured in Normalized Instance Hours, which are')\\n    print('  roughly equivalent to running an m1.medium instance for an hour.')\\n    print(\"  Billing is estimated, and may not match Amazon's system exactly.\")\\n    print()\\n\\n    # total compute-unit hours used\\n    def with_pct(usage):\\n        return (usage, _percent(usage, s['nih_billed']))\\n\\n    print('Total billed:  %9.2f  %5.1f%%' % with_pct(s['nih_billed']))\\n    print('  Total used:  %9.2f  %5.1f%%' % with_pct(s['nih_used']))\\n    print('    bootstrap: %9.2f  %5.1f%%' % with_pct(s['bootstrap_nih_used']))\\n    print('    jobs:      %9.2f  %5.1f%%' % with_pct(s['job_nih_used']))\\n    print('  Total waste: %9.2f  %5.1f%%' % with_pct(s['nih_bbnu']))\\n    print('    at end:    %9.2f  %5.1f%%' % with_pct(s['end_nih_bbnu']))\\n    print('    other:     %9.2f  %5.1f%%' % with_pct(s['other_nih_bbnu']))\\n    print()\\n\\n    if s['date_to_nih_billed']:\\n        print('Daily statistics:')\\n        print()\\n        print(' date          billed      used     waste   % waste')\\n        d = max(s['date_to_nih_billed'])\\n        while d >= min(s['date_to_nih_billed']):\\n            print(' %10s %9.2f %9.2f %9.2f     %5.1f' % (\\n                d,\\n                s['date_to_nih_billed'].get(d, 0.0),\\n                s['date_to_nih_used'].get(d, 0.0),\\n                s['date_to_nih_bbnu'].get(d, 0.0),\\n                _percent(s['date_to_nih_bbnu'].get(d, 0.0),\\n                         s['date_to_nih_billed'].get(d, 0.0))))\\n            d -= timedelta(days=1)\\n        print()\\n\\n    if s['hour_to_nih_billed']:\\n        print('Hourly statistics:')\\n        print()\\n        print(' hour              billed      used     waste   % waste')\\n        h = max(s['hour_to_nih_billed'])\\n        while h >= min(s['hour_to_nih_billed']):\\n            print(' %13s  %9.2f %9.2f %9.2f     %5.1f' % (\\n                h.strftime('%Y-%m-%d %H'),\\n                s['hour_to_nih_billed'].get(h, 0.0),\\n                s['hour_to_nih_used'].get(h, 0.0),\\n                s['hour_to_nih_bbnu'].get(h, 0.0),\\n                _percent(s['hour_to_nih_bbnu'].get(h, 0.0),\\n                         s['hour_to_nih_billed'].get(h, 0.0))))\\n            h -= timedelta(hours=1)\\n        print()\\n\\n    print('* clusters are considered to belong to the user and job that')\\n    print('  started them or last ran on them.')\\n    print()\\n\\n    # Top jobs\\n    print('Top jobs, by total time used:')\\n    for label, nih_used in sorted(s['label_to_nih_used'].items(),\\n                                  key=lambda lb_nih: (-lb_nih[1], lb_nih[0])):\\n        print('  %9.2f %s' % (nih_used, label))\\n    print()\\n\\n    print('Top jobs, by time billed but not used:')\\n    for label, nih_bbnu in sorted(\\n            s['label_to_nih_bbnu'].items(),\\n            key=lambda lb_nih1: (-lb_nih1[1], lb_nih1[0])):\\n        print('  %9.2f %s' % (nih_bbnu, label))\\n    print()\\n\\n    # Top users\\n    print('Top users, by total time used:')\\n    for owner, nih_used in sorted(s['owner_to_nih_used'].items(),\\n                                  key=lambda o_nih: (-o_nih[1], o_nih[0])):\\n        print('  %9.2f %s' % (nih_used, owner))\\n    print()\\n\\n    print('Top users, by time billed but not used:')\\n    for owner, nih_bbnu in sorted(s['owner_to_nih_bbnu'].items(),\\n                                  key=lambda o_nih2: (-o_nih2[1], o_nih2[0])):\\n        print('  %9.2f %s' % (nih_bbnu, owner))\\n    print()\\n\\n    # Top job steps\\n    print('Top job steps, by total time used (step number first):')\\n    for (label, step_num), nih_used in sorted(\\n            s['job_step_to_nih_used'].items(),\\n            key=lambda k_nih: (-k_nih[1], k_nih[0])):\\n\\n        if label:\\n            print('  %9.2f %3d %s' % (nih_used, step_num, label))\\n        else:\\n            print('  %9.2f     (non-mrjob step)' % (nih_used,))\\n    print()\\n\\n    print('Top job steps, by total time billed but not used (un-pooled only):')\\n    for (label, step_num), nih_bbnu in sorted(\\n            s['job_step_to_nih_bbnu_no_pool'].items(),\\n            key=lambda k_nih3: (-k_nih3[1], k_nih3[0])):\\n\\n        if label:\\n            print('  %9.2f %3d %s' % (nih_bbnu, step_num, label))\\n        else:\\n            print('  %9.2f     (non-mrjob step)' % (nih_bbnu,))\\n    print()\\n\\n    # Top pools\\n    print('All pools, by total time billed:')\\n    for pool, nih_billed in sorted(s['pool_to_nih_billed'].items(),\\n                                   key=lambda p_nih: (-p_nih[1], p_nih[0])):\\n        print('  %9.2f %s' % (nih_billed, pool or '(not pooled)'))\\n    print()\\n\\n    print('All pools, by total time billed but not used:')\\n    for pool, nih_bbnu in sorted(s['pool_to_nih_bbnu'].items(),\\n                                 key=lambda p_nih4: (-p_nih4[1], p_nih4[0])):\\n        print('  %9.2f %s' % (nih_bbnu, pool or '(not pooled)'))\\n    print()\\n\\n    # Top clusters\\n    print('All clusters, by total time billed:')\\n    top_clusters = sorted(s['clusters'],\\n                          key=lambda cs: (-cs['nih_billed'], cs['name']))\\n    for cs in top_clusters:\\n        print('  %9.2f %-15s %s' % (\\n            cs['nih_billed'], cs['id'], cs['name']))\\n    print()\\n\\n    print('All clusters, by time billed but not used:')\\n    top_clusters_bbnu = sorted(\\n        s['clusters'], key=lambda cs: (-cs['nih_bbnu'], cs['name']))\\n    for cs in top_clusters_bbnu:\\n        print('  %9.2f %-15s %s' % (\\n            cs['nih_bbnu'], cs['id'], cs['name']))\\n    print()\\n\\n    # Details\\n    print('Details for all clusters:')\\n    print()\\n    print(' id              state                  created             steps'\\n          '        time ran     billed    waste   user   name')\\n\\n    all_clusters = sorted(s['clusters'], key=lambda cs: cs['created'],\\n                          reverse=True)\\n\\n    for cs in all_clusters:\\n        print(' %-15s %-22s %19s %3d %17s %9.2f %9.2f %8s %s' % (\\n            cs['id'], cs['state'], cs['created'], cs['num_steps'],\\n            strip_microseconds(cs['ran']), cs['nih_used'], cs['nih_bbnu'],\\n            (cs['owner'] or ''), (cs['label'] or ('not started by mrjob'))))",
                    "func_fullName": "mrjob.tools.emr.audit_usage._print_report( stats, now )"
                },
                {
                    "func_id": 1863,
                    "func_name": "_percent",
                    "func_desc": "_percent",
                    "func_file": "audit_usage",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _percent(x, total, default=0.0):\\n    \"\"\"Return what percentage *x* is of *total*, or *default* if\\n    *total* is zero.\"\"\"\\n    if total:\\n        return 100.0 * x / total\\n    else:\\n        return default",
                    "func_fullName": "mrjob.tools.emr.audit_usage._percent( x, total, default )"
                },
                {
                    "func_id": 1865,
                    "func_name": "with_pct",
                    "func_desc": "with_pct",
                    "func_file": "audit_usage",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def with_pct(usage):\\n        return (usage, _percent(usage, s['nih_billed']))",
                    "func_fullName": "mrjob.tools.emr.audit_usage.with_pct( usage )"
                },
                {
                    "func_id": 1874,
                    "func_name": "main",
                    "func_desc": "main",
                    "func_file": "report_long_jobs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def main(args=None):\\n    now = _boto3_now()\\n\\n    arg_parser = _make_arg_parser()\\n    options = arg_parser.parse_args(args)\\n\\n    MRJob.set_up_logging(quiet=options.quiet, verbose=options.verbose)\\n\\n    log.info('getting information about running jobs')\\n\\n    min_time = timedelta(hours=options.min_hours)\\n\\n    emr_client = EMRJobRunner(**_runner_kwargs(options)).make_emr_client()\\n    cluster_summaries = _boto3_paginate(\\n        'Clusters', emr_client, 'list_clusters',\\n        ClusterStates=['STARTING', 'BOOTSTRAPPING', 'RUNNING'])\\n\\n    if not options.exclude:\\n        filtered_cluster_summaries = cluster_summaries\\n    else:\\n        filtered_cluster_summaries = _filter_clusters(\\n            cluster_summaries, emr_client, options.exclude)\\n\\n    job_info = _find_long_running_jobs(\\n        emr_client, filtered_cluster_summaries, min_time, now=now)\\n\\n    _print_report(job_info)",
                    "func_fullName": "mrjob.tools.emr.report_long_jobs.main( args )"
                },
                {
                    "func_id": 1877,
                    "func_name": "_find_long_running_jobs",
                    "func_desc": "_find_long_running_jobs",
                    "func_file": "report_long_jobs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _find_long_running_jobs(emr_client, cluster_summaries, min_time, now=None):\\n    \"\"\"Identify jobs that have been running or pending for a long time.\\n\\n    :param clusters: a list of :py:mod:`boto3` cluster summary data structures\\n    :param min_time: a :py:class:`datetime.timedelta`: report jobs running or\\n                     pending longer than this\\n    :param now: the current UTC time, as a :py:class:`datetime.datetime`.\\n                Defaults to the current time.\\n\\n    For each job that is running or pending longer than *min_time*, yields\\n    a dictionary with the following keys:\\n\\n    * *cluster_id*: the cluster's unique ID (e.g. ``j-SOMECLUSTER``)\\n    * *name*: name of the step, or the cluster when bootstrapping\\n    * *state*: state of the step (``'RUNNING'`` or ``'PENDING'``) or, if there\\n               is no step, the cluster (``'STARTING'`` or ``'BOOTSTRAPPING'``)\\n    * *time*: amount of time step was running or pending, as a\\n              :py:class:`datetime.timedelta`\\n    \"\"\"\\n    if now is None:\\n        now = _boto3_now()\\n\\n    for cs in cluster_summaries:\\n\\n        # special case for jobs that are taking a long time to bootstrap\\n        if cs['Status']['State'] in ('STARTING', 'BOOTSTRAPPING'):\\n            # there isn't a way to tell when the cluster stopped being\\n            # provisioned and started bootstrapping, so just measure\\n            # from cluster creation time\\n            created = cs['Status']['Timeline']['CreationDateTime']\\n\\n            time_running = now - created\\n\\n            if time_running >= min_time:\\n                yield({'cluster_id': cs['Id'],\\n                       'name': cs['Name'],\\n                       'state': cs['Status']['State'],\\n                       'time': time_running})\\n\\n        # the default case: running clusters\\n        if cs['Status']['State'] != 'RUNNING':\\n            continue\\n\\n        steps = list(reversed(list(_boto3_paginate(\\n            'Steps', emr_client, 'list_steps', ClusterId=cs['Id']))))\\n\\n        running_steps = [\\n            step for step in steps if step['Status']['State'] == 'RUNNING']\\n        pending_steps = [\\n            step for step in steps if step['Status']['State'] == 'PENDING']\\n\\n        if running_steps:\\n            # should be only one, but if not, we should know about it\\n            for step in running_steps:\\n\\n                start = step['Status']['Timeline']['StartDateTime']\\n\\n                time_running = now - start\\n\\n                if time_running >= min_time:\\n                    yield({'cluster_id': cs['Id'],\\n                           'name': step['Name'],\\n                           'state': step['Status']['State'],\\n                           'time': time_running})\\n\\n        # sometimes EMR says it's \"RUNNING\" but doesn't actually run steps!\\n        elif pending_steps:\\n            step = pending_steps[0]\\n\\n            # PENDING job should have run starting when the cluster\\n            # became ready, or the previous step completed\\n            start = cs['Status']['Timeline']['ReadyDateTime']\\n            for step in steps:\\n                if step['Status']['State'] == 'COMPLETED':\\n                    start = step['Status']['Timeline']['EndDateTime']\\n\\n            time_pending = now - start\\n\\n            if time_pending >= min_time:\\n                yield({'cluster_id': cs['Id'],\\n                       'name': step['Name'],\\n                       'state': step['Status']['State'],\\n                       'time': time_pending})",
                    "func_fullName": "mrjob.tools.emr.report_long_jobs._find_long_running_jobs( emr_client, cluster_summaries, min_time, now )"
                },
                {
                    "func_id": 1878,
                    "func_name": "_print_report",
                    "func_desc": "_print_report",
                    "func_file": "report_long_jobs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _print_report(job_info):\\n    \"\"\"Takes in a dictionary of info about a long-running job (see\\n    :py:func:`_find_long_running_jobs`), and prints information about it\\n    on a single (long) line.\\n    \"\"\"\\n    for ji in job_info:\\n        print('%-15s %13s for %17s (%s)' % (\\n            ji['cluster_id'],\\n            ji['state'], _format_timedelta(ji['time']),\\n            ji['name']))",
                    "func_fullName": "mrjob.tools.emr.report_long_jobs._print_report( job_info )"
                },
                {
                    "func_id": 1879,
                    "func_name": "_format_timedelta",
                    "func_desc": "_format_timedelta",
                    "func_file": "report_long_jobs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _format_timedelta(time):\\n    \"\"\"Format a timedelta for use in a columnar format. This just\\n    tweaks stuff like ``'3 days, 9:00:00'`` to line up with\\n    ``'3 days, 10:00:00'``\\n    \"\"\"\\n    result = str(strip_microseconds(time))\\n\\n    parts = result.split()\\n    if len(parts) == 3 and len(parts[-1]) == 7:\\n        return '%s %s  %s' % tuple(parts)\\n    else:\\n        return result",
                    "func_fullName": "mrjob.tools.emr.report_long_jobs._format_timedelta( time )"
                }
            ]
        },
        {
            "cluster_id": 1,
            "feature_id": 44,
            "feature_desc": "gamma=0.0608; k=17; a=0.25; combined=0.388; stability(ARI)=0.911; sep=0.150",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1197,
                    "func_name": "_get_job_steps",
                    "func_desc": "_get_job_steps",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _get_job_steps(emr_client, cluster_id, job_key):\\n    \"\"\"Efficiently fetch steps for a particular mrjob run from the EMR API.\\n\\n    :param emr_client: a boto3 EMR client. See\\n                       :py:meth:`~mrjob.emr.EMRJobRunner.make_emr_client`\\n    :param cluster_id: ID of EMR cluster to fetch steps from. See\\n                       :py:meth:`~mrjob.emr.EMRJobRunner.get_cluster_id`\\n    :param job_key: Unique key for a mrjob run. See\\n                    :py:meth:`~mrjob.runner.MRJobRunner.get_job_key`\\n    \"\"\"\\n    steps = []\\n\\n    for step in _boto3_paginate('Steps', emr_client, 'list_steps',\\n                                ClusterId=cluster_id):\\n        if step['Name'].startswith(job_key):\\n            steps.append(step)\\n        elif steps:\\n            # all steps for job will be together, so stop\\n            # when we find a non-job step\\n            break\\n\\n    return list(reversed(list(steps)))",
                    "func_fullName": "mrjob.emr._get_job_steps( emr_client, cluster_id, job_key )"
                },
                {
                    "func_id": 1254,
                    "func_name": "_add_steps_in_batch",
                    "func_desc": "_add_steps_in_batch",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _add_steps_in_batch(self):\\n        if self._opts['add_steps_in_batch'] is None:\\n            # by default, add steps in batch only when concurrent steps\\n            # are not possible\\n            return not self._image_version_gte('5.28.0')\\n        else:\\n            return self._opts['add_steps_in_batch']",
                    "func_fullName": "mrjob.emr._add_steps_in_batch( self )"
                },
                {
                    "func_id": 1255,
                    "func_name": "_steps_to_submit",
                    "func_desc": "_steps_to_submit",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _steps_to_submit(self):\\n        \"\"\"Return a step data structures to pass to ``boto3``\"\"\"\\n        # quick, add the other steps before the job spins up and\\n        # then shuts itself down! (in practice that won't happen\\n        # for several minutes)\\n        steps = []\\n\\n        if self._master_node_setup_script_path:\\n            steps.append(self._build_master_node_setup_step())\\n\\n        for n in range(self._num_steps()):\\n            steps.append(self._build_step(n))\\n\\n        return steps",
                    "func_fullName": "mrjob.emr._steps_to_submit( self )"
                },
                {
                    "func_id": 1256,
                    "func_name": "_build_step",
                    "func_desc": "_build_step",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _build_step(self, step_num):\\n        step = self._get_step(step_num)\\n\\n        if step['type'] == 'streaming':\\n            method = self._streaming_step_hadoop_jar_step\\n        elif step['type'] == 'jar':\\n            method = self._jar_step_hadoop_jar_step\\n        elif _is_spark_step_type(step['type']):\\n            method = self._spark_step_hadoop_jar_step\\n        else:\\n            raise ValueError('Bad step type: %r' % (step['type'],))\\n\\n        hadoop_jar_step = method(step_num)\\n\\n        return dict(\\n            ActionOnFailure=self._action_on_failure(),\\n            HadoopJarStep=hadoop_jar_step,\\n            Name=self._step_name(step_num),\\n        )",
                    "func_fullName": "mrjob.emr._build_step( self, step_num )"
                },
                {
                    "func_id": 1265,
                    "func_name": "_step_name",
                    "func_desc": "_step_name",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _step_name(self, step_num):\\n        \"\"\"Return something like: ``'mr_your_job Step X of Y'``\"\"\"\\n        return '%s: Step %d of %d' % (\\n            self._job_key, step_num + 1, self._num_steps())",
                    "func_fullName": "mrjob.emr._step_name( self, step_num )"
                },
                {
                    "func_id": 1269,
                    "func_name": "_get_streaming_jar_and_step_arg_prefix",
                    "func_desc": "_get_streaming_jar_and_step_arg_prefix",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_streaming_jar_and_step_arg_prefix(self):\\n        if self._opts['hadoop_streaming_jar']:\\n            if self._opts['hadoop_streaming_jar'].startswith('file://'):\\n                # special case: jar is already on EMR\\n                # relative paths are OK (though maybe not useful)\\n                return self._opts['hadoop_streaming_jar'][7:], []\\n            else:\\n                return self._upload_mgr.uri(\\n                    self._opts['hadoop_streaming_jar']), []\\n        elif version_gte(self.get_image_version(), '4'):\\n            # 4.x AMIs use an intermediary jar\\n            return _4_X_COMMAND_RUNNER_JAR, ['hadoop-streaming']\\n        else:\\n            # 2.x and 3.x AMIs just use a regular old streaming jar\\n            return _PRE_4_X_STREAMING_JAR, []",
                    "func_fullName": "mrjob.emr._get_streaming_jar_and_step_arg_prefix( self )"
                },
                {
                    "func_id": 1273,
                    "func_name": "get_job_steps",
                    "func_desc": "get_job_steps",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_job_steps(self):\\n        \"\"\"Fetch the steps submitted by this runner from the EMR API.\\n\\n        .. deprecated:: 0.7.4\\n\\n        .. versionadded:: 0.6.1\\n        \"\"\"\\n        log.warning(\\n            'get_job_steps() is deprecated and will be removed in v0.8.0')\\n\\n        return _get_job_steps(\\n            self.make_emr_client(), self.get_cluster_id(), self.get_job_key())",
                    "func_fullName": "mrjob.emr.get_job_steps( self )"
                },
                {
                    "func_id": 1274,
                    "func_name": "_wait_for_steps_to_complete",
                    "func_desc": "_wait_for_steps_to_complete",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _wait_for_steps_to_complete(self):\\n        \"\"\"Wait for every step of the job to complete, one by one.\"\"\"\\n        steps = self._steps_to_submit()\\n\\n        # clear out log interpretations if they were filled somehow\\n        self._log_interpretations = []\\n        self._mns_log_interpretation = None\\n\\n        # open SSH tunnel if cluster is already ready\\n        # (this happens with pooling). See #1115\\n        cluster = self._describe_cluster()\\n        if cluster['Status']['State'] in ('RUNNING', 'WAITING'):\\n            self._set_up_ssh_tunnel_and_hdfs()\\n\\n        for i, step in enumerate(steps):\\n            # if our step isn't already submitted, submit it\\n            if len(self._step_ids) <= i:\\n                self._add_steps_to_cluster(\\n                    steps[len(self._step_ids):i + 1])\\n\\n            step_id = self._step_ids[i]\\n            step_name = step['Name'].split(': ')[-1]\\n\\n            # treat master node setup script is treated as step -1\\n            if self._master_node_setup_script_path:\\n                step_num = i - 1\\n            else:\\n                step_num = i\\n\\n            log.info('Waiting for %s (%s) to complete...' %\\n                     (step_name, step_id))\\n\\n            self._wait_for_step_to_complete(step_id, step_num)",
                    "func_fullName": "mrjob.emr._wait_for_steps_to_complete( self )"
                },
                {
                    "func_id": 1275,
                    "func_name": "_wait_for_step_to_complete",
                    "func_desc": "_wait_for_step_to_complete",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _wait_for_step_to_complete(self, step_id, step_num=None):\\n        \"\"\"Helper for _wait_for_step_to_complete(). Wait for\\n        step with the given ID to complete, and fetch counters.\\n        If it fails, attempt to diagnose the error, and raise an\\n        exception.\\n\\n        :param step_id: the s-XXXXXXX step ID on EMR\\n        :param step_num: which step this is out of the steps\\n                         belonging to our job (0-indexed). Master node\\n                         setup script, if there is one, is step -1\\n\\n        This also adds an item to self._log_interpretations or sets\\n        self._mns_log_interpretation\\n        \"\"\"\\n        log_interpretation = dict(step_id=step_id)\\n\\n        # suppress warnings about missing job ID for script-runner.jar\\n        if step_num == -1:\\n            log_interpretation['no_job'] = True\\n            self._mns_log_interpretation = log_interpretation\\n        else:\\n            self._log_interpretations.append(log_interpretation)\\n\\n        emr_client = self.make_emr_client()\\n\\n        while True:\\n            # don't antagonize EMR's throttling\\n            log.debug('Waiting %.1f seconds...' %\\n                      self._opts['check_cluster_every'])\\n            time.sleep(self._opts['check_cluster_every'])\\n\\n            # log address of the master node once if we have it\\n            self._log_address_of_master_once()\\n\\n            step = emr_client.describe_step(\\n                ClusterId=self._cluster_id, StepId=step_id)['Step']\\n\\n            if step['Status']['State'] == 'PENDING':\\n                cluster = self._describe_cluster()\\n\\n                reason = _get_reason(cluster)\\n                reason_desc = (': %s' % reason) if reason else ''\\n\\n                # we can open the ssh tunnel if cluster is ready (see #1115)\\n                if cluster['Status']['State'] in ('RUNNING', 'WAITING'):\\n                    self._set_up_ssh_tunnel_and_hdfs()\\n\\n                log.info('  PENDING (cluster is %s%s)' % (\\n                    cluster['Status']['State'], reason_desc))\\n                continue\\n\\n            elif step['Status']['State'] == 'RUNNING':\\n\\n                time_running_desc = ''\\n\\n                start = step['Status']['Timeline'].get('StartDateTime')\\n                if start:\\n                    time_running_desc = ' for %s' % strip_microseconds(\\n                        _boto3_now() - start)\\n\\n                # now is the time to tunnel, if we haven't already\\n                self._set_up_ssh_tunnel_and_hdfs()\\n                log.info('  RUNNING%s' % time_running_desc)\\n\\n                # don't log progress for master node setup step, because\\n                # it doesn't appear in job tracker\\n                if step_num >= 0:\\n                    self._log_step_progress()\\n\\n                # it's safe to clean up our lock, cluster isn't WAITING\\n                self._release_cluster_lock()\\n\\n                continue\\n\\n            # we're done, will return at the end of this\\n            elif step['Status']['State'] == 'COMPLETED':\\n                log.info('  COMPLETED')\\n                # will fetch counters, below, and then return\\n            else:\\n                # step has failed somehow. *reason* seems to only be set\\n                # when job is cancelled (e.g. 'Job terminated')\\n                reason = _get_reason(step)\\n                reason_desc = (' (%s)' % reason) if reason else ''\\n\\n                log.info('  %s%s' % (\\n                    step['Status']['State'], reason_desc))\\n\\n                # print cluster status; this might give more context\\n                # why step didn't succeed\\n                cluster = self._describe_cluster()\\n                reason = _get_reason(cluster)\\n                reason_desc = (': %s' % reason) if reason else ''\\n                log.info('Cluster %s %s %s%s' % (\\n                    cluster['Id'],\\n                    'was' if 'ED' in cluster['Status']['State'] else 'is',\\n                    cluster['Status']['State'],\\n                    reason_desc))\\n\\n                if cluster['Status']['State'] in (\\n                        'TERMINATING', 'TERMINATED', 'TERMINATED_WITH_ERRORS'):\\n                    # was it caused by a pooled cluster self-terminating?\\n                    # (if so, raise _PooledClusterSelfTerminatedException)\\n                    self._check_for_pooled_cluster_self_termination(\\n                        cluster, step)\\n                    # was it caused by IAM roles?\\n                    self._check_for_missing_default_iam_roles(cluster)\\n                    # was it because a bootstrap action failed?\\n                    self._check_for_failed_bootstrap_action(cluster)\\n\\n            # spark steps require different log parsing. The master node\\n            # setup script is a JAR step (albeit one that never produces\\n            # counters)\\n            step_type = (\\n                self._get_step(step_num)['type'] if step_num >= 0 else 'jar')\\n\\n            # step is done (either COMPLETED, FAILED, INTERRUPTED). so\\n            # try to fetch counters. (Except for master node setup\\n            # and Spark, which has no counters.)\\n            if step['Status']['State'] != 'CANCELLED' and step_num >= 0:\\n                self._log_counters(log_interpretation, step_num)\\n\\n            if step['Status']['State'] == 'COMPLETED':\\n                return\\n\\n            if step['Status']['State'] == 'FAILED':\\n                error = self._pick_error(log_interpretation, step_type)\\n                if error:\\n                    _log_probable_cause_of_failure(log, error)\\n\\n            raise StepFailedException(\\n                step_num=step_num, num_steps=self._num_steps(),\\n                # \"Step 0 of ... failed\" looks weird\\n                step_desc=(\\n                    'Master node setup step' if step_num == -1 else None))",
                    "func_fullName": "mrjob.emr._wait_for_step_to_complete( self, step_id, step_num )"
                },
                {
                    "func_id": 1277,
                    "func_name": "_log_step_progress",
                    "func_desc": "_log_step_progress",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _log_step_progress(self):\\n        \"\"\"Tunnel to the job tracker/resource manager and log the\\n        progress of the current step.\\n\\n        (This takes no arguments; we just assume the most recent running\\n        job is ours, which should be correct for EMR.)\\n        \"\"\"\\n        progress_html = (self._progress_html_from_tunnel() or\\n                         self._progress_html_over_ssh())\\n        if not progress_html:\\n            return\\n\\n        tunnel_config = self._ssh_tunnel_config()\\n\\n        if tunnel_config['name'] == 'job tracker':\\n            map_progress, reduce_progress = (\\n                _parse_progress_from_job_tracker(progress_html))\\n            if map_progress is not None:\\n                log.info('   map %3d%% reduce %3d%%' % (\\n                    map_progress, reduce_progress))\\n        else:\\n            progress = _parse_progress_from_resource_manager(\\n                progress_html)\\n            if progress is not None:\\n                log.info('   %5.1f%% complete' % progress)",
                    "func_fullName": "mrjob.emr._log_step_progress( self )"
                },
                {
                    "func_id": 1278,
                    "func_name": "_progress_html_from_tunnel",
                    "func_desc": "_progress_html_from_tunnel",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _progress_html_from_tunnel(self):\\n        \"\"\"Fetch progress by calling :py:func:`urlopen` on our ssh tunnel, or\\n        return ``None``.\"\"\"\\n        if not self._ssh_tunnel_url:\\n            return None\\n\\n        tunnel_config = self._ssh_tunnel_config()\\n        log.debug('  Fetching progress from %s at %s' % (\\n            tunnel_config['name'], self._ssh_tunnel_url))\\n\\n        tunnel_handle = None\\n        try:\\n            tunnel_handle = urlopen(self._ssh_tunnel_url)\\n            return tunnel_handle.read()\\n        except Exception as e:\\n            log.debug('    failed: %s' % str(e))\\n            return None\\n        finally:\\n            if tunnel_handle:\\n                tunnel_handle.close()",
                    "func_fullName": "mrjob.emr._progress_html_from_tunnel( self )"
                },
                {
                    "func_id": 1306,
                    "func_name": "_build_debugging_step",
                    "func_desc": "_build_debugging_step",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _build_debugging_step(self):\\n        if self._opts['release_label']:\\n            jar = _4_X_COMMAND_RUNNER_JAR\\n            args = ['state-pusher-script']\\n        else:\\n            jar = self._script_runner_jar_uri()\\n            args = (\\n                's3://%s.elasticmapreduce/libs/state-pusher/0.1/fetch' %\\n                self._opts['region'])\\n\\n        return dict(\\n            Name='Setup Hadoop Debugging',\\n            HadoopJarStep=dict(Jar=jar, Args=args),\\n        )",
                    "func_fullName": "mrjob.emr._build_debugging_step( self )"
                },
                {
                    "func_id": 1834,
                    "func_name": "main",
                    "func_desc": "main",
                    "func_file": "diagnose",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def main(cl_args=None):\\n    arg_parser = _make_arg_parser()\\n    options = arg_parser.parse_args(cl_args)\\n\\n    MRJob.set_up_logging(quiet=options.quiet, verbose=options.verbose)\\n\\n    runner_kwargs = {k: v for k, v in options.__dict__.items()\\n                     if k not in ('quiet', 'verbose', 'step_id')}\\n\\n    runner = EMRJobRunner(**runner_kwargs)\\n    emr_client = runner.make_emr_client()\\n\\n    # pick step\\n    step = _get_step(emr_client, options.cluster_id, options.step_id)\\n\\n    if not step:\\n        raise SystemExit(1)\\n\\n    if step['Status']['State'] != 'FAILED':\\n        log.warning('step %s has state %s, not FAILED' %\\n                    (step['Id'], step['Status']['State']))\\n\\n    # interpret logs\\n    log.info('Diagnosing step %s (%s)' % (step['Id'], step['Name']))\\n\\n    log_interpretation = dict(step_id=step['Id'])\\n\\n    step_type = _infer_step_type(step)\\n\\n    error = runner._pick_error(log_interpretation, step_type)\\n\\n    # print error\\n    if error:\\n        log.error('Probable cause of failure:\\n\\n%s\\n\\n' %\\n                  _format_error(error))\\n    else:\\n        log.warning('No error detected')",
                    "func_fullName": "mrjob.tools.diagnose.main( cl_args )"
                },
                {
                    "func_id": 1835,
                    "func_name": "_get_step",
                    "func_desc": "_get_step",
                    "func_file": "diagnose",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _get_step(emr_client, cluster_id, step_id=None):\\n\\n    # just iterate backwards through steps, rather than filtering\\n    # by step ID or status. usually it'll be the last step anyhow\\n\\n    for step in _boto3_paginate('Steps', emr_client, 'list_steps',\\n                                ClusterId=cluster_id):\\n\\n        if _step_matches(step, step_id=step_id):\\n            return step\\n    else:\\n        if step_id:\\n            log.error('step %s not found on cluster %s' %\\n                      (step_id, cluster_id))\\n        else:\\n            log.error('cluster %s has no failed steps' % cluster_id)",
                    "func_fullName": "mrjob.tools.diagnose._get_step( emr_client, cluster_id, step_id )"
                },
                {
                    "func_id": 1836,
                    "func_name": "_step_matches",
                    "func_desc": "_step_matches",
                    "func_file": "diagnose",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _step_matches(step, step_id=None):\\n    if not step_id:\\n        return step['Status']['State'] == 'FAILED'\\n    else:\\n        return step['Id'] == step_id",
                    "func_fullName": "mrjob.tools.diagnose._step_matches( step, step_id )"
                },
                {
                    "func_id": 1837,
                    "func_name": "_infer_step_type",
                    "func_desc": "_infer_step_type",
                    "func_file": "diagnose",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _infer_step_type(step):\\n    args = step['Config']['Args']\\n\\n    # all that matters for log parsing is picking out Spark steps\\n    # (doesn't matter if it's spark or spark_jar or spark_script)\\n    #\\n    # and of course we don't know the logging habits of jar steps,\\n    # so we might as well use streaming's logic\\n    if '--master' in args and '--deploy-mode' in args:\\n        return 'spark'\\n    else:\\n        return 'streaming'",
                    "func_fullName": "mrjob.tools.diagnose._infer_step_type( step )"
                },
                {
                    "func_id": 1893,
                    "func_name": "_is_step_running",
                    "func_desc": "_is_step_running",
                    "func_file": "terminate_idle_clusters",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _is_step_running(step):\\n    \"\"\"Return true if the given step is currently running.\"\"\"\\n    return (step['Status']['State'] not in\\n            ('CANCELLED', 'INTERRUPTED') and\\n            step['Status']['Timeline'].get('StartDateTime') and\\n            not step['Status']['Timeline'].get('EndDateTime'))",
                    "func_fullName": "mrjob.tools.emr.terminate_idle_clusters._is_step_running( step )"
                }
            ]
        },
        {
            "cluster_id": 1,
            "feature_id": 45,
            "feature_desc": "gamma=0.0608; k=17; a=0.25; combined=0.388; stability(ARI)=0.911; sep=0.150",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1198,
                    "func_name": "_get_reason",
                    "func_desc": "_get_reason",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _get_reason(cluster_or_step):\\n    \"\"\"Get state change reason message.\"\"\"\\n    # StateChangeReason is {} before the first state change\\n    return cluster_or_step['Status']['StateChangeReason'].get('Message', '')",
                    "func_fullName": "mrjob.emr._get_reason( cluster_or_step )"
                },
                {
                    "func_id": 1208,
                    "func_name": "_obfuscate_opt",
                    "func_desc": "_obfuscate_opt",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _obfuscate_opt(self, opt_key, opt_value):\\n        \"\"\"Obfuscate AWS credentials.\"\"\"\\n        # don't need to obfuscate empty values\\n        if not opt_value:\\n            return opt_value\\n\\n        if opt_key in ('aws_secret_access_key', 'aws_session_token'):\\n            # don't expose any part of secret credentials\\n            return '...'\\n        elif opt_key == 'aws_access_key_id':\\n            if isinstance(opt_value, string_types):\\n                return '...' + opt_value[-4:]\\n            else:\\n                # don't expose aws_access_key_id if it was accidentally\\n                # put in a list or something\\n                return '...'\\n        else:\\n            return opt_value",
                    "func_fullName": "mrjob.emr._obfuscate_opt( self, opt_key, opt_value )"
                },
                {
                    "func_id": 1214,
                    "func_name": "_bash_is_bad",
                    "func_desc": "_bash_is_bad",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _bash_is_bad(self):\\n        # hopefully, there will eventually be an image version\\n        # where this issue is fixed. See #1548\\n        return self._image_version_gte(_BAD_BASH_IMAGE_VERSION)",
                    "func_fullName": "mrjob.emr._bash_is_bad( self )"
                },
                {
                    "func_id": 1215,
                    "func_name": "_default_sh_bin",
                    "func_desc": "_default_sh_bin",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _default_sh_bin(self):\\n        if self._bash_is_bad():\\n            return _BAD_BASH_SH_BIN\\n        else:\\n            return _GOOD_BASH_SH_BIN",
                    "func_fullName": "mrjob.emr._default_sh_bin( self )"
                },
                {
                    "func_id": 1216,
                    "func_name": "_sh_pre_commands",
                    "func_desc": "_sh_pre_commands",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _sh_pre_commands(self):\\n        if self._bash_is_bad() and not self._opts['sh_bin']:\\n            return ['set -e']\\n        else:\\n            return []",
                    "func_fullName": "mrjob.emr._sh_pre_commands( self )"
                },
                {
                    "func_id": 1223,
                    "func_name": "_check_input_path",
                    "func_desc": "_check_input_path",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _check_input_path(self, path):\\n        \"\"\"Add a custom check for S3 paths to ensure they're not in\\n        Glacier (which causes a cryptic error). See #1887.\"\"\"\\n        # handle non-S3 paths the usual way\\n        if not is_s3_uri(path):\\n            super(EMRJobRunner, self)._check_input_path(path)\\n            return\\n\\n        exists = False\\n\\n        for uri, obj in self.fs.s3._ls(path):\\n            exists = True\\n\\n            # we currently just look for 'ongoing-request=\"false\"'\\n            # in the *restore* field and ignore the expiration date\\n            # (if the object has expired, the *restore* field won't be set).\\n            #\\n            # See #1887 for more discussion of checking expiration.\\n            if obj.storage_class == 'GLACIER' and not (\\n                    obj.restore and _RESTORED_FROM_GLACIER in obj.restore):\\n                raise IOError(\\n                    '%s is archived in Glacier and'\\n                    ' cannot be read as input!' % uri)\\n\\n        if not exists:\\n            raise IOError(\\n                'Input path %s does not exist!' % (path,))",
                    "func_fullName": "mrjob.emr._check_input_path( self, path )"
                },
                {
                    "func_id": 1300,
                    "func_name": "_cp_to_local_cmd",
                    "func_desc": "_cp_to_local_cmd",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cp_to_local_cmd(self):\\n        \"\"\"Command to copy files from the cloud to the local directory.\"\"\"\\n        if self._opts['release_label']:\\n            # on the 4.x AMIs, hadoop isn't yet installed, so use AWS CLI\\n            return 'aws s3 cp'\\n        else:\\n            # on the 2.x and 3.x AMIs, use hadoop\\n            return 'hadoop fs -copyToLocal'",
                    "func_fullName": "mrjob.emr._cp_to_local_cmd( self )"
                },
                {
                    "func_id": 1307,
                    "func_name": "_debug_script_uri",
                    "func_desc": "_debug_script_uri",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _debug_script_uri(self):\\n        return (\\n            's3://%s.elasticmapreduce/libs/state-pusher/0.1/fetch' %\\n            self._opts['region'])",
                    "func_fullName": "mrjob.emr._debug_script_uri( self )"
                },
                {
                    "func_id": 1377,
                    "func_name": "_command",
                    "func_desc": "_command",
                    "func_file": "cmd",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _command(name, description=None):\\n    \"\"\"Decorate a function used to call a command.\\n\\n    If you don't set *description*, it won't be included in help\\n    (useful for deprecated commands).\"\"\"\\n    def decorator(f):\\n        commands[name] = f\\n        if description:\\n            descriptions[name] = description\\n        return f\\n    return decorator",
                    "func_fullName": "mrjob.cmd._command( name, description )"
                },
                {
                    "func_id": 1388,
                    "func_name": "subcommand_line",
                    "func_desc": "subcommand_line",
                    "func_file": "cmd",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def subcommand_line(name):\\n        spaces = ' ' * (longest_name - len(name))\\n        return '  %s: %s%s' % (\\n            name, spaces, descriptions[name])",
                    "func_fullName": "mrjob.cmd.subcommand_line( name )"
                },
                {
                    "func_id": 1838,
                    "func_name": "_make_arg_parser",
                    "func_desc": "_make_arg_parser",
                    "func_file": "diagnose",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _make_arg_parser():\\n    usage = '%(prog)s diagnose [opts] [--step-id STEP_ID] CLUSTER_ID'\\n    description = (\\n        'Get probable cause of failure for step on CLUSTER_ID.'\\n        ' By default we look at the last failed step')\\n    arg_parser = ArgumentParser(usage=usage, description=description)\\n\\n    _add_basic_args(arg_parser)\\n    _add_runner_args(\\n        arg_parser,\\n        _filter_by_role(EMRJobRunner.OPT_NAMES, 'connect'))\\n\\n    arg_parser.add_argument(\\n        dest='cluster_id',\\n        help='ID of cluster with failed step')\\n    arg_parser.add_argument(\\n        '--step-id', dest='step_id',\\n        help='ID of a particular failed step to diagnose')\\n\\n    _alphabetize_actions(arg_parser)\\n\\n    return arg_parser",
                    "func_fullName": "mrjob.tools.diagnose._make_arg_parser(  )"
                },
                {
                    "func_id": 1853,
                    "func_name": "_make_arg_parser",
                    "func_desc": "_make_arg_parser",
                    "func_file": "audit_usage",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _make_arg_parser():\\n    usage = '%(prog)s audit-emr-usage [options]'\\n    description = 'Print a giant report on EMR usage.'\\n\\n    arg_parser = ArgumentParser(usage=usage, description=description)\\n\\n    arg_parser.add_argument(\\n        '--max-days-ago', dest='max_days_ago', type=float, default=None,\\n        help=('Max number of days ago to look at jobs. By default, we go back'\\n              ' as far as EMR supports (currently about 2 months)'))\\n\\n    _add_basic_args(arg_parser)\\n    _add_runner_args(\\n        arg_parser,\\n        _filter_by_role(EMRJobRunner.OPT_NAMES, 'connect'))\\n\\n    _alphabetize_actions(arg_parser)\\n\\n    return arg_parser",
                    "func_fullName": "mrjob.tools.emr.audit_usage._make_arg_parser(  )"
                },
                {
                    "func_id": 1869,
                    "func_name": "_make_arg_parser",
                    "func_desc": "_make_arg_parser",
                    "func_file": "terminate_cluster",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _make_arg_parser():\\n    usage = '%(prog)s terminate-cluster [options] CLUSTER_ID'\\n    description = 'Terminate an existing EMR cluster.'\\n\\n    arg_parser = ArgumentParser(usage=usage, description=description)\\n\\n    arg_parser.add_argument(\\n        '-t', '--test', dest='test', default=False,\\n        action='store_true',\\n        help=\"Don't actually delete any files; just log that we would\")\\n\\n    arg_parser.add_argument(\\n        dest='cluster_id',\\n        help='ID of cluster to terminate')\\n\\n    _add_basic_args(arg_parser)\\n    _add_runner_args(\\n        arg_parser,\\n        _filter_by_role(EMRJobRunner.OPT_NAMES, 'connect'))\\n\\n    _alphabetize_actions(arg_parser)\\n\\n    return arg_parser",
                    "func_fullName": "mrjob.tools.emr.terminate_cluster._make_arg_parser(  )"
                },
                {
                    "func_id": 1873,
                    "func_name": "_make_arg_parser",
                    "func_desc": "_make_arg_parser",
                    "func_file": "create_cluster",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _make_arg_parser():\\n    usage = '%(prog)s create-cluster [options]'\\n    description = (\\n        'Create a persistent EMR cluster to run jobs in, and print its ID to'\\n        ' stdout.')\\n    arg_parser = ArgumentParser(usage=usage, description=description)\\n\\n    _add_basic_args(arg_parser)\\n    _add_runner_args(\\n        arg_parser,\\n        _filter_by_role(EMRJobRunner.OPT_NAMES, 'connect', 'launch'))\\n\\n    _alphabetize_actions(arg_parser)\\n\\n    return arg_parser",
                    "func_fullName": "mrjob.tools.emr.create_cluster._make_arg_parser(  )"
                },
                {
                    "func_id": 1880,
                    "func_name": "_make_arg_parser",
                    "func_desc": "_make_arg_parser",
                    "func_file": "report_long_jobs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _make_arg_parser():\\n    usage = '%(prog)s report-long-jobs [options]'\\n    description = ('Report jobs running for more than a certain number of'\\n                   ' hours (by default, %.1f). This can help catch buggy jobs'\\n                   ' and Hadoop/EMR operational issues.' % DEFAULT_MIN_HOURS)\\n\\n    arg_parser = ArgumentParser(usage=usage, description=description)\\n\\n    arg_parser.add_argument(\\n        '--min-hours', dest='min_hours', type=float,\\n        default=DEFAULT_MIN_HOURS,\\n        help=('Minimum number of hours a job can run before we report it.'\\n              ' Default: %(default)s'))\\n\\n    arg_parser.add_argument(\\n        '-x', '--exclude', action='append',\\n        help=('Exclude clusters that match the specified tags.'\\n              ' Specifed in the form TAG_KEY,TAG_VALUE.')\\n    )\\n\\n    _add_basic_args(arg_parser)\\n    _add_runner_args(\\n        arg_parser,\\n        _filter_by_role(EMRJobRunner.OPT_NAMES, 'connect')\\n    )\\n\\n    _alphabetize_actions(arg_parser)\\n\\n    return arg_parser",
                    "func_fullName": "mrjob.tools.emr.report_long_jobs._make_arg_parser(  )"
                },
                {
                    "func_id": 1897,
                    "func_name": "_make_arg_parser",
                    "func_desc": "_make_arg_parser",
                    "func_file": "terminate_idle_clusters",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _make_arg_parser():\\n    usage = '%(prog)s terminate-idle-clusters [options]'\\n    description = ('Terminate idle EMR clusters that meet the criteria'\\n                   ' passed in on the command line (or, by default,'\\n                   ' clusters that have been idle for one hour).')\\n\\n    arg_parser = ArgumentParser(usage=usage, description=description)\\n\\n    arg_parser.add_argument(\\n        '--max-mins-idle', dest='max_mins_idle',\\n        default=None, type=float,\\n        help=('Max number of minutes a cluster can go without bootstrapping,'\\n              ' running a step, or having a new step created. This will fire'\\n              ' even if there are pending steps which EMR has failed to'\\n              ' start. Make sure you set this higher than the amount of time'\\n              ' your jobs can take to start instances and bootstrap.'))\\n    arg_parser.add_argument(\\n        '--max-mins-locked', dest='max_mins_locked',\\n        type=float,\\n        help='Deprecated, does nothing')\\n    arg_parser.add_argument(\\n        '--unpooled-only', dest='unpooled_only', action='store_true',\\n        default=False,\\n        help='Only terminate un-pooled clusters')\\n    arg_parser.add_argument(\\n        '--pooled-only', dest='pooled_only', action='store_true',\\n        default=False,\\n        help='Only terminate pooled clusters')\\n    arg_parser.add_argument(\\n        '--pool-name', dest='pool_name', default=None,\\n        help='Only terminate clusters in the given named pool.')\\n    arg_parser.add_argument(\\n        '--dry-run', dest='dry_run', default=False,\\n        action='store_true',\\n        help=\"Don't actually kill idle jobs; just log that we would\")\\n\\n    _add_basic_args(arg_parser)\\n    _add_runner_args(\\n        arg_parser,\\n        _filter_by_role(EMRJobRunner.OPT_NAMES, 'connect'))\\n\\n    _alphabetize_actions(arg_parser)\\n\\n    return arg_parser",
                    "func_fullName": "mrjob.tools.emr.terminate_idle_clusters._make_arg_parser(  )"
                }
            ]
        },
        {
            "cluster_id": 1,
            "feature_id": 46,
            "feature_desc": "gamma=0.0608; k=17; a=0.25; combined=0.388; stability(ARI)=0.911; sep=0.150",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1199,
                    "func_name": "_deduplicate_emr_configurations",
                    "func_desc": "_deduplicate_emr_configurations",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _deduplicate_emr_configurations(emr_configurations):\\n    \"\"\"Takes the value of the *emr_configurations* opt, and ensures that\\n    later configs overwrite earlier ones with the same Classification.\\n\\n    Additionally, any configs that contain empty or unset Properties\\n    and Configurations will be removed (this is a way of deleting\\n    existing config dicts without replacing them).\\n\\n    You can assume that all config dicts have run through\\n    _fix_configuration_opt()\\n    \"\"\"\\n    results = OrderedDict()\\n\\n    for c in emr_configurations:\\n        results[c['Classification']] = c\\n\\n    return [c for c in results.values() if\\n            c['Properties'] or c.get('Configurations')]",
                    "func_fullName": "mrjob.emr._deduplicate_emr_configurations( emr_configurations )"
                },
                {
                    "func_id": 1200,
                    "func_name": "_fix_configuration_opt",
                    "func_desc": "_fix_configuration_opt",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _fix_configuration_opt(c):\\n    \"\"\"Return copy of *c* with *Properties* is always set\\n    (defaults to {}) and with *Configurations* is not set if empty.\\n    Convert all values to strings.\\n\\n    Raise exception on more serious problems (extra fields, wrong data\\n    type, etc).\\n\\n    This allows us to match configurations against the API, *and* catches bad\\n    configurations before they result in cryptic API errors.\\n    \"\"\"\\n    if not isinstance(c, dict):\\n        raise TypeError('configurations must be dicts, not %r' % (c,))\\n\\n    c = dict(c)  # make a copy\\n\\n    # extra keys\\n    extra_keys = (\\n        set(c) - set(['Classification', 'Configurations', 'Properties']))\\n    if extra_keys:\\n        raise ValueError('configuration opt has extra keys: %s' % ', '.join(\\n            sorted(extra_keys)))\\n\\n    # Classification\\n    if 'Classification' not in c:\\n        raise ValueError('configuration opt has no Classification')\\n\\n    if not isinstance(c['Classification'], string_types):\\n        raise TypeError('Classification must be string')\\n\\n    # Properties\\n    c.setdefault('Properties', {})\\n    if not isinstance(c['Properties'], dict):\\n        raise TypeError('Properties must be a dict')\\n\\n    c['Properties'] = dict(\\n        (str(k), str(v)) for k, v in c['Properties'].items())\\n\\n    # sub-Configurations\\n    if 'Configurations' in c:\\n        if c['Configurations']:\\n            if not isinstance(c['Configurations'], list):\\n                raise TypeError('Configurations must be a list')\\n            # recursively fix subconfigurations\\n            c['Configurations'] = [\\n                _fix_configuration_opt(sc) for sc in c['Configurations']]\\n        else:\\n            # don't keep empty configurations around\\n            del c['Configurations']\\n\\n    return c",
                    "func_fullName": "mrjob.emr._fix_configuration_opt( c )"
                },
                {
                    "func_id": 1201,
                    "func_name": "_fix_subnet_opt",
                    "func_desc": "_fix_subnet_opt",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _fix_subnet_opt(subnet):\\n    \"\"\"Return either None, a string, or a list with at least two items.\"\"\"\\n    if subnet is None:\\n        return None\\n\\n    if isinstance(subnet, string_types):\\n        return subnet\\n\\n    subnet = list(subnet)\\n    if len(subnet) == 1:\\n        return subnet[0]\\n    else:\\n        return subnet",
                    "func_fullName": "mrjob.emr._fix_subnet_opt( subnet )"
                },
                {
                    "func_id": 1205,
                    "func_name": "_default_opts",
                    "func_desc": "_default_opts",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _default_opts(cls):\\n        return combine_dicts(\\n            super(EMRJobRunner, cls)._default_opts(),\\n            dict(\\n                bootstrap_python=None,\\n                check_cluster_every=30,\\n                cleanup_on_failure=['JOB'],\\n                cloud_fs_sync_secs=5.0,\\n                docker_client_config=None,\\n                docker_image=None,\\n                image_version=_DEFAULT_IMAGE_VERSION,\\n                max_concurrent_steps=1,\\n                min_available_mb=0,\\n                min_available_virtual_cores=0,\\n                num_core_instances=0,\\n                num_task_instances=0,\\n                pool_clusters=False,\\n                pool_name='default',\\n                pool_jitter_seconds=60,\\n                pool_wait_minutes=0,\\n                region=_DEFAULT_EMR_REGION,\\n            )\\n        )",
                    "func_fullName": "mrjob.emr._default_opts( cls )"
                },
                {
                    "func_id": 1206,
                    "func_name": "_combine_opts",
                    "func_desc": "_combine_opts",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _combine_opts(self, opt_list):\\n        \"\"\"Blank out overriden *instance_fleets* and *instance_groups*\\n\\n        Convert image_version of 4.x and later to release_label.\"\"\"\\n        # blank out any instance_fleets/groups before the last config\\n        # where they are set\\n        opt_list = _blank_out_conflicting_opts(\\n            opt_list,\\n            ['instance_fleets', 'instance_groups'],\\n            self._INSTANCE_OPT_NAMES)\\n\\n        # now combine opts, with instance_groups/fleets blanked out\\n        opts = super(EMRJobRunner, self)._combine_opts(opt_list)\\n\\n        # set release_label based on image_version\\n        if (version_gte(opts['image_version'], '4') and\\n                not opts['release_label']):\\n            opts['release_label'] = 'emr-' + opts['image_version']\\n\\n        # don't keep two confs with the same Classification (see #2097)\\n        opts['emr_configurations'] = _deduplicate_emr_configurations(\\n            opts['emr_configurations'])\\n\\n        return opts",
                    "func_fullName": "mrjob.emr._combine_opts( self, opt_list )"
                },
                {
                    "func_id": 1207,
                    "func_name": "_fix_opt",
                    "func_desc": "_fix_opt",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _fix_opt(self, opt_key, opt_value, source):\\n        \"\"\"Fix and check various EMR-specific options\"\"\"\\n        opt_value = super(EMRJobRunner, self)._fix_opt(\\n            opt_key, opt_value, source)\\n\\n        # *_instance_bid_price\\n        if opt_key.endswith('_instance_bid_price'):\\n            if not opt_value:  # don't allow blank bid price\\n                return None\\n\\n            try:\\n                if not float(opt_value):\\n                    return None\\n            except ValueError:  # maybe EMR allows non-floats?\\n                pass\\n\\n            return str(opt_value)  # should be str, not a number\\n\\n        # additional_emr_info\\n        elif opt_key == 'additional_emr_info' and not isinstance(\\n                opt_value, string_types):\\n            return json.dumps(opt_value)\\n\\n        # emr_configurations\\n        elif opt_key == 'emr_configurations':\\n            return [_fix_configuration_opt(c) for c in opt_value]\\n\\n        # region\\n        elif opt_key == 'region':\\n            # don't allow blank region\\n            return opt_value or _DEFAULT_EMR_REGION\\n\\n        # subnet should be None, a string, or a multi-item list\\n        elif opt_key == 'subnet':\\n            return _fix_subnet_opt(opt_value)\\n\\n        else:\\n            return opt_value",
                    "func_fullName": "mrjob.emr._fix_opt( self, opt_key, opt_value, source )"
                },
                {
                    "func_id": 1220,
                    "func_name": "_prepare_for_launch",
                    "func_desc": "_prepare_for_launch",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _prepare_for_launch(self):\\n        \"\"\"Set up files needed for the job.\"\"\"\\n        self._check_output_not_exists()\\n        self._create_setup_wrapper_scripts()\\n        self._add_bootstrap_files_for_upload()\\n        self._add_master_node_setup_files_for_upload()\\n        self._add_job_files_for_upload()\\n        self._upload_local_files()\\n        # make sure we can see the files we copied to S3\\n        self._wait_for_s3_eventual_consistency()",
                    "func_fullName": "mrjob.emr._prepare_for_launch( self )"
                },
                {
                    "func_id": 1337,
                    "func_name": "_emr_configurations",
                    "func_desc": "_emr_configurations",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _emr_configurations(self):\\n        # don't keep two configs with the same Classification (#2097)\\n        return _deduplicate_emr_configurations(\\n            self._docker_emr_configurations() +\\n            self._opts['emr_configurations']\\n        )",
                    "func_fullName": "mrjob.emr._emr_configurations( self )"
                }
            ]
        },
        {
            "cluster_id": 1,
            "feature_id": 47,
            "feature_desc": "gamma=0.0608; k=17; a=0.25; combined=0.388; stability(ARI)=0.911; sep=0.150",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1203,
                    "func_name": "_plural",
                    "func_desc": "_plural",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _plural(n):\\n    \"\"\"Utility for logging messages\"\"\"\\n    if n == 1:\\n        return ''\\n    else:\\n        return 's'",
                    "func_fullName": "mrjob.emr._plural( n )"
                },
                {
                    "func_id": 1204,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, **kwargs):\\n        \"\"\":py:class:`~mrjob.emr.EMRJobRunner` takes the same arguments as\\n        :py:class:`~mrjob.runner.MRJobRunner`, plus some additional options\\n        which can be defaulted in :ref:`mrjob.conf <mrjob.conf>`.\\n\\n        *aws_access_key_id* and *aws_secret_access_key* are required if you\\n        haven't set them up already for boto3 (e.g. by setting the environment\\n        variables :envvar:`AWS_ACCESS_KEY_ID` and\\n        :envvar:`AWS_SECRET_ACCESS_KEY`)\\n\\n        A lengthy list of additional options can be found in\\n        :doc:`guides/emr-opts.rst`.\\n        \"\"\"\\n        super(EMRJobRunner, self).__init__(**kwargs)\\n\\n        self._fix_s3_tmp_and_log_uri_opts()\\n\\n        # use job key to make a unique tmp dir\\n        self._cloud_tmp_dir = self._opts['cloud_tmp_dir'] + self._job_key + '/'\\n\\n        # pick/validate output dir\\n        if self._output_dir:\\n            self._output_dir = self._check_and_fix_s3_dir(self._output_dir)\\n        else:\\n            self._output_dir = self._cloud_tmp_dir + 'output/'\\n\\n        # check AMI version\\n        if self._opts['image_version'].startswith('1.'):\\n            log.warning('1.x AMIs will not work because they use'\\n                        ' Python 2.5. Use a later AMI version or mrjob v0.4.2')\\n        elif not version_gte(self._opts['image_version'], '2.4.3'):\\n            log.warning(\"AMIs prior to 2.4.3 probably will not work because\"\\n                        \" they don't support Python 2.7.\")\\n        elif not self._image_version_gte('5.7.0'):\\n            if self._opts['image_id']:\\n                log.warning('AMIs prior to 5.7.0 will probably not work'\\n                            ' with custom machine images')\\n\\n        if self._opts['pool_clusters'] and not map_version(\\n                self._opts['image_version'], _IMAGE_SUPPORTS_POOLING):\\n            log.warning(\\n                \"Cluster pooling is not fully supported on AMIs prior to\"\\n                \" 2.4.8/3.1.1 due to the limit on total number of steps\")\\n\\n        if self._opts['max_concurrent_steps'] < 1:\\n            raise ValueError('max_concurrent_steps must be at least 1')\\n\\n        # manage local files that we want to upload to S3. We'll add them\\n        # to this manager just before we need them.\\n        s3_files_dir = self._cloud_tmp_dir + 'files/'\\n        self._upload_mgr = UploadDirManager(s3_files_dir)\\n\\n        # master node setup script (handled later by\\n        # _add_master_node_setup_files_for_upload())\\n        self._master_node_setup_mgr = WorkingDirManager()\\n        self._master_node_setup_script_path = None\\n\\n        # where our own logs ended up (we'll find this out once we run the job)\\n        self._s3_log_dir_uri = None\\n\\n        # did we create the cluster we're running on?\\n        self._created_cluster = False\\n\\n        # did we acquire a lock on self._cluster_id? (used for pooling)\\n        self._locked_cluster = None\\n\\n        # IDs of steps we have submitted to the cluster\\n        self._step_ids = []\\n\\n        # we don't upload the ssh key to master until it's needed\\n        self._ssh_key_is_copied = False\\n\\n        # map from cluster ID to a dictionary containing cached info about\\n        # that cluster. Includes the following keys:\\n        #\\n        # - image_version\\n        # - hadoop_version\\n        # - master_public_dns\\n        # - master_private_ip\\n        #\\n        # (we may do this for multiple cluster IDs if we join a pooled cluster\\n        # that self-terminates)\\n        self._cluster_to_cache = defaultdict(dict)\\n\\n        # set of cluster IDs for which we logged the master node's public DNS\\n        self._logged_address_of_master = set()\\n\\n        # List of dicts (one for each step) potentially containing\\n        # the keys 'history', 'step', and 'task'. These will also always\\n        # contain 'step_id' (the s-XXXXXXXX step ID on EMR).\\n        #\\n        # This will be filled by _wait_for_steps_to_complete()\\n        #\\n        # This might work better as a dictionary.\\n        self._log_interpretations = []\\n\\n        # log interpretation for master node setup step (currently we don't\\n        # use this for anything; we just want to keep it out of\\n        # self._log_interpretations)\\n        self._mns_log_interpretation = None\\n\\n        # set of step numbers (0-indexed) where we waited 5 minutes for logs to\\n        # transfer to S3 (so we don't do it twice)\\n        self._waited_for_logs_on_s3 = set()\\n\\n        # info used to match clusters. catches _pool_hash_dict()\\n        self._pool_hash_dict_cached = None\\n\\n        # add_steps_in_batch and concurrent steps don't mix\\n        if (self._add_steps_in_batch() and\\n                self._opts['max_concurrent_steps'] > 1):\\n            log.warning('add_steps_in_batch will probably not work'\\n                        ' with max_concurrent_steps > 1')\\n\\n        # min_available_* options require SSH\\n        if ((self._opts['min_available_mb'] or\\n                self._opts['min_available_virtual_cores']) and\\n                not (self._opts['ec2_key_pair'] and\\n                     self._opts['ec2_key_pair_file'])):\\n            raise ValueError('you must set up SSH (ec2_key_pair and'\\n                             ' ec2_key_pair_file) to use the'\\n                             ' min_available_* options')",
                    "func_fullName": "mrjob.emr.__init__( self, **kwargs )"
                },
                {
                    "func_id": 1217,
                    "func_name": "fs",
                    "func_desc": "fs",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def fs(self):\\n        \"\"\":py:class:`~mrjob.fs.base.Filesystem` object for SSH, S3, and the\\n        local filesystem.\\n        \"\"\"\\n        if self._fs is None:\\n            self._fs = CompositeFilesystem()\\n\\n            if self._opts['ec2_key_pair_file']:\\n                self._fs.add_fs('ssh', SSHFilesystem(\\n                    ssh_bin=self._ssh_bin(),\\n                    ssh_add_bin=self._ssh_add_bin(),\\n                    ec2_key_pair_file=self._opts['ec2_key_pair_file']))\\n\\n            self._fs.add_fs('s3', S3Filesystem(\\n                aws_access_key_id=self._opts['aws_access_key_id'],\\n                aws_secret_access_key=self._opts['aws_secret_access_key'],\\n                aws_session_token=self._opts['aws_session_token'],\\n                s3_endpoint=self._opts['s3_endpoint'],\\n                s3_region=self._opts['region'],\\n                part_size=self._upload_part_size()))\\n\\n            if self._opts['ec2_key_pair_file']:\\n                # add hadoop fs after S3 because it tries to handle all URIs\\n\\n                # we'll set hadoop_bin later, once the cluster is set up\\n                self._fs.add_fs('hadoop', HadoopFilesystem(hadoop_bin=[]))\\n\\n            self._fs.add_fs('local', LocalFilesystem())\\n\\n        return self._fs",
                    "func_fullName": "mrjob.emr.fs( self )"
                },
                {
                    "func_id": 1218,
                    "func_name": "_run",
                    "func_desc": "_run",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _run(self):\\n        self._launch()\\n        self._finish_run()",
                    "func_fullName": "mrjob.emr._run( self )"
                },
                {
                    "func_id": 1219,
                    "func_name": "_finish_run",
                    "func_desc": "_finish_run",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _finish_run(self):\\n        while True:\\n            try:\\n                self._wait_for_steps_to_complete()\\n                break\\n            except _PooledClusterSelfTerminatedException:\\n                self._relaunch()",
                    "func_fullName": "mrjob.emr._finish_run( self )"
                },
                {
                    "func_id": 1221,
                    "func_name": "_launch",
                    "func_desc": "_launch",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _launch(self):\\n        \"\"\"Set up files and then launch our job on EMR.\"\"\"\\n        self._prepare_for_launch()\\n        self._launch_emr_job()",
                    "func_fullName": "mrjob.emr._launch( self )"
                },
                {
                    "func_id": 1222,
                    "func_name": "_relaunch",
                    "func_desc": "_relaunch",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _relaunch(self):\\n        # files are already in place; just start with a fresh cluster\\n        assert not self._opts['cluster_id']\\n        self._cluster_id = None\\n        self._created_cluster = False\\n        self._step_ids = []\\n\\n        # old SSH tunnel isn't valid for this cluster (see #1549)\\n        if self._ssh_proc:\\n            self._kill_ssh_tunnel()\\n\\n        # don't try to connect to HDFS on the old cluster\\n        if hasattr(self.fs, 'hadoop'):\\n            self.fs.hadoop.set_hadoop_bin([])\\n\\n        self._launch_emr_job()",
                    "func_fullName": "mrjob.emr._relaunch( self )"
                },
                {
                    "func_id": 1249,
                    "func_name": "_add_tags",
                    "func_desc": "_add_tags",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _add_tags(self, tags, cluster_id):\\n        \"\"\"Add tags in the dict *tags* to cluster *cluster_id*. Do nothing\\n        if *tags* is empty or ``None``\"\"\"\\n        if not tags:\\n            return\\n\\n        tags_items = sorted(tags.items())\\n\\n        self.make_emr_client().add_tags(\\n            ResourceId=cluster_id,\\n            Tags=[dict(Key=k, Value=v) for k, v in tags_items])\\n\\n        log.info('Added EMR tags to cluster %s: %s' % (\\n            cluster_id,\\n            ', '.join('%s=%s' % (tag, value) for tag, value in tags_items)))",
                    "func_fullName": "mrjob.emr._add_tags( self, tags, cluster_id )"
                },
                {
                    "func_id": 1298,
                    "func_name": "_applications",
                    "func_desc": "_applications",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _applications(self, add_spark=True):\\n        \"\"\"Returns applications (*applications* option) as a set. Adds\\n        in ``Hadoop`` and ``Spark`` as needed.\"\"\"\\n        applications = set(self._opts['applications'])\\n\\n        # release_label implies 4.x AMI and later\\n        if (add_spark and self._should_bootstrap_spark() and\\n                self._opts['release_label']):\\n            # EMR allows us to have both \"spark\" and \"Spark\" applications,\\n            # which is probably not what we want\\n            if not self._has_spark_application():\\n                applications.add('Spark')\\n\\n        # patch in \"Hadoop\" unless applications is empty (e.g. 3.x AMIs)\\n        if applications:\\n            # don't add both \"Hadoop\" and \"hadoop\"\\n            if not any(a.lower() == 'hadoop' for a in applications):\\n                applications.add('Hadoop')\\n\\n        return applications",
                    "func_fullName": "mrjob.emr._applications( self, add_spark )"
                },
                {
                    "func_id": 1336,
                    "func_name": "_cmdenv",
                    "func_desc": "_cmdenv",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cmdenv(self):\\n        env = super(EMRJobRunner, self)._cmdenv()\\n\\n        return combine_dicts(self._docker_cmdenv(), env)",
                    "func_fullName": "mrjob.emr._cmdenv( self )"
                },
                {
                    "func_id": 1342,
                    "func_name": "_yrm_get",
                    "func_desc": "_yrm_get",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _yrm_get(self, path, host=None, port=None, timeout=None):\\n        \"\"\"Use curl to perform an HTTP GET on the given path on the\\n        YARN Resource Manager. Either return decoded JSON from the call,\\n        or raise an IOError\\n\\n        *path* should not start with a '/'\\n\\n        More info on the YARN REST API can be found here:\\n\\n        https://hadoop.apache.org/docs/current/hadoop-yarn/\\n            hadoop-yarn-site/ResourceManagerRest.html\\n        \"\"\"\\n        if host is None:\\n            host = self._address_of_master()\\n\\n        if port is None:\\n            port = _YARN_RESOURCE_MANAGER_PORT\\n\\n        if timeout is None:\\n            timeout = _YARN_API_TIMEOUT\\n\\n        # using urljoin() to avoid a double / when joining host/port with path\\n        yrm_url = urljoin(\\n            'http://{}:{:d}'.format(host, port),\\n            '{}/{}'.format(_YRM_BASE_PATH, path)\\n        )\\n\\n        curl_args = [\\n            'curl',  # always available on EMR\\n            '-fsS',  # fail on HTTP errors, print errors only to stderr\\n            '-m', str(timeout),  # timeout after 20 seconds\\n            yrm_url,\\n        ]\\n\\n        stdout, stderr = self.fs.ssh._ssh_run(host, curl_args)\\n\\n        return json.loads(to_unicode(stdout))",
                    "func_fullName": "mrjob.emr._yrm_get( self, path, host, port, timeout )"
                },
                {
                    "func_id": 1376,
                    "func_name": "_error",
                    "func_desc": "_error",
                    "func_file": "cmd",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _error(msg=None):\\n    if msg:\\n        print(msg, file=stderr)\\n\\n    longest_name = max(len(name) for name in descriptions)\\n\\n    def subcommand_line(name):\\n        spaces = ' ' * (longest_name - len(name))\\n        return '  %s: %s%s' % (\\n            name, spaces, descriptions[name])\\n    print(usage, file=stderr)\\n    print('\\n'.join(\\n        subcommand_line(name) for name in sorted(descriptions)), file=stderr)",
                    "func_fullName": "mrjob.cmd._error( msg )"
                },
                {
                    "func_id": 1378,
                    "func_name": "main",
                    "func_desc": "main",
                    "func_file": "cmd",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def main(args=None):\\n    args = args or argv\\n    if not args[1:] or args[1] in ('-h', '--help'):\\n        _error()\\n    elif args[1] not in commands:\\n        _error('\"%s\" is not a command' % args[1])\\n    else:\\n        commands[args[1]](args[2:])",
                    "func_fullName": "mrjob.cmd.main( args )"
                },
                {
                    "func_id": 1381,
                    "func_name": "_diagnose",
                    "func_desc": "_diagnose",
                    "func_file": "cmd",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _diagnose(args):\\n    from mrjob.tools.diagnose import main\\n    main(args)",
                    "func_fullName": "mrjob.cmd._diagnose( args )"
                },
                {
                    "func_id": 1382,
                    "func_name": "_mrboss",
                    "func_desc": "_mrboss",
                    "func_file": "cmd",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _mrboss(args):\\n    from mrjob.tools.emr.mrboss import main\\n    main(args)",
                    "func_fullName": "mrjob.cmd._mrboss( args )"
                },
                {
                    "func_id": 1384,
                    "func_name": "_s3_tmpwatch",
                    "func_desc": "_s3_tmpwatch",
                    "func_file": "cmd",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _s3_tmpwatch(args):\\n    from mrjob.tools.emr.s3_tmpwatch import main\\n    main(args)",
                    "func_fullName": "mrjob.cmd._s3_tmpwatch( args )"
                },
                {
                    "func_id": 1389,
                    "func_name": "decorator",
                    "func_desc": "decorator",
                    "func_file": "cmd",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def decorator(f):\\n        commands[name] = f\\n        if description:\\n            descriptions[name] = description\\n        return f",
                    "func_fullName": "mrjob.cmd.decorator( f )"
                },
                {
                    "func_id": 1854,
                    "func_name": "_runner_kwargs",
                    "func_desc": "_runner_kwargs",
                    "func_file": "audit_usage",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _runner_kwargs(options):\\n    kwargs = options.__dict__.copy()\\n    for unused_arg in ('quiet', 'verbose', 'max_days_ago'):\\n        del kwargs[unused_arg]\\n\\n    return kwargs",
                    "func_fullName": "mrjob.tools.emr.audit_usage._runner_kwargs( options )"
                },
                {
                    "func_id": 1868,
                    "func_name": "main",
                    "func_desc": "main",
                    "func_file": "terminate_cluster",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def main(cl_args=None):\\n    # parser command-line args\\n    arg_parser = _make_arg_parser()\\n    options = arg_parser.parse_args(cl_args)\\n\\n    MRJob.set_up_logging(quiet=options.quiet, verbose=options.verbose)\\n\\n    # create the persistent job\\n    runner = EMRJobRunner(**_runner_kwargs(options))\\n    log.debug('Terminating cluster %s' % options.cluster_id)\\n    runner.make_emr_client().terminate_job_flows(\\n        JobFlowIds=[options.cluster_id])\\n    log.info('Terminated cluster %s' % options.cluster_id)",
                    "func_fullName": "mrjob.tools.emr.terminate_cluster.main( cl_args )"
                },
                {
                    "func_id": 1870,
                    "func_name": "_runner_kwargs",
                    "func_desc": "_runner_kwargs",
                    "func_file": "terminate_cluster",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _runner_kwargs(options):\\n    kwargs = options.__dict__.copy()\\n    for unused_arg in ('cluster_id', 'quiet', 'verbose', 'test'):\\n        del kwargs[unused_arg]\\n\\n    return kwargs",
                    "func_fullName": "mrjob.tools.emr.terminate_cluster._runner_kwargs( options )"
                },
                {
                    "func_id": 1871,
                    "func_name": "main",
                    "func_desc": "main",
                    "func_file": "create_cluster",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def main(args=None):\\n    \"\"\"Run the create_cluster tool with arguments from ``sys.argv`` and\\n    printing to ``sys.stdout``.\"\"\"\\n    runner = EMRJobRunner(**_runner_kwargs(args))\\n    cluster_id = runner.make_persistent_cluster()\\n    print(cluster_id)",
                    "func_fullName": "mrjob.tools.emr.create_cluster.main( args )"
                },
                {
                    "func_id": 1872,
                    "func_name": "_runner_kwargs",
                    "func_desc": "_runner_kwargs",
                    "func_file": "create_cluster",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _runner_kwargs(cl_args=None):\\n    \"\"\"Parse command line arguments into arguments for\\n    :py:class:`EMRJobRunner`\\n    \"\"\"\\n    # parser command-line args\\n    arg_parser = _make_arg_parser()\\n    options = arg_parser.parse_args(cl_args)\\n\\n    MRJob.set_up_logging(quiet=options.quiet, verbose=options.verbose)\\n\\n    # create the persistent job\\n    kwargs = options.__dict__.copy()\\n\\n    del kwargs['quiet']\\n    del kwargs['verbose']\\n\\n    return kwargs",
                    "func_fullName": "mrjob.tools.emr.create_cluster._runner_kwargs( cl_args )"
                },
                {
                    "func_id": 1875,
                    "func_name": "_runner_kwargs",
                    "func_desc": "_runner_kwargs",
                    "func_file": "report_long_jobs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _runner_kwargs(options):\\n    \"\"\"Given the command line options, return the arguments to\\n    :py:class:`EMRJobRunner`\\n    \"\"\"\\n    kwargs = options.__dict__.copy()\\n    for unused_arg in ('quiet', 'verbose', 'min_hours', 'exclude'):\\n        del kwargs[unused_arg]\\n\\n    return kwargs",
                    "func_fullName": "mrjob.tools.emr.report_long_jobs._runner_kwargs( options )"
                },
                {
                    "func_id": 1886,
                    "func_name": "main",
                    "func_desc": "main",
                    "func_file": "terminate_idle_clusters",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def main(cl_args=None):\\n    arg_parser = _make_arg_parser()\\n    options = arg_parser.parse_args(cl_args)\\n\\n    MRJob.set_up_logging(quiet=options.quiet,\\n                         verbose=options.verbose)\\n\\n    max_mins_idle = options.max_mins_idle\\n\\n    _maybe_terminate_clusters(\\n        dry_run=options.dry_run,\\n        max_mins_idle=max_mins_idle,\\n        unpooled_only=options.unpooled_only,\\n        now=_boto3_now(),\\n        pool_name=options.pool_name,\\n        pooled_only=options.pooled_only,\\n        quiet=options.quiet,\\n        **_runner_kwargs(options)\\n    )",
                    "func_fullName": "mrjob.tools.emr.terminate_idle_clusters.main( cl_args )"
                },
                {
                    "func_id": 1887,
                    "func_name": "_runner_kwargs",
                    "func_desc": "_runner_kwargs",
                    "func_file": "terminate_idle_clusters",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _runner_kwargs(options):\\n    kwargs = options.__dict__.copy()\\n    for unused_arg in ('quiet', 'verbose', 'max_mins_idle',\\n                       'unpooled_only', 'pooled_only', 'pool_name', 'dry_run'):\\n        del kwargs[unused_arg]\\n\\n    return kwargs",
                    "func_fullName": "mrjob.tools.emr.terminate_idle_clusters._runner_kwargs( options )"
                }
            ]
        },
        {
            "cluster_id": 1,
            "feature_id": 48,
            "feature_desc": "gamma=0.0608; k=17; a=0.25; combined=0.388; stability(ARI)=0.911; sep=0.150",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1209,
                    "func_name": "_image_version_gte",
                    "func_desc": "_image_version_gte",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _image_version_gte(self, version):\\n        \"\"\"Check if the requested image version is greater than\\n        or equal to *version*. If the *release_label* opt is set,\\n        look at that instead.\\n\\n        If you're checking the actual image version of a cluster, just\\n        use :py:func:`~mrjob.compat.version_gte` and\\n        :py:meth:`get_image_version`.\\n        \"\"\"\\n        if self._opts['release_label']:\\n            return version_gte(\\n                self._opts['release_label'].lstrip('emr-'), version)\\n        else:\\n            return version_gte(self._opts['image_version'], version)",
                    "func_fullName": "mrjob.emr._image_version_gte( self, version )"
                },
                {
                    "func_id": 1301,
                    "func_name": "_manifest_download_commands",
                    "func_desc": "_manifest_download_commands",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _manifest_download_commands(self):\\n        return [\\n            ('s3://*', 'aws s3 cp'),\\n            ('*://*', 'hadoop fs -copyToLocal'),\\n        ]",
                    "func_fullName": "mrjob.emr._manifest_download_commands( self )"
                },
                {
                    "func_id": 1321,
                    "func_name": "get_image_version",
                    "func_desc": "get_image_version",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_image_version(self):\\n        \"\"\"Get the version of the AMI that our cluster is running, or ``None``.\\n        \"\"\"\\n        return self._get_cluster_info('image_version')",
                    "func_fullName": "mrjob.emr.get_image_version( self )"
                },
                {
                    "func_id": 1338,
                    "func_name": "_docker_image",
                    "func_desc": "_docker_image",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _docker_image(self):\\n        \"\"\"Special-case the \"library\" registry which is implied on Docker Hub\\n        but needs to be specified explicitly on EMR.\"\"\"\\n        image = self._opts['docker_image']\\n\\n        if not image:\\n            return None\\n        elif '/' in image:\\n            return image\\n        else:\\n            return 'library/' + image",
                    "func_fullName": "mrjob.emr._docker_image( self )"
                },
                {
                    "func_id": 1339,
                    "func_name": "_docker_registry",
                    "func_desc": "_docker_registry",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _docker_registry(self):\\n        \"\"\"Infer the trusted docker registry from the docker image.\"\"\"\\n        image = self._docker_image()\\n\\n        if not image:\\n            return None\\n        else:\\n            return image.split('/')[0]",
                    "func_fullName": "mrjob.emr._docker_registry( self )"
                },
                {
                    "func_id": 1340,
                    "func_name": "_docker_cmdenv",
                    "func_desc": "_docker_cmdenv",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _docker_cmdenv(self):\\n        image = self._docker_image()\\n\\n        if not image:\\n            return {}\\n\\n        env = dict(\\n            YARN_CONTAINER_RUNTIME_TYPE='docker',\\n            YARN_CONTAINER_RUNTIME_DOCKER_IMAGE=image,\\n        )\\n\\n        if self._opts['docker_client_config']:\\n            env['YARN_CONTAINER_RUNTIME_DOCKER_CLIENT_CONFIG'] = (\\n                self._opts['docker_client_config'])\\n\\n        if self._opts['docker_mounts']:\\n            env['YARN_CONTAINER_RUNTIME_DOCKER_MOUNTS'] = ','.join(\\n                self._opts['docker_mounts'])\\n\\n        return env",
                    "func_fullName": "mrjob.emr._docker_cmdenv( self )"
                },
                {
                    "func_id": 1341,
                    "func_name": "_docker_emr_configurations",
                    "func_desc": "_docker_emr_configurations",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _docker_emr_configurations(self):\\n        registry = self._docker_registry()\\n\\n        if not registry:\\n            return []\\n\\n        registries = ','.join(['local', registry])\\n\\n        return [\\n            dict(\\n                Classification='container-executor',\\n                Configurations=[\\n                    dict(\\n                        Classification='docker',\\n                        Properties={\\n                            'docker.trusted.registries': registries,\\n                            'docker.privileged-containers.registries': (\\n                                registries),\\n                        },\\n                    ),\\n                ],\\n                Properties={},\\n            ),\\n        ]",
                    "func_fullName": "mrjob.emr._docker_emr_configurations( self )"
                }
            ]
        },
        {
            "cluster_id": 1,
            "feature_id": 49,
            "feature_desc": "gamma=0.0608; k=17; a=0.25; combined=0.388; stability(ARI)=0.911; sep=0.150",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1210,
                    "func_name": "_fix_s3_tmp_and_log_uri_opts",
                    "func_desc": "_fix_s3_tmp_and_log_uri_opts",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _fix_s3_tmp_and_log_uri_opts(self):\\n        \"\"\"Fill in cloud_tmp_dir and cloud_log_dir (in self._opts) if they\\n        aren't already set.\\n\\n        Helper for __init__.\\n        \"\"\"\\n        # set cloud_tmp_dir by checking for existing buckets\\n        if not self._opts['cloud_tmp_dir']:\\n            self._set_cloud_tmp_dir()\\n            log.info('Using %s as our temp dir on S3' %\\n                     self._opts['cloud_tmp_dir'])\\n\\n        self._opts['cloud_tmp_dir'] = self._check_and_fix_s3_dir(\\n            self._opts['cloud_tmp_dir'])\\n\\n        # set cloud_log_dir\\n        if self._opts['cloud_log_dir']:\\n            self._opts['cloud_log_dir'] = self._check_and_fix_s3_dir(\\n                self._opts['cloud_log_dir'])\\n        else:\\n            self._opts['cloud_log_dir'] = self._opts['cloud_tmp_dir'] + 'logs/'",
                    "func_fullName": "mrjob.emr._fix_s3_tmp_and_log_uri_opts( self )"
                },
                {
                    "func_id": 1211,
                    "func_name": "_set_cloud_tmp_dir",
                    "func_desc": "_set_cloud_tmp_dir",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _set_cloud_tmp_dir(self):\\n        \"\"\"Helper for _fix_s3_tmp_and_log_uri_opts\"\"\"\\n        client = self.fs.s3.make_s3_client()\\n\\n        for bucket_name in self.fs.s3.get_all_bucket_names():\\n            if not bucket_name.startswith('mrjob-'):\\n                continue\\n\\n            bucket_region = _get_bucket_region(client, bucket_name)\\n            if bucket_region == self._opts['region']:\\n                # Regions are both specified and match\\n                log.debug(\"using existing temp bucket %s\" % bucket_name)\\n                self._opts['cloud_tmp_dir'] = 's3://%s/tmp/' % bucket_name\\n                return\\n\\n        # That may have all failed. If so, pick a name.\\n        bucket_name = 'mrjob-' + random_identifier()\\n        self._opts['cloud_tmp_dir'] = 's3://%s/tmp/' % bucket_name\\n        log.info('Auto-created temp S3 bucket %s' % bucket_name)\\n        self._wait_for_s3_eventual_consistency()",
                    "func_fullName": "mrjob.emr._set_cloud_tmp_dir( self )"
                },
                {
                    "func_id": 1212,
                    "func_name": "_s3_log_dir",
                    "func_desc": "_s3_log_dir",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _s3_log_dir(self):\\n        \"\"\"Get the URI of the log directory for this job's cluster.\"\"\"\\n        if not self._s3_log_dir_uri:\\n            cluster = self._describe_cluster()\\n            log_uri = cluster.get('LogUri')\\n            if log_uri:\\n                self._s3_log_dir_uri = '%s%s/' % (\\n                    log_uri.replace('s3n://', 's3://'), self._cluster_id)\\n\\n        return self._s3_log_dir_uri",
                    "func_fullName": "mrjob.emr._s3_log_dir( self )"
                },
                {
                    "func_id": 1213,
                    "func_name": "_check_and_fix_s3_dir",
                    "func_desc": "_check_and_fix_s3_dir",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _check_and_fix_s3_dir(self, s3_uri):\\n        \"\"\"Helper for __init__\"\"\"\\n        if not is_s3_uri(s3_uri):\\n            raise ValueError('Invalid S3 URI: %r' % s3_uri)\\n        if not s3_uri.endswith('/'):\\n            s3_uri = s3_uri + '/'\\n\\n        return s3_uri",
                    "func_fullName": "mrjob.emr._check_and_fix_s3_dir( self, s3_uri )"
                },
                {
                    "func_id": 1237,
                    "func_name": "_cleanup_cloud_tmp",
                    "func_desc": "_cleanup_cloud_tmp",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cleanup_cloud_tmp(self):\\n        # delete all the files we created on S3\\n        if self._cloud_tmp_dir:\\n            try:\\n                log.info('Removing s3 temp directory %s...' %\\n                         self._cloud_tmp_dir)\\n                self.fs.rm(self._cloud_tmp_dir)\\n                self._cloud_tmp_dir = None\\n            except Exception as e:\\n                log.exception(e)",
                    "func_fullName": "mrjob.emr._cleanup_cloud_tmp( self )"
                },
                {
                    "func_id": 1238,
                    "func_name": "_cleanup_logs",
                    "func_desc": "_cleanup_logs",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cleanup_logs(self):\\n        super(EMRJobRunner, self)._cleanup_logs()\\n\\n        # delete the log files, if it's a cluster we created (the logs\\n        # belong to the cluster)\\n        if self._s3_log_dir() and not self._opts['cluster_id'] \\\\n                and not self._opts['pool_clusters']:\\n            try:\\n                log.info('Removing log files in %s...' % self._s3_log_dir())\\n                self.fs.rm(self._s3_log_dir())\\n            except Exception as e:\\n                log.exception(e)",
                    "func_fullName": "mrjob.emr._cleanup_logs( self )"
                },
                {
                    "func_id": 1282,
                    "func_name": "_default_step_output_dir",
                    "func_desc": "_default_step_output_dir",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _default_step_output_dir(self):\\n        # put intermediate data in HDFS\\n        return 'hdfs:///tmp/mrjob/%s/step-output' % self._job_key",
                    "func_fullName": "mrjob.emr._default_step_output_dir( self )"
                },
                {
                    "func_id": 1284,
                    "func_name": "_ls_bootstrap_stderr_logs",
                    "func_desc": "_ls_bootstrap_stderr_logs",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ls_bootstrap_stderr_logs(self, action_num=None, node_id=None):\\n        \"\"\"_ls_bootstrap_stderr_logs(), with logging for each log we parse.\"\"\"\\n        if not self._read_logs():\\n            return\\n\\n        for match in _ls_emr_bootstrap_stderr_logs(\\n                self.fs,\\n                self._stream_bootstrap_log_dirs(\\n                    action_num=action_num, node_id=node_id),\\n                action_num=action_num,\\n                node_id=node_id):\\n            log.info('  Parsing boostrap stderr log: %s' % match['path'])\\n            yield match",
                    "func_fullName": "mrjob.emr._ls_bootstrap_stderr_logs( self, action_num, node_id )"
                },
                {
                    "func_id": 1285,
                    "func_name": "_stream_bootstrap_log_dirs",
                    "func_desc": "_stream_bootstrap_log_dirs",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _stream_bootstrap_log_dirs(self, action_num=None, node_id=None):\\n        \"\"\"Stream a single directory on S3 containing the relevant bootstrap\\n        stderr. Optionally, use *action_num* and *node_id* to narrow it down\\n        further.\\n        \"\"\"\\n        if action_num is None or node_id is None:\\n            s3_dir_name = 'node'\\n        else:\\n            s3_dir_name = posixpath.join(\\n                'node', node_id, 'bootstrap-actions', str(action_num + 1))\\n\\n        # dir_name=None means don't try to SSH in.\\n        #\\n        # TODO: If the failure is on the master node, we could just look in\\n        # /mnt/var/log/bootstrap-actions. However, if it's on a worker node,\\n        # we'd have to look up its internal IP using the ListInstances\\n        # API call. This *would* be a bit faster though. See #1346.\\n        return self._stream_log_dirs(\\n            'bootstrap logs',\\n            dir_name=None,  # don't SSH in\\n            s3_dir_name=s3_dir_name)",
                    "func_fullName": "mrjob.emr._stream_bootstrap_log_dirs( self, action_num, node_id )"
                },
                {
                    "func_id": 1286,
                    "func_name": "_stream_history_log_dirs",
                    "func_desc": "_stream_history_log_dirs",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _stream_history_log_dirs(self, output_dir=None):\\n        \"\"\"Yield lists of directories to look for the history log in.\"\"\"\\n\\n        if version_gte(self.get_image_version(), '4'):\\n            hdfs_dir_name = 'history'\\n            # on 4.0.0 (and possibly other versions before 4.3.0)\\n            # history logs aren't on the filesystem. See #1253\\n            dir_name = 'hadoop-mapreduce/history'\\n            s3_dir_name = 'hadoop-mapreduce/history'\\n        elif version_gte(self.get_image_version(), '3'):\\n            # on the 3.x AMIs, the history log is on HDFS only\\n            # (not even S3)\\n            hdfs_dir_name = 'history'\\n            dir_name = None\\n            s3_dir_name = None\\n        else:\\n            # 2.x AMIs don't use YARN, so no point in checking HDFS\\n            hdfs_dir_name = None\\n            dir_name = 'hadoop/history'\\n            s3_dir_name = 'jobs'\\n\\n        return self._stream_log_dirs(\\n            'history log',\\n            hdfs_dir_name=hdfs_dir_name,\\n            dir_name=dir_name,\\n            s3_dir_name=s3_dir_name)",
                    "func_fullName": "mrjob.emr._stream_history_log_dirs( self, output_dir )"
                },
                {
                    "func_id": 1287,
                    "func_name": "_stream_task_log_dirs",
                    "func_desc": "_stream_task_log_dirs",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _stream_task_log_dirs(self, application_id=None, output_dir=None):\\n        \"\"\"Get lists of directories to look for the task logs in.\"\"\"\\n        if version_gte(self.get_image_version(), '4'):\\n            # denied access on some 4.x AMIs by the yarn user, see #1244\\n            dir_name = 'hadoop-yarn/containers'\\n            s3_dir_name = 'containers'\\n        else:\\n            dir_name = 'hadoop/userlogs'\\n            s3_dir_name = 'task-attempts'\\n\\n        if application_id:\\n            dir_name = posixpath.join(dir_name, application_id)\\n            s3_dir_name = posixpath.join(s3_dir_name, application_id)\\n\\n        return self._stream_log_dirs(\\n            'task logs',\\n            dir_name=dir_name,\\n            s3_dir_name=s3_dir_name,\\n            ssh_to_workers=True)  # TODO: does this make sense on YARN?",
                    "func_fullName": "mrjob.emr._stream_task_log_dirs( self, application_id, output_dir )"
                },
                {
                    "func_id": 1288,
                    "func_name": "_get_step_log_interpretation",
                    "func_desc": "_get_step_log_interpretation",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_step_log_interpretation(self, log_interpretation, step_type):\\n        \"\"\"Fetch and interpret the step log.\"\"\"\\n        step_id = log_interpretation.get('step_id')\\n\\n        if not self._read_logs():\\n            return\\n\\n        if not step_id:\\n            log.warning(\"Can't fetch step log; missing step ID\")\\n            return\\n\\n        if self._step_type_uses_spark(step_type):\\n            # Spark also has a \"controller\" log4j log, but it doesn't\\n            # contain errors or anything else we need\\n            #\\n            # the step log is unlikely to be very much help because\\n            # Spark on EMR runs in cluster mode. See #2056\\n            #\\n            # there's generally only one log (unless the job has been running\\n            # long enough for log rotation), so use partial=False\\n            return _interpret_spark_logs(\\n                self.fs, self._ls_step_stderr_logs(step_id=step_id),\\n                partial=False)\\n        else:\\n            return (\\n                _interpret_emr_step_syslog(\\n                    self.fs, self._ls_step_syslogs(step_id=step_id)) or\\n                _interpret_emr_step_stderr(\\n                    self.fs, self._ls_step_stderr_logs(step_id=step_id))\\n            )",
                    "func_fullName": "mrjob.emr._get_step_log_interpretation( self, log_interpretation, step_type )"
                },
                {
                    "func_id": 1289,
                    "func_name": "_ls_step_syslogs",
                    "func_desc": "_ls_step_syslogs",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ls_step_syslogs(self, step_id):\\n        \"\"\"Yield step log matches, logging a message for each one.\"\"\"\\n        for match in _ls_emr_step_syslogs(\\n                self.fs, self._stream_step_log_dirs(step_id=step_id),\\n                step_id=step_id):\\n            log.info('  Parsing step log: %s' % match['path'])\\n            yield match",
                    "func_fullName": "mrjob.emr._ls_step_syslogs( self, step_id )"
                },
                {
                    "func_id": 1290,
                    "func_name": "_ls_step_stderr_logs",
                    "func_desc": "_ls_step_stderr_logs",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ls_step_stderr_logs(self, step_id):\\n        \"\"\"Yield step log matches, logging a message for each one.\"\"\"\\n        for match in _ls_emr_step_stderr_logs(\\n                self.fs, self._stream_step_log_dirs(step_id=step_id),\\n                step_id=step_id):\\n            log.info('  Parsing step log: %s' % match['path'])\\n            yield match",
                    "func_fullName": "mrjob.emr._ls_step_stderr_logs( self, step_id )"
                },
                {
                    "func_id": 1291,
                    "func_name": "_stream_step_log_dirs",
                    "func_desc": "_stream_step_log_dirs",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _stream_step_log_dirs(self, step_id):\\n        \"\"\"Get lists of directories to look for the step log in.\"\"\"\\n        return self._stream_log_dirs(\\n            'step log',\\n            dir_name=posixpath.join('hadoop', 'steps', step_id),\\n            s3_dir_name=posixpath.join('steps', step_id))",
                    "func_fullName": "mrjob.emr._stream_step_log_dirs( self, step_id )"
                },
                {
                    "func_id": 1292,
                    "func_name": "_stream_log_dirs",
                    "func_desc": "_stream_log_dirs",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _stream_log_dirs(self, log_desc, dir_name, s3_dir_name,\\n                         hdfs_dir_name=None,\\n                         ssh_to_workers=False):\\n        \"\"\"Stream log dirs for any kind of log.\\n\\n        Our general strategy is first, if SSH is enabled, to SSH into the\\n        master node (and possibly workers, if *ssh_to_workers* is set).\\n\\n        If this doesn't work, we have to look on S3. If the cluster is\\n        TERMINATING, we first wait for it to terminate (since that\\n        will trigger copying logs over).\\n        \"\"\"\\n        if not self._read_logs():\\n            return\\n\\n        # used to fetch history logs off HDFS\\n        if (hdfs_dir_name and\\n                self.fs.can_handle_path(_DEFAULT_YARN_HDFS_LOG_DIR)):\\n\\n            hdfs_log_dir = posixpath.join(\\n                _DEFAULT_YARN_HDFS_LOG_DIR, hdfs_dir_name)\\n\\n            log.info('Looking for %s in %s...' % (log_desc, hdfs_log_dir))\\n            yield [hdfs_log_dir]\\n\\n        if dir_name and self.fs.can_handle_path('ssh:///'):\\n            ssh_host = self._address_of_master()\\n            if ssh_host:\\n                hosts = [ssh_host]\\n                host_desc = ssh_host\\n                if ssh_to_workers:\\n                    try:\\n                        hosts.extend(self._ssh_worker_hosts())\\n                        host_desc += ' and task/core nodes'\\n                    except IOError:\\n                        log.warning('Could not get worker addresses for %s' %\\n                                    ssh_host)\\n\\n                path = posixpath.join(_EMR_LOG_DIR, dir_name)\\n                log.info('Looking for %s in %s on %s...' % (\\n                    log_desc, path, host_desc))\\n                yield ['ssh://%s%s%s' % (\\n                    ssh_host, '!' + host if host != ssh_host else '',\\n                    path) for host in hosts]\\n\\n        # wait for logs to be on S3\\n        self._wait_for_logs_on_s3()\\n\\n        s3_dir_name = s3_dir_name or dir_name\\n\\n        if s3_dir_name and self._s3_log_dir():\\n            cloud_log_dir = posixpath.join(self._s3_log_dir(), s3_dir_name)\\n            log.info('Looking for %s in %s...' % (log_desc, cloud_log_dir))\\n            yield [cloud_log_dir]",
                    "func_fullName": "mrjob.emr._stream_log_dirs( self, log_desc, dir_name, s3_dir_name, hdfs_dir_name, ssh_to_workers )"
                },
                {
                    "func_id": 1294,
                    "func_name": "_wait_for_logs_on_s3",
                    "func_desc": "_wait_for_logs_on_s3",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _wait_for_logs_on_s3(self):\\n        \"\"\"If the cluster is already terminating, wait for it to terminate,\\n        so that logs will be transferred to S3.\\n\\n        Don't print anything unless cluster is in the TERMINATING state.\\n        \"\"\"\\n        cluster = self._describe_cluster()\\n\\n        if cluster['Status']['State'] in (\\n                'TERMINATED', 'TERMINATED_WITH_ERRORS'):\\n            return  # already terminated\\n\\n        if cluster['Status']['State'] != 'TERMINATING':\\n            # going to need to wait for logs to get archived to S3\\n\\n            # \"step_num\" is just a unique ID for the step; using -1\\n            # for master node setup script\\n            if (self._master_node_setup_script_path and\\n                    self._mns_log_interpretation is None):\\n                step_num = -1\\n            else:\\n                step_num = len(self._log_interpretations)\\n\\n            # already did this for this step\\n            if step_num in self._waited_for_logs_on_s3:\\n                return\\n\\n            try:\\n                log.info('Waiting %d minutes for logs to transfer to S3...'\\n                         ' (ctrl-c to skip)' % _S3_LOG_WAIT_MINUTES)\\n\\n                if not self.fs.can_handle_path('ssh:///'):\\n                    log.info(\\n                        '\\n'\\n                        'To fetch logs immediately next time, set up SSH.'\\n                        ' See:\\n'\\n                        'https://pythonhosted.org/mrjob/guides'\\n                        '/emr-quickstart.html#configuring-ssh-credentials\\n')\\n\\n                time.sleep(60 * _S3_LOG_WAIT_MINUTES)\\n            except KeyboardInterrupt:\\n                pass\\n\\n            # do this even if they ctrl-c'ed; don't make them do it\\n            # for every log for this step\\n            self._waited_for_logs_on_s3.add(step_num)\\n            return\\n\\n        self._wait_for_cluster_to_terminate()",
                    "func_fullName": "mrjob.emr._wait_for_logs_on_s3( self )"
                },
                {
                    "func_id": 1379,
                    "func_name": "_audit_usage",
                    "func_desc": "_audit_usage",
                    "func_file": "cmd",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _audit_usage(args):\\n    from mrjob.tools.emr.audit_usage import main\\n    main(args)",
                    "func_fullName": "mrjob.cmd._audit_usage( args )"
                }
            ]
        },
        {
            "cluster_id": 1,
            "feature_id": 50,
            "feature_desc": "gamma=0.0608; k=17; a=0.25; combined=0.388; stability(ARI)=0.911; sep=0.150",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1225,
                    "func_name": "_add_bootstrap_files_for_upload",
                    "func_desc": "_add_bootstrap_files_for_upload",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _add_bootstrap_files_for_upload(self, persistent=False):\\n        \"\"\"Add files needed by the bootstrap script to self._upload_mgr.\\n\\n        Create the master bootstrap script if necessary.\\n\\n        persistent -- set by make_persistent_cluster()\\n        \"\"\"\\n        # all other files needed by the script are already in\\n        # _bootstrap_dir_mgr\\n        for path in self._bootstrap_dir_mgr.paths():\\n            self._upload_mgr.add(path)\\n\\n        # now that we know where the above files live, we can create\\n        # the master bootstrap script\\n        self._create_master_bootstrap_script_if_needed()\\n        if self._master_bootstrap_script_path:\\n            self._upload_mgr.add(self._master_bootstrap_script_path)\\n\\n        # make sure bootstrap action scripts are on S3\\n        for bootstrap_action in self._bootstrap_actions():\\n            self._upload_mgr.add(bootstrap_action['path'])\\n\\n        # Add max-mins-idle script if we need it\\n        if persistent or self._opts['pool_clusters']:\\n            self._upload_mgr.add(_MAX_MINS_IDLE_BOOTSTRAP_ACTION_PATH)",
                    "func_fullName": "mrjob.emr._add_bootstrap_files_for_upload( self, persistent )"
                },
                {
                    "func_id": 1283,
                    "func_name": "_check_for_failed_bootstrap_action",
                    "func_desc": "_check_for_failed_bootstrap_action",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _check_for_failed_bootstrap_action(self, cluster):\\n        \"\"\"If our bootstrap actions failed, parse the stderr to find\\n        out why.\"\"\"\\n        reason = _get_reason(cluster)\\n        action_num_and_node_id = _check_for_nonzero_return_code(reason)\\n        if not action_num_and_node_id:\\n            return\\n\\n        if not self._read_logs():\\n            return\\n\\n        # this doesn't really correspond to a step, so\\n        # don't bother storing it in self._log_interpretations\\n        bootstrap_interpretation = _interpret_emr_bootstrap_stderr(\\n            self.fs, self._ls_bootstrap_stderr_logs(**action_num_and_node_id))\\n\\n        # should be 0 or 1 errors, since we're checking a single stderr file\\n        if bootstrap_interpretation.get('errors'):\\n            error = bootstrap_interpretation['errors'][0]\\n            _log_probable_cause_of_failure(log, error)",
                    "func_fullName": "mrjob.emr._check_for_failed_bootstrap_action( self, cluster )"
                },
                {
                    "func_id": 1296,
                    "func_name": "_bootstrap_python",
                    "func_desc": "_bootstrap_python",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _bootstrap_python(self):\\n        \"\"\"Return a (possibly empty) list of parsed commands (in the same\\n        format as returned by parse_setup_cmd())'\"\"\"\\n\\n        if PY2:\\n            # Python 2 and pip are basically already installed everywhere\\n            # (Okay, there's no pip on AMIs prior to 2.4.3, but there's no\\n            # longer an easy way to get it now that apt-get is broken.)\\n            return []\\n\\n        # if bootstrap_python is None, install it for all AMIs up to 4.6.0,\\n        # and warn if it's an AMI before 3.7.0\\n        if self._opts['bootstrap_python'] or (\\n                self._opts['bootstrap_python'] is None and\\n                not self._image_version_gte('4.6.0')):\\n\\n            # we have to have at least on AMI 3.7.0. But give it a shot\\n            if not self._image_version_gte('3.7.0'):\\n                log.warning(\\n                    'bootstrapping Python 3 will probably not work on'\\n                    ' AMIs prior to 3.7.0. For an alternative, see:'\\n                    ' https://pythonhosted.org/mrjob/guides/emr-bootstrap'\\n                    '-cookbook.html#installing-python-from-source')\\n\\n            return [[\\n                'sudo yum install -y python34 python34-devel python34-pip'\\n            ]]\\n        else:\\n            return []",
                    "func_fullName": "mrjob.emr._bootstrap_python( self )"
                },
                {
                    "func_id": 1299,
                    "func_name": "_bootstrap_actions",
                    "func_desc": "_bootstrap_actions",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _bootstrap_actions(self, add_spark=True):\\n        \"\"\"Parse *bootstrap_actions* option into dictionaries with\\n        keys *path*, *args*, adding Spark bootstrap action if needed.\\n\\n        (This doesn't handle the master bootstrap script.)\\n        \"\"\"\\n        actions = list(self._opts['bootstrap_actions'])\\n\\n        # no release_label implies AMIs prior to 4.x\\n        if (add_spark and self._should_bootstrap_spark() and\\n                not self._opts['release_label']):\\n\\n            # running this action twice apparently breaks Spark's\\n            # ability to output to S3 (see #1367)\\n            if not self._has_spark_install_bootstrap_action():\\n                actions.append(_3_X_SPARK_BOOTSTRAP_ACTION)\\n\\n        results = []\\n        for action in actions:\\n            args = shlex_split(action)\\n            if not args:\\n                raise ValueError('bad bootstrap action: %r' % (action,))\\n\\n            results.append(dict(path=args[0], args=args[1:]))\\n\\n        return results",
                    "func_fullName": "mrjob.emr._bootstrap_actions( self, add_spark )"
                }
            ]
        },
        {
            "cluster_id": 1,
            "feature_id": 51,
            "feature_desc": "gamma=0.0608; k=17; a=0.25; combined=0.388; stability(ARI)=0.911; sep=0.150",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1226,
                    "func_name": "_add_master_node_setup_files_for_upload",
                    "func_desc": "_add_master_node_setup_files_for_upload",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _add_master_node_setup_files_for_upload(self):\\n        \"\"\"Add files necesary for the master node setup script to\\n        self._master_node_setup_mgr() and self._upload_mgr().\\n\\n        Create the master node setup script if necessary.\\n        \"\"\"\\n        # currently, only used by libjars; see #1336 for how we might open\\n        # this up more generally\\n        for path in self._opts['libjars']:\\n            # passthrough for libjars already on EMR\\n            if path.startswith('file:///'):\\n                continue\\n\\n            self._master_node_setup_mgr.add('file', path)\\n            self._upload_mgr.add(path)\\n\\n        self._create_master_node_setup_script_if_needed()\\n        if self._master_node_setup_script_path:\\n            self._upload_mgr.add(self._master_node_setup_script_path)",
                    "func_fullName": "mrjob.emr._add_master_node_setup_files_for_upload( self )"
                },
                {
                    "func_id": 1267,
                    "func_name": "_build_master_node_setup_step",
                    "func_desc": "_build_master_node_setup_step",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _build_master_node_setup_step(self):\\n        name = '%s: Master node setup' % self._job_key\\n        jar = self._script_runner_jar_uri()\\n        step_args = [self._upload_mgr.uri(self._master_node_setup_script_path)]\\n\\n        return dict(\\n            Name=name,\\n            ActionOnFailure=self._action_on_failure(),\\n            HadoopJarStep=dict(\\n                Jar=jar,\\n                Args=step_args,\\n            )\\n        )",
                    "func_fullName": "mrjob.emr._build_master_node_setup_step( self )"
                },
                {
                    "func_id": 1276,
                    "func_name": "_log_address_of_master_once",
                    "func_desc": "_log_address_of_master_once",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _log_address_of_master_once(self):\\n        \"\"\"Log the master node's public DNS, if we haven't already\"\"\"\\n        # Some users like to SSH in manually. See #2007\\n        if not self._cluster_id:\\n            return\\n\\n        if self._cluster_id in self._logged_address_of_master:\\n            return\\n\\n        master_dns = self._address_of_master()\\n\\n        if not master_dns:\\n            return\\n\\n        log.info('  master node is %s' % master_dns)\\n        self._logged_address_of_master.add(self._cluster_id)",
                    "func_fullName": "mrjob.emr._log_address_of_master_once( self )"
                },
                {
                    "func_id": 1302,
                    "func_name": "_create_master_node_setup_script_if_needed",
                    "func_desc": "_create_master_node_setup_script_if_needed",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _create_master_node_setup_script_if_needed(self):\\n        \"\"\"Helper for :py:meth:`_add_bootstrap_files_for_upload`.\\n\\n        If we need a master node setup script and write it into our local\\n        temp directory. Set self._master_node_setup_script_path.\\n        \"\"\"\\n        # already created\\n        if self._master_node_setup_script_path:\\n            return\\n\\n        # currently, the only thing this script does is upload files\\n        if not self._master_node_setup_mgr.paths():\\n            return\\n\\n        # create script\\n        path = os.path.join(self._get_local_tmp_dir(), 'mns.sh')\\n        contents = self._master_node_setup_script_content()\\n\\n        self._write_script(contents, path, 'master node setup script')\\n\\n        # the script itself doesn't need to be on the master node, just S3\\n        self._master_node_setup_script_path = path\\n        self._upload_mgr.add(path)",
                    "func_fullName": "mrjob.emr._create_master_node_setup_script_if_needed( self )"
                },
                {
                    "func_id": 1303,
                    "func_name": "_master_node_setup_script_content",
                    "func_desc": "_master_node_setup_script_content",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _master_node_setup_script_content(self):\\n        \"\"\"Create the contents of the master node setup script as an\\n        array of strings.\\n\\n        (prepare self._master_node_setup_mgr first)\\n        \"\"\"\\n        # TODO: this is very similar to _master_bootstrap_script_content();\\n        # merge common code\\n        out = []\\n\\n        # shebang, etc.\\n        for line in self._start_of_sh_script():\\n            out.append(line)\\n        out.append('')\\n\\n        # run commands in a block so we can redirect stdout to stderr\\n        # (e.g. to catch errors from compileall). See #370\\n        out.append('{')\\n\\n        # make working dir\\n        working_dir = self._master_node_setup_working_dir()\\n        out.append('  mkdir -p %s' % pipes.quote(working_dir))\\n        out.append('  cd %s' % pipes.quote(working_dir))\\n        out.append('')\\n\\n        for name, path in sorted(\\n                self._master_node_setup_mgr.name_to_path('file').items()):\\n            uri = self._upload_mgr.uri(path)\\n            out.append('  %s %s %s' % (\\n                self._cp_to_local_cmd(), pipes.quote(uri), pipes.quote(name)))\\n            # imitate Hadoop Distributed Cache\\n            out.append('  chmod u+rx %s' % pipes.quote(name))\\n\\n        # at some point we will probably run commands as well (see #1336)\\n\\n        out.append('} 1>&2')  # stdout -> stderr for ease of error log parsing\\n\\n        return out",
                    "func_fullName": "mrjob.emr._master_node_setup_script_content( self )"
                },
                {
                    "func_id": 1304,
                    "func_name": "_master_node_setup_working_dir",
                    "func_desc": "_master_node_setup_working_dir",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _master_node_setup_working_dir(self):\\n        \"\"\"Where to place files used by the master node setup script.\"\"\"\\n        return '/home/hadoop/%s' % self._job_key",
                    "func_fullName": "mrjob.emr._master_node_setup_working_dir( self )"
                },
                {
                    "func_id": 1322,
                    "func_name": "_address_of_master",
                    "func_desc": "_address_of_master",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _address_of_master(self):\\n        \"\"\"Get the address of the master node so we can SSH to it\"\"\"\\n        return self._get_cluster_info('master_public_dns')",
                    "func_fullName": "mrjob.emr._address_of_master( self )"
                },
                {
                    "func_id": 1323,
                    "func_name": "_master_private_ip",
                    "func_desc": "_master_private_ip",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _master_private_ip(self):\\n        \"\"\"Get the internal (\"private\") address of the master node, so we\\n        can direct our SSH tunnel to it.\"\"\"\\n        return self._get_cluster_info('master_private_ip')",
                    "func_fullName": "mrjob.emr._master_private_ip( self )"
                },
                {
                    "func_id": 1328,
                    "func_name": "_store_master_instance_info",
                    "func_desc": "_store_master_instance_info",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _store_master_instance_info(self):\\n        \"\"\"List master instance for our cluster, and cache\\n        master_private_ip.\"\"\"\\n        if not self._cluster_id:\\n            raise ValueError('cluster has not yet been created')\\n\\n        cache = self._cluster_to_cache[self._cluster_id]\\n\\n        emr_client = self.make_emr_client()\\n\\n        instances = emr_client.list_instances(\\n            ClusterId=self._cluster_id,\\n            InstanceGroupTypes=['MASTER'])['Instances']\\n\\n        if not instances:\\n            return\\n\\n        master = instances[0]\\n\\n        # can also get private DNS and public IP/DNS, but we don't use this\\n        master_private_ip = master.get('PrivateIpAddress')\\n        if master_private_ip:  # may not have been assigned yet\\n            cache['master_private_ip'] = master_private_ip",
                    "func_fullName": "mrjob.emr._store_master_instance_info( self )"
                }
            ]
        },
        {
            "cluster_id": 1,
            "feature_id": 52,
            "feature_desc": "gamma=0.0608; k=17; a=0.25; combined=0.388; stability(ARI)=0.911; sep=0.150",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1227,
                    "func_name": "_add_job_files_for_upload",
                    "func_desc": "_add_job_files_for_upload",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _add_job_files_for_upload(self):\\n        \"\"\"Add files needed for running the job (setup and input)\\n        to self._upload_mgr.\"\"\"\\n        for path in self._py_files():\\n            self._upload_mgr.add(path)\\n\\n        if self._opts['hadoop_streaming_jar']:\\n            self._upload_mgr.add(self._opts['hadoop_streaming_jar'])\\n\\n        # upload JARs and (Python) scripts run by steps\\n        for step in self._get_steps():\\n            for key in 'jar', 'script':\\n                if step.get(key):\\n                    self._upload_mgr.add(step[key])",
                    "func_fullName": "mrjob.emr._add_job_files_for_upload( self )"
                },
                {
                    "func_id": 1232,
                    "func_name": "_job_tracker_host",
                    "func_desc": "_job_tracker_host",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _job_tracker_host(self):\\n        \"\"\"The host of the job tracker/resource manager, from the master node.\\n        \"\"\"\\n        tunnel_config = self._ssh_tunnel_config()\\n\\n        if tunnel_config['localhost']:\\n            # Issue #1311: on the 2.x AMIs, we want to tunnel to the job\\n            # tracker on localhost; otherwise it won't\\n            # work on some VPC setups.\\n            return 'localhost'\\n        else:\\n            # Issue #1397: on the 3.x and 4.x AMIs we want to tunnel to the\\n            # resource manager on the master node's *internal* IP; otherwise\\n            # it work won't work on some VPC setups\\n            return self._master_private_ip()",
                    "func_fullName": "mrjob.emr._job_tracker_host( self )"
                },
                {
                    "func_id": 1235,
                    "func_name": "_job_tracker_url",
                    "func_desc": "_job_tracker_url",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _job_tracker_url(self):\\n        \"\"\"Not actually used to set up the SSH tunnel, used to run curl\\n        over SSH to fetch from the job tracker directly.\"\"\"\\n        tunnel_config = self._ssh_tunnel_config()\\n\\n        return 'http://%s:%d%s' % (\\n            self._job_tracker_host(),\\n            tunnel_config['port'],\\n            tunnel_config['path'])",
                    "func_fullName": "mrjob.emr._job_tracker_url( self )"
                },
                {
                    "func_id": 1252,
                    "func_name": "_service_role",
                    "func_desc": "_service_role",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _service_role(self):\\n        try:\\n            return (self._opts['iam_service_role'] or\\n                    get_or_create_mrjob_service_role(self.make_iam_client()))\\n        except botocore.exceptions.ClientError as ex:\\n            if _client_error_status(ex) != 403:\\n                raise\\n            log.warning(\\n                \"Can't access IAM API, trying default service role: %s\" %\\n                _FALLBACK_SERVICE_ROLE)\\n            return _FALLBACK_SERVICE_ROLE",
                    "func_fullName": "mrjob.emr._service_role( self )"
                },
                {
                    "func_id": 1266,
                    "func_name": "_upload_uri_or_remote_path",
                    "func_desc": "_upload_uri_or_remote_path",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _upload_uri_or_remote_path(self, path):\\n        \"\"\"Return where *path* will be uploaded, or, if it starts with\\n        ``'file:///'``, a local path.\"\"\"\\n        if path.startswith('file:///'):\\n            return path[7:]  # keep leading slash\\n        else:\\n            return self._upload_mgr.uri(path)",
                    "func_fullName": "mrjob.emr._upload_uri_or_remote_path( self, path )"
                },
                {
                    "func_id": 1270,
                    "func_name": "_launch_emr_job",
                    "func_desc": "_launch_emr_job",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _launch_emr_job(self):\\n        \"\"\"Create an empty cluster on EMR, and set self._cluster_id to\\n        its ID.\\n        \"\"\"\\n        # step concurrency level of a cluster we added steps to, used\\n        # for locking\\n        step_concurrency_level = None\\n\\n        # try to find a cluster from the pool. basically auto-fill\\n        # 'cluster_id' if possible and then follow normal behavior.\\n        if (self._opts['pool_clusters'] and not self._cluster_id):\\n            cluster_id, step_concurrency_level = self._find_cluster()\\n            if cluster_id:\\n                self._cluster_id = cluster_id\\n                self._locked_cluster = True\\n\\n        # create a cluster if we're not already using an existing one\\n        if not self._cluster_id:\\n            self._cluster_id = self._create_cluster()\\n            self._created_cluster = True\\n        else:\\n            log.info('Adding our job to existing cluster %s' %\\n                     self._cluster_id)\\n            self._log_address_of_master_once()\\n\\n        # now that we know which cluster it is, check for Spark support\\n        if self._has_spark_steps():\\n            self._check_cluster_spark_support()\\n\\n        # define our steps\\n        steps = self._steps_to_submit()\\n\\n        if self._add_steps_in_batch():\\n            self._add_steps_to_cluster(steps)\\n        else:\\n            # later steps will be added one at a time\\n            self._add_steps_to_cluster(steps[:1])\\n\\n        # if we locked a cluster with concurrent steps, we can release\\n        # the lock immediately\\n        if step_concurrency_level and step_concurrency_level > 1:\\n            self._release_cluster_lock()\\n\\n        # learn about how fast the cluster state switches\\n        cluster = self._describe_cluster()\\n        log.debug('Cluster has state %s' % cluster['Status']['State'])\\n\\n        # SSH FS uses sudo if we're on AMI 4.3.0+ (see #1244)\\n        if hasattr(self.fs, 'ssh') and version_gte(\\n                self.get_image_version(), '4.3.0'):\\n            self.fs.ssh.use_sudo_over_ssh()",
                    "func_fullName": "mrjob.emr._launch_emr_job( self )"
                },
                {
                    "func_id": 1305,
                    "func_name": "_script_runner_jar_uri",
                    "func_desc": "_script_runner_jar_uri",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _script_runner_jar_uri(self):\\n        return (\\n            's3://%s.elasticmapreduce/libs/script-runner/script-runner.jar' %\\n            self._opts['region'])",
                    "func_fullName": "mrjob.emr._script_runner_jar_uri( self )"
                },
                {
                    "func_id": 1383,
                    "func_name": "_report_long_jobs",
                    "func_desc": "_report_long_jobs",
                    "func_file": "cmd",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _report_long_jobs(args):\\n    from mrjob.tools.emr.report_long_jobs import main\\n    main(args)",
                    "func_fullName": "mrjob.cmd._report_long_jobs( args )"
                }
            ]
        },
        {
            "cluster_id": 1,
            "feature_id": 53,
            "feature_desc": "gamma=0.0608; k=17; a=0.25; combined=0.388; stability(ARI)=0.911; sep=0.150",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1228,
                    "func_name": "_ssh_add_bin",
                    "func_desc": "_ssh_add_bin",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ssh_add_bin(self):\\n        # the args of the ssh-add binary\\n        return self._opts['ssh_add_bin'] or ['ssh-add']",
                    "func_fullName": "mrjob.emr._ssh_add_bin( self )"
                },
                {
                    "func_id": 1229,
                    "func_name": "_ssh_bin",
                    "func_desc": "_ssh_bin",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ssh_bin(self):\\n        # the args of the ssh binary\\n        return self._opts['ssh_bin'] or ['ssh']",
                    "func_fullName": "mrjob.emr._ssh_bin( self )"
                },
                {
                    "func_id": 1230,
                    "func_name": "_set_up_ssh_tunnel_and_hdfs",
                    "func_desc": "_set_up_ssh_tunnel_and_hdfs",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _set_up_ssh_tunnel_and_hdfs(self):\\n        if hasattr(self.fs, 'hadoop'):\\n            self.fs.hadoop.set_hadoop_bin(self._ssh_hadoop_bin())\\n        self._set_up_ssh_tunnel()",
                    "func_fullName": "mrjob.emr._set_up_ssh_tunnel_and_hdfs( self )"
                },
                {
                    "func_id": 1231,
                    "func_name": "_ssh_tunnel_config",
                    "func_desc": "_ssh_tunnel_config",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ssh_tunnel_config(self):\\n        \"\"\"Look up AMI version, and return a dict with the following keys:\\n\\n        name: \"job tracker\" or \"resource manager\"\\n        path: path to start page of job tracker/resource manager\\n        port: port job tracker/resource manager is running on.\\n        \"\"\"\\n        return map_version(self.get_image_version(),\\n                           _IMAGE_VERSION_TO_SSH_TUNNEL_CONFIG)",
                    "func_fullName": "mrjob.emr._ssh_tunnel_config( self )"
                },
                {
                    "func_id": 1233,
                    "func_name": "_ssh_tunnel_args",
                    "func_desc": "_ssh_tunnel_args",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ssh_tunnel_args(self, bind_port):\\n        for opt_name in ('ec2_key_pair', 'ec2_key_pair_file',\\n                         'ssh_bind_ports'):\\n            if not self._opts[opt_name]:\\n                log.warning(\\n                    \"  You must set %s in order to set up the SSH tunnel!\"\\n                    % opt_name)\\n                self._give_up_on_ssh_tunnel = True\\n                return\\n\\n        host = self._address_of_master()\\n        if not host:\\n            return\\n\\n        return self._ssh_bin() + [\\n            '-o', 'VerifyHostKeyDNS=no',\\n            '-o', 'StrictHostKeyChecking=no',\\n            '-o', 'ExitOnForwardFailure=yes',\\n            '-o', 'UserKnownHostsFile=%s' % os.devnull,\\n        ] + self._ssh_tunnel_opts(bind_port) + [\\n            '-i', self._opts['ec2_key_pair_file'],\\n            'hadoop@%s' % host,\\n        ]",
                    "func_fullName": "mrjob.emr._ssh_tunnel_args( self, bind_port )"
                },
                {
                    "func_id": 1234,
                    "func_name": "_ssh_hadoop_bin",
                    "func_desc": "_ssh_hadoop_bin",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ssh_hadoop_bin(self):\\n        if not self._opts['ec2_key_pair_file']:\\n            return []\\n\\n        host = self._address_of_master()\\n        if not host:\\n            return []\\n\\n        return self._ssh_bin() + [\\n            '-o', 'VerifyHostKeyDNS=no',\\n            '-o', 'StrictHostKeyChecking=no',\\n            '-o', 'ExitOnForwardFailure=yes',\\n            '-o', 'UserKnownHostsFile=%s' % os.devnull,\\n            '-i', self._opts['ec2_key_pair_file'],\\n            '-q',  # don't care about SSH warnings, we just want hadoop\\n            'hadoop@%s' % host,\\n            'hadoop',\\n        ]",
                    "func_fullName": "mrjob.emr._ssh_hadoop_bin( self )"
                },
                {
                    "func_id": 1279,
                    "func_name": "_progress_html_over_ssh",
                    "func_desc": "_progress_html_over_ssh",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _progress_html_over_ssh(self):\\n        \"\"\"Fetch progress by running :command:`curl` over SSH, or return\\n        ``None``\"\"\"\\n        host = self._address_of_master()\\n\\n        if not self._opts['ec2_key_pair_file']:\\n            return None\\n\\n        if not host:\\n            return None\\n\\n        tunnel_config = self._ssh_tunnel_config()\\n        remote_url = self._job_tracker_url()\\n\\n        log.debug('  Fetching progress from %s over SSH' % (\\n            tunnel_config['name']))\\n\\n        try:\\n            stdout, _ = self.fs.ssh._ssh_run(host, ['curl', remote_url])\\n            return stdout\\n        except Exception as e:\\n            log.debug('    failed: %s' % str(e))\\n\\n        return None",
                    "func_fullName": "mrjob.emr._progress_html_over_ssh( self )"
                },
                {
                    "func_id": 1293,
                    "func_name": "_ssh_worker_hosts",
                    "func_desc": "_ssh_worker_hosts",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ssh_worker_hosts(self):\\n        \"\"\"Get the hostnames of all core and task nodes,\\n        that are currently running, so we can SSH to them through the master\\n        nodes and read their logs.\\n\\n        (This currently returns IP addresses rather than full hostnames\\n        because they're shorter.)\\n        \"\"\"\\n        emr_client = self.make_emr_client()\\n\\n        instances = _boto3_paginate(\\n            'Instances', emr_client, 'list_instances',\\n            ClusterId=self._cluster_id,\\n            InstanceGroupTypes=['CORE', 'TASK'],\\n            InstanceStates=['RUNNING'])\\n\\n        hosts = []\\n\\n        for instance in instances:\\n            hosts.append(instance['PrivateIpAddress'])\\n\\n        return hosts",
                    "func_fullName": "mrjob.emr._ssh_worker_hosts( self )"
                },
                {
                    "func_id": 1315,
                    "func_name": "_pool_hash_dict",
                    "func_desc": "_pool_hash_dict",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _pool_hash_dict(self):\\n        \"\"\"A dictionary of information that must be matched exactly to\\n        join a pooled cluster (other than mrjob version and pool name).\\n\\n        The format of this dictionary may change between mrjob versions.\\n        \"\"\"\\n        # this can be expensive because we have to read every file used in\\n        # bootstrapping, so cache it\\n        if not self._pool_hash_dict_cached:\\n            d = {}\\n\\n            # additional_emr_info\\n            d['additional_emr_info'] = self._opts['additional_emr_info']\\n\\n            # applications\\n            # (these are case-insensitive)\\n            d['applications'] = sorted(a.lower() for a in self._applications())\\n\\n            # bootstrapping\\n\\n            # bootstrap_actions\\n            d['bootstrap_actions'] = self._bootstrap_actions()\\n\\n            # bootstrap_file_md5sums\\n            d['bootstrap_file_md5sums'] = {\\n                name: self.fs.md5sum(path)\\n                for name, path\\n                in self._bootstrap_dir_mgr.name_to_path().items()\\n                if path != self._mrjob_zip_path\\n            }\\n\\n            # bootstrap_without_paths\\n            # original path doesn't matter, just contents (above) and name\\n            d['bootstrap_without_paths'] = [\\n                [\\n                    dict(type=x['type'],\\n                         name=self._bootstrap_dir_mgr.name(**x))\\n                    if isinstance(x, dict) else x\\n                    for x in cmd\\n                ]\\n                for cmd in self._bootstrap\\n            ]\\n\\n            # emr_configurations\\n            d['emr_configurations'] = self._emr_configurations()\\n\\n            # image_id\\n            d['image_id'] = self._opts['image_id']\\n\\n            # instance_collection_type\\n            # no way to compare instance groups with instance fleets\\n            # so make it part of the hash\\n            d['instance_collection_type'] = (\\n                'INSTANCE_FLEET' if self._opts['instance_fleets']\\n                else 'INSTANCE_GROUP'\\n            )\\n\\n            # release_label\\n            # use e.g. emr-2.4.9 for 2.x/3.x AMIs, even though the API wouldn't\\n            d['release_label'] = (self._opts['release_label'] or\\n                                  'emr-' + self._opts['image_version'])\\n\\n            self._pool_hash_dict_cached = d\\n\\n        return self._pool_hash_dict_cached",
                    "func_fullName": "mrjob.emr._pool_hash_dict( self )"
                },
                {
                    "func_id": 1316,
                    "func_name": "_pool_hash",
                    "func_desc": "_pool_hash",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _pool_hash(self):\\n        hash_dict = self._pool_hash_dict()\\n\\n        hash_json = json.dumps(hash_dict, sort_keys=True)\\n        if not isinstance(hash_json, bytes):\\n            hash_json = hash_json.encode('utf_8')\\n\\n        m = hashlib.md5()\\n        m.update(hash_json)\\n        return m.hexdigest()",
                    "func_fullName": "mrjob.emr._pool_hash( self )"
                },
                {
                    "func_id": 1329,
                    "func_name": "make_ec2_client",
                    "func_desc": "make_ec2_client",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def make_ec2_client(self):\\n        \"\"\"Create a :py:mod:`boto3` EC2 client.\\n\\n        :return: a :py:class:`botocore.client.EC2` wrapped in a\\n                :py:class:`mrjob.retry.RetryWrapper`\\n        \"\"\"\\n        if boto3 is None:\\n            raise ImportError('You must install boto3 to connect to EC2')\\n\\n        raw_ec2_client = boto3.client(\\n            'ec2',\\n            aws_access_key_id=self._opts['aws_access_key_id'],\\n            aws_secret_access_key=self._opts['aws_secret_access_key'],\\n            aws_session_token=self._opts['aws_session_token'],\\n            endpoint_url=_endpoint_url(self._opts['ec2_endpoint']),\\n            region_name=self._opts['region'],\\n        )\\n\\n        return _wrap_aws_client(raw_ec2_client)",
                    "func_fullName": "mrjob.emr.make_ec2_client( self )"
                }
            ]
        },
        {
            "cluster_id": 1,
            "feature_id": 54,
            "feature_desc": "gamma=0.0608; k=17; a=0.25; combined=0.388; stability(ARI)=0.911; sep=0.150",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1236,
                    "func_name": "cleanup",
                    "func_desc": "cleanup",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def cleanup(self, mode=None):\\n        super(EMRJobRunner, self).cleanup(mode=mode)\\n\\n        # always stop our SSH tunnel if it's still running\\n        self._kill_ssh_tunnel()\\n\\n        # stop the cluster if it belongs to us (it may have stopped on its\\n        # own already, but that's fine)\\n        # don't stop it if it was created due to --pool because the user\\n        # probably wants to use it again\\n        if self._cluster_id and not self._opts['cluster_id'] \\\\n                and not self._opts['pool_clusters']:\\n            log.info('Terminating cluster: %s' % self._cluster_id)\\n            try:\\n                self.make_emr_client().terminate_job_flows(\\n                    JobFlowIds=[self._cluster_id]\\n                )\\n            except Exception as e:\\n                log.exception(e)",
                    "func_fullName": "mrjob.emr.cleanup( self, mode )"
                },
                {
                    "func_id": 1240,
                    "func_name": "_wait_for_s3_eventual_consistency",
                    "func_desc": "_wait_for_s3_eventual_consistency",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _wait_for_s3_eventual_consistency(self):\\n        \"\"\"Sleep for a little while, to give S3 a chance to sync up.\\n        \"\"\"\\n        log.debug('Waiting %.1fs for S3 eventual consistency...' %\\n                  self._opts['cloud_fs_sync_secs'])\\n        time.sleep(self._opts['cloud_fs_sync_secs'])",
                    "func_fullName": "mrjob.emr._wait_for_s3_eventual_consistency( self )"
                },
                {
                    "func_id": 1324,
                    "func_name": "_get_app_versions",
                    "func_desc": "_get_app_versions",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_app_versions(self):\\n        \"\"\"Returns a map from lowercase app name to version for our cluster.\\n\\n        For apps other than Hadoop, this only works for AMI 4.x and later.\\n        \"\"\"\\n        return self._get_cluster_info('app_versions')",
                    "func_fullName": "mrjob.emr._get_app_versions( self )"
                }
            ]
        },
        {
            "cluster_id": 1,
            "feature_id": 55,
            "feature_desc": "gamma=0.0608; k=17; a=0.25; combined=0.388; stability(ARI)=0.911; sep=0.150",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1239,
                    "func_name": "_cleanup_cluster",
                    "func_desc": "_cleanup_cluster",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cleanup_cluster(self):\\n        if not self._cluster_id:\\n            # If we don't have a cluster, then we can't terminate it.\\n            return\\n\\n        emr_client = self.make_emr_client()\\n        try:\\n            log.info(\"Attempting to terminate cluster\")\\n            emr_client.terminate_job_flows(\\n                JobFlowIds=[self._cluster_id]\\n            )\\n        except Exception as e:\\n            # Something happened with boto3 and the user should know.\\n            log.exception(e)\\n            return\\n        log.info('Cluster %s successfully terminated' % self._cluster_id)",
                    "func_fullName": "mrjob.emr._cleanup_cluster( self )"
                },
                {
                    "func_id": 1241,
                    "func_name": "_wait_for_cluster_to_terminate",
                    "func_desc": "_wait_for_cluster_to_terminate",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _wait_for_cluster_to_terminate(self, cluster=None):\\n        if not cluster:\\n            cluster = self._describe_cluster()\\n\\n        log.info('Waiting for cluster (%s) to terminate...' %\\n                 cluster['Id'])\\n\\n        if (cluster['Status']['State'] == 'WAITING' and\\n                cluster['AutoTerminate']):\\n            raise Exception('Operation requires cluster to terminate, but'\\n                            ' it may never do so.')\\n\\n        while True:\\n            log.info('  %s' % cluster['Status']['State'])\\n\\n            if cluster['Status']['State'] in (\\n                    'TERMINATED', 'TERMINATED_WITH_ERRORS'):\\n                return\\n\\n            time.sleep(self._opts['check_cluster_every'])\\n            cluster = self._describe_cluster()",
                    "func_fullName": "mrjob.emr._wait_for_cluster_to_terminate( self, cluster )"
                },
                {
                    "func_id": 1248,
                    "func_name": "_create_cluster",
                    "func_desc": "_create_cluster",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _create_cluster(self, persistent=False):\\n        \"\"\"Create an empty cluster on EMR, and return the ID of that\\n        job.\\n\\n        If the ``tags`` option is set, also tags the cluster (which\\n        is a separate API call).\\n\\n        persistent -- if this is true, create the cluster with the keep_alive\\n            option, indicating the job will have to be manually terminated.\\n        \"\"\"\\n        log.debug('Creating Elastic MapReduce cluster')\\n        emr_client = self.make_emr_client()\\n\\n        kwargs = self._cluster_kwargs(persistent)\\n        log.debug('Calling run_job_flow(%s)' % (\\n            ', '.join('%s=%r' % (k, v)\\n                      for k, v in sorted(kwargs.items()))))\\n        cluster_id = emr_client.run_job_flow(**kwargs)['JobFlowId']\\n\\n        log.info('Created new cluster %s' % cluster_id)\\n\\n        # set EMR tags for the cluster\\n        tags = dict(self._opts['tags'])\\n\\n        # patch in version\\n        tags['__mrjob_version'] = mrjob.__version__\\n\\n        # patch in cluster label and owner\\n        tags['__mrjob_label'] = self._label()\\n        tags['__mrjob_owner'] = self._owner()\\n\\n        # add pooling tags\\n        if self._opts['pool_clusters']:\\n            tags['__mrjob_pool_hash'] = self._pool_hash()\\n            tags['__mrjob_pool_name'] = self._opts['pool_name']\\n\\n        self._add_tags(tags, cluster_id)\\n\\n        return cluster_id",
                    "func_fullName": "mrjob.emr._create_cluster( self, persistent )"
                },
                {
                    "func_id": 1250,
                    "func_name": "_cluster_kwargs",
                    "func_desc": "_cluster_kwargs",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cluster_kwargs(self, persistent=False):\\n        \"\"\"Build kwargs for emr_client.run_job_flow()\"\"\"\\n        kwargs = {}\\n\\n        kwargs['Name'] = self._job_key + self._cluster_name_pooling_suffix()\\n\\n        kwargs['LogUri'] = self._opts['cloud_log_dir']\\n\\n        if self._opts['release_label']:\\n            kwargs['ReleaseLabel'] = self._opts['release_label']\\n        else:\\n            kwargs['AmiVersion'] = self._opts['image_version']\\n\\n        if self._opts['image_id']:\\n            kwargs['CustomAmiId'] = self._opts['image_id']\\n\\n        # capitalizing Instances because it's just an API parameter\\n        kwargs['Instances'] = Instances = {}\\n\\n        if self._opts['zone']:\\n            Instances['Placement'] = dict(AvailabilityZone=self._opts['zone'])\\n\\n        if self._opts['instance_fleets']:\\n            Instances['InstanceFleets'] = self._opts['instance_fleets']\\n        else:\\n            Instances['InstanceGroups'] = self._instance_groups()\\n\\n        # EBS Root volume size\\n        if self._opts['ebs_root_volume_gb']:\\n            kwargs['EbsRootVolumeSize'] = self._opts['ebs_root_volume_gb']\\n\\n        # bootstrap actions\\n        kwargs['BootstrapActions'] = BootstrapActions = []\\n\\n        for i, bootstrap_action in enumerate(self._bootstrap_actions()):\\n            uri = self._upload_mgr.uri(bootstrap_action['path'])\\n            BootstrapActions.append(dict(\\n                Name=('action %d' % i),\\n                ScriptBootstrapAction=dict(\\n                    Path=uri,\\n                    Args=bootstrap_action['args'])))\\n\\n        if self._master_bootstrap_script_path:\\n            uri = self._upload_mgr.uri(self._master_bootstrap_script_path)\\n\\n            BootstrapActions.append(dict(\\n                Name='master',\\n                ScriptBootstrapAction=dict(\\n                    Path=uri,\\n                    Args=[])))\\n\\n        if persistent or self._opts['pool_clusters']:\\n            Instances['KeepJobFlowAliveWhenNoSteps'] = True\\n\\n            # use idle termination script on persistent clusters\\n            # add it last, so that we don't count bootstrapping as idle time\\n            uri = self._upload_mgr.uri(\\n                _MAX_MINS_IDLE_BOOTSTRAP_ACTION_PATH)\\n\\n            # script takes args in (integer) seconds\\n            ba_args = [str(int(self._opts['max_mins_idle'] * 60))]\\n            BootstrapActions.append(dict(\\n                Name='idle timeout',\\n                ScriptBootstrapAction=dict(\\n                    Path=uri,\\n                    Args=ba_args)))\\n\\n        if self._opts['ec2_key_pair']:\\n            Instances['Ec2KeyName'] = self._opts['ec2_key_pair']\\n\\n        kwargs['Steps'] = Steps = []\\n\\n        kwargs['StepConcurrencyLevel'] = self._opts['max_concurrent_steps']\\n\\n        if self._opts['enable_emr_debugging']:\\n            # other steps are added separately\\n            Steps.append(self._build_debugging_step())\\n\\n        if self._opts['additional_emr_info']:\\n            kwargs['AdditionalInfo'] = self._opts['additional_emr_info']\\n\\n        kwargs['VisibleToAllUsers'] = True\\n\\n        kwargs['JobFlowRole'] = self._instance_profile()\\n        kwargs['ServiceRole'] = self._service_role()\\n\\n        applications = self._applications()\\n        if applications:\\n            kwargs['Applications'] = [\\n                dict(Name=a) for a in sorted(applications)]\\n\\n        emr_configurations = self._emr_configurations()\\n        if emr_configurations:\\n            kwargs['Configurations'] = emr_configurations\\n\\n        if self._opts['subnet']:\\n            # handle lists of subnets (for instance fleets)\\n            if isinstance(self._opts['subnet'], list):\\n                Instances['Ec2SubnetIds'] = self._opts['subnet']\\n            else:\\n                Instances['Ec2SubnetId'] = self._opts['subnet']\\n\\n        return self._add_extra_cluster_params(kwargs)",
                    "func_fullName": "mrjob.emr._cluster_kwargs( self, persistent )"
                },
                {
                    "func_id": 1271,
                    "func_name": "_release_cluster_lock",
                    "func_desc": "_release_cluster_lock",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _release_cluster_lock(self):\\n        if not self._locked_cluster:\\n            return\\n\\n        emr_client = self.make_emr_client()\\n\\n        log.info('  releasing cluster lock')\\n        # this can fail, but usually it's because the cluster\\n        # started terminating, so only try releasing the lock once\\n        _attempt_to_unlock_cluster(emr_client, self._cluster_id)\\n        self._locked_cluster = False",
                    "func_fullName": "mrjob.emr._release_cluster_lock( self )"
                },
                {
                    "func_id": 1272,
                    "func_name": "_add_steps_to_cluster",
                    "func_desc": "_add_steps_to_cluster",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _add_steps_to_cluster(self, steps):\\n        \"\"\"Add steps (from _steps_to_submit()) to our cluster and append their\\n         IDs to self._step_ids\"\"\"\\n        emr_client = self.make_emr_client()\\n\\n        steps_kwargs = dict(JobFlowId=self._cluster_id, Steps=steps)\\n        log.debug('Calling add_job_flow_steps(%s)' % ','.join(\\n            ('%s=%r' % (k, v)) for k, v in steps_kwargs.items()))\\n        step_ids = emr_client.add_job_flow_steps(**steps_kwargs)['StepIds']\\n        self._step_ids.extend(step_ids)",
                    "func_fullName": "mrjob.emr._add_steps_to_cluster( self, steps )"
                },
                {
                    "func_id": 1280,
                    "func_name": "_check_for_pooled_cluster_self_termination",
                    "func_desc": "_check_for_pooled_cluster_self_termination",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _check_for_pooled_cluster_self_termination(self, cluster, step):\\n        \"\"\"If failure could have been due to a pooled cluster self-terminating,\\n        raise _PooledClusterSelfTerminatedException\"\"\"\\n        # this check might not even be relevant\\n        if not self._opts['pool_clusters']:\\n            return\\n\\n        if self._opts['cluster_id']:\\n            return\\n\\n        # if a cluster we created self-terminated, something is wrong with\\n        # the way self-termination is set up (e.g. very low idle time)\\n        if self._created_cluster:\\n            return\\n\\n        # don't check for max_mins_idle because it's possible to\\n        # join a self-terminating cluster without having max_mins_idle set\\n        # on this runner (pooling only cares about the master bootstrap script,\\n        # not other bootstrap actions)\\n\\n        # our step should be CANCELLED (not failed)\\n        if step['Status']['State'] != 'CANCELLED':\\n            return\\n\\n        # we *could* check if the step had a chance to start by checking if\\n        # step.status.timeline.startdatetime is set. This shouldn't happen in\\n        # practice, and if it did, we'd still be fine as long as the script\\n        # didn't write data to the output dir, so it's not worth the extra\\n        # code.\\n\\n        # cluster should have stopped because master node failed\\n        # could also check for\\n        # cluster.status.statechangereason.code == 'INSTANCE_FAILURE'\\n        if not _CLUSTER_SELF_TERMINATED_RE.match(_get_reason(cluster)):\\n            return\\n\\n        log.info('Pooled cluster self-terminated, trying again...')\\n        raise _PooledClusterSelfTerminatedException",
                    "func_fullName": "mrjob.emr._check_for_pooled_cluster_self_termination( self, cluster, step )"
                },
                {
                    "func_id": 1308,
                    "func_name": "make_persistent_cluster",
                    "func_desc": "make_persistent_cluster",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def make_persistent_cluster(self):\\n        if (self._cluster_id):\\n            raise ValueError(\\n                'This runner is already associated with cluster ID %s' %\\n                (self._cluster_id))\\n\\n        log.info('Creating persistent cluster to run several jobs in...')\\n\\n        self._add_bootstrap_files_for_upload(persistent=True)\\n        self._upload_local_files()\\n\\n        # make sure we can see the files we copied to S3\\n        self._wait_for_s3_eventual_consistency()\\n\\n        # don't allow user to call run()\\n        self._ran_job = True\\n\\n        self._cluster_id = self._create_cluster(persistent=True)\\n\\n        return self._cluster_id",
                    "func_fullName": "mrjob.emr.make_persistent_cluster( self )"
                },
                {
                    "func_id": 1309,
                    "func_name": "get_cluster_id",
                    "func_desc": "get_cluster_id",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_cluster_id(self):\\n        \"\"\"Get the ID of the cluster our job is running on, or ``None``.\"\"\"\\n        return self._cluster_id",
                    "func_fullName": "mrjob.emr.get_cluster_id( self )"
                },
                {
                    "func_id": 1310,
                    "func_name": "_yield_clusters_to_join",
                    "func_desc": "_yield_clusters_to_join",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _yield_clusters_to_join(self, available_cluster_ids):\\n        \"\"\"Get a list of IDs of pooled clusters that this runner can join,\\n        sorted so that the ones with the greatest CPU capacity come first.\\n\\n        yields (cluster, when_cluster_described) so we can lock clusters that\\n        we wish to join (*cluster* is the cluster description and\\n        *when_cluster_described* is a unix timestamp).\\n        \"\"\"\\n        emr_client = self.make_emr_client()\\n\\n        for cluster_id in available_cluster_ids:\\n            if not self._cluster_has_adequate_capacity(cluster_id):\\n                continue\\n\\n            # check other things about the cluster that we can't hash\\n            # (DescribeCluster)\\n            #\\n            # save cluster description so we can use it for locking\\n            when_cluster_described = time.time()\\n            cluster = emr_client.describe_cluster(\\n                ClusterId=cluster_id)['Cluster']\\n\\n            if not self._cluster_description_matches(cluster):\\n                continue\\n\\n            yield (cluster, when_cluster_described)",
                    "func_fullName": "mrjob.emr._yield_clusters_to_join( self, available_cluster_ids )"
                },
                {
                    "func_id": 1311,
                    "func_name": "_list_cluster_ids_for_pooling",
                    "func_desc": "_list_cluster_ids_for_pooling",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _list_cluster_ids_for_pooling(self, created_after=None):\\n        \"\"\"Call ListClusters, and collect cluster IDs relevant to pooling.\\n\\n        Optionally, only list clusters created after *created_after*.\\n\\n        Returns a dictionary with the following keys:\\n\\n        available: a list of IDs of clusters that we could join, based on their\\n                   state and name suffix (pool name and hash, mrjob version).\\n                   Sorted so that the cluster with the most CPU (based on\\n                   NormalizedInstanceHours) goes first\\n        matching: a set of IDs of clusters that have the right name suffix but\\n                  may or may not be in the right state to join (a superset\\n                  of *available*)\\n        in_pool: a set of IDs of clusters that are in the pool we want to join,\\n                 regardless of their state or pool hash (a superset of\\n                 *matching*)\\n        max_created: the latest creation timestamp for *any* cluster listed\\n                     (so we can call this again to get stats on newly created\\n                     clusters only)\\n        \"\"\"\\n        # a map from cluster_id to cpu_capacity\\n        available = {}\\n        matching = set()\\n        in_pool = set()\\n        max_created = None\\n\\n        name_to_match = self._opts['pool_name']\\n        suffix_to_match = self._cluster_name_pooling_suffix()\\n\\n        if self._opts['max_concurrent_steps'] > 1:\\n            states_to_match = {'RUNNING', 'WAITING'}\\n        else:\\n            states_to_match = {'WAITING'}\\n\\n        emr_client = self.make_emr_client()\\n        now = _boto3_now()\\n\\n        # you can't pass CreatedAfter=None to list_clusters()\\n        list_cluster_kwargs = dict(ClusterStates=_ACTIVE_CLUSTER_STATES)\\n        if created_after:\\n            list_cluster_kwargs['CreatedAfter'] = created_after\\n\\n        log.debug('calling list_clusters(%s)' % ', '.join(\\n            '%s=%r' % (k, v)\\n            for k, v in sorted(list_cluster_kwargs.items())))\\n\\n        for cluster in _boto3_paginate(\\n                'Clusters', emr_client, 'list_clusters',\\n                **list_cluster_kwargs):\\n\\n            cluster_id = cluster['Id']\\n            log.debug(cluster_id)\\n\\n            created = cluster['Status']['Timeline']['CreationDateTime']\\n\\n            if max_created is None or created > max_created:\\n                max_created = created\\n\\n            name = _parse_cluster_name_suffix(cluster['Name']).get('pool_name')\\n\\n            if name != name_to_match:\\n                continue\\n\\n            in_pool.add(cluster_id)\\n\\n            if not cluster['Name'].endswith(suffix_to_match):\\n                continue\\n\\n            matching.add(cluster_id)\\n\\n            if cluster['Status']['State'] not in states_to_match:\\n                continue\\n\\n            when_ready = cluster['Status']['Timeline'].get('ReadyDateTime')\\n\\n            if when_ready:\\n                hours = max(ceil((now - when_ready).total_seconds() / 3600),\\n                            1.0)\\n                cpu_capacity = cluster['NormalizedInstanceHours'] / hours\\n            else:\\n                # this probably won't happen, since we only inspect clusters\\n                # in the WAITING state\\n                cpu_capacity = 0\\n\\n            available[cluster_id] = cpu_capacity\\n\\n        # convert *available* from a dict to a sorted list\\n        available = sorted(\\n            available,\\n            key=lambda c: available[c],\\n            reverse=True)\\n\\n        return dict(\\n            available=available,\\n            in_pool=in_pool,\\n            matching=matching,\\n            max_created=max_created,\\n        )",
                    "func_fullName": "mrjob.emr._list_cluster_ids_for_pooling( self, created_after )"
                },
                {
                    "func_id": 1312,
                    "func_name": "_cluster_has_adequate_capacity",
                    "func_desc": "_cluster_has_adequate_capacity",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cluster_has_adequate_capacity(self, cluster_id):\\n        \"\"\"Check if the cluster has an instance group/fleet configuration\\n        that works as well or better.\\n\\n        This either calls ``ListInstanceFleets`` or ``ListInstanceGroups``,\\n        as appropriate\\n        \"\"\"\\n        emr_client = self.make_emr_client()\\n\\n        if (self._opts['min_available_mb'] or\\n                self._opts['min_available_virtual_cores']):\\n            cluster = emr_client.describe_cluster(\\n                ClusterId=cluster_id)['Cluster']\\n\\n            host = cluster['MasterPublicDnsName']\\n            try:\\n                log.debug('    querying clusterMetrics from %s' % host)\\n                metrics = self._yrm_get('metrics', host=host)['clusterMetrics']\\n                log.debug('      metrics: %s' %\\n                          json.dumps(metrics, sort_keys=True))\\n            except IOError as ex:\\n                log.info('    error while getting metrics for cluster %s: %s' %\\n                         (cluster_id, str(ex)))\\n                return False\\n\\n            if metrics['availableMB'] < self._opts['min_available_mb']:\\n                log.info('    too little memory')\\n                return False\\n\\n            if (metrics['availableVirtualCores'] <\\n                    self._opts['min_available_virtual_cores']):\\n                log.info('    too few virtual cores')\\n                return False\\n\\n            return True\\n        elif self._opts['instance_fleets']:\\n            try:\\n                fleets = list(_boto3_paginate(\\n                    'InstanceFleets', emr_client, 'list_instance_fleets',\\n                    ClusterId=cluster_id))\\n            except botocore.exceptions.ClientError:\\n                # this shouldn't usually happen because whether a cluster\\n                # uses instance fleets is in the pool hash\\n                log.debug('  cluster %s: does not use instance fleets' %\\n                          cluster_id)\\n                return False\\n\\n            return _instance_fleets_satisfy(\\n                fleets, self._opts['instance_fleets'])\\n        else:\\n            try:\\n                groups = list(_boto3_paginate(\\n                    'InstanceGroups', emr_client, 'list_instance_groups',\\n                    ClusterId=cluster_id))\\n            except botocore.exceptions.ClientError:\\n                # this shouldn't usually happen because whether a cluster\\n                # uses instance fleets is in the pool hash\\n                log.debug(' cluster %s: does not use instance groups' %\\n                          cluster_id)\\n                return False\\n\\n            return _instance_groups_satisfy(groups, self._instance_groups())",
                    "func_fullName": "mrjob.emr._cluster_has_adequate_capacity( self, cluster_id )"
                },
                {
                    "func_id": 1313,
                    "func_name": "_cluster_description_matches",
                    "func_desc": "_cluster_description_matches",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cluster_description_matches(self, cluster):\\n        \"\"\"Do we want to join the cluster with the given description?\"\"\"\\n        cluster_id = cluster['Id']\\n\\n        # skip if user specified a key pair and it doesn't match\\n        if (self._opts['ec2_key_pair'] and\\n                self._opts['ec2_key_pair'] !=\\n                cluster['Ec2InstanceAttributes'].get('Ec2KeyName')):\\n            log.debug('  cluster %s: ec2 key pair mismatch' % cluster_id)\\n            return False\\n\\n        # only take persistent clusters\\n        if cluster['AutoTerminate']:\\n            log.debug('  cluster %s: not persistent' % cluster_id)\\n            return False\\n\\n        # EBS root volume size\\n        if self._opts['ebs_root_volume_gb']:\\n            if 'EbsRootVolumeSize' not in cluster:\\n                log.debug('  cluster %s: EBS root volume size not set' %\\n                          cluster_id)\\n                return False\\n            elif (cluster['EbsRootVolumeSize'] <\\n                    self._opts['ebs_root_volume_gb']):\\n                log.debug('  cluster %s: EBS root volume size too small' %\\n                          cluster_id)\\n                return False\\n        else:\\n            if 'EbsRootVolumeSize' in cluster:\\n                log.debug('  cluster %s: uses non-default EBS root volume'\\n                          ' size' % cluster_id)\\n                return False\\n\\n        # subnet\\n        subnet = cluster['Ec2InstanceAttributes'].get('Ec2SubnetId')\\n        if isinstance(self._opts['subnet'], list):\\n            matches = (subnet in self._opts['subnet'])\\n        else:\\n            # empty subnet is the same as no subnet. see #1931\\n            matches = (subnet == (self._opts['subnet'] or None))\\n\\n        if not matches:\\n            log.debug('  cluster %s: subnet mismatch' % cluster_id)\\n            return\\n\\n        # step concurrency\\n        step_concurrency = cluster.get('StepConcurrencyLevel', 1)\\n        if step_concurrency > self._opts['max_concurrent_steps']:\\n            log.debug('  cluster %s: step concurrency level too high' %\\n                      cluster_id)\\n            return\\n\\n        return True",
                    "func_fullName": "mrjob.emr._cluster_description_matches( self, cluster )"
                },
                {
                    "func_id": 1314,
                    "func_name": "_find_cluster",
                    "func_desc": "_find_cluster",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _find_cluster(self):\\n        \"\"\"Find a cluster that can host this runner. Prefer clusters with more\\n        compute units. Break ties by choosing cluster with longest idle time.\\n        Return ``None`` if no suitable clusters exist.\\n        \"\"\"\\n        emr_client = self.make_emr_client()\\n\\n        start = datetime.now()\\n        wait_mins = self._opts['pool_wait_minutes']\\n        timeout_mins = self._opts['pool_timeout_minutes']\\n        pool_name = self._opts['pool_name']\\n        max_in_pool = self._opts['max_clusters_in_pool']\\n\\n        # like sleep() but also raises PoolTimeoutException if we're going to\\n        # sleep beyond the timeout\\n        def sleep_or_time_out(seconds):\\n            if (timeout_mins and (\\n                    datetime.now() + timedelta(seconds=seconds) >\\n                    start + timedelta(minutes=timeout_mins))):\\n                raise PoolTimeoutException(\\n                    'Unable to join or create a cluster within %d minutes' %\\n                    timeout_mins)\\n\\n            time.sleep(seconds)\\n\\n        log.info('Attempting to find an available cluster...')\\n        while True:\\n            cluster_ids = self._list_cluster_ids_for_pooling()\\n\\n            for cluster, when_cluster_described in (\\n                    self._yield_clusters_to_join(cluster_ids['available'])):\\n                cluster_id = cluster['Id']\\n                step_concurrency_level = cluster['StepConcurrencyLevel']\\n\\n                log.info('  Attempting to join cluster %s' % cluster_id)\\n                lock_acquired = _attempt_to_lock_cluster(\\n                    emr_client, cluster_id, self._job_key,\\n                    cluster=cluster,\\n                    when_cluster_described=when_cluster_described)\\n\\n                if lock_acquired:\\n                    return cluster_id, step_concurrency_level\\n\\n            keep_waiting = (\\n                datetime.now() < start + timedelta(minutes=wait_mins))\\n\\n            # if we haven't exhausted pool_wait_minutes, and there are\\n            # clusters we might eventually join, sleep and try again\\n            if keep_waiting and cluster_ids['matching']:\\n                log.info('No clusters in pool %r are available. Checking again'\\n                         ' in %d seconds...' % (\\n                             pool_name, int(_POOLING_SLEEP_INTERVAL)))\\n                sleep_or_time_out(_POOLING_SLEEP_INTERVAL)\\n                continue\\n\\n            # implement max_clusters_in_pool\\n            if max_in_pool:\\n                num_in_pool = len(cluster_ids['in_pool'])\\n\\n                log.info('  %d cluster%s in pool %r (max. is %d)' % (\\n                    num_in_pool, _plural(num_in_pool), pool_name, max_in_pool))\\n\\n                if num_in_pool >= max_in_pool:\\n                    log.info('Checking again in %d seconds...' % (\\n                        _POOLING_SLEEP_INTERVAL))\\n                    sleep_or_time_out(_POOLING_SLEEP_INTERVAL)\\n                    continue\\n\\n            # to avoid race conditions, double-check the clusters in the pool\\n            # if we need to satisfy max_clusters_in_pool or are trying to\\n            # bypass pool_wait_minutes\\n            if max_in_pool or (keep_waiting and not cluster_ids['matching']):\\n                jitter_seconds = randint(0, self._opts['pool_jitter_seconds'])\\n\\n                log.info('  waiting %d seconds and double-checking for'\\n                         ' newly created clusters...' % jitter_seconds)\\n                sleep_or_time_out(jitter_seconds)\\n\\n                new_cluster_ids = self._list_cluster_ids_for_pooling(\\n                    created_after=cluster_ids['max_created'])\\n\\n                new_num_in_pool = len(\\n                    cluster_ids['in_pool'] | new_cluster_ids['in_pool'])\\n\\n                log.info('    %d cluster%s in pool' % (\\n                    new_num_in_pool, _plural(new_num_in_pool)))\\n\\n                if ((not max_in_pool or new_num_in_pool < max_in_pool) and\\n                        (not keep_waiting or not new_cluster_ids['matching'])):\\n\\n                    # allow creating a new cluster\\n                    return None, None\\n\\n                log.info('Checking again in %d seconds...' % (\\n                    _POOLING_SLEEP_INTERVAL))\\n                sleep_or_time_out(_POOLING_SLEEP_INTERVAL)\\n\\n                continue\\n\\n            # pool_wait_minutes is exhausted and max_clusters_in_pool is not\\n            # set, so create a new cluster\\n            return None, None\\n\\n        # (defensive programming, in case we break out of the loop)\\n        return None, None",
                    "func_fullName": "mrjob.emr._find_cluster( self )"
                },
                {
                    "func_id": 1317,
                    "func_name": "_cluster_name_pooling_suffix",
                    "func_desc": "_cluster_name_pooling_suffix",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cluster_name_pooling_suffix(self):\\n        \"\"\"Extra info added to the cluster name, for pooling.\"\"\"\\n        if not self._opts['pool_clusters']:\\n            return ''\\n        else:\\n            return _cluster_name_suffix(\\n                self._pool_hash(), self._opts['pool_name'])",
                    "func_fullName": "mrjob.emr._cluster_name_pooling_suffix( self )"
                },
                {
                    "func_id": 1319,
                    "func_name": "_describe_cluster",
                    "func_desc": "_describe_cluster",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _describe_cluster(self):\\n        emr_client = self.make_emr_client()\\n        return emr_client.describe_cluster(\\n            ClusterId=self._cluster_id)['Cluster']",
                    "func_fullName": "mrjob.emr._describe_cluster( self )"
                },
                {
                    "func_id": 1326,
                    "func_name": "_get_cluster_info",
                    "func_desc": "_get_cluster_info",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_cluster_info(self, key):\\n        if not self._cluster_id:\\n            return None\\n\\n        cache = self._cluster_to_cache[self._cluster_id]\\n\\n        if not cache.get(key):\\n            if key == 'master_private_ip':\\n                self._store_master_instance_info()\\n            else:\\n                self._store_cluster_info()\\n\\n        return cache.get(key)",
                    "func_fullName": "mrjob.emr._get_cluster_info( self, key )"
                },
                {
                    "func_id": 1327,
                    "func_name": "_store_cluster_info",
                    "func_desc": "_store_cluster_info",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _store_cluster_info(self):\\n        \"\"\"Describe our cluster, and cache image_version, hadoop_version,\\n        and master_public_dns\"\"\"\\n        if not self._cluster_id:\\n            raise ValueError('cluster has not yet been created')\\n\\n        cache = self._cluster_to_cache[self._cluster_id]\\n\\n        cluster = self._describe_cluster()\\n\\n        # AMI version might be in RunningAMIVersion (2.x, 3.x)\\n        # or ReleaseLabel (4.x)\\n        cache['image_version'] = cluster.get('RunningAmiVersion')\\n        if not cache['image_version']:\\n            release_label = cluster.get('ReleaseLabel')\\n            if release_label:\\n                cache['image_version'] = release_label.lstrip('emr-')\\n\\n        cache['app_versions'] = dict(\\n            (a['Name'].lower(), a.get('Version'))\\n            for a in cluster['Applications'])\\n\\n        cache['collection_type'] = cluster.get(\\n            'InstanceCollectionType', 'INSTANCE_GROUP')\\n\\n        if cluster['Status']['State'] in ('RUNNING', 'WAITING'):\\n            cache['master_public_dns'] = cluster['MasterPublicDnsName']",
                    "func_fullName": "mrjob.emr._store_cluster_info( self )"
                },
                {
                    "func_id": 1380,
                    "func_name": "_create_cluster",
                    "func_desc": "_create_cluster",
                    "func_file": "cmd",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _create_cluster(args):\\n    from mrjob.tools.emr.create_cluster import main\\n    main(args)",
                    "func_fullName": "mrjob.cmd._create_cluster( args )"
                },
                {
                    "func_id": 1386,
                    "func_name": "_terminate_idle_clusters",
                    "func_desc": "_terminate_idle_clusters",
                    "func_file": "cmd",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _terminate_idle_clusters(args):\\n    from mrjob.tools.emr.terminate_idle_clusters import main\\n    main(args)",
                    "func_fullName": "mrjob.cmd._terminate_idle_clusters( args )"
                },
                {
                    "func_id": 1387,
                    "func_name": "_terminate_cluster",
                    "func_desc": "_terminate_cluster",
                    "func_file": "cmd",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _terminate_cluster(args):\\n    from mrjob.tools.emr.terminate_cluster import main\\n    main(args)",
                    "func_fullName": "mrjob.cmd._terminate_cluster( args )"
                },
                {
                    "func_id": 1855,
                    "func_name": "_clusters_to_stats",
                    "func_desc": "_clusters_to_stats",
                    "func_file": "audit_usage",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _clusters_to_stats(clusters, now=None):\\n    r\"\"\"Aggregate statistics for several clusters into a dictionary.\\n\\n    :param clusters: a sequence of dicts with the keys ``cluster``, ``steps``.\\n    :param now: the current UTC time, as a :py:class:`datetime.datetime`.\\n                Defaults to the current time.\\n\\n    Returns a dictionary with many keys, including:\\n\\n    * *summaries*: A list of dictionaries; the result of running\\n      :py:func:`_cluster_to_full_summary` on each cluster.\\n\\n    total usage:\\n\\n    * *nih_billed*: total normalized instances hours billed, for all clusters\\n    * *nih_used*: total normalized instance hours actually used for\\n      bootstrapping and running jobs.\\n    * *nih_bbnu*: total usage billed but not used (`nih_billed - nih_used`)\\n\\n    further breakdown of total usage:\\n\\n    * *bootstrap_nih_used*: total usage for bootstrapping\\n    * *end_nih_bbnu*: unused time at the end of clusters\\n    * *job_nih_used*: total usage for jobs (`nih_used - bootstrap_nih_used`)\\n    * *other_nih_bbnu*: other unused time (`nih_bbnu - end_nih_bbnu`)\\n\\n    grouping by various keys:\\n\\n    (There is a *_used*, *_billed*, and *_bbnu* version of all stats below)\\n\\n    * *date_to_nih_\\**: map from a :py:class:`datetime.date` to number\\n      of normalized instance hours on that date\\n    * *hour_to_nih_\\**: map from a :py:class:`datetime.datetime` to number\\n      of normalized instance hours during the hour starting at that time\\n    * *label_to_nih_\\**: map from jobs' labels (usually the module name of\\n      the job) to normalized instance hours, with ``None`` for\\n      non-:py:mod:`mrjob` jobs. This includes usage data for bootstrapping.\\n    * *job_step_to_nih_\\**: map from jobs' labels and step number to\\n      normalized instance hours, using ``(None, None)`` for non-:py:mod:`mrjob`\\n      jobs. This does not include bootstrapping.\\n    * *job_step_to_nih_\\*_no_pool*: Same as *job_step_to_nih_\\**, but only\\n      including non-pooled clusters.\\n    * *owner_to_nih_\\**: map from jobs' owners (usually the user who ran them)\\n      to normalized instance hours, with ``None`` for non-:py:mod:`mrjob` jobs.\\n      This includes usage data for bootstrapping.\\n    * *pool_to_nih_\\**: Map from pool name to normalized instance hours,\\n      with ``None`` for non-pooled jobs and non-:py:mod:`mrjob` jobs.\\n    \"\"\"\\n    s = {}  # stats for all clusters\\n\\n    s['clusters'] = [_cluster_to_full_summary(cluster, now=now)\\n                     for cluster in clusters]\\n\\n    # from here on out, we only process s['clusters']\\n\\n    # total usage\\n    for nih_type in ('nih_billed', 'nih_used', 'nih_bbnu'):\\n        s[nih_type] = float(sum(\\n            cs[nih_type] for cs in s['clusters']))\\n\\n    # break down by usage/waste\\n    s['bootstrap_nih_used'] = float(sum(\\n        cs['usage'][0]['nih_used'] for cs in s['clusters']\\n        if cs['usage']))\\n    s['job_nih_used'] = s['nih_used'] - s['bootstrap_nih_used']\\n    s['end_nih_bbnu'] = float(sum(\\n        cs['usage'][-1]['nih_bbnu'] for cs in s['clusters']\\n        if cs['usage']))\\n    s['other_nih_bbnu'] = s['nih_bbnu'] - s['end_nih_bbnu']\\n\\n    # stats by date/hour\\n    for interval_type in ('date', 'hour'):\\n        for nih_type in ('nih_billed', 'nih_used', 'nih_bbnu'):\\n            key = '%s_to_%s' % (interval_type, nih_type)\\n            start_to_nih = {}\\n            for cs in s['clusters']:\\n                for u in cs['usage']:\\n                    for start, nih in u[key].items():\\n                        start_to_nih.setdefault(start, 0.0)\\n                        start_to_nih[start] += nih\\n            s[key] = start_to_nih\\n\\n    # break out by label (usually script name) and owner (usually current user)\\n    for key in ('label', 'owner'):\\n        for nih_type in ('nih_used', 'nih_billed', 'nih_bbnu'):\\n            key_to_nih = {}\\n            for cs in s['clusters']:\\n                for u in cs['usage']:\\n                    key_to_nih.setdefault(u[key], 0.0)\\n                    key_to_nih[u[key]] += u[nih_type]\\n            s['%s_to_%s' % (key, nih_type)] = key_to_nih\\n\\n    # break down by job step. separate out un-pooled jobs\\n    for nih_type in ('nih_used', 'nih_billed', 'nih_bbnu'):\\n        job_step_to_nih = {}\\n        job_step_to_nih_no_pool = {}\\n        for cs in s['clusters']:\\n            for u in cs['usage'][1:]:\\n                job_step = (u['label'], u['step_num'])\\n                job_step_to_nih.setdefault(job_step, 0.0)\\n                job_step_to_nih[job_step] += u[nih_type]\\n                if not cs['pool']:\\n                    job_step_to_nih_no_pool.setdefault(job_step, 0.0)\\n                    job_step_to_nih_no_pool[job_step] += u[nih_type]\\n\\n            s['job_step_to_%s' % nih_type] = job_step_to_nih\\n            s['job_step_to_%s_no_pool' % nih_type] = job_step_to_nih_no_pool\\n\\n    # break down by pool\\n    for nih_type in ('nih_used', 'nih_billed', 'nih_bbnu'):\\n        pool_to_nih = {}\\n        for cs in s['clusters']:\\n            pool_to_nih.setdefault(cs['pool'], 0.0)\\n            pool_to_nih[cs['pool']] += cs[nih_type]\\n\\n        s['pool_to_%s' % nih_type] = pool_to_nih\\n\\n    return s",
                    "func_fullName": "mrjob.tools.emr.audit_usage._clusters_to_stats( clusters, now )"
                },
                {
                    "func_id": 1856,
                    "func_name": "_cluster_to_full_summary",
                    "func_desc": "_cluster_to_full_summary",
                    "func_file": "audit_usage",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _cluster_to_full_summary(cluster, now=None):\\n    \"\"\"Convert a cluster to a full summary for use in creating a report,\\n    including billing/usage information.\\n\\n    :param cluster: a :py:mod:`boto3` cluster data structure\\n    :param now: the current UTC time, as a :py:class:`datetime.datetime`.\\n                Defaults to the current time.\\n\\n    Returns a dictionary with the keys from\\n    :py:func:`cluster_to_basic_summary` plus:\\n\\n    * *nih_billed*: total normalized instances hours billed for this cluster\\n    * *nih_used*: total normalized instance hours actually used for\\n      bootstrapping and running jobs.\\n    * *nih_bbnu*: total usage billed but not used (`nih_billed - nih_used`)\\n    * *usage*: job-specific usage information, returned by\\n      :py:func:`_cluster_to_usage_data`.\\n    \"\"\"\\n    cs = _cluster_to_basic_summary(cluster, now=now)\\n\\n    cs['usage'] = _cluster_to_usage_data(\\n        cluster, basic_summary=cs, now=now)\\n\\n    # add up billing info\\n    cs['nih_billed'] = float(sum(u['nih_billed'] for u in cs['usage']))\\n\\n    for nih_type in ('nih_used', 'nih_bbnu'):\\n        cs[nih_type] = float(sum(u[nih_type] for u in cs['usage']))\\n\\n    return cs",
                    "func_fullName": "mrjob.tools.emr.audit_usage._cluster_to_full_summary( cluster, now )"
                },
                {
                    "func_id": 1857,
                    "func_name": "_cluster_to_basic_summary",
                    "func_desc": "_cluster_to_basic_summary",
                    "func_file": "audit_usage",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _cluster_to_basic_summary(cluster, now=None):\\n    \"\"\"Extract fields such as creation time, owner, etc. from the cluster.\\n\\n    :param cluster: a :py:mod:`boto3` cluster data structure\\n    :param now: the current UTC time, as a :py:class:`datetime.datetime`.\\n                Defaults to the current time.\\n\\n    Returns a dictionary with the following keys. These will be ``None`` if the\\n    corresponding field in the cluster is unavailable.\\n\\n    * *created*: UTC `datetime.datetime` that the cluster was created,\\n      or ``None``\\n    * *end*: UTC `datetime.datetime` that the cluster finished, or ``None``\\n    * *id*: cluster ID, or ``None`` (this should never happen)\\n    * *label*: The label for the cluster (usually the module name of the\\n      :py:class:`~mrjob.job.MRJob` script that started it), or\\n      ``None`` for non-:py:mod:`mrjob` clusters.\\n    * *name*: cluster name, or ``None`` (this should never happen)\\n    * *nih*: number of normalized instance hours cluster *would* use if it\\n      ran to the end of the next full hour (\\n    * *num_steps*: Number of steps in the cluster.\\n    * *owner*: The owner for the cluster (usually the user that started it),\\n      or ``None`` for non-:py:mod:`mrjob` clusters.\\n    * *pool*: pool name (e.g. ``'default'``) if the cluster is pooled,\\n      otherwise ``None``.\\n    * *ran*: How long the cluster ran, or has been running, as a\\n      :py:class:`datetime.timedelta`. This will be ``timedelta(0)`` if\\n      the cluster hasn't started.\\n    * *ready*: UTC `datetime.datetime` that the cluster finished\\n      bootstrapping, or ``None``\\n    * *state*: The cluster's state as a string (e.g. ``'RUNNING'``)\\n    \"\"\"\\n    if now is None:\\n        now = _boto3_now()\\n\\n    bcs = {}  # basic cluster summary to fill in\\n\\n    bcs['id'] = cluster['Id']\\n    bcs['name'] = cluster['Name']\\n\\n    Status = cluster['Status']\\n    Timeline = Status.get('Timeline', {})\\n\\n    bcs['created'] = Timeline.get('CreationDateTime')\\n    bcs['ready'] = Timeline.get('ReadyDateTime')\\n    bcs['end'] = Timeline.get('EndDateTime')\\n\\n    if bcs['created']:\\n        bcs['ran'] = (bcs['end'] or now) - bcs['created']\\n    else:\\n        bcs['ran'] = timedelta(0)\\n\\n    bcs['state'] = Status.get('State')\\n\\n    bcs['num_steps'] = len(cluster['Steps'])\\n\\n    bcs['pool'] = _pool_name(cluster)\\n\\n    m = _JOB_KEY_RE.match(bcs['name'] or '')\\n    if m:\\n        bcs['label'], bcs['owner'] = m.group(1), m.group(2)\\n    else:\\n        bcs['label'], bcs['owner'] = None, None\\n\\n    bcs['nih'] = float(cluster.get('NormalizedInstanceHours', 0))\\n\\n    return bcs",
                    "func_fullName": "mrjob.tools.emr.audit_usage._cluster_to_basic_summary( cluster, now )"
                },
                {
                    "func_id": 1861,
                    "func_name": "_yield_clusters",
                    "func_desc": "_yield_clusters",
                    "func_file": "audit_usage",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _yield_clusters(max_days_ago=None, now=None, **runner_kwargs):\\n    \"\"\"Get relevant cluster information from EMR.\\n\\n    :param float max_days_ago: If set, don't fetch clusters created longer\\n                               than this many days ago.\\n    :param now: the current UTC time, as a :py:class:`datetime.datetime`.\\n                Defaults to the current time.\\n    :param runner_kwargs: keyword args to pass through to\\n                          :py:class:`~mrjob.emr.EMRJobRunner`\\n    \"\"\"\\n    if now is None:\\n        now = _boto3_now()\\n\\n    emr_client = EMRJobRunner(**runner_kwargs).make_emr_client()\\n\\n    # if --max-days-ago is set, only look at recent jobs\\n    created_after = None\\n    if max_days_ago is not None:\\n        created_after = now - timedelta(days=max_days_ago)\\n\\n    # use _DELAY to sleep 1 second after each API call (see #1091). Could\\n    # implement some sort of connection wrapper for this if it becomes more\\n    # generally useful.\\n    list_clusters_kwargs = dict(_delay=_DELAY)\\n    if created_after is not None:\\n        list_clusters_kwargs['CreatedAfter'] = created_after\\n\\n    for cluster_summary in _boto3_paginate(\\n            'Clusters', emr_client, 'list_clusters', **list_clusters_kwargs):\\n\\n        cluster_id = cluster_summary['Id']\\n\\n        cluster = emr_client.describe_cluster(ClusterId=cluster_id)['Cluster']\\n        sleep(_DELAY)\\n\\n        cluster['Steps'] = list(reversed(list(_boto3_paginate(\\n            'Steps', emr_client, 'list_steps',\\n            ClusterId=cluster_id, _delay=_DELAY))))\\n\\n        yield cluster",
                    "func_fullName": "mrjob.tools.emr.audit_usage._yield_clusters( max_days_ago, now, **runner_kwargs )"
                },
                {
                    "func_id": 1876,
                    "func_name": "_filter_clusters",
                    "func_desc": "_filter_clusters",
                    "func_file": "report_long_jobs",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _filter_clusters(cluster_summaries, emr_client, exclude_strings):\\n    \"\"\" Filter out clusters that have tags matching any specified in\\n    exclude_strings.\\n    :param cluster_summaries: a list of :py:mod:`boto3` cluster summary data\\n                              structures\\n    :param exclude_strings: A list of strings of the form TAG_KEY,TAG_VALUE\\n    \"\"\"\\n    exclude_as_dicts = []\\n    for exclude_string in exclude_strings:\\n        exclude_key, exclude_value = exclude_string.split(',')\\n        exclude_as_dicts.append({'Key': exclude_key, 'Value': exclude_value})\\n\\n    for cs in cluster_summaries:\\n        cluster_id = cs['Id']\\n        cluster_tags = emr_client.describe_cluster(\\n            ClusterId=cluster_id)['Cluster']['Tags']\\n        for cluster_tag in cluster_tags:\\n            if cluster_tag in exclude_as_dicts:\\n                break\\n        else:\\n            yield cs",
                    "func_fullName": "mrjob.tools.emr.report_long_jobs._filter_clusters( cluster_summaries, emr_client, exclude_strings )"
                },
                {
                    "func_id": 1888,
                    "func_name": "_maybe_terminate_clusters",
                    "func_desc": "_maybe_terminate_clusters",
                    "func_file": "terminate_idle_clusters",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _maybe_terminate_clusters(dry_run=False,\\n                              max_mins_idle=None,\\n                              now=None,\\n                              pool_name=None,\\n                              pooled_only=False,\\n                              unpooled_only=False,\\n                              quiet=False,\\n                              **kwargs):\\n    if now is None:\\n        now = _boto3_now()\\n\\n    # old default behavior\\n    if max_mins_idle is None:\\n        max_mins_idle = _DEFAULT_MAX_MINS_IDLE\\n\\n    runner = EMRJobRunner(**kwargs)\\n    emr_client = runner.make_emr_client()\\n\\n    num_starting = 0\\n    num_bootstrapping = 0\\n    num_done = 0\\n    num_idle = 0\\n    num_pending = 0\\n    num_running = 0\\n\\n    # include RUNNING to catch clusters with PENDING jobs that\\n    # never ran (see #365).\\n    for cluster_summary in _boto3_paginate(\\n            'Clusters', emr_client, 'list_clusters',\\n            ClusterStates=['WAITING', 'RUNNING']):\\n\\n        cluster_id = cluster_summary['Id']\\n\\n        # check if cluster is done\\n        if _is_cluster_done(cluster_summary):\\n            num_done += 1\\n            continue\\n\\n        # check if cluster is starting\\n        if _is_cluster_starting(cluster_summary):\\n            num_starting += 1\\n            continue\\n\\n        # check if cluster is bootstrapping\\n        if _is_cluster_bootstrapping(cluster_summary):\\n            num_bootstrapping += 1\\n            continue\\n\\n        # need steps to learn more about cluster\\n        steps = list(reversed(list(_boto3_paginate(\\n            'Steps', emr_client, 'list_steps',\\n            ClusterId=cluster_id))))\\n\\n        if any(_is_step_running(step) for step in steps):\\n            num_running += 1\\n            continue\\n\\n        # cluster is idle\\n        time_idle = now - _time_last_active(cluster_summary, steps)\\n        is_pending = _cluster_has_pending_steps(steps)\\n\\n        # need to get actual cluster to see tags\\n        cluster = emr_client.describe_cluster(ClusterId=cluster_id)['Cluster']\\n\\n        pool = _pool_name(cluster)\\n\\n        if is_pending:\\n            num_pending += 1\\n        else:\\n            num_idle += 1\\n\\n        log.debug(\\n            'cluster %s %s for %s, %s (%s) - %s' %\\n            (cluster_id,\\n             'pending' if is_pending else 'idle',\\n             strip_microseconds(time_idle),\\n             ('unpooled' if pool is None else 'in %s pool' % pool),\\n             cluster_summary['Name'],\\n             'protected' if cluster['TerminationProtected'] else 'unprotected',\\n             ))\\n\\n        # filter out clusters that don't meet our criteria\\n        if (max_mins_idle is not None and\\n                time_idle <= timedelta(minutes=max_mins_idle)):\\n            continue\\n\\n        if (pooled_only and pool is None):\\n            continue\\n\\n        if (unpooled_only and pool is not None):\\n            continue\\n\\n        if (pool_name is not None and pool != pool_name):\\n            continue\\n\\n        if cluster['TerminationProtected']:\\n            continue\\n\\n        # terminate idle cluster\\n        _terminate_and_notify(\\n            runner=runner,\\n            cluster_id=cluster_id,\\n            cluster_name=cluster_summary['Name'],\\n            num_steps=len(steps),\\n            is_pending=is_pending,\\n            time_idle=time_idle,\\n            dry_run=dry_run,\\n            quiet=quiet)\\n\\n    log.info(\\n        'Cluster statuses: %d starting, %d bootstrapping, %d running,'\\n        ' %d pending, %d idle, %d done' % (\\n            num_starting, num_bootstrapping, num_running,\\n            num_pending, num_idle, num_done))",
                    "func_fullName": "mrjob.tools.emr.terminate_idle_clusters._maybe_terminate_clusters( dry_run, max_mins_idle, now, pool_name, pooled_only, unpooled_only, quiet, **kwargs )"
                },
                {
                    "func_id": 1889,
                    "func_name": "_is_cluster_done",
                    "func_desc": "_is_cluster_done",
                    "func_file": "terminate_idle_clusters",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _is_cluster_done(cluster):\\n    \"\"\"Return True if the given cluster is done running.\"\"\"\\n    return bool(cluster['Status']['State'] == 'TERMINATING' or\\n                cluster['Status']['Timeline'].get('EndDateTime'))",
                    "func_fullName": "mrjob.tools.emr.terminate_idle_clusters._is_cluster_done( cluster )"
                },
                {
                    "func_id": 1890,
                    "func_name": "_is_cluster_starting",
                    "func_desc": "_is_cluster_starting",
                    "func_file": "terminate_idle_clusters",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _is_cluster_starting(cluster_summary):\\n    return cluster_summary['Status']['State'] == 'STARTING'",
                    "func_fullName": "mrjob.tools.emr.terminate_idle_clusters._is_cluster_starting( cluster_summary )"
                },
                {
                    "func_id": 1891,
                    "func_name": "_is_cluster_bootstrapping",
                    "func_desc": "_is_cluster_bootstrapping",
                    "func_file": "terminate_idle_clusters",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _is_cluster_bootstrapping(cluster_summary):\\n    \"\"\"Return ``True`` if *cluster_summary* is currently bootstrapping.\"\"\"\\n    return (cluster_summary['Status']['State'] != 'STARTING' and\\n            not cluster_summary['Status']['Timeline'].get('ReadyDateTime'))",
                    "func_fullName": "mrjob.tools.emr.terminate_idle_clusters._is_cluster_bootstrapping( cluster_summary )"
                },
                {
                    "func_id": 1892,
                    "func_name": "_is_cluster_running",
                    "func_desc": "_is_cluster_running",
                    "func_file": "terminate_idle_clusters",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _is_cluster_running(steps):\\n    return any(_is_step_running(step) for step in steps)",
                    "func_fullName": "mrjob.tools.emr.terminate_idle_clusters._is_cluster_running( steps )"
                },
                {
                    "func_id": 1894,
                    "func_name": "_cluster_has_pending_steps",
                    "func_desc": "_cluster_has_pending_steps",
                    "func_file": "terminate_idle_clusters",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _cluster_has_pending_steps(steps):\\n    \"\"\"Does *cluster* have any steps in the ``PENDING`` state?\"\"\"\\n    return any(step['Status']['State'] == 'PENDING' for step in steps)",
                    "func_fullName": "mrjob.tools.emr.terminate_idle_clusters._cluster_has_pending_steps( steps )"
                }
            ]
        },
        {
            "cluster_id": 1,
            "feature_id": 56,
            "feature_desc": "gamma=0.0608; k=17; a=0.25; combined=0.388; stability(ARI)=0.911; sep=0.150",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1257,
                    "func_name": "_streaming_step_hadoop_jar_step",
                    "func_desc": "_streaming_step_hadoop_jar_step",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _streaming_step_hadoop_jar_step(self, step_num):\\n        jar, step_arg_prefix = self._get_streaming_jar_and_step_arg_prefix()\\n\\n        args = (step_arg_prefix +\\n                self._hadoop_streaming_jar_args(step_num))\\n\\n        return dict(Jar=jar, Args=args)",
                    "func_fullName": "mrjob.emr._streaming_step_hadoop_jar_step( self, step_num )"
                },
                {
                    "func_id": 1258,
                    "func_name": "_jar_step_hadoop_jar_step",
                    "func_desc": "_jar_step_hadoop_jar_step",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _jar_step_hadoop_jar_step(self, step_num):\\n        step = self._get_step(step_num)\\n\\n        jar = self._upload_uri_or_remote_path(step['jar'])\\n\\n        args = (\\n            self._interpolate_jar_step_args(step['args'], step_num))\\n\\n        hadoop_jar_step = dict(Jar=jar, Args=args)\\n\\n        if step.get('main_class'):\\n            hadoop_jar_step['MainClass'] = step['main_class']\\n\\n        return hadoop_jar_step",
                    "func_fullName": "mrjob.emr._jar_step_hadoop_jar_step( self, step_num )"
                },
                {
                    "func_id": 1259,
                    "func_name": "_spark_step_hadoop_jar_step",
                    "func_desc": "_spark_step_hadoop_jar_step",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_step_hadoop_jar_step(self, step_num):\\n        return dict(\\n            Jar=self._spark_jar(),\\n            Args=self._args_for_spark_step(step_num))",
                    "func_fullName": "mrjob.emr._spark_step_hadoop_jar_step( self, step_num )"
                },
                {
                    "func_id": 1260,
                    "func_name": "_interpolate_spark_script_path",
                    "func_desc": "_interpolate_spark_script_path",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _interpolate_spark_script_path(self, path):\\n        if path in self._working_dir_mgr.paths():\\n            return self._dest_in_wd_mirror(\\n                path, self._working_dir_mgr.name('file', path)) or path\\n        else:\\n            return self._upload_mgr.uri(path)",
                    "func_fullName": "mrjob.emr._interpolate_spark_script_path( self, path )"
                },
                {
                    "func_id": 1261,
                    "func_name": "_find_spark_submit_bin",
                    "func_desc": "_find_spark_submit_bin",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _find_spark_submit_bin(self):\\n        if version_gte(self.get_image_version(), '4'):\\n            return ['spark-submit']\\n        else:\\n            return [_3_X_SPARK_SUBMIT]",
                    "func_fullName": "mrjob.emr._find_spark_submit_bin( self )"
                },
                {
                    "func_id": 1262,
                    "func_name": "_spark_master",
                    "func_desc": "_spark_master",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_master(self):\\n        # hard-coded for EMR\\n        return 'yarn'",
                    "func_fullName": "mrjob.emr._spark_master( self )"
                },
                {
                    "func_id": 1263,
                    "func_name": "_spark_deploy_mode",
                    "func_desc": "_spark_deploy_mode",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_deploy_mode(self):\\n        # hard-coded for EMR; otherwise it can't access S3\\n        return 'cluster'",
                    "func_fullName": "mrjob.emr._spark_deploy_mode( self )"
                },
                {
                    "func_id": 1264,
                    "func_name": "_spark_jar",
                    "func_desc": "_spark_jar",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _spark_jar(self):\\n        if version_gte(self.get_image_version(), '4'):\\n            return _4_X_COMMAND_RUNNER_JAR\\n        else:\\n            return self._script_runner_jar_uri()",
                    "func_fullName": "mrjob.emr._spark_jar( self )"
                },
                {
                    "func_id": 1268,
                    "func_name": "_libjar_paths",
                    "func_desc": "_libjar_paths",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _libjar_paths(self):\\n        results = []\\n\\n        # libjars should be in the working dir of the master node setup\\n        # script path, unless they refer to paths directly (file:///)\\n        for path in self._opts['libjars']:\\n            if path.startswith('file:///'):\\n                results.append(path[7:])  # keep leading slash\\n            else:\\n                results.append(posixpath.join(\\n                    self._master_node_setup_working_dir(),\\n                    self._master_node_setup_mgr.name('file', path)))\\n\\n        return results",
                    "func_fullName": "mrjob.emr._libjar_paths( self )"
                },
                {
                    "func_id": 1297,
                    "func_name": "_should_bootstrap_spark",
                    "func_desc": "_should_bootstrap_spark",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _should_bootstrap_spark(self):\\n        \"\"\"Return *bootstrap_spark* option if set; otherwise return\\n        true if our job has Spark steps.\"\"\"\\n        if self._opts['bootstrap_spark'] is None:\\n            return self._has_spark_steps()\\n        else:\\n            return bool(self._opts['bootstrap_spark'])",
                    "func_fullName": "mrjob.emr._should_bootstrap_spark( self )"
                },
                {
                    "func_id": 1320,
                    "func_name": "get_hadoop_version",
                    "func_desc": "get_hadoop_version",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_hadoop_version(self):\\n        return self._get_app_versions().get('hadoop')",
                    "func_fullName": "mrjob.emr.get_hadoop_version( self )"
                },
                {
                    "func_id": 1331,
                    "func_name": "_uses_spark",
                    "func_desc": "_uses_spark",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _uses_spark(self):\\n        \"\"\"Does this runner use Spark, based on steps, bootstrap actions,\\n        and EMR applications? If so, we'll need more memory.\"\"\"\\n        return (self._has_spark_steps() or\\n                self._has_spark_install_bootstrap_action() or\\n                self._has_spark_application() or\\n                self._opts['bootstrap_spark'])",
                    "func_fullName": "mrjob.emr._uses_spark( self )"
                },
                {
                    "func_id": 1332,
                    "func_name": "_has_spark_install_bootstrap_action",
                    "func_desc": "_has_spark_install_bootstrap_action",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _has_spark_install_bootstrap_action(self):\\n        \"\"\"Does it look like this runner has a spark bootstrap install\\n        action set? (Anything ending in \"/install-spark\" counts.)\"\"\"\\n        return any(ba['path'].endswith('/install-spark')\\n                   for ba in self._bootstrap_actions(add_spark=False))",
                    "func_fullName": "mrjob.emr._has_spark_install_bootstrap_action( self )"
                },
                {
                    "func_id": 1333,
                    "func_name": "_has_spark_application",
                    "func_desc": "_has_spark_application",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _has_spark_application(self):\\n        \"\"\"Does this runner have \"Spark\" in its *applications* option?\"\"\"\\n        return any(a.lower() == 'spark'\\n                   for a in self._applications(add_spark=False))",
                    "func_fullName": "mrjob.emr._has_spark_application( self )"
                },
                {
                    "func_id": 1334,
                    "func_name": "_check_cluster_spark_support",
                    "func_desc": "_check_cluster_spark_support",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _check_cluster_spark_support(self):\\n        \"\"\"Issue a warning if our cluster doesn't support Spark.\\n\\n        This should only be called if you are going to run one or more\\n        Spark steps.\\n        \"\"\"\\n        message = self._cluster_spark_support_warning()\\n        if message:\\n            log.warning(message)",
                    "func_fullName": "mrjob.emr._check_cluster_spark_support( self )"
                },
                {
                    "func_id": 1335,
                    "func_name": "_cluster_spark_support_warning",
                    "func_desc": "_cluster_spark_support_warning",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cluster_spark_support_warning(self):\\n        \"\"\"Helper for _check_cluster_spark_support().\"\"\"\\n        image_version = self.get_image_version()\\n\\n        if not version_gte(image_version, _MIN_SPARK_AMI_VERSION):\\n            suggested_version = (\\n                _MIN_SPARK_AMI_VERSION if PY2 else _MIN_SPARK_PY3_AMI_VERSION)\\n            return ('  AMI version %s does not support Spark;\\n'\\n                    '  (try --image-version %s or later)' % (\\n                        image_version, suggested_version))\\n\\n        if not version_gte(image_version, _MIN_SPARK_PY3_AMI_VERSION):\\n            if PY2:\\n                # even though this version of Spark \"works\" with Python 2,\\n                # it doesn't work well\\n                return ('  AMI version %s has an old version of Spark\\n'\\n                        ' and does not correctly determine when a Spark'\\n                        ' job has failed\\n'\\n                        'Try --image-version %s or later)' % (\\n                            image_version, _MIN_SPARK_PY3_AMI_VERSION))\\n            else:\\n                # this version of Spark doesn't support Python 3 at all!\\n                return ('  AMI version %s does not support Python 3 on Spark\\n'\\n                        '  (try --image-version %s or later)' % (\\n                            image_version, _MIN_SPARK_PY3_AMI_VERSION))\\n\\n        emr_client = self.make_emr_client()\\n\\n        too_small_msg = ('  instance type %s is too small for Spark;'\\n                         ' your job may stall forever')\\n\\n        if self._get_collection_type() == 'INSTANCE_FLEET':\\n            fleets = list(_boto3_paginate(\\n                'InstanceFleets', emr_client, 'list_instance_fleets',\\n                ClusterId=self.get_cluster_id()))\\n\\n            for fleet in fleets:\\n                # master doesn't matter if it's not running tasks\\n                if fleet['InstanceFleetType'] == 'MASTER' and len(fleets) > 1:\\n                    continue\\n\\n                for spec in fleet['InstanceTypeSpecifications']:\\n                    mem = EC2_INSTANCE_TYPE_TO_MEMORY.get(spec['InstanceType'])\\n                    if mem and mem < _MIN_SPARK_INSTANCE_MEMORY:\\n                        return (too_small_msg % spec['InstanceType'])\\n        else:\\n            # instance groups\\n            igs = list(_boto3_paginate(\\n                'InstanceGroups', emr_client, 'list_instance_groups',\\n                ClusterId=self.get_cluster_id()))\\n\\n            for ig in igs:\\n                # master doesn't matter if it's not running tasks\\n                if ig['InstanceGroupType'] == 'MASTER' and len(igs) > 1:\\n                    continue\\n\\n                mem = EC2_INSTANCE_TYPE_TO_MEMORY.get(ig['InstanceType'])\\n                if mem and mem < _MIN_SPARK_INSTANCE_MEMORY:\\n                    return (too_small_msg % ig['InstanceType'])\\n\\n        return None",
                    "func_fullName": "mrjob.emr._cluster_spark_support_warning( self )"
                },
                {
                    "func_id": 1385,
                    "func_name": "_spark_submit",
                    "func_desc": "_spark_submit",
                    "func_file": "cmd",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _spark_submit(args):\\n    from mrjob.tools.spark_submit import main\\n    main(args)",
                    "func_fullName": "mrjob.cmd._spark_submit( args )"
                }
            ]
        },
        {
            "cluster_id": 1,
            "feature_id": 57,
            "feature_desc": "gamma=0.0608; k=17; a=0.25; combined=0.388; stability(ARI)=0.911; sep=0.150",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1343,
                    "func_name": "sleep_or_time_out",
                    "func_desc": "sleep_or_time_out",
                    "func_file": "emr",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def sleep_or_time_out(seconds):\\n            if (timeout_mins and (\\n                    datetime.now() + timedelta(seconds=seconds) >\\n                    start + timedelta(minutes=timeout_mins))):\\n                raise PoolTimeoutException(\\n                    'Unable to join or create a cluster within %d minutes' %\\n                    timeout_mins)\\n\\n            time.sleep(seconds)",
                    "func_fullName": "mrjob.emr.sleep_or_time_out( seconds )"
                },
                {
                    "func_id": 1858,
                    "func_name": "_cluster_to_usage_data",
                    "func_desc": "_cluster_to_usage_data",
                    "func_file": "audit_usage",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _cluster_to_usage_data(cluster, basic_summary=None, now=None):\\n    r\"\"\"Break billing/usage information for a cluster down by job.\\n\\n    :param cluster: a :py:mod:`boto3` cluster data structure\\n    :param basic_summary: a basic summary of the cluster, returned by\\n                          :py:func:`_cluster_to_basic_summary`. If this\\n                          is ``None``, we'll call\\n                          :py:func:`_cluster_to_basic_summary` ourselves.\\n    :param now: the current UTC time, as a :py:class:`datetime.datetime`.\\n                Defaults to the current time.\\n\\n    Returns a list of dictionaries containing usage information, one for\\n    bootstrapping, and one for each step that ran or is currently running. If\\n    the cluster hasn't started yet, return ``[]``.\\n\\n    Usage dictionaries have the following keys:\\n\\n    * *end*: when the job finished running, or *now* if it's still running.\\n    * *end_billing*: the effective end of the job for billing purposes, either\\n      when the next job starts, the current time if the job\\n      is still running, or the end of the next full hour\\n      in the cluster.\\n    * *nih_billed*: normalized instances hours billed for this job or\\n      bootstrapping step\\n    * *nih_used*: normalized instance hours actually used for running\\n      the job or bootstrapping\\n    * *nih_bbnu*: usage billed but not used (`nih_billed - nih_used`)\\n    * *date_to_nih_\\**: map from a :py:class:`datetime.date` to number\\n      of normalized instance hours billed/used/billed but not used on that date\\n    * *hour_to_nih_\\**: map from a :py:class:`datetime.datetime` to number\\n      of normalized instance hours billed/used/billed but not used during\\n      the hour starting at that time\\n    * *label*: job's label (usually the module name of the job), or for the\\n      bootstrapping step, the label of the cluster\\n    * *owner*: job's owner (usually the user that started it), or for the\\n      bootstrapping step, the owner of the cluster\\n    * *start*: when the job or bootstrapping step started, as a\\n      :py:class:`datetime.datetime`\\n    \"\"\"\\n    bcs = basic_summary or _cluster_to_basic_summary(cluster)\\n\\n    if now is None:\\n        now = _boto3_now()\\n\\n    if not bcs['created']:\\n        return []\\n\\n    # EMR no longer bills by the full hour, but NormalizedInstanceHours\\n    # still works that way\\n    full_hours = math.ceil(timedelta.total_seconds(bcs['ran']) / 60.0 / 60.0)\\n    nih_per_sec = bcs['nih'] / (full_hours * 3600.0)\\n\\n    # EMR bills by the full second, and at least one minute per cluster\\n    cluster_end_billing = bcs['created'] + max(\\n        _round_up_to_next_second(bcs['ran']), timedelta(minutes=1))\\n\\n    intervals = []\\n\\n    # make a fake step for cluster startup and bootstrapping, so we don't\\n    # consider that wasted.\\n    intervals.append({\\n        'label': bcs['label'],\\n        'owner': bcs['owner'],\\n        'start': bcs['created'],\\n        'end': bcs['ready'] or bcs['end'] or now,\\n        'step_num': None,\\n    })\\n\\n    for step in cluster['Steps']:\\n        Status = step['Status']\\n        Timeline = Status.get('Timeline', {})\\n\\n        # we've reached the last step that's actually run\\n        if not Timeline.get('StartDateTime'):\\n            break\\n\\n        step_start = Timeline['StartDateTime']\\n\\n        step_end = Timeline.get('EndDateTime')\\n        if step_end is None:\\n            # step started running and was cancelled. credit it for 0 usage\\n            if bcs['end']:\\n                step_end = step_start\\n            # step is still running\\n            else:\\n                step_end = now\\n\\n        m = _STEP_NAME_RE.match(step['Name'])\\n        if m:\\n            step_label = m.group(1)\\n            step_owner = m.group(2)\\n            step_num = int(m.group(6))\\n        else:\\n            step_label, step_owner, step_num = None, None, None\\n\\n        intervals.append({\\n            'label': step_label,\\n            'owner': step_owner,\\n            'start': step_start,\\n            'end': step_end,\\n            'step_num': step_num,\\n        })\\n\\n    # fill in end_billing\\n    for i in range(len(intervals) - 1):\\n        intervals[i]['end_billing'] = intervals[i + 1]['start']\\n\\n    intervals[-1]['end_billing'] = cluster_end_billing\\n\\n    # fill normalized usage information\\n    for interval in intervals:\\n\\n        interval['nih_used'] = (\\n            nih_per_sec *\\n            timedelta.total_seconds(interval['end'] - interval['start']))\\n\\n        interval['date_to_nih_used'] = dict(\\n            (d, nih_per_sec * secs)\\n            for d, secs\\n            in _subdivide_interval_by_date(interval['start'],\\n                                           interval['end']).items())\\n\\n        interval['hour_to_nih_used'] = dict(\\n            (d, nih_per_sec * secs)\\n            for d, secs\\n            in _subdivide_interval_by_hour(interval['start'],\\n                                           interval['end']).items())\\n\\n        interval['nih_billed'] = (\\n            nih_per_sec * timedelta.total_seconds(\\n                interval['end_billing'] - interval['start']))\\n\\n        interval['date_to_nih_billed'] = dict(\\n            (d, nih_per_sec * secs)\\n            for d, secs\\n            in _subdivide_interval_by_date(interval['start'],\\n                                           interval['end_billing']).items())\\n\\n        interval['hour_to_nih_billed'] = dict(\\n            (d, nih_per_sec * secs)\\n            for d, secs\\n            in _subdivide_interval_by_hour(interval['start'],\\n                                           interval['end_billing']).items())\\n\\n        # time billed but not used\\n        interval['nih_bbnu'] = interval['nih_billed'] - interval['nih_used']\\n\\n        interval['date_to_nih_bbnu'] = {}\\n        for d, nih_billed in interval['date_to_nih_billed'].items():\\n            nih_bbnu = nih_billed - interval['date_to_nih_used'].get(d, 0.0)\\n            if nih_bbnu:\\n                interval['date_to_nih_bbnu'][d] = nih_bbnu\\n\\n        interval['hour_to_nih_bbnu'] = {}\\n        for d, nih_billed in interval['hour_to_nih_billed'].items():\\n            nih_bbnu = nih_billed - interval['hour_to_nih_used'].get(d, 0.0)\\n            if nih_bbnu:\\n                interval['hour_to_nih_bbnu'][d] = nih_bbnu\\n\\n    return intervals",
                    "func_fullName": "mrjob.tools.emr.audit_usage._cluster_to_usage_data( cluster, basic_summary, now )"
                },
                {
                    "func_id": 1859,
                    "func_name": "_subdivide_interval_by_date",
                    "func_desc": "_subdivide_interval_by_date",
                    "func_file": "audit_usage",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _subdivide_interval_by_date(start, end):\\n    \"\"\"Convert a time interval to a map from :py:class:`datetime.date` to\\n    the number of seconds within the interval on that date.\\n\\n    *start* and *end* are :py:class:`datetime.datetime` objects.\\n    \"\"\"\\n    if start.date() == end.date():\\n        date_to_secs = {start.date(): timedelta.total_seconds(end - start)}\\n    else:\\n        date_to_secs = {}\\n\\n        date_to_secs[start.date()] = timedelta.total_seconds(\\n            datetime(start.year, start.month, start.day, tzinfo=start.tzinfo) +\\n            timedelta(days=1) - start)\\n\\n        date_to_secs[end.date()] = timedelta.total_seconds(\\n            end - datetime(end.year, end.month, end.day, tzinfo=end.tzinfo))\\n\\n        # fill in dates in the middle\\n        cur_date = start.date() + timedelta(days=1)\\n        while cur_date < end.date():\\n            date_to_secs[cur_date] = timedelta.total_seconds(timedelta(days=1))\\n            cur_date += timedelta(days=1)\\n\\n    # remove zeros\\n    date_to_secs = dict(\\n        (d, secs) for d, secs in date_to_secs.items() if secs)\\n\\n    return date_to_secs",
                    "func_fullName": "mrjob.tools.emr.audit_usage._subdivide_interval_by_date( start, end )"
                },
                {
                    "func_id": 1860,
                    "func_name": "_subdivide_interval_by_hour",
                    "func_desc": "_subdivide_interval_by_hour",
                    "func_file": "audit_usage",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _subdivide_interval_by_hour(start, end):\\n    \"\"\"Convert a time interval to a map from hours (represented as\\n    :py:class:`datetime.datetime` for the start of the hour) to the number of\\n    seconds during that hour that are within the interval\\n\\n    *start* and *end* are :py:class:`datetime.datetime` objects.\\n    \"\"\"\\n    start_hour = start.replace(minute=0, second=0, microsecond=0)\\n    end_hour = end.replace(minute=0, second=0, microsecond=0)\\n\\n    if start_hour == end_hour:\\n        hour_to_secs = {start_hour: timedelta.total_seconds(end - start)}\\n    else:\\n        hour_to_secs = {}\\n\\n        hour_to_secs[start_hour] = timedelta.total_seconds(\\n            start_hour + timedelta(hours=1) - start)\\n\\n        hour_to_secs[end_hour] = timedelta.total_seconds(end - end_hour)\\n\\n        # fill in dates in the middle\\n        cur_hour = start_hour + timedelta(hours=1)\\n        while cur_hour < end_hour:\\n            hour_to_secs[cur_hour] = timedelta.total_seconds(\\n                timedelta(hours=1))\\n            cur_hour += timedelta(hours=1)\\n\\n    # remove zeros\\n    hour_to_secs = dict(\\n        (h, secs) for h, secs in hour_to_secs.items() if secs)\\n\\n    return hour_to_secs",
                    "func_fullName": "mrjob.tools.emr.audit_usage._subdivide_interval_by_hour( start, end )"
                },
                {
                    "func_id": 1864,
                    "func_name": "_round_up_to_next_second",
                    "func_desc": "_round_up_to_next_second",
                    "func_file": "audit_usage",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _round_up_to_next_second(td):\\n    \"\"\"Round up to the next second because that's how EMR bills.\"\"\"\\n    if td.microseconds:\\n        return strip_microseconds(td) + timedelta(seconds=1)\\n    else:\\n        return td",
                    "func_fullName": "mrjob.tools.emr.audit_usage._round_up_to_next_second( td )"
                },
                {
                    "func_id": 1895,
                    "func_name": "_time_last_active",
                    "func_desc": "_time_last_active",
                    "func_file": "terminate_idle_clusters",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _time_last_active(cluster_summary, steps):\\n    \"\"\"When did something last happen with the given cluster?\\n\\n    Things we look at:\\n\\n    * cluster's ``CreationDateTime`` (always set)\\n    * cluster's ``ReadyDateTime`` (i.e. when bootstrapping finished)\\n    * step's ``CreationDateTime`` for any step\\n    * step's ``StartDateTime`` for any step\\n    * step's ``EndDateTime`` for any step\\n\\n    This is not really meant to be run on clusters which are currently\\n    running, or done.\\n    \"\"\"\\n    timestamps = []\\n\\n    for key in 'CreationDateTime', 'ReadyDateTime':\\n        value = cluster_summary['Status']['Timeline'].get(key)\\n        if value:\\n            timestamps.append(value)\\n\\n    for step in steps:\\n        for key in 'CreationDateTime', 'StartDateTime', 'EndDateTime':\\n            value = step['Status']['Timeline'].get(key)\\n            if value:\\n                timestamps.append(value)\\n\\n    return max(timestamps)",
                    "func_fullName": "mrjob.tools.emr.terminate_idle_clusters._time_last_active( cluster_summary, steps )"
                }
            ]
        },
        {
            "cluster_id": 34,
            "feature_id": 58,
            "feature_desc": "gamma=0.0743; k=3; a=0.25; combined=0.459; stability(ARI)=1.000; sep=0.112",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 950,
                    "func_name": "_IDENTITY_MAPPER",
                    "func_desc": "_IDENTITY_MAPPER",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _IDENTITY_MAPPER(key, value):\\n    yield key, value",
                    "func_fullName": "mrjob.step._IDENTITY_MAPPER( key, value )"
                },
                {
                    "func_id": 951,
                    "func_name": "_IDENTITY_REDUCER",
                    "func_desc": "_IDENTITY_REDUCER",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _IDENTITY_REDUCER(key, values):\\n    for value in values:\\n        yield key, value",
                    "func_fullName": "mrjob.step._IDENTITY_REDUCER( key, values )"
                },
                {
                    "func_id": 975,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(\\n            self, reason=None, step_num=None, num_steps=None,\\n            step_desc=None, last_step_num=None):\\n        \"\"\"Initialize a reason for step failure.\\n\\n        :param string reason: brief explanation of which step failed\\n        :param int step_num: which step failed (0-indexed)\\n        :param int num_steps: number of steps in the job\\n        :param string step_desc: description of step (if we don't like the\\n                                 default \"Step X of Y\")\\n        :param int last_step_num: if one of a range of steps failed, the\\n                                  (0-indexed) last step in that range. If this\\n                                  equals *step_num*, will be ignored.\\n\\n        *reason* should not be several lines long; use ``log.error(...)``\\n        for that.\\n        \"\"\"\\n        self.reason = reason\\n        self.step_num = step_num\\n        self.num_steps = num_steps\\n        self.step_desc = step_desc\\n\\n        # we only need this for streaming steps run by the Spark harness,\\n        # so don't create noise\\n        if last_step_num is None or last_step_num == step_num:\\n            self.last_step_num = None\\n        else:\\n            self.last_step_num = last_step_num",
                    "func_fullName": "mrjob.step.__init__( self, reason, step_num, num_steps, step_desc, last_step_num )"
                },
                {
                    "func_id": 976,
                    "func_name": "__str__",
                    "func_desc": "__str__",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __str__(self):\\n        \"\"\"Human-readable version of the exception. Note that this 1-indexes\\n        *step_num*.\"\"\"\\n        if self.step_desc:\\n            step_desc = self.step_desc\\n        else:\\n            if self.step_num is not None:\\n                # 1-index step numbers\\n                if self.last_step_num is not None:\\n                    step_name = 'Steps %d-%d' % (\\n                        self.step_num + 1, self.last_step_num + 1)\\n                else:\\n                    step_name = 'Step %d' % (self.step_num + 1)\\n\\n                if self.num_steps:\\n                    step_desc = '%s of %d' % (step_name, self.num_steps)\\n                else:\\n                    step_desc = step_name\\n            else:\\n                step_desc = 'Step'\\n\\n        if self.reason:\\n            return '%s failed: %s' % (step_desc, self.reason)\\n        else:\\n            return '%s failed' % step_desc",
                    "func_fullName": "mrjob.step.__str__( self )"
                },
                {
                    "func_id": 977,
                    "func_name": "__repr__",
                    "func_desc": "__repr__",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __repr__(self):\\n        return '%s(%s)' % (\\n            self.__class__.__name__,\\n            ', '.join(('%s=%r' % (k, getattr(self, k))\\n                       for k in self._FIELDS\\n                       if getattr(self, k) is not None)))",
                    "func_fullName": "mrjob.step.__repr__( self )"
                },
                {
                    "func_id": 978,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, **kwargs):\\n        # limit which keyword args can be specified\\n        bad_kwargs = sorted(set(kwargs) - set(_JOB_STEP_PARAMS))\\n        if bad_kwargs:\\n            raise TypeError('MRStep() got an unexpected keyword argument %r' %\\n                            bad_kwargs[0])\\n\\n        if not set(kwargs) & set(_JOB_STEP_FUNC_PARAMS):\\n            raise ValueError(\"Step has no mappers and no reducers\")\\n\\n        self.has_explicit_mapper = any(\\n            value for name, value in kwargs.items()\\n            if name in _MAPPER_FUNCS)\\n\\n        self.has_explicit_combiner = any(\\n            value for name, value in kwargs.items()\\n            if name in _COMBINER_FUNCS)\\n\\n        self.has_explicit_reducer = any(\\n            value for name, value in kwargs.items()\\n            if name in _REDUCER_FUNCS)\\n\\n        steps = dict((f, None) for f in _JOB_STEP_PARAMS)\\n\\n        steps.update(kwargs)\\n\\n        def _check_conflict(func, other_funcs):\\n            if steps[func]:\\n                for other_func in other_funcs:\\n                    if steps[other_func] and other_func != func:\\n                        raise ValueError(\"Can't specify both %s and %s\" % (\\n                            func, other_func))\\n\\n        _check_conflict('mapper_cmd', _MAPPER_FUNCS)\\n        _check_conflict('mapper_raw', ('mapper', 'mapper_pre_filter'))\\n        _check_conflict('combiner_cmd', _COMBINER_FUNCS)\\n        _check_conflict('reducer_cmd', _REDUCER_FUNCS)\\n\\n        self._steps = steps",
                    "func_fullName": "mrjob.step.__init__( self, **kwargs )"
                },
                {
                    "func_id": 979,
                    "func_name": "__repr__",
                    "func_desc": "__repr__",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __repr__(self):\\n        not_none = dict((k, v) for k, v in self._steps.items()\\n                        if v is not None)\\n        return '%s(%s)' % (\\n            self.__class__.__name__,\\n            ', '.join('%s=%r' % (k, v) for k, v in not_none.items()))",
                    "func_fullName": "mrjob.step.__repr__( self )"
                },
                {
                    "func_id": 980,
                    "func_name": "__eq__",
                    "func_desc": "__eq__",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __eq__(self, other):\\n        return (isinstance(other, MRStep) and self._steps == other._steps)",
                    "func_fullName": "mrjob.step.__eq__( self, other )"
                },
                {
                    "func_id": 981,
                    "func_name": "__getitem__",
                    "func_desc": "__getitem__",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __getitem__(self, key):\\n        # always be prepared to run a mapper, since Hadoop Streaming requires\\n        # it\\n        if key == 'mapper' and self._steps['mapper'] is None:\\n            return _IDENTITY_MAPPER\\n        # identity reducer should only show up if you specified 'reducer_init',\\n        # 'reducer_final', or 'reducer_pre_filter', but not 'reducer' itself\\n        if (key == 'reducer' and self._steps['reducer'] is None and\\n                self.has_explicit_reducer):\\n            return _IDENTITY_REDUCER\\n        # identity combiner should only show up if you specified\\n        # 'combiner_init', 'combiner_final', or 'combiner_pre_filter', but not\\n        # 'combiner' itself\\n        if (key == 'combiner' and self._steps['combiner'] is None and\\n                self.has_explicit_combiner):\\n            return _IDENTITY_REDUCER\\n        return self._steps[key]",
                    "func_fullName": "mrjob.step.__getitem__( self, key )"
                },
                {
                    "func_id": 986,
                    "func_name": "description",
                    "func_desc": "description",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def description(self, step_num=0):\\n        desc = {'type': 'streaming'}\\n        # Use a mapper if:\\n        #   - the user writes one\\n        #   - it is the first step and we don't want to mess up protocols\\n        #   - there are only combiners and we don't want to mess up protocols\\n        if (step_num == 0 or\\n                self.has_explicit_mapper or\\n                self.has_explicit_combiner):\\n            desc['mapper'] = self.render_mapper()\\n        if self.has_explicit_combiner:\\n            desc['combiner'] = self.render_combiner()\\n        if self.has_explicit_reducer:\\n            desc['reducer'] = self.render_reducer()\\n        if self._steps['mapper_raw']:\\n            desc['input_manifest'] = True\\n        # TODO: verify this is a dict, convert booleans to strings\\n        if self._steps['jobconf']:\\n            desc['jobconf'] = self._steps['jobconf']\\n\\n        return desc",
                    "func_fullName": "mrjob.step.description( self, step_num )"
                },
                {
                    "func_id": 987,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, **kwargs):\\n        \"\"\"Set all attributes to the corresponding value in *kwargs*, or the\\n        default value. Raise :py:class:`TypeError` for unknown arguments or\\n        values with the wrong type.\"\"\"\\n        bad_kwargs = sorted(set(kwargs) - set(self._STEP_ATTRS))\\n        if bad_kwargs:\\n            raise TypeError('%s() got unexpected keyword arguments: %s' % (\\n                self.__class__.__name__, ', '.join(bad_kwargs)))\\n\\n        for k in self._STEP_ATTRS:\\n            v = kwargs.get(k)\\n            if v is None:\\n                v = self._default(k)\\n            elif k in self._STEP_ATTR_TYPES:\\n                attr_type = self._STEP_ATTR_TYPES[k]\\n\\n                if attr_type is callable:\\n                    if not callable(v):\\n                        raise TypeError('%s is not callable: %r' % (k, v))\\n                elif not isinstance(v, attr_type):\\n                    raise TypeError('%s is not an instance of %r: %r' % (\\n                        k, self._STEP_ATTR_TYPES[k], v))\\n\\n            setattr(self, k, v)",
                    "func_fullName": "mrjob.step.__init__( self, **kwargs )"
                },
                {
                    "func_id": 988,
                    "func_name": "__repr__",
                    "func_desc": "__repr__",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __repr__(self):\\n        kwargs = dict(\\n            (k, getattr(self, k))\\n            for k in self._STEP_ATTR_TYPES if hasattr(self, k))\\n\\n        return '%s(%s)' % (\\n            self.__class__.__name__, ', '.join(\\n                '%s=%s' % (k, v)\\n                for k, v in sorted(kwargs.items())\\n                if v != self._default(k)))",
                    "func_fullName": "mrjob.step.__repr__( self )"
                },
                {
                    "func_id": 989,
                    "func_name": "__eq__",
                    "func_desc": "__eq__",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __eq__(self, other):\\n        return (isinstance(other, self.__class__) and\\n                all(getattr(self, key) == getattr(other, key)\\n                    for key in set(self._STEP_ATTRS)))",
                    "func_fullName": "mrjob.step.__eq__( self, other )"
                },
                {
                    "func_id": 990,
                    "func_name": "_default",
                    "func_desc": "_default",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _default(self, k):\\n        if k in self._STEP_ATTR_DEFAULTS:\\n            return self._STEP_ATTR_DEFAULTS[k]()\\n        else:\\n            return None",
                    "func_fullName": "mrjob.step._default( self, k )"
                },
                {
                    "func_id": 991,
                    "func_name": "description",
                    "func_desc": "description",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def description(self, step_num=0):\\n        \"\"\"Return a dictionary representation of this step. See\\n        :ref:`steps-format` for examples.\"\"\"\\n        result = dict(\\n            (k, getattr(self, k))\\n            for k in self._STEP_ATTRS\\n            if k not in self._HIDDEN_ATTRS\\n        )\\n        result['type'] = self._STEP_TYPE\\n\\n        return result",
                    "func_fullName": "mrjob.step.description( self, step_num )"
                },
                {
                    "func_id": 992,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, jar, **kwargs):\\n        super(JarStep, self).__init__(jar=jar, **kwargs)",
                    "func_fullName": "mrjob.step.__init__( self, jar, **kwargs )"
                },
                {
                    "func_id": 993,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, spark, **kwargs):\\n        super(SparkStep, self).__init__(spark=spark, **kwargs)",
                    "func_fullName": "mrjob.step.__init__( self, spark, **kwargs )"
                },
                {
                    "func_id": 994,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, jar, main_class, **kwargs):\\n        super(SparkJarStep, self).__init__(\\n            jar=jar, main_class=main_class, **kwargs)",
                    "func_fullName": "mrjob.step.__init__( self, jar, main_class, **kwargs )"
                },
                {
                    "func_id": 995,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, script, **kwargs):\\n        super(SparkScriptStep, self).__init__(script=script, **kwargs)",
                    "func_fullName": "mrjob.step.__init__( self, script, **kwargs )"
                },
                {
                    "func_id": 1795,
                    "func_name": "_eio_to_eof",
                    "func_desc": "_eio_to_eof",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _eio_to_eof(pty_master):\\n    \"\"\"Yield lines from a PTY, gracefully handling an ``IOError`` with\\n    ``errno == EIO`` as end-of-file.\"\"\"\\n    try:\\n        for line in pty_master:\\n            yield line\\n    except IOError as e:\\n        # this is just the PTY's way of saying goodbye\\n        if e.errno == errno.EIO:\\n            return\\n        else:\\n            raise",
                    "func_fullName": "mrjob.logs.step._eio_to_eof( pty_master )"
                }
            ]
        },
        {
            "cluster_id": 34,
            "feature_id": 59,
            "feature_desc": "gamma=0.0743; k=3; a=0.25; combined=0.459; stability(ARI)=1.000; sep=0.112",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 973,
                    "func_name": "_is_spark_step_type",
                    "func_desc": "_is_spark_step_type",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _is_spark_step_type(step_type):\\n    \"\"\"Does the given step type indicate that it uses Spark?\"\"\"\\n    return step_type.split('_')[0] == 'spark'",
                    "func_fullName": "mrjob.step._is_spark_step_type( step_type )"
                },
                {
                    "func_id": 974,
                    "func_name": "_is_pyspark_step_type",
                    "func_desc": "_is_pyspark_step_type",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _is_pyspark_step_type(step_type):\\n    \"\"\"Does the given step type indicate that it uses Spark and Python?\"\"\"\\n    return step_type in ('spark', 'spark_script')",
                    "func_fullName": "mrjob.step._is_pyspark_step_type( step_type )"
                },
                {
                    "func_id": 996,
                    "func_name": "_check_conflict",
                    "func_desc": "_check_conflict",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def _check_conflict(func, other_funcs):\\n            if steps[func]:\\n                for other_func in other_funcs:\\n                    if steps[other_func] and other_func != func:\\n                        raise ValueError(\"Can't specify both %s and %s\" % (\\n                            func, other_func))",
                    "func_fullName": "mrjob.step._check_conflict( func, other_funcs )"
                },
                {
                    "func_id": 1786,
                    "func_name": "_ls_emr_step_syslogs",
                    "func_desc": "_ls_emr_step_syslogs",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _ls_emr_step_syslogs(fs, log_dir_stream, step_id=None):\\n    \"\"\"Yield matching step logs, optionally filtering by *step_id*.\\n    Yields dicts with the keys:\\n\\n    path: path/URI of step file\\n    step_id: step_id in *path* (must match *step_id* if set)\\n    \"\"\"\\n    matches = _ls_logs(fs, log_dir_stream, _match_emr_step_syslog_path,\\n                       step_id=step_id)\\n\\n    return sorted(matches, key=_match_sort_key)",
                    "func_fullName": "mrjob.logs.step._ls_emr_step_syslogs( fs, log_dir_stream, step_id )"
                },
                {
                    "func_id": 1787,
                    "func_name": "_ls_emr_step_stderr_logs",
                    "func_desc": "_ls_emr_step_stderr_logs",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _ls_emr_step_stderr_logs(fs, log_dir_stream, step_id=None):\\n    \"\"\"Yield matching step logs, optionally filtering by *step_id*.\\n    Yields dicts with the keys:\\n\\n    path: path/URI of step file\\n    step_id: step_id in *path* (must match *step_id* if set)\\n    \"\"\"\\n    matches = _ls_logs(fs, log_dir_stream, _match_emr_step_stderr_path,\\n                       step_id=step_id)\\n\\n    # we basically want to tail the stderr log, so search rotated\\n    # logs in reverse\\n    return sorted(matches, key=_match_sort_key, reverse=True)",
                    "func_fullName": "mrjob.logs.step._ls_emr_step_stderr_logs( fs, log_dir_stream, step_id )"
                },
                {
                    "func_id": 1788,
                    "func_name": "_match_sort_key",
                    "func_desc": "_match_sort_key",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _match_sort_key(m):\\n    \"\"\"sort key which treats empty timestamp as most recent\"\"\"\\n    return (m['timestamp'] is None, m['timestamp'] or '')",
                    "func_fullName": "mrjob.logs.step._match_sort_key( m )"
                },
                {
                    "func_id": 1789,
                    "func_name": "_match_emr_step_syslog_path",
                    "func_desc": "_match_emr_step_syslog_path",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _match_emr_step_syslog_path(path, step_id=None):\\n    \"\"\"Match path of a step syslog, optionally filtering by *step_id*.\\n\\n    If there is a match, return a dict with the keys *step_id* and\\n    *timestamp* (a string). Otherwise, returns None.\\n    \"\"\"\\n    return _match_emr_step_log_path(path, 'syslog', step_id=step_id)",
                    "func_fullName": "mrjob.logs.step._match_emr_step_syslog_path( path, step_id )"
                },
                {
                    "func_id": 1790,
                    "func_name": "_match_emr_step_stderr_path",
                    "func_desc": "_match_emr_step_stderr_path",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _match_emr_step_stderr_path(path, step_id=None):\\n    \"\"\"Match path of a step stderr log, optionally filtering by *step_id*.\\n\\n    If there is a match, return a dict with the keys *step_id* and\\n    *timestamp* (a string). Otherwise, returns None.\\n    \"\"\"\\n    return _match_emr_step_log_path(path, 'stderr', step_id=step_id)",
                    "func_fullName": "mrjob.logs.step._match_emr_step_stderr_path( path, step_id )"
                },
                {
                    "func_id": 1791,
                    "func_name": "_match_emr_step_log_path",
                    "func_desc": "_match_emr_step_log_path",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _match_emr_step_log_path(path, log_type, step_id=None):\\n    \"\"\"Helper for :py:func:`_match_emr_step_syslog_path` and\\n    :py:func:`_match_emr_step_stderr_path`\\n    \"\"\"\\n    m = _EMR_STEP_LOG_PATH_RE.match(path)\\n    if not m:\\n        return None\\n\\n    if m.group('log_type') != log_type:\\n        return None\\n\\n    if not (step_id is None or m.group('step_id') == step_id):\\n        return None\\n\\n    return dict(step_id=m.group('step_id'), timestamp=m.group('timestamp'))",
                    "func_fullName": "mrjob.logs.step._match_emr_step_log_path( path, log_type, step_id )"
                },
                {
                    "func_id": 1792,
                    "func_name": "_interpret_emr_step_syslog",
                    "func_desc": "_interpret_emr_step_syslog",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _interpret_emr_step_syslog(fs, matches):\\n    \"\"\"Extract information from step syslog (see :py:func:`_parse_step_log()`),\\n    which may be split into several chunks by timestamp\"\"\"\\n    # going to merge results for each log into final result\\n    errors = []\\n    result = {}\\n\\n    for match in matches:\\n        path = match['path']\\n\\n        interpretation = _parse_step_syslog(_cat_log_lines(fs, path))\\n\\n        result.update(interpretation)\\n        for error in result.get('errors') or ():\\n            if 'hadoop_error' in error:\\n                error['hadoop_error']['path'] = path\\n            _add_implied_task_id(error)\\n            errors.append(error)\\n\\n    _add_implied_job_id(result)\\n    if errors:\\n        result['errors'] = errors\\n\\n    return result",
                    "func_fullName": "mrjob.logs.step._interpret_emr_step_syslog( fs, matches )"
                },
                {
                    "func_id": 1793,
                    "func_name": "_interpret_new_dataproc_step_stderr",
                    "func_desc": "_interpret_new_dataproc_step_stderr",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _interpret_new_dataproc_step_stderr(step_interpretation, new_lines):\\n    \"\"\"Incrementally update *step_interpretation* (a dict) with information\\n    from new lines read from Hadoop job driver output on Dataproc.\"\"\"\\n    return _parse_step_syslog_from_log4j_records(\\n        _parse_hadoop_log4j_records(new_lines),\\n        step_interpretation)",
                    "func_fullName": "mrjob.logs.step._interpret_new_dataproc_step_stderr( step_interpretation, new_lines )"
                },
                {
                    "func_id": 1794,
                    "func_name": "_interpret_emr_step_stderr",
                    "func_desc": "_interpret_emr_step_stderr",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _interpret_emr_step_stderr(fs, matches):\\n    \"\"\"Extract information from step stderr (see\\n    :py:func:`~mrjob.logs.task._parse_task_stderr()`),\\n    which may be split into several chunks by timestamp\"\"\"\\n    for match in matches:\\n        path = match['path']\\n\\n        error = _parse_task_stderr(_cat_log_lines(fs, path))\\n\\n        if error:\\n            error['path'] = path\\n            # We're essentially just tailing the stderr log, so stop when we\\n            # find an error.\\n            return dict(errors=[dict(task_error=error)])\\n\\n    return {}",
                    "func_fullName": "mrjob.logs.step._interpret_emr_step_stderr( fs, matches )"
                },
                {
                    "func_id": 1796,
                    "func_name": "_interpret_hadoop_jar_command_stderr",
                    "func_desc": "_interpret_hadoop_jar_command_stderr",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _interpret_hadoop_jar_command_stderr(lines, record_callback=None):\\n    \"\"\"Parse *lines* from the ``hadoop jar`` command's stderr (lines can be\\n    either bytes or unicode). Works like :py:func:`_parse_step_syslog` (same\\n    return format) with a few extra features to handle the output of the\\n    ``hadoop jar`` command on the fly:\\n\\n    - Pre-filters non-log4j stuff from Hadoop Streaming so it doesn't\\n      get treated as part of a multi-line message\\n    - Optionally calls *record_callback* for each log4j record (see\\n      :py:func:`~mrjob.logs.log4j._parse_hadoop_log4j_records`).\\n    \"\"\"\\n    def pre_filter(line):\\n        return bool(_HADOOP_STREAMING_NON_LOG4J_LINE_RE.match(line))\\n\\n    def yield_records():\\n        for record in _parse_hadoop_log4j_records(\\n                lines, pre_filter=pre_filter):\\n            if record_callback:\\n                record_callback(record)\\n            yield record\\n\\n    result = _parse_step_syslog_from_log4j_records(yield_records())\\n\\n    _add_implied_job_id(result)\\n    for error in result.get('errors') or ():\\n        _add_implied_task_id(error)\\n\\n    return result",
                    "func_fullName": "mrjob.logs.step._interpret_hadoop_jar_command_stderr( lines, record_callback )"
                },
                {
                    "func_id": 1797,
                    "func_name": "_parse_step_syslog",
                    "func_desc": "_parse_step_syslog",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_step_syslog(lines):\\n    \"\"\"Parse the syslog from the ``hadoop jar`` command.\\n\\n    Returns a dictionary which potentially contains the following keys:\\n\\n    application_id: a string like 'application_1449857544442_0002'. Only\\n        set on YARN\\n    counters: a map from counter group -> counter -> amount, or None if\\n        no counters found (only YARN prints counters)\\n    errors: a list of errors, with the following keys:\\n        hadoop_error:\\n            message: lines of error, as as string\\n            start_line: first line of log containing the error (0-indexed)\\n            num_lines: # of lines of log containing the error\\n        attempt_id: ID of task attempt with this error\\n    job_id: a string like 'job_201512112247_0003'. Should always be set\\n    output_dir: a URI like 'hdfs:///user/hadoop/tmp/my-output-dir'. Should\\n        always be set on success.\\n    \"\"\"\\n    return _parse_step_syslog_from_log4j_records(\\n        _parse_hadoop_log4j_records(lines))",
                    "func_fullName": "mrjob.logs.step._parse_step_syslog( lines )"
                },
                {
                    "func_id": 1798,
                    "func_name": "_parse_step_syslog_from_log4j_records",
                    "func_desc": "_parse_step_syslog_from_log4j_records",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_step_syslog_from_log4j_records(records, step_interpretation=None):\\n    \"\"\"Pulls errors, counters, IDs, etc. from log4j records\\n    emitted by Hadoop.\\n\\n    This powers :py:func:`_parse_step_syslog` and\\n    :py:func:`_interpret_hadoop_jar_command_stderr`.\\n    \"\"\"\\n    if step_interpretation is None:\\n        result = {}\\n    else:\\n        result = step_interpretation\\n\\n    for record in records:\\n        message = record['message']\\n\\n        # counters\\n        if _is_counter_log4j_record(record):\\n            result['counters'] = _parse_indented_counters(\\n                message.splitlines())\\n            continue\\n\\n        # output_dir\\n        m = _OUTPUT_DIRECTORY_RE.match(message)\\n        if m:\\n            result['output_dir'] = m.group('output_dir')\\n            continue\\n\\n        # application_id\\n        m = _SUBMITTED_APPLICATION_RE.match(message)\\n        if m:\\n            result['application_id'] = m.group('application_id')\\n            continue\\n\\n        # job_id\\n        m = _RUNNING_JOB_RE.match(message)\\n        if m:\\n            result['job_id'] = m.group('job_id')\\n            continue\\n\\n        # progress\\n        m = _JOB_PROGRESS_RE.match(message)\\n        if m:\\n            result['progress'] = dict(\\n                map=int(m.group('map')),\\n                reduce=int(m.group('reduce')),\\n                message=message,\\n            )\\n\\n        # invalid jar\\n        m = _NOT_A_VALID_JAR_RE.match(message)\\n        if m:\\n            error = dict(\\n                hadoop_error=dict(\\n                    message=message,\\n                    num_lines=record['num_lines'],\\n                    start_line=record['start_line'],\\n                ),\\n            )\\n            result.setdefault('errors', [])\\n            result['errors'].append(error)\\n\\n        # task failure\\n        m = _TASK_ATTEMPT_FAILED_RE.match(message)\\n        if m:\\n            error_str = '\\n'.join(message.splitlines()[1:])\\n            if not error_str:  # if no exception, print something\\n                error_str = message\\n\\n            error = dict(\\n                attempt_id=m.group('attempt_id'),\\n                hadoop_error=dict(\\n                    message=error_str,\\n                    num_lines=record['num_lines'],\\n                    start_line=record['start_line'],\\n                )\\n            )\\n\\n            result.setdefault('errors', [])\\n            result['errors'].append(error)\\n\\n    return result",
                    "func_fullName": "mrjob.logs.step._parse_step_syslog_from_log4j_records( records, step_interpretation )"
                },
                {
                    "func_id": 1799,
                    "func_name": "_is_counter_log4j_record",
                    "func_desc": "_is_counter_log4j_record",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _is_counter_log4j_record(record):\\n    \"\"\"Is this the record containing counters?\\n\\n    (HadoopJobRunner doesn't want to log this because it's very long\\n    and it has its own way of displaying counters.)\\n    \"\"\"\\n    return bool(_INDENTED_COUNTERS_MESSAGE_RE.match(record['message']))",
                    "func_fullName": "mrjob.logs.step._is_counter_log4j_record( record )"
                },
                {
                    "func_id": 1800,
                    "func_name": "_parse_indented_counters",
                    "func_desc": "_parse_indented_counters",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_indented_counters(lines):\\n    \"\"\"Parse counters in the indented format output/logged by the\\n    Hadoop binary.\\n\\n    Takes input as lines (should not include log record stuff) and\\n    returns a map from counter group to counter to amount.\\n    \"\"\"\\n    counters = {}  # map group -> counter -> amount\\n    group = None\\n    group_indent = None\\n\\n    for line in lines:\\n        if not (group is None or group_indent is None):\\n            m = _INDENTED_COUNTER_RE.match(line)\\n            if m and len(m.group('indent')) > group_indent:\\n\\n                counter = m.group('counter')\\n                amount = int(m.group('amount'))\\n\\n                counters.setdefault(group, {})\\n                counters[group][counter] = amount\\n\\n                continue\\n\\n        m = _INDENTED_COUNTER_GROUP_RE.match(line)\\n        if m:\\n            group = m.group('group')\\n            group_indent = len(m.group('indent'))\\n\\n        elif not _INDENTED_COUNTERS_MESSAGE_RE.match(line):\\n            log.warning('unexpected counter line: %s' % line)\\n\\n    return counters",
                    "func_fullName": "mrjob.logs.step._parse_indented_counters( lines )"
                },
                {
                    "func_id": 1801,
                    "func_name": "_log_line_from_driver",
                    "func_desc": "_log_line_from_driver",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _log_line_from_driver(line, level=None):\\n    \"\"\"Log ``'  <line>'``. *line* should be a string.\\n\\n    Optionally specify a logging level (default is logging.INFO).\\n    \"\"\"\\n    log.log(level or logging.INFO, '  %s' % line)",
                    "func_fullName": "mrjob.logs.step._log_line_from_driver( line, level )"
                },
                {
                    "func_id": 1802,
                    "func_name": "_log_log4j_record",
                    "func_desc": "_log_log4j_record",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _log_log4j_record(record):\\n    \"\"\"Log a log4j message at the appropriate logging level\"\"\"\\n    level = getattr(logging, record.get('level') or '', None)\\n    _log_line_from_driver(record['message'], level=level)",
                    "func_fullName": "mrjob.logs.step._log_log4j_record( record )"
                },
                {
                    "func_id": 1804,
                    "func_name": "yield_records",
                    "func_desc": "yield_records",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def yield_records():\\n        for record in _parse_hadoop_log4j_records(\\n                lines, pre_filter=pre_filter):\\n            if record_callback:\\n                record_callback(record)\\n            yield record",
                    "func_fullName": "mrjob.logs.step.yield_records(  )"
                }
            ]
        },
        {
            "cluster_id": 34,
            "feature_id": 60,
            "feature_desc": "gamma=0.0743; k=3; a=0.25; combined=0.459; stability(ARI)=1.000; sep=0.112",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 982,
                    "func_name": "_render_substep",
                    "func_desc": "_render_substep",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _render_substep(self, cmd_key, pre_filter_key):\\n        if self._steps[cmd_key]:\\n            cmd = self._steps[cmd_key]\\n            if not isinstance(cmd, string_types):\\n                cmd = cmd_line(cmd)\\n            if (pre_filter_key and self._steps[pre_filter_key]):\\n                raise ValueError('Cannot specify both %s and %s' % (\\n                    cmd_key, pre_filter_key))\\n            return {'type': 'command', 'command': cmd}\\n        else:\\n            substep = {'type': 'script'}\\n            if (pre_filter_key and\\n                    self._steps[pre_filter_key]):\\n                substep['pre_filter'] = self._steps[pre_filter_key]\\n            return substep",
                    "func_fullName": "mrjob.step._render_substep( self, cmd_key, pre_filter_key )"
                },
                {
                    "func_id": 983,
                    "func_name": "render_mapper",
                    "func_desc": "render_mapper",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def render_mapper(self):\\n        return self._render_substep('mapper_cmd', 'mapper_pre_filter')",
                    "func_fullName": "mrjob.step.render_mapper( self )"
                },
                {
                    "func_id": 984,
                    "func_name": "render_combiner",
                    "func_desc": "render_combiner",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def render_combiner(self):\\n        return self._render_substep('combiner_cmd', 'combiner_pre_filter')",
                    "func_fullName": "mrjob.step.render_combiner( self )"
                },
                {
                    "func_id": 985,
                    "func_name": "render_reducer",
                    "func_desc": "render_reducer",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def render_reducer(self):\\n        return self._render_substep('reducer_cmd', 'reducer_pre_filter')",
                    "func_fullName": "mrjob.step.render_reducer( self )"
                },
                {
                    "func_id": 1803,
                    "func_name": "pre_filter",
                    "func_desc": "pre_filter",
                    "func_file": "step",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def pre_filter(line):\\n        return bool(_HADOOP_STREAMING_NON_LOG4J_LINE_RE.match(line))",
                    "func_fullName": "mrjob.logs.step.pre_filter( line )"
                }
            ]
        },
        {
            "cluster_id": 27,
            "feature_id": 61,
            "feature_desc": "gamma=0.0100; k=1; a=0.25; combined=-0.050; stability(ARI)=1.000; sep=0.722",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1028,
                    "func_name": "get_or_create_mrjob_service_role",
                    "func_desc": "get_or_create_mrjob_service_role",
                    "func_file": "iam",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def get_or_create_mrjob_service_role(client):\\n    \"\"\"Look for a usable service role for EMR, and if there is none,\\n    create one. Either way, return that role's name.\"\"\"\\n\\n    # look for matching role. Must have same policy document\\n    # and attached role policy\\n    for role in _boto3_paginate('Roles', client, 'list_roles'):\\n        if _role_matches(client, role, _MRJOB_SERVICE_ROLE,\\n                         _EMR_SERVICE_ROLE_POLICY_ARN):\\n            return role['RoleName']\\n\\n    # no matches, create it ourselves\\n    role_name = _create_mrjob_role_with_attached_policy(\\n        client, _MRJOB_SERVICE_ROLE, _EMR_SERVICE_ROLE_POLICY_ARN)\\n\\n    log.info('Auto-created service role %s' % role_name)\\n\\n    return role_name",
                    "func_fullName": "mrjob.iam.get_or_create_mrjob_service_role( client )"
                },
                {
                    "func_id": 1029,
                    "func_name": "get_or_create_mrjob_instance_profile",
                    "func_desc": "get_or_create_mrjob_instance_profile",
                    "func_file": "iam",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def get_or_create_mrjob_instance_profile(client):\\n    \"\"\"Look for a usable instance profile for EMR, and if there is none,\\n    create one.\"\"\"\\n    # look for matching instance profile. Must point to a role with\\n    # the right policy document and attached role policy\\n    for profile in _boto3_paginate(\\n            'InstanceProfiles', client, 'list_instance_profiles'):\\n        roles = profile['Roles']\\n        if len(roles) != 1:\\n            continue\\n        if _role_matches(client, roles[0], _MRJOB_INSTANCE_PROFILE_ROLE,\\n                         _EMR_INSTANCE_PROFILE_POLICY_ARN):\\n            return profile['InstanceProfileName']\\n\\n    # create a new role, and wrap it in an instance profile\\n    # with the same name\\n    name = _create_mrjob_role_with_attached_policy(\\n        client, _MRJOB_INSTANCE_PROFILE_ROLE, _EMR_INSTANCE_PROFILE_POLICY_ARN)\\n\\n    client.create_instance_profile(InstanceProfileName=name)\\n    client.add_role_to_instance_profile(InstanceProfileName=name,\\n                                        RoleName=name)\\n\\n    log.info('Auto-created instance profile %s' % name)\\n\\n    return name",
                    "func_fullName": "mrjob.iam.get_or_create_mrjob_instance_profile( client )"
                },
                {
                    "func_id": 1030,
                    "func_name": "_role_matches",
                    "func_desc": "_role_matches",
                    "func_file": "iam",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _role_matches(client, role, role_document, policy_arn):\\n    \"\"\"Does the given role data structure have the given policy document\\n    and the given policy ARN attached?\\n\\n    (Roles can have up to two policy ARNs attached, but we don't need this\\n    functionality.)\\n    \"\"\"\\n    if role['AssumeRolePolicyDocument'] != role_document:\\n        return False\\n\\n    # not bothering to paginate these because we're only checking for\\n    # a single one. As of 2015-05-29, the max allowed was two anyway.\\n    policy_resp = client.list_attached_role_policies(RoleName=role['RoleName'])\\n    policies = policy_resp['AttachedPolicies']\\n    return len(policies) == 1 and policies[0]['PolicyArn'] == policy_arn",
                    "func_fullName": "mrjob.iam._role_matches( client, role, role_document, policy_arn )"
                },
                {
                    "func_id": 1031,
                    "func_name": "_create_mrjob_role_with_attached_policy",
                    "func_desc": "_create_mrjob_role_with_attached_policy",
                    "func_file": "iam",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _create_mrjob_role_with_attached_policy(client, role_document, policy_arn):\\n    \"\"\"Create a new role with a random name starting with ``mrjob-`` that\\n    has the given policy document and the given policy ARN attached.\\n\\n    (Roles can have up to two policy ARNs attached, but we don't need this\\n    functionality.)\\n    \"\"\"\\n    # create role\\n    role_name = 'mrjob-' + random_identifier()\\n\\n    client.create_role(AssumeRolePolicyDocument=json.dumps(role_document),\\n                       RoleName=role_name)\\n    client.attach_role_policy(PolicyArn=policy_arn,\\n                              RoleName=role_name)\\n\\n    return role_name",
                    "func_fullName": "mrjob.iam._create_mrjob_role_with_attached_policy( client, role_document, policy_arn )"
                }
            ]
        },
        {
            "cluster_id": 5,
            "feature_id": 62,
            "feature_desc": "gamma=0.0743; k=5; a=0.25; combined=0.487; stability(ARI)=1.000; sep=0.160",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1344,
                    "func_name": "is_uri",
                    "func_desc": "is_uri",
                    "func_file": "parse",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def is_uri(uri):\\n    r\"\"\"Return True if *uri* is a URI and contains ``://``\\n    (we only care about URIs that can describe files)\\n    \"\"\"\\n    return '://' in uri and bool(urlparse(uri).scheme)",
                    "func_fullName": "mrjob.parse.is_uri( uri )"
                },
                {
                    "func_id": 1345,
                    "func_name": "is_s3_uri",
                    "func_desc": "is_s3_uri",
                    "func_file": "parse",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def is_s3_uri(uri):\\n    \"\"\"Return True if *uri* can be parsed into an S3 URI, False otherwise.\"\"\"\\n    try:\\n        parse_s3_uri(uri)\\n        return True\\n    except ValueError:\\n        return False",
                    "func_fullName": "mrjob.parse.is_s3_uri( uri )"
                },
                {
                    "func_id": 1346,
                    "func_name": "parse_s3_uri",
                    "func_desc": "parse_s3_uri",
                    "func_file": "parse",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def parse_s3_uri(uri):\\n    \"\"\"Parse an S3 URI into (bucket, key)\\n\\n    >>> parse_s3_uri('s3://walrus/tmp/')\\n    ('walrus', 'tmp/')\\n\\n    If ``uri`` is not an S3 URI, raise a ValueError\\n    \"\"\"\\n    components = urlparse(uri)\\n    if (components.scheme not in ('s3', 's3n', 's3a') or\\n        '/' not in components.path):  # noqa\\n\\n        raise ValueError('Invalid S3 URI: %s' % uri)\\n\\n    return components.netloc, components.path[1:]",
                    "func_fullName": "mrjob.parse.parse_s3_uri( uri )"
                },
                {
                    "func_id": 1347,
                    "func_name": "to_uri",
                    "func_desc": "to_uri",
                    "func_file": "parse",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def to_uri(path_or_uri):\\n    \"\"\"If *path_or_uri* is not a URI already, convert it to a ``file:///``\\n    URI.\"\"\"\\n    if is_uri(path_or_uri):\\n        return path_or_uri\\n    else:\\n        return urljoin('file:', pathname2url(abspath(path_or_uri)))",
                    "func_fullName": "mrjob.parse.to_uri( path_or_uri )"
                },
                {
                    "func_id": 1348,
                    "func_name": "urlparse",
                    "func_desc": "urlparse",
                    "func_file": "parse",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def urlparse(urlstring, scheme='', allow_fragments=True, *args, **kwargs):\\n    \"\"\"A wrapper for :py:func:`urlparse.urlparse` that splits the fragment\\n    correctly in all URIs, not just Web-related ones.\\n    This behavior was fixed in the Python 2.7.4 standard library but we have\\n    to back-port it for previous versions.\\n    \"\"\"\\n    (scheme, netloc, path, params, query, fragment) = (\\n        urlparse_buggy(urlstring, scheme, allow_fragments, *args, **kwargs))\\n\\n    if allow_fragments and '#' in path and not fragment:\\n        path, fragment = path.split('#', 1)\\n\\n    return ParseResult(scheme, netloc, path, params, query, fragment)",
                    "func_fullName": "mrjob.parse.urlparse( urlstring, scheme, allow_fragments, *args, **kwargs )"
                },
                {
                    "func_id": 1831,
                    "func_name": "_add_implied_job_id",
                    "func_desc": "_add_implied_job_id",
                    "func_file": "ids",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _add_implied_job_id(d):\\n    \"\"\"If *d* has *task_id* or *application_id* but not *job_id*,\\n    add it.\\n\\n    (We don't infer application_id from job_id because application_id\\n    only exists on YARN)\\n\\n    .. note::\\n\\n       Don't use this with Spark, where job and application IDs don't\\n       necessarily match.\\n    \"\"\"\\n    if not d.get('job_id'):\\n        if d.get('task_id'):\\n            d['job_id'] = _to_job_id(d['task_id'])\\n        elif d.get('application_id'):\\n            d['job_id'] = _to_job_id(d['application_id'])",
                    "func_fullName": "mrjob.logs.ids._add_implied_job_id( d )"
                },
                {
                    "func_id": 1833,
                    "func_name": "_to_job_id",
                    "func_desc": "_to_job_id",
                    "func_file": "ids",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _to_job_id(task_id):\\n    \"\"\"Convert e.g. ``'task_201601081945_0005_m_000005'``\\n    or ``'application_201601081945_0005'`` to\\n    to ``'job_201601081945_0005'``.\"\"\"\\n    return 'job_' + '_'.join(task_id.split('_')[1:3])",
                    "func_fullName": "mrjob.logs.ids._to_job_id( task_id )"
                }
            ]
        },
        {
            "cluster_id": 5,
            "feature_id": 63,
            "feature_desc": "gamma=0.0743; k=5; a=0.25; combined=0.487; stability(ARI)=1.000; sep=0.160",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1349,
                    "func_name": "_parse_port_range_list",
                    "func_desc": "_parse_port_range_list",
                    "func_file": "parse",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_port_range_list(range_list_str):\\n    all_ranges = []\\n    for range_str in range_list_str.split(','):\\n        if ':' in range_str:\\n            a, b = [int(x) for x in range_str.split(':')]\\n            all_ranges.extend(range(a, b + 1))\\n        else:\\n            all_ranges.append(int(range_str))\\n    return all_ranges",
                    "func_fullName": "mrjob.parse._parse_port_range_list( range_list_str )"
                },
                {
                    "func_id": 1350,
                    "func_name": "parse_mr_job_stderr",
                    "func_desc": "parse_mr_job_stderr",
                    "func_file": "parse",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def parse_mr_job_stderr(stderr, counters=None):\\n    \"\"\"Parse counters and status messages out of MRJob output.\\n\\n    :param stderr: a filehandle, a list of lines (bytes), or bytes\\n    :param counters: Counters so far, to update; a map from group (string to\\n                     counter name (string) to count.\\n\\n    Returns a dictionary with the keys *counters*, *statuses*, *other*:\\n\\n    - *counters*: counters so far; same format as above\\n    - *statuses*: a list of status messages encountered\\n    - *other*: lines (strings) that aren't either counters or status messages\\n    \"\"\"\\n    # For the corresponding code in Hadoop Streaming, see ``incrCounter()`` in\\n    # http://svn.apache.org/viewvc/hadoop/mapreduce/trunk/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java?view=markup  # noqa\\n    from mrjob.py2 import to_unicode\\n    if isinstance(stderr, bytes):\\n        stderr = BytesIO(stderr)\\n\\n    if counters is None:\\n        counters = {}\\n    statuses = []\\n    other = []\\n\\n    for line in stderr:\\n        m = _COUNTER_RE.match(line.rstrip(b'\\r\\n'))\\n        if m:\\n            group, counter, amount_str = m.groups()\\n\\n            # don't leave these as bytes on Python 3\\n            group = to_unicode(group)\\n            counter = to_unicode(counter)\\n\\n            counters.setdefault(group, {})\\n            counters[group].setdefault(counter, 0)\\n            counters[group][counter] += int(amount_str)\\n            continue\\n\\n        m = _STATUS_RE.match(line.rstrip(b'\\r\\n'))\\n        if m:\\n            # don't leave as bytes on Python 3\\n            statuses.append(to_unicode(m.group(1)))\\n            continue\\n\\n        other.append(to_unicode(line))\\n\\n    return {'counters': counters, 'statuses': statuses, 'other': other}",
                    "func_fullName": "mrjob.parse.parse_mr_job_stderr( stderr, counters )"
                },
                {
                    "func_id": 1351,
                    "func_name": "_parse_progress_from_job_tracker",
                    "func_desc": "_parse_progress_from_job_tracker",
                    "func_file": "parse",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_progress_from_job_tracker(html_bytes):\\n    \"\"\"Pull (map_percent, reduce_percent) from running job from job tracker\\n    HTML as floats, or return (None, None).\\n\\n    This assumes at most one running job (designed for EMR).\\n    \"\"\"\\n    # snip out the Running Jobs section (ignore the header)\\n    start = html_bytes.rfind(b'Running Jobs')\\n    if start == -1:\\n        return None, None\\n    end = html_bytes.find(b'Jobs', start + len(b'Running Jobs'))\\n    if end == -1:\\n        end = None\\n\\n    html_bytes = html_bytes[start:end]\\n\\n    # search it for percents\\n    matches = _JOB_TRACKER_HTML_RE.findall(html_bytes)\\n    if len(matches) >= 2:\\n        return float(matches[0]), float(matches[1])\\n    else:\\n        return None, None",
                    "func_fullName": "mrjob.parse._parse_progress_from_job_tracker( html_bytes )"
                },
                {
                    "func_id": 1352,
                    "func_name": "_parse_progress_from_resource_manager",
                    "func_desc": "_parse_progress_from_resource_manager",
                    "func_file": "parse",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_progress_from_resource_manager(html_bytes):\\n    \"\"\"Pull progress_precent for running job from job tracker HTML, as a\\n    float, or return None.\\n\\n    This assumes at most one running job (designed for EMR).\\n    \"\"\"\\n    # this is for EMR and assumes only one running job\\n    for line in html_bytes.splitlines():\\n        m = _RESOURCE_MANAGER_JS_RE.match(line)\\n        if m:\\n            return float(m.group('percent'))\\n\\n    return None",
                    "func_fullName": "mrjob.parse._parse_progress_from_resource_manager( html_bytes )"
                },
                {
                    "func_id": 1724,
                    "func_name": "_sum_counters",
                    "func_desc": "_sum_counters",
                    "func_file": "counters",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _sum_counters(*counters_list):\\n    \"\"\"Combine many maps from group to counter to amount.\"\"\"\\n    result = {}\\n\\n    for counters in counters_list:\\n        for group, counter_to_amount in counters.items():\\n            for counter, amount in counter_to_amount.items():\\n                result.setdefault(group, {})\\n                result[group].setdefault(counter, 0)\\n                result[group][counter] += amount\\n\\n    return result",
                    "func_fullName": "mrjob.logs.counters._sum_counters( *counters_list )"
                },
                {
                    "func_id": 1764,
                    "func_name": "_parse_yarn_history_log",
                    "func_desc": "_parse_yarn_history_log",
                    "func_file": "history",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_yarn_history_log(lines):\\n    \"\"\"Collect useful info from a YARN history file, dealing gracefully\\n    with unexpected data structures.\\n\\n    This returns a dictionary which may contain the following keys:\\n\\n    attempt_to_container_id: map from attempt_id to container_id (used\\n        to find task logs corresponding to failed attempts)\\n    counters: map from group to counter to amount. If job failed, we sum\\n        counters for succesful tasks\\n    errors: a list of dictionaries with the keys:\\n        hadoop_error:\\n            message: lines of error, as as string\\n            start_line: first line of log containing the error (0-indexed)\\n            num_lines: # of lines of log containing the error\\n        task_id: ID of task with this error\\n        attempt_id: ID of task attempt with this error\\n    \"\"\"\\n    result = {}\\n    task_to_counters = {}  # used for successful tasks in failed jobs\\n\\n    for line_num, line in enumerate(lines):\\n        # empty space or \"Avro-Json\" header\\n        if not line.startswith('{'):\\n            continue\\n\\n        try:\\n            record = json.loads(line)\\n        except:\\n            continue\\n\\n        record_type = record.get('type')\\n        if not isinstance(record_type, string_types):\\n            continue\\n\\n        # extract events. Looks like there's just one per record\\n        event_record = record.get('event')\\n        if not isinstance(event_record, dict):\\n            continue\\n        events = [e for e in record['event'].values()\\n                  if isinstance(e, dict)]\\n\\n        # update container_id -> attempt_id mapping\\n        for event in events:\\n            if 'attemptId' in event and 'containerId' in event:\\n                result.setdefault('attempt_to_container_id', {})\\n                result['attempt_to_container_id'][\\n                    event['attemptId']] = event['containerId']\\n\\n        if record_type.endswith('_ATTEMPT_FAILED'):\\n            for event in events:\\n                err_msg = event.get('error')\\n                if not (err_msg and isinstance(err_msg, string_types)):\\n                    continue\\n\\n                error = dict(\\n                    hadoop_error=dict(\\n                        message=err_msg,\\n                        start_line=line_num,\\n                        num_lines=1))\\n\\n                if isinstance(event.get('taskid'), string_types):\\n                    error['task_id'] = event['taskid']\\n\\n                if isinstance(event.get('attemptId'), string_types):\\n                    error['attempt_id'] = event['attemptId']\\n\\n                result.setdefault('errors', [])\\n                result['errors'].append(error)\\n\\n        elif record_type == 'TASK_FINISHED':\\n            for event in events:\\n                task_id = event.get('taskid')\\n                if not isinstance(task_id, string_types):\\n                    continue\\n\\n                counters_record = event.get('counters')\\n                if not isinstance(counters_record, dict):\\n                    continue\\n\\n                task_to_counters[task_id] = _extract_yarn_counters(\\n                    counters_record)\\n\\n        elif record_type == 'JOB_FINISHED':\\n            for event in events:\\n                # mapCounters and reduceCounters are also available\\n                counters_record = event.get('totalCounters')\\n                if not isinstance(counters_record, dict):\\n                    continue\\n\\n                result['counters'] = _extract_yarn_counters(counters_record)\\n\\n    # if job failed, patch together counters from successful tasks\\n    if 'counters' not in result and task_to_counters:\\n        result['counters'] = _sum_counters(*task_to_counters.values())\\n\\n    return result",
                    "func_fullName": "mrjob.logs.history._parse_yarn_history_log( lines )"
                },
                {
                    "func_id": 1765,
                    "func_name": "_extract_yarn_counters",
                    "func_desc": "_extract_yarn_counters",
                    "func_file": "history",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _extract_yarn_counters(counters_record):\\n    \"\"\"Convert Avro-Json counter data structure to our\\n    group -> counter -> amount format.\\n\\n    This deals gracefully with unexpected data structures.\\n    \"\"\"\\n    if not isinstance(counters_record, dict):\\n        return {}\\n\\n    group_records = counters_record.get('groups')\\n    if not isinstance(group_records, list):\\n        return {}\\n\\n    counters = {}\\n\\n    for group_record in group_records:\\n        if not isinstance(group_record, dict):\\n            continue\\n\\n        group = group_record.get('displayName')\\n        if not isinstance(group, string_types):\\n            continue\\n\\n        counter_records = group_record.get('counts')\\n        if not isinstance(counter_records, list):\\n            continue\\n\\n        for counter_record in counter_records:\\n            counter = counter_record.get('displayName')\\n            if not isinstance(counter, string_types):\\n                continue\\n\\n            # in YARN, counters can have an amount of 0. The Hadoop command\\n            # prints them out, so we'll parse them\\n            amount = counter_record.get('value')\\n            if not (isinstance(amount, integer_types)):\\n                continue\\n\\n            counters.setdefault(group, {})\\n            counters[group].setdefault(counter, 0)\\n            counters[group][counter] += amount\\n\\n    return counters",
                    "func_fullName": "mrjob.logs.history._extract_yarn_counters( counters_record )"
                },
                {
                    "func_id": 1766,
                    "func_name": "_parse_pre_yarn_history_log",
                    "func_desc": "_parse_pre_yarn_history_log",
                    "func_file": "history",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_pre_yarn_history_log(lines):\\n    \"\"\"Collect useful info from a pre-YARN history file.\\n\\n    See :py:func:`_parse_yarn_history_log` for return format.\\n    \"\"\"\\n    # tantalizingly, STATE_STRING contains the split (URI and line numbers)\\n    # read, but only for successful tasks, which doesn't help with debugging\\n    result = {}\\n    task_to_counters = {}  # used for successful tasks in failed jobs\\n\\n    for record in _parse_pre_yarn_history_records(lines):\\n        fields = record['fields']\\n\\n        # if job is successful, we get counters for the entire job at the end\\n        if record['type'] == 'Job' and 'COUNTERS' in fields:\\n            result['counters'] = _parse_pre_yarn_counters(fields['COUNTERS'])\\n\\n        # otherwise, compile counters for each successful task\\n        #\\n        # Note: this apparently records a higher total than the task tracker\\n        # (possibly some tasks are duplicates?). Couldn't figure out the logic\\n        # behind this while looking at the history file\\n        elif (record['type'] == 'Task' and\\n              'COUNTERS' in fields and 'TASKID' in fields):\\n            task_id = fields['TASKID']\\n            counters = _parse_pre_yarn_counters(fields['COUNTERS'])\\n\\n            task_to_counters[task_id] = counters\\n\\n        # only want FAILED (not KILLED) tasks with non-blank errors\\n        elif (record['type'] in ('MapAttempt', 'ReduceAttempt') and\\n              'TASK_ATTEMPT_ID' in fields and\\n              fields.get('TASK_STATUS') == 'FAILED' and\\n              fields.get('ERROR')):\\n            result.setdefault('errors', [])\\n            result['errors'].append(dict(\\n                hadoop_error=dict(\\n                    message=fields['ERROR'],\\n                    start_line=record['start_line'],\\n                    num_lines=record['num_lines']),\\n                attempt_id=fields['TASK_ATTEMPT_ID']))\\n\\n    # if job failed, patch together counters from successful tasks\\n    if 'counters' not in result and task_to_counters:\\n        result['counters'] = _sum_counters(*task_to_counters.values())\\n\\n    return result",
                    "func_fullName": "mrjob.logs.history._parse_pre_yarn_history_log( lines )"
                },
                {
                    "func_id": 1767,
                    "func_name": "_parse_pre_yarn_history_records",
                    "func_desc": "_parse_pre_yarn_history_records",
                    "func_file": "history",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_pre_yarn_history_records(lines):\\n    r\"\"\"Yield records from the given sequence of lines. For example,\\n    a line like this:\\n\\n    Task TASKID=\"task_201512311928_0001_m_000003\" \\\\n    TASK_TYPE=\"MAP\" START_TIME=\"1451590341378\" \\\\n    SPLITS=\"/default-rack/172\\.31\\.22\\.226\" .\\n\\n    into a record like:\\n\\n    {\\n        'fields': {'TASKID': 'task_201512311928_0001_m_00000',\\n                   'TASK_TYPE': 'MAP',\\n                   'START_TIME': '1451590341378',\\n                   'SPLITS': '/default-rack/172.31.22.226'},\\n        'type': 'Task',\\n        'line_num': 0,\\n        'num_lines': 1,\\n    }\\n\\n    This handles unescaping values, but doesn't do the further\\n    unescaping needed to process counters. It can also handle multi-line\\n    records (e.g. for Java stack traces).\\n    \"\"\"\\n    def yield_record_strings(lines):\\n        record_lines = []\\n        start_line = 0\\n\\n        for line_num, line in enumerate(lines):\\n            record_lines.append(line)\\n            if line.endswith(' .\\n'):\\n                yield start_line, len(record_lines), ''.join(record_lines)\\n                record_lines = []\\n                start_line = line_num + 1\\n\\n    for start_line, num_lines, record_str in yield_record_strings(lines):\\n        record_match = _PRE_YARN_HISTORY_RECORD.match(record_str)\\n\\n        if not record_match:\\n            continue\\n\\n        record_type = record_match.group('type')\\n        key_pairs = record_match.group('key_pairs')\\n\\n        fields = {}\\n        for m in _PRE_YARN_HISTORY_KEY_PAIR.finditer(key_pairs):\\n            key = m.group('key')\\n            value = _pre_yarn_history_unescape(m.group('escaped_value'))\\n\\n            fields[key] = value\\n\\n        yield dict(\\n            fields=fields,\\n            num_lines=num_lines,\\n            start_line=start_line,\\n            type=record_type,\\n        )",
                    "func_fullName": "mrjob.logs.history._parse_pre_yarn_history_records( lines )"
                },
                {
                    "func_id": 1768,
                    "func_name": "_parse_pre_yarn_counters",
                    "func_desc": "_parse_pre_yarn_counters",
                    "func_file": "history",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_pre_yarn_counters(counters_str):\\n    \"\"\"Parse a COUNTERS field from a pre-YARN history file.\\n\\n    Returns a map from group to counter to amount.\\n    \"\"\"\\n    counters = {}\\n\\n    for group_match in _PRE_YARN_COUNTER_GROUP_RE.finditer(counters_str):\\n        group_name = _pre_yarn_history_unescape(\\n            group_match.group('group_name'))\\n\\n        group_counters = {}\\n\\n        for counter_match in _PRE_YARN_COUNTER_RE.finditer(\\n                group_match.group('counter_list_str')):\\n\\n            counter_name = _pre_yarn_history_unescape(\\n                counter_match.group('counter_name'))\\n            amount = int(counter_match.group('amount'))\\n\\n            group_counters[counter_name] = amount\\n\\n        counters[group_name] = group_counters\\n\\n    return counters",
                    "func_fullName": "mrjob.logs.history._parse_pre_yarn_counters( counters_str )"
                },
                {
                    "func_id": 1769,
                    "func_name": "_pre_yarn_history_unescape",
                    "func_desc": "_pre_yarn_history_unescape",
                    "func_file": "history",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _pre_yarn_history_unescape(s):\\n    \"\"\"Un-escape string from a pre-YARN history file.\"\"\"\\n    return _PRE_YARN_HISTORY_ESCAPE_RE.sub(r'\\1', s)",
                    "func_fullName": "mrjob.logs.history._pre_yarn_history_unescape( s )"
                },
                {
                    "func_id": 1770,
                    "func_name": "yield_record_strings",
                    "func_desc": "yield_record_strings",
                    "func_file": "history",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def yield_record_strings(lines):\\n        record_lines = []\\n        start_line = 0\\n\\n        for line_num, line in enumerate(lines):\\n            record_lines.append(line)\\n            if line.endswith(' .\\n'):\\n                yield start_line, len(record_lines), ''.join(record_lines)\\n                record_lines = []\\n                start_line = line_num + 1",
                    "func_fullName": "mrjob.logs.history.yield_record_strings( lines )"
                }
            ]
        },
        {
            "cluster_id": 5,
            "feature_id": 64,
            "feature_desc": "gamma=0.0743; k=5; a=0.25; combined=0.487; stability(ARI)=1.000; sep=0.160",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1722,
                    "func_name": "_format_counters",
                    "func_desc": "_format_counters",
                    "func_file": "counters",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _format_counters(counters, indent='\\t', desc='Counters'):\\n    \"\"\"Convert a map from group -> counter name -> amount to a message\\n    similar to that printed by the Hadoop binary, with no trailing newline.\\n    \"\"\"\\n    num_counters = sum(len(counter_to_amount)\\n                       for group, counter_to_amount in counters.items())\\n    message = '%s: %d' % (desc, num_counters)\\n\\n    for group, group_counters in sorted(counters.items()):\\n        if group_counters:\\n            message += '\\n%s%s' % (indent, group)\\n            for counter, amount in sorted(group_counters.items()):\\n                message += '\\n%s%s%s=%d' % (indent, indent, counter, amount)\\n\\n    return message",
                    "func_fullName": "mrjob.logs.counters._format_counters( counters, indent, desc )"
                },
                {
                    "func_id": 1723,
                    "func_name": "_pick_counters",
                    "func_desc": "_pick_counters",
                    "func_file": "counters",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _pick_counters(log_interpretation):\\n    \"\"\"Pick counters from a dictionary possibly containing\\n    step and history interpretations.\"\"\"\\n    for log_type in 'step', 'history':\\n        counters = log_interpretation.get(log_type, {}).get('counters')\\n        if counters:\\n            return counters\\n    else:\\n        return {}",
                    "func_fullName": "mrjob.logs.counters._pick_counters( log_interpretation )"
                },
                {
                    "func_id": 1751,
                    "func_name": "_pick_counters",
                    "func_desc": "_pick_counters",
                    "func_file": "mixin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _pick_counters(self, log_interpretation, step_type):\\n        \"\"\"Pick counters from our log interpretation, interpreting\\n        history logs if need be.\"\"\"\\n        if self._step_type_uses_spark(step_type):\\n            return {}\\n\\n        counters = _pick_counters(log_interpretation)\\n\\n        if self._read_logs():\\n            if not counters:\\n                log.info('Attempting to fetch counters from logs...')\\n                self._interpret_step_logs(log_interpretation, step_type)\\n                counters = _pick_counters(log_interpretation)\\n\\n            if not counters:\\n                self._interpret_history_log(log_interpretation)\\n                counters = _pick_counters(log_interpretation)\\n\\n        return counters",
                    "func_fullName": "mrjob.logs.mixin._pick_counters( self, log_interpretation, step_type )"
                },
                {
                    "func_id": 1816,
                    "func_name": "sort_key",
                    "func_desc": "sort_key",
                    "func_file": "errors",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def sort_key(error):\\n        spark_error = error['spark_error']\\n        msg = spark_error.get('message') or ''\\n        num_lines = spark_error.get('num_lines') or 1\\n\\n        return (\\n            _PYTHON_TRACEBACK_HEADER in msg,\\n            num_lines > 1,\\n            -num_lines,\\n        )",
                    "func_fullName": "mrjob.logs.errors.sort_key( error )"
                },
                {
                    "func_id": 1817,
                    "func_name": "sort_key",
                    "func_desc": "sort_key",
                    "func_file": "errors",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def sort_key(key_and_error):\\n        key, error = key_and_error\\n\\n        # key[0] is 'container_id' or 'time_key'\\n        return (bool(error.get('task_error')),\\n                key[0] == 'container_id',\\n                key[1:])",
                    "func_fullName": "mrjob.logs.errors.sort_key( key_and_error )"
                },
                {
                    "func_id": 2512,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "errors",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, msg, doc, pos, end=None):\\n        ValueError.__init__(self, errmsg(msg, doc, pos, end=end))\\n        self.msg = msg\\n        self.doc = doc\\n        self.pos = pos\\n        self.end = end\\n        self.lineno, self.colno = linecol(doc, pos)\\n        if end is not None:\\n            self.endlineno, self.endcolno = linecol(doc, end)\\n        else:\\n            self.endlineno, self.endcolno = None, None",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.errors.__init__( self, msg, doc, pos, end )"
                },
                {
                    "func_id": 2513,
                    "func_name": "__reduce__",
                    "func_desc": "__reduce__",
                    "func_file": "errors",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __reduce__(self):\\n        return self.__class__, (self.msg, self.doc, self.pos, self.end)",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.errors.__reduce__( self )"
                },
                {
                    "func_id": 1818,
                    "func_name": "_sort_by_recency",
                    "func_desc": "_sort_by_recency",
                    "func_file": "ids",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _sort_by_recency(ds):\\n    \"\"\"Sort the given list/sequence of dicts containing IDs so that the\\n    most recent ones come first (e.g. to find the best error, or the best\\n    log file to look for an error in).\\n    \"\"\"\\n    return sorted(ds, key=_time_sort_key, reverse=True)",
                    "func_fullName": "mrjob.logs.ids._sort_by_recency( ds )"
                },
                {
                    "func_id": 1819,
                    "func_name": "_sort_for_spark",
                    "func_desc": "_sort_for_spark",
                    "func_file": "ids",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _sort_for_spark(ds):\\n    \"\"\"Sort the given list/sequence of dicts in forward order by task/container\\n    ID, with most recent attempts first. This is used for finding errors on\\n    Spark, see #2056.\\n    \"\"\"\\n    return (\\n        sorted(\\n            sorted(\\n                sorted(\\n                    ds, key=_attempt_num, reverse=True),\\n                key=_container_num),\\n            key=_step_sort_key, reverse=True))",
                    "func_fullName": "mrjob.logs.ids._sort_for_spark( ds )"
                },
                {
                    "func_id": 1820,
                    "func_name": "_time_sort_key",
                    "func_desc": "_time_sort_key",
                    "func_file": "ids",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _time_sort_key(d):\\n    \"\"\"Sort key to sort the given dictionaries containing IDs roughly by time\\n    (earliest first).\\n\\n    We consider higher attempt_nums \"later\" than higher task_nums (of the\\n    same step type) because fatal errors usually occur on the final\\n    attempt of a task.\\n\\n    If we can, we convert (YARN) container IDs to attempt IDs. Unconverted\\n    container IDs are considered more \"recent\" than any task/attempt ID\\n    (these usually come from task logs).\\n    \"\"\"\\n    # when parsing task syslogs on YARN, we may end up with\\n    # container_id and nothing else. container IDs match with job ID\\n    # but aren't directly comparable to task and attempt IDs\\n\\n    # But if we couldn't parse the history file (for example because\\n    # we're using YARN on EMR and the only way to get it is SSHing in and\\n    # finding it on HDFS), we can use the container ID to infer the\\n    # job ID. After that, we just assume that errors with a container\\n    # ID must be better (they usually include the task error, after all),\\n    # so we treat them as more recent.\\n\\n    # break ID like\\n    # {application,attempt,task,job}_201601081945_0005[_m[_000005[_0]]]\\n    # into its component parts\\n    #\\n    # in practice, errors don't have job or application ID attached to\\n    # them (and we're only sorting errors from the same job/application)\\n    return (\\n        _timestamp(d),\\n        _step_num(d),\\n        d.get('container_id') or '',\\n        _task_type(d),\\n        _attempt_num(d),\\n        _task_num(d),\\n    )",
                    "func_fullName": "mrjob.logs.ids._time_sort_key( d )"
                },
                {
                    "func_id": 1821,
                    "func_name": "_step_sort_key",
                    "func_desc": "_step_sort_key",
                    "func_file": "ids",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _step_sort_key(d):\\n    \"\"\"Sort by timestamp and step\"\"\"\\n    return (\\n        _timestamp(d),\\n        _step_num(d),\\n    )",
                    "func_fullName": "mrjob.logs.ids._step_sort_key( d )"
                },
                {
                    "func_id": 1822,
                    "func_name": "_id_part",
                    "func_desc": "_id_part",
                    "func_file": "ids",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _id_part(id, i):\\n    \"\"\"Get the ith part of some container ID, e.g.\\n    container_1567013493699_0003_01_000002. If *id* is None\\n    or too short, return ''\"\"\"\\n    parts = (id or '').split('_')\\n\\n    if i >= len(parts):\\n        return ''\\n    else:\\n        return parts[i]",
                    "func_fullName": "mrjob.logs.ids._id_part( id, i )"
                },
                {
                    "func_id": 1823,
                    "func_name": "_any_id",
                    "func_desc": "_any_id",
                    "func_file": "ids",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _any_id(d):\\n    return (d.get('attempt_id') or\\n            d.get('task_id') or d.get('job_id') or\\n            d.get('application_id') or\\n            d.get('container_id'))",
                    "func_fullName": "mrjob.logs.ids._any_id( d )"
                },
                {
                    "func_id": 1824,
                    "func_name": "_timestamp",
                    "func_desc": "_timestamp",
                    "func_file": "ids",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _timestamp(d):\\n    \"\"\"cluster timestamp. the first number in any ID.\\n\\n    For example, in container_1567013493699_0003_01_000002, it's\\n    1567013493699.\"\"\"\\n    return _id_part(_any_id(d), 1)",
                    "func_fullName": "mrjob.logs.ids._timestamp( d )"
                },
                {
                    "func_id": 1825,
                    "func_name": "_step_num",
                    "func_desc": "_step_num",
                    "func_file": "ids",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _step_num(d):\\n    \"\"\"Number that indicates this is the nth task to be launched on the\\n    cluster. The second number in any ID.\\n\\n    For example, in container_1567013493699_0003_01_000002, it's 0003.\\n    \"\"\"\\n    return _id_part(_any_id(d), 2)",
                    "func_fullName": "mrjob.logs.ids._step_num( d )"
                },
                {
                    "func_id": 1827,
                    "func_name": "_task_num",
                    "func_desc": "_task_num",
                    "func_file": "ids",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _task_num(d):\\n    \"\"\"In task or attempt IDs, a unique number for the individual\\n    task within the map or reduce phase.\\n\\n    For example, in\\n    task_201601081945_0005_m_000005, it's 000005\\n    \"\"\"\\n    return _id_part(d.get('attempt_id') or d.get('task_id'), 4)",
                    "func_fullName": "mrjob.logs.ids._task_num( d )"
                },
                {
                    "func_id": 1828,
                    "func_name": "_container_num",
                    "func_desc": "_container_num",
                    "func_file": "ids",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _container_num(d):\\n    \"\"\"In container IDs, a unique number for the task assigned to the\\n    container. Could be a map, a reduce, or part of a Spark\\n    task; YARN doesn't care.\\n\\n    For example, in container_1567013493699_0003_01_000002, it's 000002.\\n    \"\"\"\\n    return _id_part(d.get('container_id'), 4)",
                    "func_fullName": "mrjob.logs.ids._container_num( d )"
                },
                {
                    "func_id": 1829,
                    "func_name": "_attempt_num",
                    "func_desc": "_attempt_num",
                    "func_file": "ids",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _attempt_num(d):\\n    \"\"\"Which attempt this is for a particular task; typically tasks that\\n    fail are retried one or two times. This applies to both attempt\\n    and container IDs\\n\\n    In container_1567013493699_0003_01_000002, attempt num is 01\\n\\n    In attempt_201601081945_0005_m_000005_0, attempt num is 0\\n    \"\"\"\\n    return (_id_part(d.get('attempt_id'), 5) or\\n            _id_part(d.get('container_id'), 3))",
                    "func_fullName": "mrjob.logs.ids._attempt_num( d )"
                }
            ]
        },
        {
            "cluster_id": 5,
            "feature_id": 65,
            "feature_desc": "gamma=0.0743; k=5; a=0.25; combined=0.487; stability(ARI)=1.000; sep=0.160",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1734,
                    "func_name": "_log_parsing_task_log",
                    "func_desc": "_log_parsing_task_log",
                    "func_file": "mixin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _log_parsing_task_log(log_path):\\n    log.info('  Parsing task log: %s' % log_path)",
                    "func_fullName": "mrjob.logs.mixin._log_parsing_task_log( log_path )"
                },
                {
                    "func_id": 1748,
                    "func_name": "_stream_history_log_dirs",
                    "func_desc": "_stream_history_log_dirs",
                    "func_file": "mixin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _stream_history_log_dirs(self, output_dir=None):\\n        \"\"\"Yield lists of directories (usually, URIs) to search for history\\n        logs in.\\n\\n        Usually, you'll want to add logging messages (e.g.\\n        'Searching for history logs in ...'\\n\\n        :param output_dir: Output directory for step (optional), to look\\n            for logs (e.g. on Cloudera).\\n        \"\"\"\\n        return ()",
                    "func_fullName": "mrjob.logs.mixin._stream_history_log_dirs( self, output_dir )"
                },
                {
                    "func_id": 1749,
                    "func_name": "_stream_task_log_dirs",
                    "func_desc": "_stream_task_log_dirs",
                    "func_file": "mixin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _stream_task_log_dirs(self, application_id=None, output_dir=None):\\n        \"\"\"Yield lists of directories (usually, URIs) to search for task\\n        logs in.\\n\\n        Usually, you'll want to add logging messages (e.g.\\n        'Searching for task syslogs in...')\\n\\n        :param application_id: YARN application ID (optional), so we can ls\\n            the relevant subdirectory of `userlogs/` rather than the whole\\n            thing\\n        :param output_dir: Output directory for step (optional), to look\\n            for logs (e.g. on Cloudera).\\n        \"\"\"\\n        # sometimes pre-YARN logs are organized by job ID, but not always,\\n        # so we don't bother with job_id; just ls() the entire userlogs\\n        # dir and depend on regexes to find the right subdir.\\n        return ()",
                    "func_fullName": "mrjob.logs.mixin._stream_task_log_dirs( self, application_id, output_dir )"
                },
                {
                    "func_id": 1750,
                    "func_name": "_get_step_log_interpretation",
                    "func_desc": "_get_step_log_interpretation",
                    "func_file": "mixin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_step_log_interpretation(self, log_interpretation, step_type):\\n        \"\"\"Return interpretation of the step log. Either implement\\n        this, or fill ``'step'`` yourself (e.g. from Hadoop binary's\\n        output.\"\"\"\\n        return None",
                    "func_fullName": "mrjob.logs.mixin._get_step_log_interpretation( self, log_interpretation, step_type )"
                },
                {
                    "func_id": 1753,
                    "func_name": "_logs_needed_to_pick_error",
                    "func_desc": "_logs_needed_to_pick_error",
                    "func_file": "mixin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _logs_needed_to_pick_error(self, step_type):\\n        \"\"\"We don't need all the logs when interpreting Spark steps\"\"\"\\n        if self._step_type_uses_spark(step_type):\\n            if self._spark_deploy_mode() == 'cluster':\\n                return ('step', 'task')\\n            else:\\n                return ('step',)\\n        else:\\n            return ('step', 'history', 'task')",
                    "func_fullName": "mrjob.logs.mixin._logs_needed_to_pick_error( self, step_type )"
                },
                {
                    "func_id": 1754,
                    "func_name": "_interpret_history_log",
                    "func_desc": "_interpret_history_log",
                    "func_file": "mixin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _interpret_history_log(self, log_interpretation):\\n        \"\"\"Fetch history log and add 'history' to log_interpretation.\"\"\"\\n        if 'history' in log_interpretation:\\n            return   # already interpreted\\n\\n        if not self._read_logs():\\n            return  # nothing to do\\n\\n        step_interpretation = log_interpretation.get('step') or {}\\n\\n        job_id = step_interpretation.get('job_id')\\n        if not job_id:\\n            if not log_interpretation.get('no_job'):\\n                log.warning(\"Can't fetch history log; missing job ID\")\\n            return\\n\\n        output_dir = step_interpretation.get('output_dir')\\n\\n        log_interpretation['history'] = _interpret_history_log(\\n            self.fs, self._ls_history_logs(\\n                job_id=job_id, output_dir=output_dir))",
                    "func_fullName": "mrjob.logs.mixin._interpret_history_log( self, log_interpretation )"
                },
                {
                    "func_id": 1755,
                    "func_name": "_ls_history_logs",
                    "func_desc": "_ls_history_logs",
                    "func_file": "mixin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ls_history_logs(self, job_id=None, output_dir=None):\\n        \"\"\"Yield history log matches, logging a message for each one.\"\"\"\\n        if not self._read_logs():\\n            return\\n\\n        for match in _ls_history_logs(\\n                self.fs,\\n                self._stream_history_log_dirs(output_dir=output_dir),\\n                job_id=job_id):\\n            log.info('  Parsing history log: %s' % match['path'])\\n            yield match",
                    "func_fullName": "mrjob.logs.mixin._ls_history_logs( self, job_id, output_dir )"
                },
                {
                    "func_id": 1756,
                    "func_name": "_interpret_step_logs",
                    "func_desc": "_interpret_step_logs",
                    "func_file": "mixin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _interpret_step_logs(self, log_interpretation, step_type):\\n        \"\"\"Add *step* to the log interpretation, if it's not already there.\"\"\"\\n        if 'step' in log_interpretation:\\n            return\\n\\n        if not self._read_logs():\\n            return\\n\\n        step_interpretation = self._get_step_log_interpretation(\\n            log_interpretation, step_type)\\n        if step_interpretation:\\n            log_interpretation['step'] = step_interpretation",
                    "func_fullName": "mrjob.logs.mixin._interpret_step_logs( self, log_interpretation, step_type )"
                },
                {
                    "func_id": 1757,
                    "func_name": "_interpret_task_logs",
                    "func_desc": "_interpret_task_logs",
                    "func_file": "mixin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _interpret_task_logs(\\n            self, log_interpretation, step_type, error_attempt_ids=(),\\n            partial=True):\\n        \"\"\"Fetch task syslogs and stderr, and add 'task' to interpretation.\"\"\"\\n        if 'task' in log_interpretation and (\\n                partial or not log_interpretation['task'].get('partial')):\\n            return   # already interpreted\\n\\n        if not self._read_logs():\\n            return\\n\\n        step_interpretation = log_interpretation.get('step') or {}\\n\\n        application_id = step_interpretation.get('application_id')\\n        job_id = step_interpretation.get('job_id')\\n        output_dir = step_interpretation.get('output_dir')\\n\\n        yarn = uses_yarn(self.get_hadoop_version())\\n\\n        attempt_to_container_id = log_interpretation.get('history', {}).get(\\n            'attempt_to_container_id', {})\\n\\n        if yarn:\\n            if not application_id:\\n                if not log_interpretation.get('no_job'):\\n                    log.warning(\\n                        \"Can't fetch task logs; missing application ID\")\\n                return\\n        else:\\n            if not job_id:\\n                if not log_interpretation.get('no_job'):\\n                    log.warning(\"Can't fetch task logs; missing job ID\")\\n                return\\n\\n        if self._step_type_uses_spark(step_type):\\n            interpret_func = _interpret_spark_logs\\n        else:\\n            interpret_func = _interpret_task_logs\\n\\n        log_interpretation['task'] = interpret_func(\\n            self.fs,\\n            self._ls_task_logs(\\n                step_type,\\n                application_id=application_id,\\n                job_id=job_id,\\n                output_dir=output_dir,\\n                error_attempt_ids=error_attempt_ids,\\n                attempt_to_container_id=attempt_to_container_id,\\n            ),\\n            partial=partial,\\n            log_callback=_log_parsing_task_log)",
                    "func_fullName": "mrjob.logs.mixin._interpret_task_logs( self, log_interpretation, step_type, error_attempt_ids, partial )"
                },
                {
                    "func_id": 1758,
                    "func_name": "_ls_task_logs",
                    "func_desc": "_ls_task_logs",
                    "func_file": "mixin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ls_task_logs(self, step_type,\\n                      application_id=None, job_id=None, output_dir=None,\\n                      error_attempt_ids=None, attempt_to_container_id=None):\\n        \"\"\"Yield task log matches.\"\"\"\\n        if not self._read_logs():\\n            return\\n\\n        if self._step_type_uses_spark(step_type):\\n            ls_func = _ls_spark_task_logs\\n        else:\\n            ls_func = _ls_task_logs\\n\\n        # logging messages are handled by a callback in _interpret_task_logs()\\n        matches = ls_func(\\n            self.fs,\\n            self._stream_task_log_dirs(\\n                application_id=application_id, output_dir=output_dir),\\n            application_id=application_id,\\n            job_id=job_id,\\n            error_attempt_ids=error_attempt_ids,\\n            attempt_to_container_id=attempt_to_container_id,\\n        )\\n\\n        for match in matches:\\n            yield match",
                    "func_fullName": "mrjob.logs.mixin._ls_task_logs( self, step_type, application_id, job_id, output_dir, error_attempt_ids, attempt_to_container_id )"
                },
                {
                    "func_id": 1759,
                    "func_name": "_log_counters",
                    "func_desc": "_log_counters",
                    "func_file": "mixin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _log_counters(self, log_interpretation, step_num):\\n        \"\"\"Utility for logging counters (if any) for a step.\"\"\"\\n        step_type = self._get_step(step_num)['type']\\n\\n        if not self._step_type_uses_spark(step_type):\\n            counters = self._pick_counters(\\n                log_interpretation, step_type)\\n            if counters:\\n                log.info(_format_counters(counters))\\n            elif self._read_logs():\\n                # should only log this if we actually looked for counters\\n                log.warning('No counters found')",
                    "func_fullName": "mrjob.logs.mixin._log_counters( self, log_interpretation, step_num )"
                },
                {
                    "func_id": 1760,
                    "func_name": "_read_logs",
                    "func_desc": "_read_logs",
                    "func_file": "mixin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _read_logs(self):\\n        \"\"\"If this is false, we shouldn't attempt to list or cat logs.\"\"\"\\n        return self._opts.get('read_logs', True)",
                    "func_fullName": "mrjob.logs.mixin._read_logs( self )"
                },
                {
                    "func_id": 1761,
                    "func_name": "_ls_history_logs",
                    "func_desc": "_ls_history_logs",
                    "func_file": "history",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _ls_history_logs(fs, log_dir_stream, job_id=None):\\n    \"\"\"Yield matching files, optionally filtering by *job_id*. Yields dicts\\n    with the keys:\\n\\n    job_id: job_id in path (must match *job_id* if set)\\n    path: path/URI of log file\\n    yarn: true if this is a YARN log file\\n\\n    *log_dir_stream* is a sequence of lists of log dirs. For each list, we'll\\n    look in all directories, and if we find any logs, we'll stop. (The\\n    assumption is that subsequent lists of log dirs would have copies\\n    of the same logs, just in a different location.\\n    \"\"\"\\n    return _ls_logs(fs, log_dir_stream, _match_history_log_path,\\n                    job_id=job_id)",
                    "func_fullName": "mrjob.logs.history._ls_history_logs( fs, log_dir_stream, job_id )"
                },
                {
                    "func_id": 1762,
                    "func_name": "_match_history_log_path",
                    "func_desc": "_match_history_log_path",
                    "func_file": "history",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _match_history_log_path(path, job_id=None):\\n    \"\"\"Yield paths/uris of all job history files in the given directories,\\n    optionally filtering by *job_id*.\\n    \"\"\"\\n    m = _HISTORY_LOG_PATH_RE.match(path)\\n    if not m:\\n        return None\\n\\n    if not (job_id is None or m.group('job_id') == job_id):\\n        return None\\n\\n    # TODO: couldn't manage to include .jhist in regex; an optional\\n    # group has less priority than a non-greedy match, apparently\\n    return dict(job_id=m.group('job_id'), yarn='.jhist' in m.group('suffix'))",
                    "func_fullName": "mrjob.logs.history._match_history_log_path( path, job_id )"
                },
                {
                    "func_id": 1763,
                    "func_name": "_interpret_history_log",
                    "func_desc": "_interpret_history_log",
                    "func_file": "history",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _interpret_history_log(fs, matches):\\n    \"\"\"Extract counters and errors from history log.\\n\\n    Matches is a list of dicts with the keys *job_id* and *yarn*\\n    (see :py:func:`_ls_history_logs()`)\\n\\n    We expect *matches* to contain at most one match; further matches\\n    will be ignored.\\n\\n    Returns a dictionary with the keys *counters* and *errors*.\\n    \"\"\"\\n    # we expect to go through this for loop 0 or 1 times\\n    for match in matches:\\n        path = match['path']\\n\\n        if match['yarn']:\\n            # not yet implemented\\n            result = _parse_yarn_history_log(_cat_log_lines(fs, path))\\n        else:\\n            result = _parse_pre_yarn_history_log(_cat_log_lines(fs, path))\\n\\n        # patch path, task_id, etc. into errors\\n        for error in result.get('errors') or ():\\n            if 'hadoop_error' in error:\\n                error['hadoop_error']['path'] = path\\n            _add_implied_task_id(error)\\n\\n        return result\\n\\n    return {}",
                    "func_fullName": "mrjob.logs.history._interpret_history_log( fs, matches )"
                },
                {
                    "func_id": 1771,
                    "func_name": "_ls_task_logs",
                    "func_desc": "_ls_task_logs",
                    "func_file": "task",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _ls_task_logs(fs, log_dir_stream, application_id=None, job_id=None,\\n                  error_attempt_ids=None, attempt_to_container_id=None):\\n    \"\"\"Yield matching logs, optionally filtering by application_id\\n    or job_id.\\n\\n    This will yield matches for stderr logs first, followed by syslogs. stderr\\n    logs will have a 'syslog' field pointing to the match for the\\n    corresponding syslog (stderr logs without a corresponding syslog won't be\\n    included).\\n    \"\"\"\\n    return _ls_task_logs_helper(\\n        fs, log_dir_stream, is_spark=False,\\n        application_id=application_id, job_id=job_id,\\n        error_attempt_ids=error_attempt_ids,\\n        attempt_to_container_id=attempt_to_container_id)",
                    "func_fullName": "mrjob.logs.task._ls_task_logs( fs, log_dir_stream, application_id, job_id, error_attempt_ids, attempt_to_container_id )"
                },
                {
                    "func_id": 1772,
                    "func_name": "_ls_spark_task_logs",
                    "func_desc": "_ls_spark_task_logs",
                    "func_file": "task",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _ls_spark_task_logs(\\n        fs, log_dir_stream, application_id=None, job_id=None,\\n        error_attempt_ids=None, attempt_to_container_id=None):\\n    \"\"\"Yield matching Spark logs, optionally filtering by application_id\\n    or job_id.\\n\\n    This will yield matches for stderr logs only. stderr\\n    logs will have a 'stdout' field pointing to the match for the\\n    corresponding stdout file; whether we process this depends on the content\\n    of the stderr file.\\n    \"\"\"\\n    return _ls_task_logs_helper(\\n        fs, log_dir_stream, is_spark=True,\\n        application_id=application_id, job_id=job_id,\\n        error_attempt_ids=error_attempt_ids,\\n        attempt_to_container_id=attempt_to_container_id)",
                    "func_fullName": "mrjob.logs.task._ls_spark_task_logs( fs, log_dir_stream, application_id, job_id, error_attempt_ids, attempt_to_container_id )"
                },
                {
                    "func_id": 1773,
                    "func_name": "_ls_task_logs_helper",
                    "func_desc": "_ls_task_logs_helper",
                    "func_file": "task",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _ls_task_logs_helper(fs, log_dir_stream, is_spark,\\n                         application_id=None, job_id=None,\\n                         error_attempt_ids=None, attempt_to_container_id=None):\\n    \"\"\"Helper for _ls_task_logs() and _ls_spark_task_logs().\\n\\n    *syslog_type* is the type of the log to pair with stderr logs.\\n\\n    This is actually a bit weird; on Spark, 'stderr' is the equivalent\\n    of syslog in Streaming, and 'stdout' is the equivlend of Streaming's\\n    stderr.\\n\\n    For Streaming, we want stderr logs with corresponding syslogs, and if after\\n    listing all task logs we don't find any, syslogs.\\n\\n    For Spark, we want stderr logs (which are equivalent to Streaming syslogs)\\n    with corresponding stdouts, and if after listing all task logs we don't\\n    find any, stderr logs without corresponding stdouts.\\n    \"\"\"\\n    syslog_type = 'stdout' if is_spark else 'syslog'\\n\\n    error_attempt_ids = error_attempt_ids or ()\\n\\n    # figure out subdirs to look for logs in\\n    if attempt_to_container_id:\\n        # YARN\\n        subdirs = [\\n            attempt_to_container_id[a] for a in error_attempt_ids\\n            if a in attempt_to_container_id]\\n    else:\\n        subdirs = list(error_attempt_ids)\\n\\n    # only look in subdirs corresponding to failed attempts\\n    if subdirs:\\n        log_subdir_stream = ([\\n            fs.join(log_dir, subdir)\\n            for subdir in subdirs\\n            for log_dir in log_dir_list\\n        ] for log_dir_list in log_dir_stream)\\n    else:\\n        log_subdir_stream = log_dir_stream\\n\\n    key_to_type_to_match = defaultdict(dict)\\n\\n    # less desirable errors to yield if we don't find the ones we want\\n    other_matches = []\\n\\n    for match in _ls_logs(\\n            fs, log_subdir_stream, _match_task_log_path, is_spark,\\n            application_id=application_id,\\n            job_id=job_id):\\n\\n        log_key = _log_key(match)\\n        log_type = match['log_type']\\n\\n        if log_type not in ('stderr', syslog_type):\\n            continue  # don't care\\n\\n        type_to_match = key_to_type_to_match[log_key]\\n\\n        if log_type in type_to_match:\\n            continue  # already seen\\n\\n        type_to_match[log_type] = match\\n\\n        # yield stderrs with syslogs as we find them\\n        if 'stderr' in type_to_match and syslog_type in type_to_match:\\n            stderr_match = type_to_match['stderr']\\n            syslog_match = type_to_match[syslog_type]\\n\\n            stderr_match[syslog_type] = syslog_match\\n\\n            yield stderr_match\\n\\n        if log_type == ('stderr' if is_spark else syslog_type):\\n            other_matches.append(match)\\n\\n    # yield logs that don't have both syslog and stderr\\n    for other_match in other_matches:\\n        if syslog_type not in other_match:  # already yielded\\n            yield other_match",
                    "func_fullName": "mrjob.logs.task._ls_task_logs_helper( fs, log_dir_stream, is_spark, application_id, job_id, error_attempt_ids, attempt_to_container_id )"
                },
                {
                    "func_id": 1774,
                    "func_name": "_log_key",
                    "func_desc": "_log_key",
                    "func_file": "task",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _log_key(match):\\n    \"\"\"Helper method for _ls_task_logs() and _ls_spark_task_logs().\"\"\"\\n    return tuple((k, v) for k, v in sorted(match.items())\\n                 if k not in ('log_type', 'path'))",
                    "func_fullName": "mrjob.logs.task._log_key( match )"
                },
                {
                    "func_id": 1775,
                    "func_name": "_match_task_log_path",
                    "func_desc": "_match_task_log_path",
                    "func_file": "task",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _match_task_log_path(path, application_id=None, job_id=None):\\n    \"\"\"Is this the path/URI of a task log? (Including Spark)\\n\\n    If so, return a dictionary containing application_id and container_id\\n    (on YARN) or attempt_id (on pre-YARN Hadoop), plus log_type (either\\n    stdout, stderr, or syslog).\\n\\n    Otherwise, return None\\n\\n    Optionally, filter by application_id (YARN) or job_id (pre-YARN).\\n    \"\"\"\\n    from .ids import _to_job_id\\n    m = _PRE_YARN_TASK_LOG_PATH_RE.match(path)\\n    if m:\\n        if job_id and job_id != _to_job_id(m.group('attempt_id')):\\n            return None  # matches, but wrong job_id\\n\\n        return dict(\\n            attempt_id=m.group('attempt_id'),\\n            log_type=m.group('log_type'))\\n\\n    m = _YARN_TASK_LOG_PATH_RE.match(path)\\n    if m:\\n        if application_id and application_id != m.group('application_id'):\\n            return None  # matches, but wrong application_id\\n\\n        return dict(\\n            application_id=m.group('application_id'),\\n            container_id=m.group('container_id'),\\n            log_type=m.group('log_type'))\\n\\n    return None",
                    "func_fullName": "mrjob.logs.task._match_task_log_path( path, application_id, job_id )"
                },
                {
                    "func_id": 1776,
                    "func_name": "_interpret_task_logs",
                    "func_desc": "_interpret_task_logs",
                    "func_file": "task",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _interpret_task_logs(fs, matches, partial=True, log_callback=None):\\n    \"\"\"Look for errors in task syslog/stderr.\\n\\n    If *partial* is true (the default), stop when we find the first error\\n    that includes a *task_error*.\\n\\n    If *log_callback* is set, every time we're about to parse a\\n        file, call it with a single argument, the path of that file\\n\\n    Returns a dictionary possibly containing the key 'errors', which\\n    is a dict containing:\\n\\n    hadoop_error:\\n        message: string containing error message and Java exception\\n        num_lines: number of lines in syslog this takes up\\n        path: syslog we read this error from\\n        start_line: where in syslog exception starts (0-indexed)\\n    split: (optional)\\n        path: URI of input file task was processing\\n        num_lines: (optional) number of lines in split\\n        start_line: (optional) first line of split (0-indexed)\\n    task_error:\\n        message: command and error message from task, as a string\\n        num_lines: number of lines in stderr this takes up\\n        path: stderr we read this from\\n        start_line: where in stderr error message starts (0-indexed)\\n\\n    In addition, if *partial* is set to true (and we found an error),\\n    this dictionary will contain the key *partial*, set to True.\\n    \"\"\"\\n    result = {}\\n    syslogs_parsed = set()\\n\\n    for match in matches:\\n        error = {}\\n\\n        # are is this match for a stderr file, or a syslog?\\n        if match.get('syslog'):\\n            stderr_path = match['path']\\n            syslog_path = match['syslog']['path']\\n        else:\\n            stderr_path = None\\n            syslog_path = match['path']\\n\\n        if stderr_path:\\n            if log_callback:\\n                log_callback(stderr_path)\\n            task_error = _parse_task_stderr(_cat_log_lines(fs, stderr_path))\\n\\n            if task_error:\\n                task_error['path'] = stderr_path\\n                error['task_error'] = task_error\\n            else:\\n                continue  # can parse syslog independently later\\n\\n        # already parsed this syslog in conjunction with an earlier task error\\n        if syslog_path in syslogs_parsed:\\n            continue\\n\\n        if log_callback:\\n            log_callback(syslog_path)\\n        syslog_error = _parse_task_syslog(_cat_log_lines(fs, syslog_path))\\n        syslogs_parsed.add(syslog_path)\\n\\n        if not syslog_error.get('hadoop_error'):\\n            # if no entry in Hadoop syslog, probably just noise\\n            continue\\n\\n        error.update(syslog_error)\\n        error['hadoop_error']['path'] = syslog_path\\n\\n        # patch in IDs we learned from path\\n        for id_key in 'attempt_id', 'container_id':\\n            if id_key in match:\\n                error[id_key] = match[id_key]\\n        _add_implied_task_id(error)\\n\\n        result.setdefault('errors', [])\\n        result['errors'].append(error)\\n\\n        if partial:\\n            result['partial'] = True\\n            break\\n\\n    return result",
                    "func_fullName": "mrjob.logs.task._interpret_task_logs( fs, matches, partial, log_callback )"
                },
                {
                    "func_id": 1777,
                    "func_name": "_parse_task_syslog",
                    "func_desc": "_parse_task_syslog",
                    "func_file": "task",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_task_syslog(lines):\\n    \"\"\"Parse an error out of a syslog file (or a Spark stderr file).\\n\\n    Returns a dict, possibly containing the following keys:\\n\\n    check_stdout:\\n        if true, we should look for task errors in the corresponding\\n        'stdout' file. Used for Spark logs.\\n    hadoop_error:\\n        message: string containing error message and Java exception\\n        num_lines: number of lines in syslog this takes up\\n        start_line: where in syslog exception starts (0-indexed)\\n    split: (optional)\\n        path: URI of input file task was processing\\n        num_lines: (optional) number of lines in split\\n        start_line: (optional) first line of split (0-indexed)\\n    \"\"\"\\n    from .log4j import _parse_hadoop_log4j_records\\n    return _parse_task_syslog_records(_parse_hadoop_log4j_records(lines))",
                    "func_fullName": "mrjob.logs.task._parse_task_syslog( lines )"
                },
                {
                    "func_id": 1778,
                    "func_name": "_parse_task_syslog_records",
                    "func_desc": "_parse_task_syslog_records",
                    "func_file": "task",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_task_syslog_records(records):\\n    \"\"\"Helper for _parse_task_syslog(); takes log4j records rather than\\n    lines\"\"\"\\n    result = {}\\n\\n    for record in records:\\n        message = record['message']\\n\\n        m = _OPENING_FOR_READING_RE.match(message)\\n        if m:\\n            result['split'] = dict(path=m.group('path'))\\n            continue\\n\\n        m = _YARN_INPUT_SPLIT_RE.match(message)\\n        if m:\\n            result['split'] = dict(\\n                path=m.group('path'),\\n                start_line=int(m.group('start_line')),\\n                num_lines=int(m.group('num_lines')))\\n            continue\\n\\n        m = _JAVA_TRACEBACK_RE.search(message)\\n        if m:\\n            result['hadoop_error'] = dict(\\n                message=message,\\n                num_lines=record['num_lines'],\\n                start_line=record['start_line'],\\n            )\\n            break  # nothing to do once we've found the error\\n\\n        if (record['logger'] == _SPARK_APP_MASTER_LOGGER and\\n                record['level'] == 'ERROR'):\\n            m = _SPARK_APP_EXITED_RE.match(message)\\n            if m:\\n                result['hadoop_error'] = dict(\\n                    message=message,\\n                    num_lines=record['num_lines'],\\n                    start_line=record['start_line'],\\n                )\\n                result['check_stdout'] = True\\n                break  # nothing else to do once we've found the error\\n\\n    return result",
                    "func_fullName": "mrjob.logs.task._parse_task_syslog_records( records )"
                },
                {
                    "func_id": 1779,
                    "func_name": "_parse_task_stderr",
                    "func_desc": "_parse_task_stderr",
                    "func_file": "task",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_task_stderr(lines):\\n    \"\"\"Attempt to explain any error in task stderr, be it a Python\\n    exception or a problem with a setup command (see #1203).\\n\\n    Looks for '+ ' followed by a command line, and then the command's\\n    stderr. If there are no such lines (because we're not using a setup\\n    script), assumes the entire file contents are the cause of error.\\n\\n    Returns a task error dictionary with the following keys, or None\\n    if the file is empty.\\n\\n    message: a string (e.g. Python command line followed by Python traceback)\\n    start_line: where in lines message appears (0-indexed)\\n    num_lines: how may lines the message takes up\\n    \"\"\"\\n    task_error = None\\n    stack_trace_start_line = None\\n\\n    for line_num, line in enumerate(lines):\\n        line = line.rstrip('\\r\\n')\\n\\n        # ignore \"subprocess failed\" stack trace\\n        if _SUBPROCESS_FAILED_STACK_TRACE_START.match(line):\\n            stack_trace_start_line = line_num\\n            continue\\n\\n        # once we detect a stack trace, keep ignoring lines until\\n        # we find a non-indented one\\n        if stack_trace_start_line is not None:\\n            if line.lstrip() != line:\\n                continue\\n            else:\\n                stack_trace_start_line = None\\n\\n        # ignore warnings about initializing log4j, counters, etc.\\n        if any(ir.match(line) for ir in _TASK_STDERR_IGNORE_RES):\\n            # ignored lines shouldn't count as part of the line range\\n            if task_error and task_error.get('num_lines') is None:\\n                task_error['num_lines'] = line_num - task_error['start_line']\\n            continue\\n        elif not task_error or line.startswith('+ '):\\n            # stderr log should only contain counters and status\\n            # messages in local mode\\n            task_error = dict(\\n                message=line,\\n                start_line=line_num)\\n        else:\\n            task_error['message'] += '\\n' + line\\n            task_error['num_lines'] = None\\n\\n    if task_error:\\n        if task_error.get('num_lines') is None:\\n            end_line = stack_trace_start_line or (line_num + 1)\\n            task_error['num_lines'] = end_line - task_error['start_line']\\n        return task_error\\n    else:\\n        return None",
                    "func_fullName": "mrjob.logs.task._parse_task_stderr( lines )"
                },
                {
                    "func_id": 1809,
                    "func_name": "_is_probably_task_error",
                    "func_desc": "_is_probably_task_error",
                    "func_file": "errors",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _is_probably_task_error(error):\\n    \"\"\"Used to identify task errors.\"\"\"\\n    return ('subprocess failed' in\\n            error.get('hadoop_error', {}).get('message', ''))",
                    "func_fullName": "mrjob.logs.errors._is_probably_task_error( error )"
                },
                {
                    "func_id": 1811,
                    "func_name": "_log_probable_cause_of_failure",
                    "func_desc": "_log_probable_cause_of_failure",
                    "func_file": "errors",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _log_probable_cause_of_failure(log, error):\\n    \"\"\"Log \"probable cause of failure\" log message.\"\"\"\\n    log.error('\\nProbable cause of failure:\\n\\n%s\\n\\n' % _format_error(error))",
                    "func_fullName": "mrjob.logs.errors._log_probable_cause_of_failure( log, error )"
                },
                {
                    "func_id": 1826,
                    "func_name": "_task_type",
                    "func_desc": "_task_type",
                    "func_file": "ids",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _task_type(d):\\n    \"\"\"In task or attempt IDs, whether this is a map or reduce\\n    (not used by Spark).\\n\\n    For example, in\\n    attempt_201601081945_0005_m_000005_0, it's \"m\"\\n    \"\"\"\\n    return _id_part(d.get('attempt_id') or d.get('task_id'), 3)",
                    "func_fullName": "mrjob.logs.ids._task_type( d )"
                },
                {
                    "func_id": 1830,
                    "func_name": "_add_implied_task_id",
                    "func_desc": "_add_implied_task_id",
                    "func_file": "ids",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _add_implied_task_id(d):\\n    \"\"\"If *d* (a dictionary) has *attempt_id* but not *task_id*, add it.\\n\\n    Use this on errors.\\n    \"\"\"\\n    # NOTE: container IDs look similar to task IDs, but there's a single\\n    # numbering system for both map and reduce tasks\\n    if d.get('attempt_id') and not d.get('task_id'):\\n        d['task_id'] = _attempt_id_to_task_id(\\n            d['attempt_id'])",
                    "func_fullName": "mrjob.logs.ids._add_implied_task_id( d )"
                },
                {
                    "func_id": 1832,
                    "func_name": "_attempt_id_to_task_id",
                    "func_desc": "_attempt_id_to_task_id",
                    "func_file": "ids",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _attempt_id_to_task_id(attempt_id):\\n    \"\"\"Convert e.g. ``'attempt_201601081945_0005_m_000005_0'``\\n    to ``'task_201601081945_0005_m_000005'``\"\"\"\\n    return 'task_' + '_'.join(attempt_id.split('_')[1:5])",
                    "func_fullName": "mrjob.logs.ids._attempt_id_to_task_id( attempt_id )"
                }
            ]
        },
        {
            "cluster_id": 5,
            "feature_id": 66,
            "feature_desc": "gamma=0.0743; k=5; a=0.25; combined=0.487; stability(ARI)=1.000; sep=0.160",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1752,
                    "func_name": "_pick_error",
                    "func_desc": "_pick_error",
                    "func_file": "mixin",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _pick_error(self, log_interpretation, step_type):\\n        \"\"\"Pick probable cause of failure (only call this if job fails).\"\"\"\\n        from mrjob.logs.errors import _pick_error_attempt_ids\\n        from mrjob.logs.errors import _pick_error\\n        logs_needed = self._logs_needed_to_pick_error(step_type)\\n\\n        if self._read_logs() and not all(\\n                log_type in log_interpretation for log_type in logs_needed):\\n            log.info('Scanning logs for probable cause of failure...')\\n\\n            if 'step' in logs_needed:\\n                self._interpret_step_logs(log_interpretation, step_type)\\n\\n            if 'history' in logs_needed:\\n                self._interpret_history_log(log_interpretation)\\n\\n            if 'task' in logs_needed:\\n                error_attempt_ids = _pick_error_attempt_ids(log_interpretation)\\n\\n                self._interpret_task_logs(\\n                    log_interpretation, step_type, error_attempt_ids)\\n\\n        return _pick_error(log_interpretation)",
                    "func_fullName": "mrjob.logs.mixin._pick_error( self, log_interpretation, step_type )"
                },
                {
                    "func_id": 1805,
                    "func_name": "_pick_error",
                    "func_desc": "_pick_error",
                    "func_file": "errors",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _pick_error(log_interpretation):\\n    \"\"\"Pick most recent error from a dictionary possibly containing\\n    step, history, and task interpretations. Returns None if there\\n    are no errors.\\n    \"\"\"\\n    errors = _extract_errors(log_interpretation)\\n\\n    # no point in merging spark errors, which may not be tied to a container\\n    # because they're not even necessarily on Hadoop\\n    spark_errors = _pick_spark_errors(errors)\\n    if spark_errors:\\n        return spark_errors[0]\\n\\n    # otherwise, merge hadoop/task errors and pick the most recent one\\n    attempt_to_container_id = log_interpretation.get('history', {}).get(\\n        'attempt_to_container_id', {})\\n\\n    merged_errors = _merge_and_sort_errors(errors, attempt_to_container_id)\\n    if merged_errors:\\n        return merged_errors[0]\\n\\n    return None",
                    "func_fullName": "mrjob.logs.errors._pick_error( log_interpretation )"
                },
                {
                    "func_id": 1806,
                    "func_name": "_extract_errors",
                    "func_desc": "_extract_errors",
                    "func_file": "errors",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _extract_errors(log_interpretation):\\n    \"\"\"Extract all errors from *log_interpretation*, in no particular order.\"\"\"\\n    errors = []\\n\\n    for log_type in ('step', 'history', 'task'):\\n        errors.extend(\\n            log_interpretation.get(log_type, {}).get('errors') or ())\\n\\n    return errors",
                    "func_fullName": "mrjob.logs.errors._extract_errors( log_interpretation )"
                },
                {
                    "func_id": 1807,
                    "func_name": "_pick_spark_errors",
                    "func_desc": "_pick_spark_errors",
                    "func_file": "errors",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _pick_spark_errors(errors):\\n    \"\"\"Pick the shortest Spark error with a traceback.\"\"\"\\n    def sort_key(error):\\n        spark_error = error['spark_error']\\n        msg = spark_error.get('message') or ''\\n        num_lines = spark_error.get('num_lines') or 1\\n\\n        return (\\n            _PYTHON_TRACEBACK_HEADER in msg,\\n            num_lines > 1,\\n            -num_lines,\\n        )\\n\\n    return sorted(\\n        (e for e in errors if e.get('spark_error')),\\n        key=sort_key, reverse=True)",
                    "func_fullName": "mrjob.logs.errors._pick_spark_errors( errors )"
                },
                {
                    "func_id": 1808,
                    "func_name": "_pick_error_attempt_ids",
                    "func_desc": "_pick_error_attempt_ids",
                    "func_file": "errors",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _pick_error_attempt_ids(log_interpretation):\\n    \"\"\"Pick error attempt IDs from step and history logs, so we know which\\n    task logs to look at (most relevant first)\"\"\"\\n    errors = _extract_errors(log_interpretation)\\n\\n    attempt_to_container_id = log_interpretation.get('history', {}).get(\\n        'attempt_to_container_id', {})\\n\\n    errors = _merge_and_sort_errors(errors, attempt_to_container_id)\\n\\n    errors.sort(key=_is_probably_task_error, reverse=True)\\n\\n    return list(unique(\\n        error['attempt_id'] for error in errors\\n        if error.get('attempt_id')))",
                    "func_fullName": "mrjob.logs.errors._pick_error_attempt_ids( log_interpretation )"
                },
                {
                    "func_id": 1810,
                    "func_name": "_merge_and_sort_errors",
                    "func_desc": "_merge_and_sort_errors",
                    "func_file": "errors",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _merge_and_sort_errors(errors, attempt_to_container_id=None):\\n    \"\"\"Merge errors from one or more lists of errors and then return\\n    them, sorted by recency.\\n\\n    We allow None in place of an error list.\\n    \"\"\"\\n    from .ids import _time_sort_key\\n    attempt_to_container_id = attempt_to_container_id or {}\\n\\n    key_to_error = {}\\n\\n    for error in errors:\\n        # merge by container_id if we know it\\n        container_id = error.get('container_id') or (\\n            attempt_to_container_id.get(error.get('attempt_id')))\\n\\n        if container_id:\\n            key = ('container_id', container_id)\\n        else:\\n            key = ('time_key', _time_sort_key(error))\\n\\n        key_to_error.setdefault(key, {})\\n        # assume redundant fields will match\\n        key_to_error[key].update(error)\\n\\n    # wrap sort key to prioritize task errors. See #1429\\n    def sort_key(key_and_error):\\n        key, error = key_and_error\\n\\n        # key[0] is 'container_id' or 'time_key'\\n        return (bool(error.get('task_error')),\\n                key[0] == 'container_id',\\n                key[1:])\\n\\n    return [error for _, error in\\n            sorted(key_to_error.items(), key=sort_key, reverse=True)]",
                    "func_fullName": "mrjob.logs.errors._merge_and_sort_errors( errors, attempt_to_container_id )"
                },
                {
                    "func_id": 1812,
                    "func_name": "_format_error",
                    "func_desc": "_format_error",
                    "func_file": "errors",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _format_error(error):\\n    \"\"\"Return string to log/print explaining the given error.\"\"\"\\n    # it's just sad if we error while trying to explain an error\\n    try:\\n        return _format_error_helper(error)\\n    except:\\n        return json.dumps(error, indent=2, sort_keys=True)",
                    "func_fullName": "mrjob.logs.errors._format_error( error )"
                },
                {
                    "func_id": 1813,
                    "func_name": "_format_error_helper",
                    "func_desc": "_format_error_helper",
                    "func_file": "errors",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _format_error_helper(error):\\n    \"\"\"Return string to log/print explaining the given error.\"\"\"\\n    result = ''\\n\\n    spark_error = error.get('spark_error')\\n    if spark_error:\\n        spark_error = _trim_spark_error(spark_error)\\n\\n        result += spark_error.get('message', '')\\n\\n        if spark_error.get('path'):\\n            result += '\\n\\n(from %s)' % _describe_source(spark_error)\\n\\n        # spark errors typically include both java and Python stacktraces,\\n        # so it's not helpful to print hadoop/task errors (and there probably\\n        # wouldn't be any)\\n    else:\\n        hadoop_error = error.get('hadoop_error')\\n        if hadoop_error:\\n            result += hadoop_error.get('message', '')\\n\\n            if hadoop_error.get('path'):\\n                result += '\\n\\n(from %s)' % _describe_source(hadoop_error)\\n\\n        # for practical purposes, there's always a Java error with a message,\\n        # so don't worry too much about spacing.\\n\\n        task_error = error.get('task_error')\\n        if task_error:\\n            if hadoop_error:\\n                result += '\\n\\ncaused by:\\n\\n%s' % (\\n                    task_error.get('message', ''))\\n            else:\\n                result += task_error.get('message', '')\\n\\n            if task_error.get('path'):\\n                result += '\\n\\n(from %s)' % _describe_source(task_error)\\n\\n    split = error.get('split')\\n    if split and split.get('path'):\\n        result += '\\n\\nwhile reading input from %s' % _describe_source(split)\\n\\n    return result",
                    "func_fullName": "mrjob.logs.errors._format_error_helper( error )"
                },
                {
                    "func_id": 1814,
                    "func_name": "_describe_source",
                    "func_desc": "_describe_source",
                    "func_file": "errors",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _describe_source(d):\\n    \"\"\"return either '<path>' or 'line N of <path>' or 'lines M-N of <path>'.\\n    \"\"\"\\n    path = d.get('path') or ''\\n\\n    if 'num_lines' in d and 'start_line' in d:\\n        if d['num_lines'] == 1:\\n            return 'line %d of %s' % (d['start_line'] + 1, path)\\n        else:\\n            return 'lines %d-%d of %s' % (\\n                d['start_line'] + 1, d['start_line'] + d['num_lines'], path)\\n    else:\\n        return path",
                    "func_fullName": "mrjob.logs.errors._describe_source( d )"
                },
                {
                    "func_id": 1815,
                    "func_name": "_trim_spark_error",
                    "func_desc": "_trim_spark_error",
                    "func_file": "errors",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _trim_spark_error(spark_error):\\n    \"\"\"If *spark_error* contains a stack trace followed by a blank line,\\n    trim off the blank line and everything that follows.\"\"\"\\n    spark_error = dict(spark_error)\\n\\n    parts = spark_error['message'].split('\\n\\n')\\n\\n    if len(parts) > 1 and _PYTHON_TRACEBACK_HEADER in parts[0]:\\n        spark_error['message'] = parts[0]\\n        spark_error['num_lines'] = len(parts[0].split('\\n')) + 1\\n\\n    return spark_error",
                    "func_fullName": "mrjob.logs.errors._trim_spark_error( spark_error )"
                },
                {
                    "func_id": 2508,
                    "func_name": "linecol",
                    "func_desc": "linecol",
                    "func_file": "errors",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def linecol(doc, pos):\\n    lineno = doc.count('\\n', 0, pos) + 1\\n    if lineno == 1:\\n        colno = pos + 1\\n    else:\\n        colno = pos - doc.rindex('\\n', 0, pos)\\n    return lineno, colno",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.errors.linecol( doc, pos )"
                },
                {
                    "func_id": 2509,
                    "func_name": "errmsg",
                    "func_desc": "errmsg",
                    "func_file": "errors",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def errmsg(msg, doc, pos, end=None):\\n    lineno, colno = linecol(doc, pos)\\n    msg = msg.replace('%r', repr(doc[pos:pos + 1]))\\n    if end is None:\\n        fmt = '%s: line %d column %d (char %d)'\\n        return fmt % (msg, lineno, colno, pos)\\n    endlineno, endcolno = linecol(doc, end)\\n    fmt = '%s: line %d column %d - line %d column %d (char %d - %d)'\\n    return fmt % (msg, lineno, colno, endlineno, endcolno, pos, end)",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.errors.errmsg( msg, doc, pos, end )"
                }
            ]
        },
        {
            "cluster_id": 22,
            "feature_id": 67,
            "feature_desc": "gamma=0.0000; k=1; a=0.25; combined=1.000; stability(ARI)=1.000; sep=1.000",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1390,
                    "func_name": "describe_base_emr_images",
                    "func_desc": "describe_base_emr_images",
                    "func_file": "ami",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def describe_base_emr_images(ec2_client):\\n    \"\"\"Fetch a list of Amazon Linux AMI images that are usable with EMR,\\n    with the most recent first. This can take several seconds.\\n\\n    :param ec2_client: a boto3 EC2 client, which can be obtained from\\n                       :py:meth:`mrjob.emr.EMRJobRunner.make_ec2_client()`\\n                       or ``boto3.client('ec2')``\\n\\n    For the sake of consistency, we have somewhat stricter requirements\\n    than `the AWS documentation <https://docs.aws.amazon.com/emr/latest/\\\\n    ManagementGuide/emr-custom-ami.html#emr-custom-ami-considerations>`_.\\n    Specifically:\\n\\n    * Amazon Linux (not Amazon Linux 2)\\n    * HVM virtualization\\n    * x86_64 architecture\\n    * single EBS volume\\n      * standard volume type (not GP2)\\n    * stable version (no \"testing\" or \"rc\", only numbers and dots)\\n\\n    This only returns images going back to September 2016 (prior to that,\\n    EC2 used a different naming convention).\\n\\n    This returns a dictionary for each image, in the same response format as\\n    `ec2_client.describe_images() <https://boto3.amazonaws.com/v1/\\\\n    documentation/api/latest/reference/services/ec2.html#EC2.Client\\\\n    .describe_images>`_. The\\n    *ImageId* field contains the AMI ID, and *Description* contains\\n    a human-readable description.\\n    \"\"\"\\n    # DescribeImages' filtering is imperfect and slow, but this helps a bit\\n    images = ec2_client.describe_images(\\n        Owners=['amazon'],\\n        Filters=[\\n            dict(Name='architecture', Values=['x86_64']),\\n            dict(Name='root-device-type', Values=['ebs']),\\n            dict(Name='virtualization-type', Values=['hvm']),\\n        ],\\n    )['Images']\\n\\n    # perform further filtering by name to pick out Amazon Linux\\n    images = [img for img in images\\n              if _EMR_BASE_AMI_NAME_RE.match(img.get('Name') or '')]\\n\\n    # filter out any images that have multiple volumes\\n    # (this is implied by the naming convention, but just in case)\\n    images = [img for img in images\\n              if len(img.get('BlockDeviceMappings') or []) == 1]\\n\\n    # require a CreationDate\\n    images = [img for img in images if img['CreationDate']]\\n\\n    # put most recent images first\\n    images.sort(key=lambda img: img['CreationDate'], reverse=True)\\n\\n    return images",
                    "func_fullName": "mrjob.ami.describe_base_emr_images( ec2_client )"
                }
            ]
        },
        {
            "cluster_id": 28,
            "feature_id": 68,
            "feature_desc": "gamma=0.2478; k=7; a=0.25; combined=0.500; stability(ARI)=1.000; sep=0.472",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1391,
                    "func_name": "_pool_tags",
                    "func_desc": "_pool_tags",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _pool_tags(hash, name):\\n    \"\"\"Return a dict with \"hidden\" tags to add to the given cluster.\"\"\"\\n    return dict(__mrjob_pool_hash=hash, __mrjob_pool_name=name)",
                    "func_fullName": "mrjob.pool._pool_tags( hash, name )"
                },
                {
                    "func_id": 1392,
                    "func_name": "_extract_tags",
                    "func_desc": "_extract_tags",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _extract_tags(cluster):\\n    \"\"\"Pull the tags from a cluster, as a dict.\"\"\"\\n    return {t['Key']: t['Value'] for t in cluster.get('Tags') or []}",
                    "func_fullName": "mrjob.pool._extract_tags( cluster )"
                },
                {
                    "func_id": 1393,
                    "func_name": "_pool_name",
                    "func_desc": "_pool_name",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _pool_name(cluster):\\n    tags = _extract_tags(cluster)\\n    return tags.get('__mrjob_pool_name')",
                    "func_fullName": "mrjob.pool._pool_name( cluster )"
                }
            ]
        },
        {
            "cluster_id": 28,
            "feature_id": 69,
            "feature_desc": "gamma=0.2478; k=7; a=0.25; combined=0.500; stability(ARI)=1.000; sep=0.472",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1394,
                    "func_name": "_cluster_name_suffix",
                    "func_desc": "_cluster_name_suffix",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _cluster_name_suffix(hash, name):\\n    fields = [mrjob.__version__, name, hash]\\n    return ' pooling:%s' % ','.join(fields)",
                    "func_fullName": "mrjob.pool._cluster_name_suffix( hash, name )"
                },
                {
                    "func_id": 1395,
                    "func_name": "_parse_cluster_name_suffix",
                    "func_desc": "_parse_cluster_name_suffix",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_cluster_name_suffix(cluster_name):\\n    \"\"\"Return a dictionary possibly containing the keys:\\n\\n    mrjob_version: version of mrjob that created this cluster\\n    pool_hash: hash representing bootstrap setup etc.\\n    pool_name: name of the cluster pool\\n\\n    If the cluster is not pooled or we can't parse its pooling suffix,\\n    return ``{}``.\\n    \"\"\"\\n    # return version, hash, and name from cluster pool suffix\\n\\n    i = cluster_name.find(' pooling:')\\n    if i == -1:\\n        return {}\\n\\n    suffix = cluster_name[i + len(' pooling:'):]\\n\\n    parts = suffix.split(',', 3)\\n\\n    if len(parts) == 3:\\n        return dict(\\n            mrjob_version=parts[0],\\n            pool_name=parts[1],\\n            pool_hash=parts[2],\\n        )\\n    else:\\n        return {}",
                    "func_fullName": "mrjob.pool._parse_cluster_name_suffix( cluster_name )"
                },
                {
                    "func_id": 1414,
                    "func_name": "_attempt_to_unlock_cluster",
                    "func_desc": "_attempt_to_unlock_cluster",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _attempt_to_unlock_cluster(emr_client, cluster_id):\\n    \"\"\"Release our lock on the given pooled cluster. Only do this if you know\\n    the cluster is currently running steps (so other jobs won't try to\\n    join the cluster).\\n\\n    Returns True if successful, False if not (usually, this means the\\n    cluster terminated). Cluster locks eventually release themselves,\\n    so if releasing a lock fails for whatever reason, it's not worth\\n    releasing it again.\\n\\n    Locks expire after a minute anyway (which is less time than it takes to\\n    run most jobs), so this is mostly useful for preventing problems\\n    due to clock skew. Also makes unit testing more straightforward.\\n    \"\"\"\\n    try:\\n        emr_client.remove_tags(ResourceId=cluster_id, TagKeys=[_POOL_LOCK_KEY])\\n        return True\\n    except ClientError as ex:\\n        log.debug('removing tags failed: %r' % ex)\\n        return False",
                    "func_fullName": "mrjob.pool._attempt_to_unlock_cluster( emr_client, cluster_id )"
                }
            ]
        },
        {
            "cluster_id": 28,
            "feature_id": 70,
            "feature_desc": "gamma=0.2478; k=7; a=0.25; combined=0.500; stability(ARI)=1.000; sep=0.472",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1396,
                    "func_name": "_instance_groups_satisfy",
                    "func_desc": "_instance_groups_satisfy",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _instance_groups_satisfy(actual_igs, requested_igs):\\n    \"\"\"Do the actual instance groups from a cluster satisfy the requested\\n    ones, for the purpose of pooling?\\n    \"\"\"\\n    # the format of *requested_igs* is here:\\n    #     http://docs.aws.amazon.com/ElasticMapReduce/latest/API/API_InstanceGroup.html  # noqa\\n    # and the format of *actual_igs* is here:\\n    #     http://docs.aws.amazon.com/ElasticMapReduce/latest/API/API_ListInstanceGroups.html  # noqa\\n\\n    # verify format of requested_igs\\n    if not (isinstance(requested_igs, (list, tuple)) and\\n            all(isinstance(req_ig, dict) and 'InstanceRole' in req_ig\\n                for req_ig in requested_igs)):\\n        log.debug('    bad instance_groups config')\\n        return False\\n\\n    # a is a map from role to actual instance groups\\n    a = defaultdict(list)\\n    for ig in actual_igs:\\n        a[ig['InstanceGroupType']].append(ig)\\n\\n    # r is a map from role to request (should be only one per role)\\n    r = {req.get('InstanceRole'): req for req in requested_igs}\\n\\n    # updated request to account for extra instance groups\\n    # see #1630 for what we do when roles don't match\\n    if set(a) - set(r):\\n        r = _add_missing_roles_to_request(set(a) - set(r), r,\\n                                          ['InstanceCount'])\\n\\n    if set(a) != set(r):\\n        log.debug(\"    missing instance group roles\")\\n        return False\\n\\n    for role in r:\\n        if not _igs_for_same_role_satisfy(a[role], r[role]):\\n            return False\\n\\n    return True",
                    "func_fullName": "mrjob.pool._instance_groups_satisfy( actual_igs, requested_igs )"
                },
                {
                    "func_id": 1401,
                    "func_name": "_instance_fleets_satisfy",
                    "func_desc": "_instance_fleets_satisfy",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _instance_fleets_satisfy(actual_fleets, req_fleets):\\n    \"\"\"Common code for :py:func:`\\n    :py:func:`_instance_groups_satisfy_fleets` and\\n    :py:func:`_instance_groups_satisfy`.\"\"\"\\n    # verify format of requested_igs\\n    if not (isinstance(req_fleets, (list, tuple)) and\\n            all(isinstance(req_ft, dict) and 'InstanceFleetType' in req_ft\\n                for req_ft in req_fleets)):\\n        log.debug('    bad instance_fleets config')\\n        return False\\n\\n    # a is a map from role to actual instance fleet\\n    # (unlike with groups, there can never be more than one fleet per role)\\n    a = {f['InstanceFleetType']: f for f in actual_fleets}\\n\\n    # r is a map from role to request (should be only one per role)\\n    r = {f['InstanceFleetType']: f for f in req_fleets}\\n\\n    # updated request to account for extra instance groups\\n    # see #1630 for what we do when roles don't match\\n    if set(a) - set(r):\\n        r = _add_missing_roles_to_request(\\n            set(a) - set(r), r,\\n            ['TargetOnDemandCapacity', 'TargetSpotCapacity'])\\n\\n    if set(a) != set(r):\\n        log.debug(\"    missing instance fleet roles\")\\n        return False\\n\\n    for role in r:\\n        if not _fleet_for_same_role_satisfies(a[role], r[role]):\\n            return False\\n\\n    return True",
                    "func_fullName": "mrjob.pool._instance_fleets_satisfy( actual_fleets, req_fleets )"
                },
                {
                    "func_id": 1406,
                    "func_name": "_add_missing_roles_to_request",
                    "func_desc": "_add_missing_roles_to_request",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _add_missing_roles_to_request(\\n        missing_roles, role_to_req, req_count_fields):\\n    \"\"\"Helper for :py:func:`_igs_satisfy_request`. Add requests for\\n    *missing_roles* to *role_to_ig* so that we have a better chance of\\n    matching the cluster's actual instance groups.\"\"\"\\n    # see #1630 for discussion\\n\\n    # don't worry about modifying *role_to_req*; this is\\n    # a helper func\\n\\n    if 'CORE' in missing_roles and list(role_to_req) == ['MASTER']:\\n        # both core and master have to satisfy master-only request\\n        role_to_req['CORE'] = role_to_req['MASTER']\\n\\n    if 'TASK' in missing_roles and 'CORE' in role_to_req:\\n        # make sure tasks won't crash on the task instances,\\n        # but don't require the same amount of CPU\\n        role_to_req['TASK'] = dict(role_to_req['CORE'])\\n        for req_count_field in req_count_fields:\\n            role_to_req['TASK'][req_count_field] = 0\\n\\n    return role_to_req",
                    "func_fullName": "mrjob.pool._add_missing_roles_to_request( missing_roles, role_to_req, req_count_fields )"
                }
            ]
        },
        {
            "cluster_id": 28,
            "feature_id": 71,
            "feature_desc": "gamma=0.2478; k=7; a=0.25; combined=0.500; stability(ARI)=1.000; sep=0.472",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1397,
                    "func_name": "_igs_for_same_role_satisfy",
                    "func_desc": "_igs_for_same_role_satisfy",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _igs_for_same_role_satisfy(actual_igs, requested_ig):\\n    \"\"\"Does the *actual* list of instance groups satisfy the *requested*\\n    one?\\n    \"\"\"\\n    # bid price/on-demand\\n    if not all(_ig_satisfies_bid_price(ig, requested_ig) for ig in actual_igs):\\n        return False\\n\\n    # memory\\n    if not all(_ig_satisfies_mem(ig, requested_ig) for ig in actual_igs):\\n        return False\\n\\n    # EBS volumes\\n    if not all(_ebs_satisfies(ig, requested_ig) for ig in actual_igs):\\n        return False\\n\\n    # CPU (this returns # of compute units or None)\\n    return _igs_satisfy_cpu(actual_igs, requested_ig)",
                    "func_fullName": "mrjob.pool._igs_for_same_role_satisfy( actual_igs, requested_ig )"
                },
                {
                    "func_id": 1398,
                    "func_name": "_ig_satisfies_bid_price",
                    "func_desc": "_ig_satisfies_bid_price",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _ig_satisfies_bid_price(actual_ig, requested_ig):\\n    \"\"\"Does the actual instance group definition satisfy the bid price\\n    (or lack thereof) of the requested instance group?\\n    \"\"\"\\n    # _instance_groups_satisfy() already verified *requested_ig* is a dict\\n\\n    # on-demand instances satisfy every bid price\\n    if actual_ig['Market'] == 'ON_DEMAND':\\n        return True\\n\\n    if requested_ig.get('Market', 'ON_DEMAND') == 'ON_DEMAND':\\n        log.debug('    spot instance, requested on-demand')\\n        return False\\n\\n    if actual_ig['BidPrice'] == requested_ig.get('BidPrice'):\\n        return True\\n\\n    try:\\n        if float(actual_ig['BidPrice']) >= float(requested_ig.get('BidPrice')):\\n            return True\\n        else:\\n            # low bid prices mean cluster is more likely to be\\n            # yanked away\\n            log.debug('    bid price too low')\\n            return False\\n    except ValueError:\\n        log.debug('    non-float bid price')\\n        return False",
                    "func_fullName": "mrjob.pool._ig_satisfies_bid_price( actual_ig, requested_ig )"
                },
                {
                    "func_id": 1399,
                    "func_name": "_ig_satisfies_mem",
                    "func_desc": "_ig_satisfies_mem",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _ig_satisfies_mem(actual_ig, requested_ig):\\n    \"\"\"Does the actual instance group satisfy the memory requirements of\\n    the requested instance group?\"\"\"\\n    actual_type = actual_ig['InstanceType']\\n    requested_type = requested_ig.get('InstanceType')\\n\\n    # this works even for unknown instance types\\n    if actual_type == requested_type:\\n        return True\\n\\n    try:\\n        if (EC2_INSTANCE_TYPE_TO_MEMORY[actual_type] >=\\n                EC2_INSTANCE_TYPE_TO_MEMORY[requested_type]):\\n            return True\\n        else:\\n            log.debug('    too little memory')\\n            return False\\n    except KeyError:\\n        log.debug('    unknown instance type')\\n        return False",
                    "func_fullName": "mrjob.pool._ig_satisfies_mem( actual_ig, requested_ig )"
                },
                {
                    "func_id": 1400,
                    "func_name": "_igs_satisfy_cpu",
                    "func_desc": "_igs_satisfy_cpu",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _igs_satisfy_cpu(actual_igs, requested_ig):\\n    \"\"\"Does the list of actual instance groups satisfy the CPU requirements\\n    of the requested instance group?\\n    \"\"\"\\n    requested_type = requested_ig.get('InstanceType')\\n    num_requested = requested_ig.get('InstanceCount')\\n\\n    if not isinstance(num_requested, integer_types):\\n        return False\\n\\n    # count number of compute units (cu)\\n    if requested_type in EC2_INSTANCE_TYPE_TO_COMPUTE_UNITS:\\n        requested_cu = (\\n            num_requested * EC2_INSTANCE_TYPE_TO_COMPUTE_UNITS[requested_type])\\n\\n        # don't require instances to be running; we'd be worse off if\\n        # we started our own cluster from scratch. (This can happen if\\n        # the previous job finished while some task instances were\\n        # still being provisioned.)\\n        actual_cu = sum(\\n            ig['RunningInstanceCount'] *\\n            EC2_INSTANCE_TYPE_TO_COMPUTE_UNITS.get(ig['InstanceType'], 0.0)\\n            for ig in actual_igs)\\n    else:\\n        # unknown instance type, just count # of matching instances\\n        requested_cu = num_requested\\n        actual_cu = sum(ig['RunningInstanceCount'] for ig in actual_igs\\n                        if ig['InstanceType'] == requested_type)\\n\\n    if actual_cu >= requested_cu:\\n        return True\\n    else:\\n        log.debug('    not enough compute units')\\n        return False",
                    "func_fullName": "mrjob.pool._igs_satisfy_cpu( actual_igs, requested_ig )"
                }
            ]
        },
        {
            "cluster_id": 28,
            "feature_id": 72,
            "feature_desc": "gamma=0.2478; k=7; a=0.25; combined=0.500; stability(ARI)=1.000; sep=0.472",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1402,
                    "func_name": "_fleet_for_same_role_satisfies",
                    "func_desc": "_fleet_for_same_role_satisfies",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _fleet_for_same_role_satisfies(actual_fleet, req_fleet):\\n    # match up instance types\\n    actual_specs = {spec['InstanceType']: spec\\n                    for spec in actual_fleet['InstanceTypeSpecifications']}\\n    try:\\n        req_specs = {spec['InstanceType']: spec\\n                     for spec in req_fleet['InstanceTypeConfigs']}\\n    except (TypeError, KeyError):\\n        return False\\n\\n    if set(actual_specs) - set(req_specs):\\n        log.debug('    fleet may include wrong instance types')\\n        return False\\n\\n    if not all(_fleet_spec_satsifies(actual_specs[t], req_specs[t])\\n               for t in actual_specs):\\n        return False\\n\\n    # capacity\\n    actual_on_demand = actual_fleet.get('ProvisionedOnDemandCapacity', 0)\\n    req_on_demand = req_fleet.get('TargetOnDemandCapacity', 0)\\n\\n    if not isinstance(req_on_demand, integer_types):\\n        return False\\n\\n    if req_on_demand > actual_on_demand:\\n        log.debug('    not enough on-demand capacity')\\n        return False\\n\\n    actual_spot = actual_fleet.get('ProvisionedSpotCapacity', 0)\\n    req_spot = req_fleet.get('TargetSpotCapacity', 0)\\n\\n    if not isinstance(req_spot, integer_types):\\n        return False\\n\\n    # allow extra on-demand instances to serve as spot instances\\n    if req_spot > actual_spot + (actual_on_demand - req_on_demand):\\n        log.debug('    not enough spot capacity')\\n\\n    # handle TERMINATE_CLUSTER timeout action. This really doesn't play\\n    # well with pooling anyhow\\n    if _get_timeout_action(actual_fleet) == 'TERMINATE_CLUSTER':\\n        if _get_timeout_action(req_fleet) != 'TERMINATE_CLUSTER':\\n            log.debug('    self-terminating fleet not requested')\\n            return False\\n\\n        if (_get_timeout_duration(actual_fleet) <\\n                _get_timeout_duration(req_fleet)):\\n            log.debug('    fleet may self-terminate prematurely')\\n            return False\\n\\n    return True",
                    "func_fullName": "mrjob.pool._fleet_for_same_role_satisfies( actual_fleet, req_fleet )"
                },
                {
                    "func_id": 1403,
                    "func_name": "_get_timeout_action",
                    "func_desc": "_get_timeout_action",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _get_timeout_action(fleet):\\n    return fleet.get(\\n        'LaunchSpecifications', {}).get(\\n        'SpotSpecification', {}).get(\\n        'TimeoutAction')",
                    "func_fullName": "mrjob.pool._get_timeout_action( fleet )"
                },
                {
                    "func_id": 1404,
                    "func_name": "_get_timeout_duration",
                    "func_desc": "_get_timeout_duration",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _get_timeout_duration(fleet):\\n    return fleet.get(\\n        'LaunchSpecifications', {}).get(\\n        'SpotSpecification', {}).get(\\n        'TimeoutDurationMinutes', 0.0)",
                    "func_fullName": "mrjob.pool._get_timeout_duration( fleet )"
                }
            ]
        },
        {
            "cluster_id": 28,
            "feature_id": 73,
            "feature_desc": "gamma=0.2478; k=7; a=0.25; combined=0.500; stability(ARI)=1.000; sep=0.472",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1405,
                    "func_name": "_fleet_spec_satsifies",
                    "func_desc": "_fleet_spec_satsifies",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _fleet_spec_satsifies(actual_spec, req_spec):\\n    \"\"\"Make sure the specification for the given instance type is as\\n    good or better than the requested spec.\\n\\n    Specs must have the same weight, but \"better\" EBS configurations are\\n    accepted.\\n\\n    Bid price must either be higher or the *actual* bid price\\n    must be same as on-demand\\n    \"\"\"\\n    if (actual_spec.get('WeightedCapacity', 1) !=\\n            req_spec.get('WeightedCapacity', 1)):\\n        log.debug('    different weighted capacity for same instance type')\\n        return False\\n\\n    if not _ebs_satisfies(actual_spec, req_spec):\\n        return False\\n\\n    # bid price is the max, don't worry about it\\n    if actual_spec.get('BidPriceAsPercentageOfOnDemandPrice', 100) >= 100:\\n        return True\\n\\n    # absolute bid price\\n    req_bid_price = req_spec.get('BidPrice')\\n    if req_bid_price is not None:\\n        actual_bid_price = actual_spec.get('BidPrice')\\n        if actual_bid_price is None:\\n            log.debug('    no bid price specified')\\n            return False\\n\\n        try:\\n            if not float(actual_bid_price) >= float(req_bid_price):\\n                log.debug('    bid price too low')\\n                return False\\n        except TypeError:\\n            log.debug('    non-numeric bid price')\\n            return False\\n\\n    # relative bid price\\n    req_bid_percent = req_spec.get('BidPriceAsPercentageOfOnDemandPrice')\\n    if not isinstance(req_spec, (integer_types, float)):\\n        return False\\n\\n    if req_bid_percent:\\n        actual_bid_percent = actual_spec.get(\\n            'BidPriceAsPercentageOfOnDemandPrice')\\n        if actual_bid_percent is None:\\n            log.debug('    no bid price as % of on-demand price')\\n            return False\\n\\n        if req_bid_percent > actual_bid_percent:\\n            log.debug('    bid price as % of on-demand price too low')\\n            return False\\n\\n    return True",
                    "func_fullName": "mrjob.pool._fleet_spec_satsifies( actual_spec, req_spec )"
                },
                {
                    "func_id": 1407,
                    "func_name": "_ebs_satisfies",
                    "func_desc": "_ebs_satisfies",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _ebs_satisfies(actual, request):\\n    \"\"\"Does *actual* have EBS volumes that satisfy *request*.\\n\\n    *actual* is either an instance group from ``ListInstanceGroups``\\n    or an instance fleet spec from ``ListInstanceFleets`` (format\\n    is the same).\\n\\n    *request* is either the ``InstanceGroups`` or ``InstanceFleets``\\n    param to ``RunJobFlow``\\n\\n    If *request* doesn't have an EBS Configuration, we return\\n    True.\\n\\n    If *request* requests EBS optimization, *actual* should provide it.\\n\\n    Finally, *actual* should have the same or better block devices\\n    as those in *request* (same volume type, at least as much IOPS\\n    and volume size).\\n    \"\"\"\\n    req_ebs_config = request.get('EbsConfiguration')\\n\\n    if not req_ebs_config:\\n        return True\\n\\n    if (req_ebs_config.get('EbsOptimized') and\\n            not actual.get('EbsOptimized')):\\n        log.debug('    need EBS-optimized instances')\\n        return False\\n\\n    req_device_configs = req_ebs_config.get('EbsBlockDeviceConfigs')\\n\\n    if not req_device_configs:\\n        return True\\n\\n    if not (isinstance(req_device_configs, (list, tuple)) and\\n            all(isinstance(rdc, dict) for rdc in req_device_configs)):\\n        return False\\n\\n    req_volumes = []\\n\\n    for req_device_config in req_device_configs:\\n        volume = req_device_config['VolumeSpecification']\\n        num_volumes = req_device_config.get('VolumesPerInstance', 1)\\n\\n        req_volumes.extend([volume] * num_volumes)\\n\\n    actual_volumes = [\\n        bd.get('VolumeSpecification', {})\\n        for bd in actual.get('EbsBlockDevices', [])]\\n\\n    return _ebs_volumes_satisfy(actual_volumes, req_volumes)",
                    "func_fullName": "mrjob.pool._ebs_satisfies( actual, request )"
                },
                {
                    "func_id": 1408,
                    "func_name": "_ebs_volumes_satisfy",
                    "func_desc": "_ebs_volumes_satisfy",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _ebs_volumes_satisfy(actual_volumes, req_volumes):\\n    \"\"\"Does the given list of actual EBS volumes satisfy the given request?\\n\\n    Just compare them one by one (we want each actual device to be\\n    bigger/faster; just having the same amount of capacity or iops\\n    isn't enough).\\n    \"\"\"\\n    if not isinstance(req_volumes, (list, tuple)):\\n        return False\\n\\n    if len(req_volumes) > len(actual_volumes):\\n        log.debug('    more EBS volumes requested than available')\\n        return False\\n\\n    return all(_ebs_volume_satisfies(a, r)\\n               for a, r in zip(actual_volumes, req_volumes))",
                    "func_fullName": "mrjob.pool._ebs_volumes_satisfy( actual_volumes, req_volumes )"
                },
                {
                    "func_id": 1409,
                    "func_name": "_ebs_volume_satisfies",
                    "func_desc": "_ebs_volume_satisfies",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _ebs_volume_satisfies(actual_volume, req_volume):\\n    \"\"\"Does the given actual EBS volume satisfy the given request?\"\"\"\\n    if not isinstance(req_volume, dict):\\n        return False\\n\\n    if req_volume.get('VolumeType') != actual_volume.get('VolumeType'):\\n        log.debug('    wrong EBS volume type')\\n        return False\\n\\n    if not req_volume.get('SizeInGB', 0) <= actual_volume.get('SizeInGB', 0):\\n        log.debug('    EBS volume too small')\\n        return False\\n\\n    # Iops isn't really \"optional\"; it has to be set if volume type is\\n    # io1 and not set otherwise\\n    if not (req_volume.get('Iops', 0) <= actual_volume.get('Iops', 0)):\\n        log.debug('    EBS volume too slow')\\n        return False\\n\\n    return True",
                    "func_fullName": "mrjob.pool._ebs_volume_satisfies( actual_volume, req_volume )"
                }
            ]
        },
        {
            "cluster_id": 28,
            "feature_id": 74,
            "feature_desc": "gamma=0.2478; k=7; a=0.25; combined=0.500; stability(ARI)=1.000; sep=0.472",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1410,
                    "func_name": "_make_cluster_lock",
                    "func_desc": "_make_cluster_lock",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _make_cluster_lock(job_key, expiry):\\n    \"\"\"Return the contents of a tag used to lock a cluster.\\n\\n    *expiry* is the unix timestamp for when the lock is no longer valid\"\"\"\\n    return '%s %.6f' % (job_key, expiry)",
                    "func_fullName": "mrjob.pool._make_cluster_lock( job_key, expiry )"
                },
                {
                    "func_id": 1411,
                    "func_name": "_parse_cluster_lock",
                    "func_desc": "_parse_cluster_lock",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _parse_cluster_lock(lock):\\n    \"\"\"Return (job_key, expiry) or raise ValueError\\n\\n    Raises TypeError if *lock* is not a string.\\n    \"\"\"\\n    if not isinstance(lock, (string_types)):\\n        raise TypeError\\n\\n    job_key, expiry_str = lock.split(' ')\\n\\n    try:\\n        expiry = float(expiry_str)\\n    except TypeError:\\n        raise ValueError\\n\\n    return job_key, expiry",
                    "func_fullName": "mrjob.pool._parse_cluster_lock( lock )"
                },
                {
                    "func_id": 1412,
                    "func_name": "_get_cluster_lock",
                    "func_desc": "_get_cluster_lock",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _get_cluster_lock(cluster):\\n    return _extract_tags(cluster).get(_POOL_LOCK_KEY)",
                    "func_fullName": "mrjob.pool._get_cluster_lock( cluster )"
                },
                {
                    "func_id": 1413,
                    "func_name": "_attempt_to_lock_cluster",
                    "func_desc": "_attempt_to_lock_cluster",
                    "func_file": "pool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _attempt_to_lock_cluster(\\n        emr_client, cluster_id, job_key,\\n        cluster=None, when_cluster_described=None):\\n    \"\"\"Attempt to lock the given pooled cluster using EMR tags.\\n\\n    You may optionally include *cluster* (a cluster description) and\\n    *when_cluster_described*, to save an API call to ``DescribeCluster``\\n\\n    If the cluster's StepConcurrency Level is 1, locking considers the cluster\\n    available if it's in the WAITING state. this means we should not release\\n    our lock until our step(s) have started running, which can take several\\n    seconds.\\n\\n    Otherwise, steps can run concurrently, so locking\\n    considers the cluster available if it's in the WAITING or RUNNING state.\\n    Additionally, it makes a ``ListSteps`` API call to verify that the cluster\\n    doesn't already have as many active steps as it can run simultaneously.\\n    Because other jobs looking to join the cluster will also count steps,\\n    we can release our lock as soon as we add our steps.\\n    \"\"\"\\n    log.debug('Attempting to lock cluster %s for %.1f seconds' % (\\n        cluster_id, _CLUSTER_LOCK_SECS))\\n\\n    if cluster is None:\\n        cluster = emr_client.describe_cluster(ClusterId=cluster_id)['Cluster']\\n\\n    if when_cluster_described is None:\\n        start = time.time()\\n    else:\\n        start = when_cluster_described\\n\\n    if cluster['StepConcurrencyLevel'] == 1:\\n        step_accepting_states = ['WAITING']\\n    else:\\n        step_accepting_states = ['RUNNING', 'WAITING']\\n\\n    # check if there is a non-expired lock\\n    state = cluster['Status']['State']\\n\\n    if state not in step_accepting_states:\\n        # this could happen if the cluster were TERMINATING, for example\\n        log.info('  cluster is not accepting steps, state is %s' % state)\\n        return False\\n\\n    lock = _get_cluster_lock(cluster)\\n\\n    if lock:\\n        expiry = None\\n        try:\\n            their_job_key, expiry = _parse_cluster_lock(lock)\\n        except ValueError:\\n            log.info('  ignoring invalid pool lock: %s' % lock)\\n\\n        if expiry and expiry > start:\\n            log.info('  locked by %s for %.1f seconds' % (\\n                their_job_key, expiry - start))\\n            return False\\n\\n    # add our lock\\n    our_lock = _make_cluster_lock(job_key, start + _CLUSTER_LOCK_SECS)\\n\\n    log.debug('  adding tag to cluster %s:' % cluster_id)\\n    log.debug('    %s=%s' % (_POOL_LOCK_KEY, our_lock))\\n    emr_client.add_tags(\\n        ResourceId=cluster_id,\\n        Tags=[dict(Key=_POOL_LOCK_KEY, Value=our_lock)]\\n    )\\n\\n    if time.time() - start > _ADD_TAG_BEFORE:\\n        log.info('  took too long to tag cluster with lock')\\n        return False\\n\\n    # wait, then check if our lock is still there\\n    log.info(\"  waiting %.1f seconds to ensure lock wasn't overwritten\" %\\n             _WAIT_AFTER_ADD_TAG)\\n    time.sleep(_WAIT_AFTER_ADD_TAG)\\n\\n    # check if our lock is still there\\n    cluster = emr_client.describe_cluster(ClusterId=cluster_id)['Cluster']\\n\\n    state = cluster['Status']['State']\\n\\n    if state not in step_accepting_states:\\n        # this could happen if the cluster were TERMINATING, for example\\n        log.info('  cluster is not accepting steps, state is %s' % state)\\n        return False\\n\\n    if cluster['StepConcurrencyLevel'] > 1:\\n        # is cluster already full of steps?\\n        num_active_steps = len(list(_boto3_paginate(\\n            'Steps', emr_client, 'list_steps',\\n            ClusterId=cluster_id,\\n            StepStates=['PENDING', 'RUNNING'])))\\n\\n        if num_active_steps >= cluster['StepConcurrencyLevel']:\\n            log.info(\\n                '  cluster already has %d active steps' % num_active_steps)\\n            return\\n\\n    lock = _get_cluster_lock(cluster)\\n\\n    if lock is None:\\n        log.info('  lock was removed')\\n        return False\\n    elif lock != our_lock:\\n        their_job_desc = 'other job'\\n        try:\\n            their_job_desc, expiry = _parse_cluster_lock(lock)\\n        except ValueError:\\n            pass\\n\\n        log.info('  lock was overwritten by %s' % their_job_desc)\\n        return False\\n\\n    # make sure we have enough time to add steps and have them run\\n    # before the lock expires\\n    if time.time() > start + _CHECK_TAG_BEFORE:\\n        log.info('  took too long to check for lock')\\n        return False\\n\\n    log.info('  lock acquired')\\n    return True",
                    "func_fullName": "mrjob.pool._attempt_to_lock_cluster( emr_client, cluster_id, job_key, cluster, when_cluster_described )"
                }
            ]
        },
        {
            "cluster_id": 29,
            "feature_id": 75,
            "feature_desc": "gamma=0.0100; k=1; a=0.25; combined=0.850; stability(ARI)=1.000; sep=0.161",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1539,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "ssh",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, ssh_bin, ec2_key_pair_file, ssh_add_bin=None):\\n        \"\"\"\\n        :param ssh_bin: path to ``ssh`` binary\\n        :param ec2_key_pair_file: path to an SSH keyfile\\n        :param ssh_add_bin: path to ``ssh-add`` binary if any\\n        \"\"\"\\n        super(SSHFilesystem, self).__init__()\\n        self._ssh_bin = ssh_bin\\n        self._ec2_key_pair_file = ec2_key_pair_file\\n        if self._ec2_key_pair_file is None:\\n            raise ValueError('ec2_key_pair_file must be a path')\\n\\n        self._ssh_add_bin = ssh_add_bin or ['ssh-add']\\n\\n        # keep track of hosts we've already copied the key pair to\\n        self._hosts_with_key_pair_file = set()\\n\\n        # keep track of which hosts we've copied our key to, and\\n        # what the (random) name of the key file is on that host\\n        self._host_to_key_filename = {}\\n\\n        # should we use sudo (for EMR)? Enable with use_sudo_over_ssh().\\n        self._sudo = False",
                    "func_fullName": "mrjob.fs.ssh.__init__( self, ssh_bin, ec2_key_pair_file, ssh_add_bin )"
                },
                {
                    "func_id": 1540,
                    "func_name": "_ssh_cmd_args",
                    "func_desc": "_ssh_cmd_args",
                    "func_file": "ssh",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ssh_cmd_args(self, address, cmd_args):\\n        \"\"\"Return an ssh command that would run the given command on\\n        the given *address*.\\n\\n        Address consists of one or most hosts, joined by '!' (so that\\n        we can reach hosts only accessible through an internal network).\\n\\n        We assume that any host we SSH into is a UNIX system, and that\\n        we don't need sudo to run ssh itself. We also assume the username\\n        is always ``hadoop``.\\n        \"\"\"\\n        args = []\\n\\n        for i, host in enumerate(address.split('!')):\\n\\n            args.extend(self._ssh_bin)\\n\\n            if i == 0:\\n                args.extend(['-i', self._ec2_key_pair_file])\\n\\n            known_hosts_file = os.devnull if i == 0 else '/dev/null'\\n\\n            args.extend(\\n                [\\n                    '-o', 'UserKnownHostsFile=' + known_hosts_file,\\n                    '-o', 'StrictHostKeyChecking=no',\\n                    '-o', 'VerifyHostKeyDNS=no',\\n                    '-A',\\n                    'hadoop@' + host,\\n                ]\\n            )\\n\\n        if self._sudo:\\n            args.append('sudo')\\n\\n        args.extend(cmd_args)\\n\\n        return args",
                    "func_fullName": "mrjob.fs.ssh._ssh_cmd_args( self, address, cmd_args )"
                },
                {
                    "func_id": 1541,
                    "func_name": "_ssh_launch",
                    "func_desc": "_ssh_launch",
                    "func_file": "ssh",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ssh_launch(self, address, cmd_args):\\n        \"\"\"Copy SSH keys if necessary, then launch the given command\\n        over SSH and return a Popen.\"\"\"\\n        if '!' in address:\\n            self._ssh_add_key()\\n\\n        args = self._ssh_cmd_args(address, cmd_args)\\n\\n        log.debug('  > ' + cmd_line(args))\\n        try:\\n            return Popen(args, stdout=PIPE, stderr=PIPE)\\n        except OSError as ex:\\n            raise IOError(ex.strerror)",
                    "func_fullName": "mrjob.fs.ssh._ssh_launch( self, address, cmd_args )"
                },
                {
                    "func_id": 1542,
                    "func_name": "_ssh_run",
                    "func_desc": "_ssh_run",
                    "func_file": "ssh",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ssh_run(self, address, cmd_args):\\n        \"\"\"Run the given SSH command, and raise an IOError if it fails.\\n        Return ``(stdout, stderr)``\\n\\n        Use this for commands with a bounded amount of output.\\n        \"\"\"\\n        p = self._ssh_launch(address, cmd_args)\\n\\n        stdout, stderr = p.communicate()\\n\\n        if p.returncode != 0:\\n            raise IOError(to_unicode(stderr))\\n\\n        return stdout, stderr",
                    "func_fullName": "mrjob.fs.ssh._ssh_run( self, address, cmd_args )"
                },
                {
                    "func_id": 1543,
                    "func_name": "_ssh_finish_run",
                    "func_desc": "_ssh_finish_run",
                    "func_file": "ssh",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ssh_finish_run(self, p):\\n        \"\"\"Close file handles and do error handling on a ``Popen``\\n        who we've read stdout from but done nothing else.\"\"\"\\n        stderr = p.stderr.read()\\n\\n        p.stdout.close()\\n        p.stderr.close()\\n\\n        returncode = p.wait()\\n\\n        if returncode != 0:\\n            raise IOError(stderr)",
                    "func_fullName": "mrjob.fs.ssh._ssh_finish_run( self, p )"
                },
                {
                    "func_id": 1544,
                    "func_name": "_ssh_add_key",
                    "func_desc": "_ssh_add_key",
                    "func_file": "ssh",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ssh_add_key(self):\\n        \"\"\"Add ``self._ec2_key_pair_file`` to the ssh agent with ``ssh-add``.\\n        \"\"\"\\n        args = self._ssh_add_bin + [\\n            '-t', '60', self._ec2_key_pair_file]\\n\\n        log.debug('  > ' + cmd_line(args))\\n\\n        try:\\n            p = Popen(args, stdout=PIPE, stderr=PIPE)\\n        except OSError as ex:\\n            raise IOError(ex.strerror)\\n\\n        stdout, stderr = p.communicate()\\n\\n        if p.returncode != 0:\\n            raise IOError(to_unicode(stderr))",
                    "func_fullName": "mrjob.fs.ssh._ssh_add_key( self )"
                },
                {
                    "func_id": 1545,
                    "func_name": "can_handle_path",
                    "func_desc": "can_handle_path",
                    "func_file": "ssh",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def can_handle_path(self, path):\\n        return _SSH_URI_RE.match(path) is not None",
                    "func_fullName": "mrjob.fs.ssh.can_handle_path( self, path )"
                },
                {
                    "func_id": 1546,
                    "func_name": "du",
                    "func_desc": "du",
                    "func_file": "ssh",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def du(self, path_glob):\\n        raise IOError()  # not implemented",
                    "func_fullName": "mrjob.fs.ssh.du( self, path_glob )"
                },
                {
                    "func_id": 1547,
                    "func_name": "ls",
                    "func_desc": "ls",
                    "func_file": "ssh",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def ls(self, path_glob):\\n        m = _SSH_URI_RE.match(path_glob)\\n        addr = m.group('hostname')\\n        path_to_ls = m.group('filesystem_path')\\n\\n        p = self._ssh_launch(\\n            addr, ['find', '-L', path_to_ls, '-type', 'f'])\\n\\n        for line in p.stdout:\\n            path = to_unicode(line).rstrip('\\n')\\n            yield 'ssh://%s%s' % (addr, path)\\n\\n        self._ssh_finish_run(p)",
                    "func_fullName": "mrjob.fs.ssh.ls( self, path_glob )"
                },
                {
                    "func_id": 1548,
                    "func_name": "md5sum",
                    "func_desc": "md5sum",
                    "func_file": "ssh",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def md5sum(self, path):\\n        raise IOError()  # not implemented",
                    "func_fullName": "mrjob.fs.ssh.md5sum( self, path )"
                },
                {
                    "func_id": 1549,
                    "func_name": "_cat_file",
                    "func_desc": "_cat_file",
                    "func_file": "ssh",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cat_file(self, path):\\n        from mrjob.cat import decompress\\n        m = _SSH_URI_RE.match(path)\\n        addr = m.group('hostname')\\n        fs_path = m.group('filesystem_path')\\n\\n        p = self._ssh_launch(addr, ['cat', fs_path])\\n\\n        for chunk in decompress(p.stdout, fs_path):\\n            yield chunk\\n\\n        self._ssh_finish_run(p)",
                    "func_fullName": "mrjob.fs.ssh._cat_file( self, path )"
                },
                {
                    "func_id": 1550,
                    "func_name": "mkdir",
                    "func_desc": "mkdir",
                    "func_file": "ssh",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mkdir(self, path):\\n        raise IOError()  # not implemented",
                    "func_fullName": "mrjob.fs.ssh.mkdir( self, path )"
                },
                {
                    "func_id": 1551,
                    "func_name": "exists",
                    "func_desc": "exists",
                    "func_file": "ssh",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def exists(self, path_glob):\\n        # just fall back on ls(); it's smart\\n        try:\\n            return any(self.ls(path_glob))\\n        except IOError:\\n            return False",
                    "func_fullName": "mrjob.fs.ssh.exists( self, path_glob )"
                },
                {
                    "func_id": 1552,
                    "func_name": "rm",
                    "func_desc": "rm",
                    "func_file": "ssh",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def rm(self, path_glob):\\n        raise IOError()  # not implemented",
                    "func_fullName": "mrjob.fs.ssh.rm( self, path_glob )"
                },
                {
                    "func_id": 1553,
                    "func_name": "touchz",
                    "func_desc": "touchz",
                    "func_file": "ssh",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def touchz(self, path):\\n        raise IOError()  # not implemented",
                    "func_fullName": "mrjob.fs.ssh.touchz( self, path )"
                },
                {
                    "func_id": 1554,
                    "func_name": "use_sudo_over_ssh",
                    "func_desc": "use_sudo_over_ssh",
                    "func_file": "ssh",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def use_sudo_over_ssh(self, sudo=True):\\n        \"\"\"Use this to turn on *sudo* (we do this depending on the AMI\\n        version on EMR).\"\"\"\\n        self._sudo = sudo",
                    "func_fullName": "mrjob.fs.ssh.use_sudo_over_ssh( self, sudo )"
                }
            ]
        },
        {
            "cluster_id": 11,
            "feature_id": 76,
            "feature_desc": "gamma=0.0100; k=1; a=0.25; combined=0.477; stability(ARI)=1.000; sep=0.135",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1658,
                    "func_name": "_endpoint_url",
                    "func_desc": "_endpoint_url",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _endpoint_url(host_or_uri):\\n    \"\"\"If *host_or_uri* is non-empty and isn't a URI, prepend ``'https://'``.\\n\\n    Otherwise, pass through as-is.\\n    \"\"\"\\n    if not host_or_uri:\\n        return host_or_uri\\n    elif is_uri(host_or_uri):\\n        return host_or_uri\\n    else:\\n        return 'https://' + host_or_uri",
                    "func_fullName": "mrjob.fs.s3._endpoint_url( host_or_uri )"
                },
                {
                    "func_id": 1659,
                    "func_name": "_get_bucket_region",
                    "func_desc": "_get_bucket_region",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _get_bucket_region(client, bucket_name):\\n    \"\"\"Look up the given bucket's location constraint and translate\\n    it to a region name.\"\"\"\\n    resp = client.get_bucket_location(Bucket=bucket_name)\\n    return resp['LocationConstraint'] or _S3_REGION_WITH_NO_LOCATION_CONSTRAINT",
                    "func_fullName": "mrjob.fs.s3._get_bucket_region( client, bucket_name )"
                },
                {
                    "func_id": 1679,
                    "func_name": "_is_permanent_boto3_error",
                    "func_desc": "_is_permanent_boto3_error",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _is_permanent_boto3_error(ex):\\n    \"\"\"Used to disable S3Filesystem when boto3 is installed but\\n    credentials aren't set up.\"\"\"\\n    return isinstance(ex, botocore.exceptions.NoCredentialsError)",
                    "func_fullName": "mrjob.fs.s3._is_permanent_boto3_error( ex )"
                },
                {
                    "func_id": 1680,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, aws_access_key_id=None, aws_secret_access_key=None,\\n                 aws_session_token=None, s3_endpoint=None, s3_region=None,\\n                 part_size=None):\\n        super(S3Filesystem, self).__init__()\\n        self._s3_endpoint_url = _endpoint_url(s3_endpoint)\\n        self._s3_region = s3_region\\n        self._aws_access_key_id = aws_access_key_id\\n        self._aws_secret_access_key = aws_secret_access_key\\n        self._aws_session_token = aws_session_token\\n        self._part_size = part_size",
                    "func_fullName": "mrjob.fs.s3.__init__( self, aws_access_key_id, aws_secret_access_key, aws_session_token, s3_endpoint, s3_region, part_size )"
                },
                {
                    "func_id": 1681,
                    "func_name": "can_handle_path",
                    "func_desc": "can_handle_path",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def can_handle_path(self, path):\\n        return is_s3_uri(path)",
                    "func_fullName": "mrjob.fs.s3.can_handle_path( self, path )"
                },
                {
                    "func_id": 1682,
                    "func_name": "du",
                    "func_desc": "du",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def du(self, path_glob):\\n        \"\"\"Get the size of all files matching path_glob.\"\"\"\\n        return sum(key.size for uri, key in self._ls(path_glob))",
                    "func_fullName": "mrjob.fs.s3.du( self, path_glob )"
                },
                {
                    "func_id": 1683,
                    "func_name": "ls",
                    "func_desc": "ls",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def ls(self, path_glob):\\n        \"\"\"Recursively yield the URIs of S3 keys matching the given glob.\\n\\n        *path_glob* can include ``?`` to match single characters or\\n        ``*`` to match 0 or more characters. Both ``?`` and ``*`` can match\\n        ``/``.\\n        \"\"\"\\n        for uri, key in self._ls(path_glob):\\n            yield uri",
                    "func_fullName": "mrjob.fs.s3.ls( self, path_glob )"
                },
                {
                    "func_id": 1684,
                    "func_name": "_ls",
                    "func_desc": "_ls",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _ls(self, path_glob):\\n        \"\"\"Helper method for :py:meth:`ls`; yields tuples of\\n        ``(uri, key)`` where *key* is the corresponding boto3 s3.ObjectSummary.\\n        \"\"\"\\n        # clean up the  base uri to ensure we have pass boto3 an s3:// URI\\n        # (not s3n://)\\n        scheme = urlparse(path_glob).scheme\\n\\n        # support globs\\n        glob_match = GLOB_RE.match(path_glob)\\n\\n        # we're going to search for all keys starting with base_uri\\n        if glob_match:\\n            # cut it off at first wildcard\\n            base_uri = glob_match.group(1)\\n        else:\\n            base_uri = path_glob\\n\\n        bucket_name, base_name = parse_s3_uri(base_uri)\\n\\n        # allow subdirectories of the path/glob\\n        if path_glob and not path_glob.endswith('/'):\\n            dir_glob = path_glob + '/*'\\n        else:\\n            dir_glob = path_glob + '*'\\n\\n        try:\\n            bucket = self.get_bucket(bucket_name)\\n        except botocore.exceptions.ClientError as ex:\\n            if _client_error_status(ex) == 404:  # treat nonexistent as empty\\n                return\\n            raise\\n\\n        for key in bucket.objects.filter(Prefix=base_name):\\n            uri = \"%s://%s/%s\" % (scheme, bucket_name, key.key)\\n\\n            # enforce globbing\\n            if not (fnmatch.fnmatchcase(uri, path_glob) or\\n                    fnmatch.fnmatchcase(uri, dir_glob)):\\n                continue\\n\\n            yield uri, key",
                    "func_fullName": "mrjob.fs.s3._ls( self, path_glob )"
                },
                {
                    "func_id": 1685,
                    "func_name": "md5sum",
                    "func_desc": "md5sum",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def md5sum(self, path):\\n        k = self._get_s3_key(path)\\n        if not k:\\n            raise IOError('Key %r does not exist' % (path,))\\n        return k.e_tag.strip('\"')",
                    "func_fullName": "mrjob.fs.s3.md5sum( self, path )"
                },
                {
                    "func_id": 1686,
                    "func_name": "_cat_file",
                    "func_desc": "_cat_file",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _cat_file(self, path):\\n        # stream lines from the s3 key\\n        s3_key = self._get_s3_key(path)\\n        body = s3_key.get()['Body']\\n\\n        return decompress(body, path)",
                    "func_fullName": "mrjob.fs.s3._cat_file( self, path )"
                },
                {
                    "func_id": 1687,
                    "func_name": "exists",
                    "func_desc": "exists",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def exists(self, path_glob):\\n        \"\"\"Does the given path exist?\\n\\n        If dest is a directory (ends with a \"/\"), we check if there are\\n        any files starting with that path.\\n        \"\"\"\\n        # just fall back on _ls(); it's smart\\n        return any(self._ls(path_glob))",
                    "func_fullName": "mrjob.fs.s3.exists( self, path_glob )"
                },
                {
                    "func_id": 1688,
                    "func_name": "mkdir",
                    "func_desc": "mkdir",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mkdir(self, path):\\n        \"\"\"Make a directory. This doesn't actually create directories on S3\\n        (because there is no such thing), but it will create the corresponding\\n        bucket if it doesn't exist.\\n        \"\"\"\\n        bucket_name, key_name = parse_s3_uri(path)\\n\\n        client = self.make_s3_client()\\n\\n        try:\\n            client.head_bucket(Bucket=bucket_name)\\n        except botocore.exceptions.ClientError as ex:\\n            if _client_error_status(ex) != 404:\\n                raise\\n\\n            self.create_bucket(bucket_name)",
                    "func_fullName": "mrjob.fs.s3.mkdir( self, path )"
                },
                {
                    "func_id": 1689,
                    "func_name": "put",
                    "func_desc": "put",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def put(self, src, path):\\n        \"\"\"Uploads a local file to a specific destination.\"\"\"\\n        s3_key = self._get_s3_key(path)\\n\\n        # if part_size is None or 0, disable multipart upload\\n        part_size = self._part_size or _HUGE_PART_SIZE\\n\\n        s3_key.upload_file(\\n            src,\\n            Config=boto3.s3.transfer.TransferConfig(\\n                multipart_chunksize=part_size,\\n                multipart_threshold=part_size,\\n            ),\\n        )",
                    "func_fullName": "mrjob.fs.s3.put( self, src, path )"
                },
                {
                    "func_id": 1690,
                    "func_name": "rm",
                    "func_desc": "rm",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def rm(self, path_glob):\\n        \"\"\"Remove all files matching the given glob.\"\"\"\\n        for uri, key in self._ls(path_glob):\\n            log.debug('deleting ' + uri)\\n            key.delete()",
                    "func_fullName": "mrjob.fs.s3.rm( self, path_glob )"
                },
                {
                    "func_id": 1691,
                    "func_name": "touchz",
                    "func_desc": "touchz",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def touchz(self, path):\\n        \"\"\"Make an empty file in the given location. Raises an error if\\n        a non-empty file already exists in that location.\"\"\"\\n        key = self._get_s3_key(path)\\n\\n        data = None\\n        try:\\n            data = key.get()\\n        except botocore.exceptions.ClientError as ex:\\n            # okay if key doesn't exist\\n            if _client_error_status(ex) != 404:\\n                raise\\n\\n        if data and data['ContentLength'] != 0:\\n            raise OSError('Non-empty file %r already exists!' % (path,))\\n\\n        key.put(Body=b'')",
                    "func_fullName": "mrjob.fs.s3.touchz( self, path )"
                },
                {
                    "func_id": 1692,
                    "func_name": "make_s3_resource",
                    "func_desc": "make_s3_resource",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def make_s3_resource(self, region_name=None):\\n        \"\"\"Create a :py:mod:`boto3` S3 resource, with its client\\n        wrapped in a :py:class:`mrjob.retry.RetryWrapper`\\n\\n        :param region: region to use to choose S3 endpoint\\n\\n        It's best to use :py:meth:`get_bucket` because it chooses the\\n        appropriate S3 endpoint automatically. If you are trying to get\\n        bucket metadata, use :py:meth:`make_s3_client`.\\n        \"\"\"\\n        # give a non-cryptic error message if boto3 isn't installed\\n        if boto3 is None:\\n            raise ImportError('You must install boto3 to connect to S3')\\n\\n        kwargs = self._client_kwargs(region_name)\\n\\n        s3_resource = boto3.resource('s3', **kwargs)\\n        s3_resource.meta.client = _wrap_aws_client(s3_resource.meta.client)\\n\\n        return s3_resource",
                    "func_fullName": "mrjob.fs.s3.make_s3_resource( self, region_name )"
                },
                {
                    "func_id": 1693,
                    "func_name": "make_s3_client",
                    "func_desc": "make_s3_client",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def make_s3_client(self, region_name=None):\\n        \"\"\"Create a :py:mod:`boto3` S3 client,\\n        wrapped in a :py:class:`mrjob.retry.RetryWrapper`\\n\\n        :param region: region to use to choose S3 endpoint.\\n        \"\"\"\\n        # give a non-cryptic error message if boto3 isn't installed\\n        if boto3 is None:\\n            raise ImportError('You must install boto3 to connect to S3')\\n\\n        kwargs = self._client_kwargs(region_name or self._s3_region)\\n\\n        return _wrap_aws_client(boto3.client('s3', **kwargs))",
                    "func_fullName": "mrjob.fs.s3.make_s3_client( self, region_name )"
                },
                {
                    "func_id": 1694,
                    "func_name": "_client_kwargs",
                    "func_desc": "_client_kwargs",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _client_kwargs(self, region_name):\\n        \"\"\"Keyword args for creating resources or clients.\"\"\"\\n\\n        return dict(\\n            aws_access_key_id=self._aws_access_key_id,\\n            aws_secret_access_key=self._aws_secret_access_key,\\n            aws_session_token=self._aws_session_token,\\n            endpoint_url=self._s3_endpoint_url,\\n            region_name=(region_name or self._s3_region),\\n        )",
                    "func_fullName": "mrjob.fs.s3._client_kwargs( self, region_name )"
                },
                {
                    "func_id": 1695,
                    "func_name": "get_bucket",
                    "func_desc": "get_bucket",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_bucket(self, bucket_name):\\n        \"\"\"Get the (:py:mod:`boto3`) bucket, connecting through the\\n        appropriate endpoint.\"\"\"\\n        client = self.make_s3_client()\\n\\n        try:\\n            region_name = _get_bucket_region(client, bucket_name)\\n        except botocore.exceptions.ClientError as ex:\\n            # it's possible to have access to a bucket but not access\\n            # to its location metadata. This happens on the 'elasticmapreduce'\\n            # bucket, for example (see #1170)\\n            if _client_error_status(ex) != 403:\\n                raise\\n            log.warning('Could not infer endpoint for bucket %s; '\\n                        'assuming defaults', bucket_name)\\n            region_name = None\\n\\n        resource = self.make_s3_resource(region_name)\\n        return resource.Bucket(bucket_name)",
                    "func_fullName": "mrjob.fs.s3.get_bucket( self, bucket_name )"
                },
                {
                    "func_id": 1696,
                    "func_name": "_get_s3_key",
                    "func_desc": "_get_s3_key",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _get_s3_key(self, uri):\\n        \"\"\"Get the boto3 s3.Object matching the given S3 uri, or\\n        return None if that key doesn't exist.\\n\\n        uri is an S3 URI: ``s3://foo/bar``\\n        \"\"\"\\n        bucket_name, key_name = parse_s3_uri(uri)\\n        return self.get_bucket(bucket_name).Object(key_name)",
                    "func_fullName": "mrjob.fs.s3._get_s3_key( self, uri )"
                },
                {
                    "func_id": 1697,
                    "func_name": "get_all_bucket_names",
                    "func_desc": "get_all_bucket_names",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_all_bucket_names(self):\\n        \"\"\"Get a list of the names of all buckets owned by this user\\n        on S3.\\n        \"\"\"\\n        c = self.make_s3_client()\\n        return [b['Name'] for b in c.list_buckets()['Buckets']]",
                    "func_fullName": "mrjob.fs.s3.get_all_bucket_names( self )"
                },
                {
                    "func_id": 1698,
                    "func_name": "create_bucket",
                    "func_desc": "create_bucket",
                    "func_file": "s3",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def create_bucket(self, bucket_name, region=None):\\n        \"\"\"Create a bucket on S3 with a location constraint\\n        matching the given region.\\n        \"\"\"\\n        client = self.make_s3_client()\\n\\n        params = dict(Bucket=bucket_name)\\n\\n        if region is None:\\n            region = self._s3_region\\n\\n        # CreateBucketConfiguration can't be empty, so don't set it\\n        # unless there's a location constraint (see #1927)\\n        if region and region != _S3_REGION_WITH_NO_LOCATION_CONSTRAINT:\\n            params['CreateBucketConfiguration'] = dict(\\n                LocationConstraint=region)\\n\\n        client.create_bucket(**params)",
                    "func_fullName": "mrjob.fs.s3.create_bucket( self, bucket_name, region )"
                },
                {
                    "func_id": 1881,
                    "func_name": "main",
                    "func_desc": "main",
                    "func_file": "s3_tmpwatch",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def main(cl_args=None):\\n    arg_parser = _make_arg_parser()\\n    options = arg_parser.parse_args(cl_args)\\n\\n    MRJob.set_up_logging(quiet=options.quiet, verbose=options.verbose)\\n\\n    time_old = _process_time(options.time_untouched)\\n\\n    for path in options.uris:\\n        _s3_cleanup(path, time_old,\\n                    dry_run=options.test,\\n                    **_runner_kwargs(options))",
                    "func_fullName": "mrjob.tools.emr.s3_tmpwatch.main( cl_args )"
                },
                {
                    "func_id": 1882,
                    "func_name": "_s3_cleanup",
                    "func_desc": "_s3_cleanup",
                    "func_file": "s3_tmpwatch",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _s3_cleanup(glob_path, time_old, dry_run=False, **runner_kwargs):\\n    \"\"\"Delete all files older than *time_old* in *path*.\\n\\n    If *dry_run* is true, then just log the files that need to be\\n    deleted without actually deleting them\\n    \"\"\"\\n    runner = EMRJobRunner(**runner_kwargs)\\n\\n    log.info('Deleting all files in %s that are older than %s' %\\n             (glob_path, time_old))\\n\\n    for path, key in runner.fs.s3._ls(glob_path):\\n        age = _boto3_now() - key.last_modified\\n        if age > time_old:\\n            # Delete it\\n            log.info('Deleting %s; is %s old' % (path, age))\\n            if not dry_run:\\n                key.delete()",
                    "func_fullName": "mrjob.tools.emr.s3_tmpwatch._s3_cleanup( glob_path, time_old, dry_run, **runner_kwargs )"
                },
                {
                    "func_id": 1883,
                    "func_name": "_runner_kwargs",
                    "func_desc": "_runner_kwargs",
                    "func_file": "s3_tmpwatch",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _runner_kwargs(options):\\n    \"\"\"Options to pass to the EMRJobRunner.\"\"\"\\n    kwargs = options.__dict__.copy()\\n    for unused_arg in ('quiet', 'verbose', 'test'):\\n        del kwargs[unused_arg]\\n\\n    return kwargs",
                    "func_fullName": "mrjob.tools.emr.s3_tmpwatch._runner_kwargs( options )"
                },
                {
                    "func_id": 1884,
                    "func_name": "_process_time",
                    "func_desc": "_process_time",
                    "func_file": "s3_tmpwatch",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _process_time(time):\\n    if time[-1] == 'm':\\n        return timedelta(minutes=int(time[:-1]))\\n    elif time[-1] == 'h':\\n        return timedelta(hours=int(time[:-1]))\\n    elif time[-1] == 'd':\\n        return timedelta(days=int(time[:-1]))\\n    else:\\n        return timedelta(hours=int(time))",
                    "func_fullName": "mrjob.tools.emr.s3_tmpwatch._process_time( time )"
                },
                {
                    "func_id": 1885,
                    "func_name": "_make_arg_parser",
                    "func_desc": "_make_arg_parser",
                    "func_file": "s3_tmpwatch",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _make_arg_parser():\\n    usage = '%(prog)s s3-tmpwatch [options] TIME_UNTOUCHED URI [URI ...]'\\n    description = (\\n        'Delete all files at one or more URIs that are older than a'\\n        ' specified time.')\\n\\n    arg_parser = ArgumentParser(usage=usage, description=description)\\n\\n    arg_parser.add_argument(\\n        '-t', '--test', dest='test', default=False,\\n        action='store_true',\\n        help=\"Don't actually delete any files; just log that we would\")\\n\\n    arg_parser.add_argument(\\n        dest='time_untouched',\\n        help='The time threshold for removing'\\n        ' files. A number with an optional'\\n        ' single-character suffix specifying the units: m for minutes, h for'\\n        ' hours, d for days. If no suffix is specified, time is in hours.')\\n\\n    arg_parser.add_argument(\\n        dest='uris', nargs='+',\\n        help='s3:// URIs specifying where to delete old files')\\n\\n    _add_basic_args(arg_parser)\\n    _add_runner_args(\\n        arg_parser,\\n        set(['region', 's3_endpoint']),\\n    )\\n\\n    _alphabetize_actions(arg_parser)\\n\\n    return arg_parser",
                    "func_fullName": "mrjob.tools.emr.s3_tmpwatch._make_arg_parser(  )"
                },
                {
                    "func_id": 2387,
                    "func_name": "__init__",
                    "func_desc": "__init__",
                    "func_file": "exceptions",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def __init__(self, reason):\\n        self.msg = str(reason)\\n        super(ArchiveLoadFailed, self).__init__(self.msg)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.exceptions.__init__( self, reason )"
                }
            ]
        },
        {
            "cluster_id": 2,
            "feature_id": 77,
            "feature_desc": "gamma=0.0100; k=1; a=0.25; combined=0.400; stability(ARI)=1.000; sep=0.151",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1898,
                    "func_name": "main",
                    "func_desc": "main",
                    "func_file": "spark_wordcount_script",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def main():\\n    # read in input, output path\\n    args = sys.argv[1:]\\n\\n    if len(args) != 2:\\n        raise ValueError\\n\\n    inputPath, outputPath = args\\n\\n    sc = SparkContext(appName='mrjob Spark wordcount script')\\n\\n    lines = sc.textFile(inputPath)\\n\\n    # lines.flatMap(WORD_RE.findall) doesn't work on Spark 1.6.2; apparently\\n    # it can't serialize instance methods?\\n    counts = (\\n        lines.flatMap(lambda line: [w.lower() for w in WORD_RE.findall(line)])\\n        .map(lambda word: (word, 1))\\n        .reduceByKey(add))\\n\\n    counts.saveAsTextFile(outputPath)\\n\\n    sc.stop()",
                    "func_fullName": "mrjob.examples.spark_wordcount_script.main(  )"
                },
                {
                    "func_id": 1904,
                    "func_name": "steps",
                    "func_desc": "steps",
                    "func_file": "mr_next_word_stats",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def steps(self):\\n        return [MRStep(mapper=self.m_find_words,\\n                       combiner=self.c_combine_counts,\\n                       reducer=self.r_sum_counts),\\n                MRStep(reducer=self.r_compute_stats)]",
                    "func_fullName": "mrjob.examples.mr_next_word_stats.steps( self )"
                },
                {
                    "func_id": 1905,
                    "func_name": "m_find_words",
                    "func_desc": "m_find_words",
                    "func_file": "mr_next_word_stats",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def m_find_words(self, _, line):\\n        \"\"\"Tokenize lines, and look for pairs of adjacent words.\\n\\n        Yield (prev_word, word), 1 and (prev_word, '*'), 1 for each pair\\n        \"\"\"\\n        prev_word = None\\n\\n        for word in WORD_RE.findall(line):\\n            word = word.lower()\\n\\n            if prev_word is not None:\\n                # total up the number of times prev_word appears\\n                # and the number of times next_word appears after it\\n                yield (prev_word, '*'), 1\\n                yield (prev_word, word), 1\\n\\n            prev_word = word",
                    "func_fullName": "mrjob.examples.mr_next_word_stats.m_find_words( self, _, line )"
                },
                {
                    "func_id": 1906,
                    "func_name": "c_combine_counts",
                    "func_desc": "c_combine_counts",
                    "func_file": "mr_next_word_stats",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def c_combine_counts(self, key, counts):\\n        \"\"\"Sum up all those 1s before passing data off to the reducer\"\"\"\\n        yield key, sum(counts)",
                    "func_fullName": "mrjob.examples.mr_next_word_stats.c_combine_counts( self, key, counts )"
                },
                {
                    "func_id": 1907,
                    "func_name": "r_sum_counts",
                    "func_desc": "r_sum_counts",
                    "func_file": "mr_next_word_stats",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def r_sum_counts(self, key, counts):\\n        \"\"\"Compute the number of times each pair of words appears, and the\\n        number of times the first word in a pair appears, and send it to\\n        a reducer that keys on the first word in the pair.\\n        \"\"\"\\n        count = sum(counts)\\n\\n        prev_word, word = key\\n\\n        if word == '*':\\n            # we want total to arrive at r_compute_stats first, so\\n            # prefix it with \"A\", which comes before \"B\"\\n            yield prev_word, ('A: total', count)\\n        else:\\n            yield prev_word, ('B: stats', (word, count))",
                    "func_fullName": "mrjob.examples.mr_next_word_stats.r_sum_counts( self, key, counts )"
                },
                {
                    "func_id": 1908,
                    "func_name": "r_compute_stats",
                    "func_desc": "r_compute_stats",
                    "func_file": "mr_next_word_stats",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def r_compute_stats(self, prev_word, value):\\n        \"\"\"For each pair of words, compute how many times it appears,\\n        how many times the first word appears in a pair, and the percentage\\n        of time the second word follows the first.\\n\\n        This relies on values appearing in sorted order; we need the total\\n        number of times the first word appears before we can compute the\\n        percentage for each second word.\\n        \"\"\"\\n        total = None\\n\\n        for value_type, data in value:\\n            if value_type == 'A: total':\\n                total = data\\n            else:\\n                assert value_type == 'B: stats'\\n                word, count = data\\n                # A comes before B, so total should already be set\\n                percent = 100.0 * count / total\\n                yield (prev_word, word), (total, count, percent)",
                    "func_fullName": "mrjob.examples.mr_next_word_stats.r_compute_stats( self, prev_word, value )"
                },
                {
                    "func_id": 1952,
                    "func_name": "steps",
                    "func_desc": "steps",
                    "func_file": "mr_spark_wordcount_script",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def steps(self):\\n        return [\\n            SparkScriptStep(\\n                script=join(dirname(__file__), 'spark_wordcount_script.py'),\\n                args=[INPUT, OUTPUT],\\n            ),\\n        ]",
                    "func_fullName": "mrjob.examples.mr_spark_wordcount_script.steps( self )"
                },
                {
                    "func_id": 1959,
                    "func_name": "spark",
                    "func_desc": "spark",
                    "func_file": "mr_spark_wordcount",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def spark(self, input_path, output_path):\\n        # Spark may not be available where script is launched\\n        from pyspark import SparkContext\\n\\n        sc = SparkContext(appName='mrjob Spark wordcount script')\\n\\n        lines = sc.textFile(input_path)\\n\\n        counts = (\\n            lines.flatMap(self.get_words)\\n            .map(lambda word: (word, 1))\\n            .reduceByKey(add))\\n\\n        counts.saveAsTextFile(output_path)\\n\\n        sc.stop()",
                    "func_fullName": "mrjob.examples.mr_spark_wordcount.spark( self, input_path, output_path )"
                },
                {
                    "func_id": 1960,
                    "func_name": "get_words",
                    "func_desc": "get_words",
                    "func_file": "mr_spark_wordcount",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_words(self, line):\\n        return [w.lower() for w in WORD_RE.findall(line)]",
                    "func_fullName": "mrjob.examples.mr_spark_wordcount.get_words( self, line )"
                },
                {
                    "func_id": 1965,
                    "func_name": "mapper_pre_filter",
                    "func_desc": "mapper_pre_filter",
                    "func_file": "mr_words_containing_u_freq_count",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper_pre_filter(self):\\n        # no need to account for grep exiting with status 1 if no matches,\\n        # since pre-filters are piped into another process. Compare to\\n        # mr_grep.py\\n        return 'grep -i u'",
                    "func_fullName": "mrjob.examples.mr_words_containing_u_freq_count.mapper_pre_filter( self )"
                },
                {
                    "func_id": 1966,
                    "func_name": "mapper",
                    "func_desc": "mapper",
                    "func_file": "mr_words_containing_u_freq_count",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper(self, _, line):\\n        for word in WORD_RE.findall(line):\\n            yield (word.lower(), 1)",
                    "func_fullName": "mrjob.examples.mr_words_containing_u_freq_count.mapper( self, _, line )"
                },
                {
                    "func_id": 1967,
                    "func_name": "combiner",
                    "func_desc": "combiner",
                    "func_file": "mr_words_containing_u_freq_count",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def combiner(self, word, counts):\\n        yield (word, sum(counts))",
                    "func_fullName": "mrjob.examples.mr_words_containing_u_freq_count.combiner( self, word, counts )"
                },
                {
                    "func_id": 1968,
                    "func_name": "reducer",
                    "func_desc": "reducer",
                    "func_file": "mr_words_containing_u_freq_count",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def reducer(self, word, counts):\\n        yield (word, sum(counts))",
                    "func_fullName": "mrjob.examples.mr_words_containing_u_freq_count.reducer( self, word, counts )"
                },
                {
                    "func_id": 1981,
                    "func_name": "mapper",
                    "func_desc": "mapper",
                    "func_file": "mr_word_freq_count",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper(self, _, line):\\n        for word in WORD_RE.findall(line):\\n            yield (word.lower(), 1)",
                    "func_fullName": "mrjob.examples.mr_word_freq_count.mapper( self, _, line )"
                },
                {
                    "func_id": 1982,
                    "func_name": "combiner",
                    "func_desc": "combiner",
                    "func_file": "mr_word_freq_count",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def combiner(self, word, counts):\\n        yield (word, sum(counts))",
                    "func_fullName": "mrjob.examples.mr_word_freq_count.combiner( self, word, counts )"
                },
                {
                    "func_id": 1983,
                    "func_name": "reducer",
                    "func_desc": "reducer",
                    "func_file": "mr_word_freq_count",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def reducer(self, word, counts):\\n        yield (word, sum(counts))",
                    "func_fullName": "mrjob.examples.mr_word_freq_count.reducer( self, word, counts )"
                }
            ]
        },
        {
            "cluster_id": 14,
            "feature_id": 78,
            "feature_desc": "gamma=0.0100; k=1; a=0.25; combined=0.400; stability(ARI)=1.000; sep=0.177",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1909,
                    "func_name": "parse_doc_filename",
                    "func_desc": "parse_doc_filename",
                    "func_file": "mr_text_classifier",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def parse_doc_filename(input_uri):\\n    \"\"\"Parse a filename like ``some_id-cat1-cat2-not_cat3.txt`` into\\n    ``dict(id='some_id', cats=dict(cat1=True, cat2=True, cat3=False))``\\n    \"\"\"\\n    # get filename without extension\\n    from mrjob.util import file_ext\\n    name_with_ext = posixpath.basename(input_uri)\\n    name = name_with_ext[:-len(file_ext(name_with_ext))]\\n\\n    parts = name.split('-')\\n\\n    doc_id = parts[0]\\n    cats = {}\\n\\n    for part in parts[1:]:\\n        if part.startswith('not_'):\\n            cats[part[4:]] = False\\n        else:\\n            cats[part] = True\\n\\n    return dict(id=doc_id, cats=cats)",
                    "func_fullName": "mrjob.examples.mr_text_classifier.parse_doc_filename( input_uri )"
                },
                {
                    "func_id": 1910,
                    "func_name": "count_ngrams",
                    "func_desc": "count_ngrams",
                    "func_file": "mr_text_classifier",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def count_ngrams(text, max_ngram_size, stop_words):\\n    \"\"\"Break text down into ngrams, and return a dictionary mapping\\n    (n, ngram) to number of times that ngram occurs.\\n\\n    n: ngram size (\"foo\" is a 1-gram, \"foo bar baz\" is a 3-gram)\\n    ngram: the ngram, as a space-separated string or None to indicate the\\n        ANY ngram (basically the number of words in the document).\\n\\n    Args:\\n    text -- text, as a unicode\\n    max_ngram_size -- maximum size of ngrams to consider\\n    stop_words -- a collection of words (in lowercase) to remove before\\n        parsing out ngrams (e.g. \"the\", \"and\")\\n    \"\"\"\\n    if not isinstance(stop_words, set):\\n        stop_words = set(stop_words)\\n\\n    words = [word.lower() for word in WORD_RE.findall(text)\\n             if word.lower() not in stop_words]\\n\\n    ngram_counts = defaultdict(int)\\n\\n    for i in range(len(words)):\\n        for n in range(1, max_ngram_size + 1):\\n            if i + n <= len(words):\\n                ngram = ' '.join(words[i:i + n])\\n                ngram_counts[(n, ngram)] += 1\\n\\n    # add counts for ANY ngram\\n    for n in range(1, max_ngram_size + 1):\\n        ngram_counts[(n, None)] = len(words) - n + 1\\n\\n    return ngram_counts",
                    "func_fullName": "mrjob.examples.mr_text_classifier.count_ngrams( text, max_ngram_size, stop_words )"
                },
                {
                    "func_id": 1919,
                    "func_name": "steps",
                    "func_desc": "steps",
                    "func_file": "mr_text_classifier",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def steps(self):\\n        \"\"\"Conceptually, the steps are:\\n        1. Parse documents into ngrams\\n        2. Group by ngram to get a frequency count for each ngram, and to\\n           exclude very rare ngrams\\n        3. Send all ngram information to one \"global\" reducer so we can\\n           assign scores for each category and ngram\\n        4. Group scores and documents by ngram and compute score for that\\n           ngram for that document. Exclude very common ngrams to save memory.\\n        5. Average together scores for each document to get its score for\\n           each category.\\n\\n        The documents themselves are passed through from step 1 to step 5.\\n        Ngram scoring information is passed through from step 4 to step 5.\\n        \"\"\"\\n        return [MRStep(mapper_raw=self.parse_doc,\\n                       reducer=self.count_ngram_freq),\\n                MRStep(reducer=self.score_ngrams),\\n                MRStep(reducer=self.score_documents_by_ngram),\\n                MRStep(reducer=self.score_documents)]",
                    "func_fullName": "mrjob.examples.mr_text_classifier.steps( self )"
                },
                {
                    "func_id": 1920,
                    "func_name": "configure_args",
                    "func_desc": "configure_args",
                    "func_file": "mr_text_classifier",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def configure_args(self):\\n        \"\"\"Add command-line options specific to this script.\"\"\"\\n        super(MRTextClassifier, self).configure_args()\\n\\n        self.add_passthru_arg(\\n            '--min-df', dest='min_df', default=2, type=int,\\n            help=('min number of documents an n-gram must appear in for us to'\\n                  ' count it. Default: %(default)s'))\\n        self.add_passthru_arg(\\n            '--max-df', dest='max_df', default=10000000, type=int,\\n            help=('max number of documents an n-gram may appear in for us to'\\n                  ' count it (this keeps reducers from running out of memory).'\\n                  ' Default: %(default)s'))\\n        self.add_passthru_arg(\\n            '--max-ngram-size', dest='max_ngram_size',\\n            default=DEFAULT_MAX_NGRAM_SIZE, type=int,\\n            help='maximum phrase length to consider')\\n        self.add_passthru_arg(\\n            '--stop-words', dest='stop_words',\\n            default=', '.join(DEFAULT_STOP_WORDS),\\n            help=(\"comma-separated list of words to ignore. For example, \"\\n                  \"--stop-words 'in, the' would cause 'hole in the wall' to be\"\\n                  \" parsed as ['hole', 'wall']. Default: %(default)s\"))\\n        self.add_passthru_arg(\\n            '--short-doc-threshold', dest='short_doc_threshold',\\n            type=int, default=None,\\n            help=('Normally, for each n-gram size, we take the average score'\\n                  ' over all n-grams that appear. This allows us to penalize'\\n                  ' short documents by using this threshold as the denominator'\\n                  ' rather than the actual number of n-grams.'))\\n        self.add_passthru_arg(\\n            '--no-test-set', dest='no_test_set',\\n            action='store_true', default=False,\\n            help=(\"Choose about half of the documents to be the testing set\"\\n                  \" (don't use them to train the classifier) based on a SHA1\"\\n                  \" hash of their text\"))",
                    "func_fullName": "mrjob.examples.mr_text_classifier.configure_args( self )"
                },
                {
                    "func_id": 1921,
                    "func_name": "load_args",
                    "func_desc": "load_args",
                    "func_file": "mr_text_classifier",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def load_args(self, args):\\n        \"\"\"Parse stop_words option.\"\"\"\\n        super(MRTextClassifier, self).load_args(args)\\n\\n        self.stop_words = set()\\n        if self.options.stop_words:\\n            self.stop_words.update(\\n                s.strip().lower() for s in self.options.stop_words.split(','))",
                    "func_fullName": "mrjob.examples.mr_text_classifier.load_args( self, args )"
                },
                {
                    "func_id": 1922,
                    "func_name": "parse_doc",
                    "func_desc": "parse_doc",
                    "func_file": "mr_text_classifier",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def parse_doc(self, input_path, input_uri):\\n        \"\"\"Mapper: parse documents and emit ngram information.\\n\\n        Input: Text files whose name contains a unique document ID and\\n        category information (see :py:func:`parse_doc_filename`).\\n\\n        Output:\\n        ``('ngram', (n, ngram)), (count, cats)`` OR\\n        ``('doc', doc_id), doc``\\n\\n        n: ngram length\\n        ngram: ngram encoded encoded as a string (e.g. \"pad thai\")\\n            or None to indicate ANY ngram.\\n        count:  # of times an ngram appears in the document\\n        cats: a map from category name to a boolean indicating whether it's\\n            this document is in the category\\n\\n        doc_id: (hopefully) unique document ID\\n        doc: the encoded document. We'll fill these fields:\\n            ngram_counts: map from (n, ngram) to  # of times ngram appears\\n                in the document, using (n, None) to represent the total\\n                number of times ANY ngram of that size appears (essentially\\n                number of words)\\n            in_test_set: boolean indicating if this doc is in the test set\\n            id: SHA1 hash of doc text (if not already filled)\\n        \"\"\"\\n        # fill *id* and *cats*\\n        doc = parse_doc_filename(input_uri)\\n\\n        with open(input_path) as f:\\n            text = to_unicode(f.read())\\n\\n        # pick test/training docs\\n        if self.options.no_test_set:\\n            doc['in_test_set'] = False\\n        else:\\n            doc_hash = hashlib.sha1(text.encode('utf-8')).hexdigest()\\n            doc['in_test_set'] = bool(int(doc_hash[-1], 16) % 2)\\n\\n        # map from (n, ngram) to number of times it appears\\n        ngram_counts = count_ngrams(\\n            text, self.options.max_ngram_size, self.stop_words)\\n\\n        # yield the number of times the ngram appears in this doc\\n        # and the categories for this document, so we can train the classifier\\n        if not doc['in_test_set']:\\n            for (n, ngram), count in ngram_counts.items():\\n                yield ('ngram', (n, ngram)), (count, doc['cats'])\\n\\n        # yield the document itself, for safekeeping\\n        doc['ngram_counts'] = list(ngram_counts.items())\\n        yield ('doc', doc['id']), doc",
                    "func_fullName": "mrjob.examples.mr_text_classifier.parse_doc( self, input_path, input_uri )"
                },
                {
                    "func_id": 1923,
                    "func_name": "count_ngram_freq",
                    "func_desc": "count_ngram_freq",
                    "func_file": "mr_text_classifier",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def count_ngram_freq(self, type_and_key, values):\\n        \"\"\"Reducer: Combine information about how many times each ngram\\n        appears for docs in/not in each category. Dump ngrams that appear\\n        in very few documents (according to --min-df switch). If two documents\\n        have the same ID, increment a counter and only keep one; otherwise\\n        pass docs through unchanged.\\n\\n        Input (see parse_doc() for details):\\n        ('ngram', (n, ngram)), (count, cats) OR\\n        ('doc', doc_id), doc\\n\\n        Output:\\n        ('global', None), ((n, ngram), (cat_to_df, cat_to_tf)) OR\\n        ('doc', doc_id), doc\\n        n: ngram length\\n        ngram: ngram encoded encoded as a string (e.g. \"pad thai\")\\n            or None to indicate ANY ngram.\\n        cat_to_df: list of tuples of ((cat_name, is_in_category), df); df\\n            is  # of documents of this type that the ngram appears in\\n        cat_to_tf: list of tuples of ((cat_name, is_in_category), df); tf\\n            is  # of time the ngram appears in docs of this type\\n        doc_id: unique document ID\\n        doc: the encoded document\\n        \"\"\"\\n        key_type, key = type_and_key\\n\\n        # pass documents through\\n        if key_type == 'doc':\\n            doc_id = key\\n            docs = list(values)\\n            # if two documents end up with the same key, only keep one\\n            if len(docs) > 1:\\n                self.increment_counter(\\n                    'Document key collision', str(doc_id))\\n            yield ('doc', doc_id), docs[0]\\n            return\\n\\n        assert key_type == 'ngram'\\n        n, ngram = key\\n\\n        # total # of docs this ngram appears in\\n        total_df = 0\\n        # map from (cat, is_in_cat) to\\n        # number of documents in this cat it appears in (df), or\\n        # number of times it appears in documents of this type (tf)\\n        cat_to_df = defaultdict(int)\\n        cat_to_tf = defaultdict(int)\\n\\n        for count, cats in values:\\n            total_df += 1\\n            for cat in cats.items():\\n                cat_to_df[cat] += 1\\n                cat_to_tf[cat] += count\\n\\n        # don't bother with very rare ngrams\\n        if total_df < self.options.min_df:\\n            return\\n\\n        yield (('global', None),\\n               ((n, ngram),\\n                (list(cat_to_df.items()), list(cat_to_tf.items()))))",
                    "func_fullName": "mrjob.examples.mr_text_classifier.count_ngram_freq( self, type_and_key, values )"
                },
                {
                    "func_id": 1924,
                    "func_name": "score_ngrams",
                    "func_desc": "score_ngrams",
                    "func_file": "mr_text_classifier",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def score_ngrams(self, type_and_key, values):\\n        \"\"\"Reducer: Look at all ngrams together, and assign scores by\\n        ngram and category. Also farm out documents to the reducer for\\n        any ngram they contain, and pass documents through to the next\\n        step.\\n\\n        To score an ngram for a category, we compare the probability of any\\n        given ngram being our ngram for documents in the category against\\n        documents not in the category. The score is just the log of the\\n        ratio of probabilities (the \"log difference\")\\n\\n        Input (see count_ngram_freq() for details):\\n        ('global', None), ((n, ngram), (cat_to_df, cat_to_tf)) OR\\n        ('doc', doc_id), doc\\n\\n        Output:\\n        ('doc', doc_id), document OR\\n        ('ngram', (n, ngram)), ('doc_id', doc_id) OR\\n        ('ngram', (n, ngram)), ('cat_to_score', cat_to_score)\\n\\n        n: ngram length\\n        ngram: ngram encoded encoded as a string (e.g. \"pad thai\")\\n            or None to indicate ANY ngram.\\n        cat_to_score: map from (cat_name, is_in_category) to score for\\n            this ngram\\n        doc_id: unique document ID\\n        doc: the encoded document\\n        \"\"\"\\n        key_type, key = type_and_key\\n        if key_type == 'doc':\\n            doc_id = key\\n            doc = list(values)[0]\\n            # pass document through\\n            yield ('doc', doc_id), doc\\n\\n            # send document to reducer for every ngram it contains\\n            for (n, ngram), count in doc['ngram_counts']:\\n                # don't bother even creating a reducer for the ANY ngram\\n                # because we'd have to send all documents to it.\\n                if ngram is None:\\n                    continue\\n                yield (('ngram', (n, ngram)),\\n                       ('doc_id', doc_id))\\n\\n            return\\n\\n        assert key_type == 'global'\\n        ngram_to_info = dict(\\n            ((n, ngram),\\n             (dict((tuple(cat), df) for cat, df in cat_to_df),\\n              dict((tuple(cat), tf) for cat, tf in cat_to_tf)))\\n            for (n, ngram), (cat_to_df, cat_to_tf)\\n            in values)\\n\\n        # m = # of possible ngrams of any given type. This is not a very\\n        # rigorous estimate, but it's good enough\\n        m = len(ngram_to_info)\\n\\n        for (n, ngram), info in ngram_to_info.items():\\n            # do this even for the special ANY ngram; it's useful\\n            # as a normalization factor.\\n            cat_to_df, cat_to_tf = info\\n\\n            # get the total # of documents and terms for ngrams of this size\\n            cat_to_d, cat_to_t = ngram_to_info[(n, None)]\\n\\n            # calculate the probability of any given term being\\n            # this term for documents of each type\\n            cat_to_p = {}\\n            for cat, t in cat_to_t.items():\\n                tf = cat_to_tf.get(cat) or 0\\n                # use Laplace's rule of succession to estimate p. See:\\n                # http://en.wikipedia.org/wiki/Rule_of_succession#Generalization_to_any_number_of_possibilities\\n                cat_to_p[cat] = (tf + (2.0 / m)) / (t + 2)\\n\\n            cats = set(cat for cat, in_cat in cat_to_t)\\n            cat_to_score = {}\\n            for cat in cats:\\n                p_if_in = cat_to_p.get((cat, True), 1.0 / m)\\n                p_if_out = cat_to_p.get((cat, False), 1.0 / m)\\n                # take the log difference of probabilities\\n                score = math.log(p_if_in) - math.log(p_if_out)\\n                cat_to_score[cat] = score\\n\\n            yield (('ngram', (n, ngram)),\\n                   ('cat_to_score', cat_to_score))",
                    "func_fullName": "mrjob.examples.mr_text_classifier.score_ngrams( self, type_and_key, values )"
                },
                {
                    "func_id": 1925,
                    "func_name": "score_documents_by_ngram",
                    "func_desc": "score_documents_by_ngram",
                    "func_file": "mr_text_classifier",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def score_documents_by_ngram(self, type_and_key, types_and_values):\\n        \"\"\"Reducer: For all documents that contain a given ngram, send\\n        scoring info to that document. Also pass documents and scoring\\n        info through as-is\\n\\n        Input (see score_ngrams() for details):\\n        ('doc', doc_id), doc OR\\n        ('ngram', (n, ngram)), ('doc_id', doc_id) OR\\n        ('ngram', (n, ngram)), ('cat_to_score', cat_to_score)\\n\\n        Output:\\n        ('doc', doc_id), ('doc', doc)\\n        ('doc', doc_id), ('scores', ((n, ngram), cat_to_score))\\n        ('cat_to_score', (n, ngram)), cat_to_score\\n\\n        n: ngram length\\n        ngram: ngram encoded encoded as a string (e.g. \"pad thai\")\\n            or None to indicate ANY ngram.\\n        cat_to_score: map from (cat_name, is_in_category) to score for\\n            this ngram\\n        doc_id: unique document ID\\n        doc: the encoded document\\n        \"\"\"\\n        key_type, key = type_and_key\\n\\n        # pass documents through\\n        if key_type == 'doc':\\n            doc_id = key\\n            doc = list(types_and_values)[0]\\n            yield ('doc', doc_id), ('doc', doc)\\n            return\\n\\n        assert key_type == 'ngram'\\n        n, ngram = key\\n\\n        doc_ids = []\\n        cat_to_score = None\\n\\n        for value_type, value in types_and_values:\\n            if value_type == 'cat_to_score':\\n                cat_to_score = value\\n                continue\\n\\n            assert value_type == 'doc_id'\\n            doc_ids.append(value)\\n\\n            if len(doc_ids) > self.options.max_df:\\n                self.increment_counter('Exceeded max df', repr((n, ngram)))\\n                return\\n\\n        # skip ngrams that are too rare to score\\n        if cat_to_score is None:\\n            return\\n\\n        # send score info for this ngram to this document\\n        for doc_id in doc_ids:\\n            yield ('doc', doc_id), ('scores', ((n, ngram), cat_to_score))\\n\\n        # keep scoring info\\n        yield ('cat_to_score', (n, ngram)), cat_to_score",
                    "func_fullName": "mrjob.examples.mr_text_classifier.score_documents_by_ngram( self, type_and_key, types_and_values )"
                },
                {
                    "func_id": 1926,
                    "func_name": "score_documents",
                    "func_desc": "score_documents",
                    "func_file": "mr_text_classifier",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def score_documents(self, type_and_key, types_and_values):\\n        \"\"\"Reducer: combine all scoring information for each document, and\\n        add it to the document. Also pass ngram scores through as-is.\\n\\n        To score a document, we essentially take a weighted average of all\\n        the scores for ngrams of each size, and then sum together those\\n        averages. ngrams that aren't scored (because they're very rare or\\n        very common) are considered to have a score of zero. Using averages\\n        allows us to be insensitive to document size. There is a penalty\\n        for very small documents.\\n\\n        Input (see score_ngrams() for details):\\n        ('doc', doc_id), ('doc', doc)\\n        ('doc', doc_id), ('scores', ((n, ngram), cat_to_score))\\n        ('cat_to_score', (n, ngram)), cat_to_score\\n\\n        Output:\\n        ('doc', doc_id), doc\\n        ('cat_to_score', (n, ngram)), cat_to_score\\n\\n        n: ngram length\\n        ngram: ngram encoded encoded as a string (e.g. \"pad thai\")\\n            or None to indicate ANY ngram.\\n        cat_to_score: map from (cat_name, is_in_category) to score for\\n            this ngram\\n        doc_id: unique document ID\\n        doc: the encoded document. this will contain an extra field\\n            'cat_to_score', and will no longer have the 'ngram_counts' field.\\n        \"\"\"\\n        key_type, key = type_and_key\\n\\n        # pass through cat_to_score\\n        if key_type == 'cat_to_score':\\n            cat_to_score = list(types_and_values)[0]\\n            yield ('cat_to_score', key), cat_to_score\\n            return\\n\\n        assert key_type == 'doc'\\n        doc_id = key\\n\\n        # store the document and scoring info\\n        doc = None\\n        ngrams_and_scores = []\\n\\n        for value_type, value in types_and_values:\\n            if value_type == 'doc':\\n                doc = value\\n                continue\\n\\n            assert value_type == 'scores'\\n            ((n, ngram), cat_to_score) = value\\n            ngrams_and_scores.append(((n, ngram), cat_to_score))\\n\\n        # total scores for each ngram size\\n        ngram_counts = dict(((n, ngram), count)\\n                            for (n, ngram), count in doc['ngram_counts'])\\n\\n        cat_to_n_to_total_score = defaultdict(lambda: defaultdict(float))\\n\\n        for (n, ngram), cat_to_score in ngrams_and_scores:\\n            tf = ngram_counts[(n, ngram)]\\n            for cat, score in cat_to_score.items():\\n                cat_to_n_to_total_score[cat][n] += score * tf\\n\\n        # average scores for each ngram size\\n        cat_to_score = {}\\n        for cat, n_to_total_score in cat_to_n_to_total_score.items():\\n            total_score_for_cat = 0\\n            for n, total_score in n_to_total_score.items():\\n                total_t = ngram_counts[(n, None)]\\n                total_score_for_cat += (\\n                    total_score /\\n                    max(total_t, self.options.short_doc_threshold or 0, 1))\\n            cat_to_score[cat] = total_score_for_cat\\n\\n        # add scores to the document, and get rid of ngram_counts\\n        doc['cat_to_score'] = cat_to_score\\n        del doc['ngram_counts']\\n\\n        yield ('doc', doc_id), doc",
                    "func_fullName": "mrjob.examples.mr_text_classifier.score_documents( self, type_and_key, types_and_values )"
                }
            ]
        },
        {
            "cluster_id": 9,
            "feature_id": 79,
            "feature_desc": "gamma=0.0100; k=1; a=0.25; combined=0.400; stability(ARI)=1.000; sep=0.201",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1939,
                    "func_name": "mapper_init",
                    "func_desc": "mapper_init",
                    "func_file": "mr_count_lines_right",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper_init(self):\\n        self.num_lines = 0",
                    "func_fullName": "mrjob.examples.mr_count_lines_right.mapper_init( self )"
                },
                {
                    "func_id": 1940,
                    "func_name": "mapper",
                    "func_desc": "mapper",
                    "func_file": "mr_count_lines_right",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper(self, _, line):\\n        self.num_lines += 1",
                    "func_fullName": "mrjob.examples.mr_count_lines_right.mapper( self, _, line )"
                },
                {
                    "func_id": 1941,
                    "func_name": "mapper_final",
                    "func_desc": "mapper_final",
                    "func_file": "mr_count_lines_right",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper_final(self):\\n        yield None, self.num_lines",
                    "func_fullName": "mrjob.examples.mr_count_lines_right.mapper_final( self )"
                },
                {
                    "func_id": 1942,
                    "func_name": "reducer",
                    "func_desc": "reducer",
                    "func_file": "mr_count_lines_right",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def reducer(self, key, values):\\n        yield key, sum(values)",
                    "func_fullName": "mrjob.examples.mr_count_lines_right.reducer( self, key, values )"
                },
                {
                    "func_id": 2004,
                    "func_name": "mapper",
                    "func_desc": "mapper",
                    "func_file": "mr_count_lines_by_file",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper(self, _, line):\\n        yield jobconf_from_env('mapreduce.map.input.file'), 1",
                    "func_fullName": "mrjob.examples.mr_count_lines_by_file.mapper( self, _, line )"
                },
                {
                    "func_id": 2005,
                    "func_name": "reducer",
                    "func_desc": "reducer",
                    "func_file": "mr_count_lines_by_file",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def reducer(self, path, ones):\\n        yield path, sum(ones)",
                    "func_fullName": "mrjob.examples.mr_count_lines_by_file.reducer( self, path, ones )"
                },
                {
                    "func_id": 2032,
                    "func_name": "mapper_init",
                    "func_desc": "mapper_init",
                    "func_file": "mr_count_lines_wrong",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper_init(self):\\n        self.num_lines = 0",
                    "func_fullName": "mrjob.examples.mr_count_lines_wrong.mapper_init( self )"
                },
                {
                    "func_id": 2033,
                    "func_name": "mapper",
                    "func_desc": "mapper",
                    "func_file": "mr_count_lines_wrong",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper(self, _, line):\\n        self.num_lines += 1",
                    "func_fullName": "mrjob.examples.mr_count_lines_wrong.mapper( self, _, line )"
                },
                {
                    "func_id": 2034,
                    "func_name": "mapper_final",
                    "func_desc": "mapper_final",
                    "func_file": "mr_count_lines_wrong",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper_final(self):\\n        yield None, self.num_lines",
                    "func_fullName": "mrjob.examples.mr_count_lines_wrong.mapper_final( self )"
                }
            ]
        },
        {
            "cluster_id": 24,
            "feature_id": 80,
            "feature_desc": "gamma=0.0100; k=1; a=0.25; combined=0.400; stability(ARI)=1.000; sep=0.154",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1947,
                    "func_name": "configure_args",
                    "func_desc": "configure_args",
                    "func_file": "mr_spark_most_used_word",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def configure_args(self):\\n        super(MRSparkMostUsedWord, self).configure_args()\\n\\n        # allow for alternate stop words file\\n        self.add_file_arg(\\n            '--stop-words-file',\\n            dest='stop_words_file',\\n            default=None,\\n            help='alternate stop words file. lowercase words, one per line',\\n        )",
                    "func_fullName": "mrjob.examples.mr_spark_most_used_word.configure_args( self )"
                },
                {
                    "func_id": 1948,
                    "func_name": "spark",
                    "func_desc": "spark",
                    "func_file": "mr_spark_most_used_word",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def spark(self, input_path, output_path):\\n        from pyspark import SparkContext\\n\\n        sc = SparkContext()\\n\\n        lines = sc.textFile(input_path)\\n\\n        # do a word frequency count\\n        words_and_ones = lines.mapPartitions(self.get_words)\\n        word_counts = words_and_ones.reduceByKey(add)\\n\\n        # pick pair with highest count (now in count, word format)\\n        max_count, word = word_counts.map(lambda w_c: (w_c[1], w_c[0])).max()\\n\\n        # output our word\\n        output = sc.parallelize([json.dumps(word)])\\n        output.saveAsTextFile(output_path)\\n\\n        sc.stop()",
                    "func_fullName": "mrjob.examples.mr_spark_most_used_word.spark( self, input_path, output_path )"
                },
                {
                    "func_id": 1949,
                    "func_name": "get_words",
                    "func_desc": "get_words",
                    "func_file": "mr_spark_most_used_word",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def get_words(self, line_iterator):\\n        # this only happens once per partition\\n        stop_words = self.load_stop_words()\\n\\n        for line in line_iterator:\\n            for word in WORD_RE.findall(line):\\n                word = word.lower()\\n                if word not in stop_words:\\n                    yield (word, 1)",
                    "func_fullName": "mrjob.examples.mr_spark_most_used_word.get_words( self, line_iterator )"
                },
                {
                    "func_id": 1950,
                    "func_name": "load_stop_words",
                    "func_desc": "load_stop_words",
                    "func_file": "mr_spark_most_used_word",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def load_stop_words(self):\\n        # this should only be called inside executors (i.e. inside functions\\n        # passed to RDDs)\\n        stop_words_path = self.options.stop_words_file or 'stop_words.txt'\\n\\n        with open(stop_words_path) as f:\\n            return set(line.strip() for line in f)",
                    "func_fullName": "mrjob.examples.mr_spark_most_used_word.load_stop_words( self )"
                },
                {
                    "func_id": 2013,
                    "func_name": "configure_args",
                    "func_desc": "configure_args",
                    "func_file": "mr_most_used_word",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def configure_args(self):\\n        super(MRMostUsedWord, self).configure_args()\\n\\n        # allow for alternate stop words file\\n        self.add_file_arg(\\n            '--stop-words-file',\\n            dest='stop_words_file',\\n            default=None,\\n            help='alternate stop words file. lowercase words, one per line',\\n        )",
                    "func_fullName": "mrjob.examples.mr_most_used_word.configure_args( self )"
                },
                {
                    "func_id": 2014,
                    "func_name": "mapper_init",
                    "func_desc": "mapper_init",
                    "func_file": "mr_most_used_word",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper_init(self):\\n        stop_words_path = self.options.stop_words_file or 'stop_words.txt'\\n\\n        with open(stop_words_path) as f:\\n            self.stop_words = set(line.strip() for line in f)",
                    "func_fullName": "mrjob.examples.mr_most_used_word.mapper_init( self )"
                },
                {
                    "func_id": 2015,
                    "func_name": "mapper_get_words",
                    "func_desc": "mapper_get_words",
                    "func_file": "mr_most_used_word",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper_get_words(self, _, line):\\n        # yield each word in the line\\n        for word in WORD_RE.findall(line):\\n            word = word.lower()\\n            if word not in self.stop_words:\\n                yield (word, 1)",
                    "func_fullName": "mrjob.examples.mr_most_used_word.mapper_get_words( self, _, line )"
                },
                {
                    "func_id": 2016,
                    "func_name": "combiner_count_words",
                    "func_desc": "combiner_count_words",
                    "func_file": "mr_most_used_word",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def combiner_count_words(self, word, counts):\\n        # sum the words we've seen so far\\n        yield (word, sum(counts))",
                    "func_fullName": "mrjob.examples.mr_most_used_word.combiner_count_words( self, word, counts )"
                },
                {
                    "func_id": 2017,
                    "func_name": "reducer_count_words",
                    "func_desc": "reducer_count_words",
                    "func_file": "mr_most_used_word",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def reducer_count_words(self, word, counts):\\n        # send all (num_occurrences, word) pairs to the same reducer.\\n        # num_occurrences is so we can easily use Python's max() function.\\n        yield None, (sum(counts), word)",
                    "func_fullName": "mrjob.examples.mr_most_used_word.reducer_count_words( self, word, counts )"
                },
                {
                    "func_id": 2018,
                    "func_name": "reducer_find_max_word",
                    "func_desc": "reducer_find_max_word",
                    "func_file": "mr_most_used_word",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def reducer_find_max_word(self, _, word_count_pairs):\\n        # each item of word_count_pairs is (count, word),\\n        # so yielding one results in key=counts, value=word\\n        try:\\n            yield max(word_count_pairs)\\n        except ValueError:\\n            pass",
                    "func_fullName": "mrjob.examples.mr_most_used_word.reducer_find_max_word( self, _, word_count_pairs )"
                },
                {
                    "func_id": 2019,
                    "func_name": "steps",
                    "func_desc": "steps",
                    "func_file": "mr_most_used_word",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def steps(self):\\n        return [\\n            MRStep(mapper_init=self.mapper_init,\\n                   mapper=self.mapper_get_words,\\n                   combiner=self.combiner_count_words,\\n                   reducer=self.reducer_count_words),\\n            MRStep(reducer=self.reducer_find_max_word)\\n        ]",
                    "func_fullName": "mrjob.examples.mr_most_used_word.steps( self )"
                }
            ]
        },
        {
            "cluster_id": 31,
            "feature_id": 81,
            "feature_desc": "gamma=0.0100; k=1; a=0.25; combined=0.200; stability(ARI)=1.000; sep=0.250",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1955,
                    "func_name": "configure_args",
                    "func_desc": "configure_args",
                    "func_file": "mr_grep",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def configure_args(self):\\n        super(MRGrepJob, self).configure_args()\\n\\n        self.add_passthru_arg(\\n            '-e', '--expression',\\n            required=True,\\n            help=('Expression to search for. Required.'))",
                    "func_fullName": "mrjob.examples.mr_grep.configure_args( self )"
                },
                {
                    "func_id": 1956,
                    "func_name": "mapper_cmd",
                    "func_desc": "mapper_cmd",
                    "func_file": "mr_grep",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper_cmd(self):\\n        # grep will return exit status 1 if no matching lines are found\\n        return cmd_line([\\n            'sh', '-c',\\n            'grep -e %s || RC=$?; if [ $RC -ne 1 ]; then (exit $RC); fi' % (\\n                cmd_line([self.options.expression]))])",
                    "func_fullName": "mrjob.examples.mr_grep.mapper_cmd( self )"
                }
            ]
        },
        {
            "cluster_id": 25,
            "feature_id": 82,
            "feature_desc": "gamma=0.0100; k=1; a=0.25; combined=0.200; stability(ARI)=1.000; sep=0.250",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1970,
                    "func_name": "spark",
                    "func_desc": "spark",
                    "func_file": "mr_sparkaboom",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def spark(self, input_path, output_path):\\n        # Spark may not be available where script is launched\\n        from pyspark import SparkContext\\n\\n        sc = SparkContext(appName='mrjob Spark wordcount script')\\n\\n        lines = sc.textFile(input_path)\\n\\n        def kaboom(line):\\n            raise Exception('KABOOM')\\n\\n        # make sure the exception happens inside Spark, not just at the\\n        # top-level client\\n\\n        # strangely, Spark 1.2 thinks this is all good. Probably not something\\n        # we can fix.\\n        kaboomed_lines = lines.flatMap(kaboom)\\n        kaboomed_lines.saveAsTextFile(output_path)\\n\\n        sc.stop()",
                    "func_fullName": "mrjob.examples.mr_sparkaboom.spark( self, input_path, output_path )"
                },
                {
                    "func_id": 1971,
                    "func_name": "kaboom",
                    "func_desc": "kaboom",
                    "func_file": "mr_sparkaboom",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "        def kaboom(line):\\n            raise Exception('KABOOM')",
                    "func_fullName": "mrjob.examples.mr_sparkaboom.kaboom( line )"
                }
            ]
        },
        {
            "cluster_id": 26,
            "feature_id": 83,
            "feature_desc": "gamma=0.0100; k=1; a=0.25; combined=0.200; stability(ARI)=1.000; sep=0.250",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1974,
                    "func_name": "mapper",
                    "func_desc": "mapper",
                    "func_file": "mr_nick_nack",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper(self, _, line):\\n        for word in WORD_RE.findall(line):\\n            yield (word.lower(), 1)",
                    "func_fullName": "mrjob.examples.mr_nick_nack.mapper( self, _, line )"
                },
                {
                    "func_id": 1975,
                    "func_name": "reducer",
                    "func_desc": "reducer",
                    "func_file": "mr_nick_nack",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def reducer(self, word, counts):\\n        total = sum(counts)\\n        yield None, '\\t'.join([word[0], json.dumps(word), json.dumps(total)])",
                    "func_fullName": "mrjob.examples.mr_nick_nack.reducer( self, word, counts )"
                }
            ]
        },
        {
            "cluster_id": 10,
            "feature_id": 84,
            "feature_desc": "gamma=0.0000; k=1; a=0.25; combined=1.000; stability(ARI)=1.000; sep=1.000",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1977,
                    "func_name": "mapper_init",
                    "func_desc": "mapper_init",
                    "func_file": "mr_boom",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper_init(self):\\n        raise Exception('BOOM')",
                    "func_fullName": "mrjob.examples.mr_boom.mapper_init( self )"
                }
            ]
        },
        {
            "cluster_id": 18,
            "feature_id": 85,
            "feature_desc": "gamma=0.0100; k=1; a=0.25; combined=0.400; stability(ARI)=1.000; sep=0.174",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1988,
                    "func_name": "configure_args",
                    "func_desc": "configure_args",
                    "func_file": "mr_log_sampler",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def configure_args(self):\\n        super(MRLogSampler, self).configure_args()\\n        self.add_passthru_arg(\\n            '--sample-size',\\n            type=int,\\n            help='Number of entries to sample.'\\n        )\\n        self.add_passthru_arg(\\n            '--expected-length',\\n            type=int,\\n            help=(\"Number of entries you expect in the log. If not specified,\"\\n                  \" we'll pass every line to the reducer.\")\\n        )",
                    "func_fullName": "mrjob.examples.mr_log_sampler.configure_args( self )"
                },
                {
                    "func_id": 1989,
                    "func_name": "load_args",
                    "func_desc": "load_args",
                    "func_file": "mr_log_sampler",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def load_args(self, args):\\n        super(MRLogSampler, self).load_args(args)\\n\\n        if self.options.sample_size is None:\\n            self.arg_parser.error('You must specify the --sample-size')\\n        else:\\n            self.sample_size = self.options.sample_size\\n\\n        # If we have an expected length, we can estimate the sampling\\n        # probability for the mapper, so that the reducer doesn't have to\\n        # process all records. Otherwise, pass everything thru to the reducer.\\n        if self.options.expected_length is None:\\n            self.sampling_probability = 1\\n        else:\\n            # We should be able to bound this probability by using the binomial\\n            # distribution, but I haven't figured it out yet. So, let's just\\n            # fudge it.\\n            self.sampling_probability = (float(self.sample_size) *\\n                                         SAMPLING_FUDGE_FACTOR /\\n                                         self.options.expected_length)",
                    "func_fullName": "mrjob.examples.mr_log_sampler.load_args( self, args )"
                },
                {
                    "func_id": 1990,
                    "func_name": "mapper",
                    "func_desc": "mapper",
                    "func_file": "mr_log_sampler",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper(self, _, line):\\n        \"\"\"\\n        For each log line, with probability self.sampling_probability,\\n        yield a None key, and (random seed, line) as the value, so that\\n        the values get sorted randomly and fed into a single reducer.\\n\\n        Args:\\n            line - raw log line\\n\\n        Yields:\\n            key - None\\n            value - (random seed, line)\\n        \"\"\"\\n        if random.random() < self.sampling_probability:\\n            seed = '%20i' % random.randint(0, sys.maxsize)\\n            yield None, (seed, line)",
                    "func_fullName": "mrjob.examples.mr_log_sampler.mapper( self, _, line )"
                },
                {
                    "func_id": 1991,
                    "func_name": "reducer",
                    "func_desc": "reducer",
                    "func_file": "mr_log_sampler",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def reducer(self, _, values):\\n        \"\"\"\\n        Now that the values have a random number attached,\\n        they'll come in in random order, so we yield the\\n        first n lines, and return early.\\n\\n        Args:\\n            values - generator of (random_seed, line) pairs\\n\\n        Yields:\\n            key - None\\n            value - random sample of log lines\\n        \"\"\"\\n        for line_num, (seed, line) in enumerate(values):\\n            yield None, line\\n\\n            # enumerate() is 0-indexed, so add 1\\n            if line_num + 1 >= self.sample_size:\\n                break",
                    "func_fullName": "mrjob.examples.mr_log_sampler.reducer( self, _, values )"
                }
            ]
        },
        {
            "cluster_id": 36,
            "feature_id": 86,
            "feature_desc": "gamma=0.0100; k=1; a=0.25; combined=0.400; stability(ARI)=1.000; sep=0.203",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 1997,
                    "func_name": "configure_args",
                    "func_desc": "configure_args",
                    "func_file": "mr_jar_step_example",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def configure_args(self):\\n        super(MRJarStepExample, self).configure_args()\\n\\n        self.add_passthru_arg(\\n            '--use-main-class', dest='use_main_class',\\n            default=False, action='store_true')\\n\\n        self.add_passthru_arg(\\n            '--examples-jar', dest='examples_jar',\\n            default=_EXAMPLES_JAR_URI,\\n            help=('URI of Hadoop example jar. Usually the default'\\n                  ' is fine, but on EMR, you should use '\\n                  ' file:///home/hadoop/hadoop-examples.jar'\\n                  ' with 3.x AMIs and earlier')\\n        )",
                    "func_fullName": "mrjob.examples.mr_jar_step_example.configure_args( self )"
                },
                {
                    "func_id": 1998,
                    "func_name": "steps",
                    "func_desc": "steps",
                    "func_file": "mr_jar_step_example",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def steps(self):\\n        jar = self.options.examples_jar\\n\\n        if self.options.use_main_class:\\n            jar_step = JarStep(\\n                jar=jar,\\n                args=[GENERIC_ARGS, INPUT, OUTPUT],\\n                main_class=_WORDCOUNT_MAIN_CLASS,\\n            )\\n        else:\\n            jar_step = JarStep(\\n                jar=jar,\\n                args=['wordcount', GENERIC_ARGS, INPUT, OUTPUT],\\n            )\\n\\n        return [\\n            jar_step,\\n            MRStep(\\n                mapper=self.mapper,\\n                combiner=self.reducer,\\n                reducer=self.reducer,\\n            )\\n        ]",
                    "func_fullName": "mrjob.examples.mr_jar_step_example.steps( self )"
                },
                {
                    "func_id": 1999,
                    "func_name": "mapper",
                    "func_desc": "mapper",
                    "func_file": "mr_jar_step_example",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def mapper(self, key, freq):\\n        yield int(freq), 1",
                    "func_fullName": "mrjob.examples.mr_jar_step_example.mapper( self, key, freq )"
                },
                {
                    "func_id": 2000,
                    "func_name": "reducer",
                    "func_desc": "reducer",
                    "func_file": "mr_jar_step_example",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def reducer(self, freq, counts):\\n        yield freq, sum(counts)",
                    "func_fullName": "mrjob.examples.mr_jar_step_example.reducer( self, freq, counts )"
                },
                {
                    "func_id": 2001,
                    "func_name": "pick_protocols",
                    "func_desc": "pick_protocols",
                    "func_file": "mr_jar_step_example",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def pick_protocols(self, step_num, step_type):\\n        \"\"\"Use RawProtocol to read output from the jar.\"\"\"\\n        read, write = super(MRJarStepExample, self).pick_protocols(\\n            step_num, step_type)\\n        if (step_num, step_type) == (1, 'mapper'):\\n            read = RawProtocol().read\\n\\n        return read, write",
                    "func_fullName": "mrjob.examples.mr_jar_step_example.pick_protocols( self, step_num, step_type )"
                }
            ]
        },
        {
            "cluster_id": 30,
            "feature_id": 87,
            "feature_desc": "gamma=0.0100; k=1; a=0.25; combined=0.400; stability(ARI)=1.000; sep=0.201",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 2020,
                    "func_name": "standardize_phone_number",
                    "func_desc": "standardize_phone_number",
                    "func_file": "mr_phone_to_url",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def standardize_phone_number(number):\\n    \"\"\"put *number* in a standard format, and convert it to a :py:class:`str`.\\n    \"\"\"\\n    number_sep = PHONE_SEP_RE.split(number)\\n    number = b''.join(number_sep).decode('ascii')\\n    if len(number) > 7:\\n        if number[-1] not in '0123456789':\\n            number = number[:-1]\\n        if number[0] not in '0123456789':\\n            number = number[1:]\\n    if len(number) <= 10:\\n        return \"+1\" + number\\n    else:\\n        return \"+\" + number",
                    "func_fullName": "mrjob.examples.mr_phone_to_url.standardize_phone_number( number )"
                },
                {
                    "func_id": 2025,
                    "func_name": "steps",
                    "func_desc": "steps",
                    "func_file": "mr_phone_to_url",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def steps(self):\\n        return [\\n            MRStep(mapper_raw=self.extract_phone_and_url_mapper,\\n                   reducer=self.count_by_host_reducer),\\n            MRStep(reducer=self.pick_best_url_reducer),\\n        ]",
                    "func_fullName": "mrjob.examples.mr_phone_to_url.steps( self )"
                },
                {
                    "func_id": 2026,
                    "func_name": "extract_phone_and_url_mapper",
                    "func_desc": "extract_phone_and_url_mapper",
                    "func_file": "mr_phone_to_url",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def extract_phone_and_url_mapper(self, wet_path, wet_uri):\\n        \"\"\"Read in .wet file, and extract phone ant URL\\n        \"\"\"\\n        from warcio.archiveiterator import ArchiveIterator\\n\\n        with open(wet_path, 'rb') as f:\\n            for record in ArchiveIterator(f):\\n                if record.rec_type != 'conversion':\\n                    continue\\n\\n                headers = record.rec_headers\\n                if headers.get_header('Content-Type') != 'text/plain':\\n                    continue\\n\\n                url = headers.get_header('WARC-Target-URI')\\n                if not url:\\n                    continue\\n\\n                host = urlparse(url).netloc\\n\\n                payload = record.content_stream().read()\\n                for phone in PHONE_RE.findall(payload):\\n                    phone = standardize_phone_number(phone)\\n                    yield host, (phone, url)",
                    "func_fullName": "mrjob.examples.mr_phone_to_url.extract_phone_and_url_mapper( self, wet_path, wet_uri )"
                },
                {
                    "func_id": 2027,
                    "func_name": "count_by_host_reducer",
                    "func_desc": "count_by_host_reducer",
                    "func_file": "mr_phone_to_url",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def count_by_host_reducer(self, host, phone_urls):\\n        phone_urls = list(islice(phone_urls, MAX_PHONES_PER_HOST + 1))\\n\\n        # don't bother with directories, etc.\\n        host_phone_count = len(phone_urls)\\n        if host_phone_count > MAX_PHONES_PER_HOST:\\n            return\\n\\n        for phone, url in phone_urls:\\n            yield phone, (url, host_phone_count)",
                    "func_fullName": "mrjob.examples.mr_phone_to_url.count_by_host_reducer( self, host, phone_urls )"
                },
                {
                    "func_id": 2028,
                    "func_name": "pick_best_url_reducer",
                    "func_desc": "pick_best_url_reducer",
                    "func_file": "mr_phone_to_url",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def pick_best_url_reducer(self, phone, urls_with_count):\\n        # pick the url that appears on a host with the least number of\\n        # phone numbers, breaking ties by choosing the shortest URL\\n        # and the one that comes first alphabetically\\n        urls_with_count = sorted(\\n            urls_with_count, key=lambda uc: (uc[1], -len(uc[0]), uc[0]))\\n\\n        yield phone, urls_with_count[0][0]",
                    "func_fullName": "mrjob.examples.mr_phone_to_url.pick_best_url_reducer( self, phone, urls_with_count )"
                }
            ]
        },
        {
            "cluster_id": 42,
            "feature_id": 88,
            "feature_desc": "gamma=0.0100; k=1; a=0.25; combined=0.496; stability(ARI)=1.000; sep=0.190",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 2152,
                    "func_name": "main",
                    "func_desc": "main",
                    "func_file": "cli",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def main(args=None):\\n    parser = ArgumentParser(description='warcio utils',\\n                            formatter_class=RawTextHelpFormatter)\\n\\n    parser.add_argument('-V', '--version', action='version', version=get_version())\\n\\n    subparsers = parser.add_subparsers(dest='cmd')\\n    subparsers.required = True\\n\\n    index = subparsers.add_parser('index', help='WARC/ARC Indexer')\\n    index.add_argument('inputs', nargs='*', help='input file(s); default is stdin')\\n    index.add_argument('-f', '--fields', default='offset,warc-type,warc-target-uri',\\n            help='fields to include in json output; supported values are \"offset\", '\\n                 '\"length\", \"filename\", \"http:status\", \"http:{http-header}\" '\\n                 '(arbitrary http header), and \"{warc-header}\" (arbitrary warc '\\n                 'record header)')\\n    index.add_argument('-o', '--output', help='output file; default is stdout')\\n    index.set_defaults(func=indexer)\\n\\n    recompress = subparsers.add_parser('recompress', help='Recompress an existing WARC or ARC',\\n                                       description='Read an existing, possibly broken WARC ' +\\n                                                   'and correctly recompress it to fix any compression errors\\n' +\\n                                                   'Also convert any ARC file to a standard compressed WARC file')\\n    recompress.add_argument('filename')\\n    recompress.add_argument('output')\\n    recompress.add_argument('-v', '--verbose', action='store_true')\\n    recompress.set_defaults(func=recompressor)\\n\\n    extract = subparsers.add_parser('extract', help='Extract WARC/ARC Record')\\n    extract.add_argument('filename')\\n    extract.add_argument('offset')\\n    group = extract.add_mutually_exclusive_group()\\n    group.add_argument('--payload', action='store_true', help='output only record payload (after content and transfer decoding, if applicable)')\\n    group.add_argument('--headers', action='store_true', help='output only record headers (and http headers, if applicable)')\\n\\n    extract.set_defaults(func=extractor)\\n\\n    check = subparsers.add_parser('check', help='WARC digest checker')\\n    check.add_argument('inputs', nargs='+')\\n    check.add_argument('-v', '--verbose', action='store_true')\\n    check.set_defaults(func=checker)\\n\\n    cmd = parser.parse_args(args=args)\\n    cmd.func(cmd)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.cli.main( args )"
                },
                {
                    "func_id": 2153,
                    "func_name": "get_version",
                    "func_desc": "get_version",
                    "func_file": "cli",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def get_version():\\n    return '%(prog)s ' + version('warcio')",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.cli.get_version(  )"
                },
                {
                    "func_id": 2154,
                    "func_name": "indexer",
                    "func_desc": "indexer",
                    "func_file": "cli",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def indexer(cmd):\\n    inputs = cmd.inputs or ('-',)  # default to stdin\\n    _indexer = Indexer(cmd.fields, inputs, cmd.output)\\n    _indexer.process_all()",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.cli.indexer( cmd )"
                },
                {
                    "func_id": 2155,
                    "func_name": "checker",
                    "func_desc": "checker",
                    "func_file": "cli",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def checker(cmd):\\n    _checker = Checker(cmd)\\n    sys.exit(_checker.process_all())",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.cli.checker( cmd )"
                },
                {
                    "func_id": 2156,
                    "func_name": "extractor",
                    "func_desc": "extractor",
                    "func_file": "cli",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def extractor(cmd):\\n    _extractor = Extractor(cmd.filename, cmd.offset)\\n    _extractor.extract(cmd.payload, cmd.headers)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.cli.extractor( cmd )"
                },
                {
                    "func_id": 2157,
                    "func_name": "recompressor",
                    "func_desc": "recompressor",
                    "func_file": "cli",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def recompressor(cmd):\\n    _recompressor = Recompressor(cmd.filename, cmd.output, cmd.verbose)\\n    _recompressor.recompress()",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.cli.recompressor( cmd )"
                },
                {
                    "func_id": 2158,
                    "func_name": "version",
                    "func_desc": "version",
                    "func_file": "cli",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def version(package):\\n        return pkg_resources.get_distribution(package).version",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.cli.version( package )"
                },
                {
                    "func_id": 2228,
                    "func_name": "iso_date_to_datetime",
                    "func_desc": "iso_date_to_datetime",
                    "func_file": "timeutils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def iso_date_to_datetime(string, tz_aware=False):\\n    \"\"\"\\n    >>> iso_date_to_datetime('2013-12-26T10:11:12Z')\\n    datetime.datetime(2013, 12, 26, 10, 11, 12)\\n\\n    >>> iso_date_to_datetime('2013-12-26T10:11:12.456789Z')\\n    datetime.datetime(2013, 12, 26, 10, 11, 12, 456789)\\n\\n    >>> iso_date_to_datetime('2013-12-26T10:11:12.30Z')\\n    datetime.datetime(2013, 12, 26, 10, 11, 12, 300000)\\n\\n    >>> iso_date_to_datetime('2013-12-26T10:11:12.00001Z')\\n    datetime.datetime(2013, 12, 26, 10, 11, 12, 10)\\n\\n    >>> iso_date_to_datetime('2013-12-26T10:11:12.000001Z')\\n    datetime.datetime(2013, 12, 26, 10, 11, 12, 1)\\n\\n    >>> iso_date_to_datetime('2013-12-26T10:11:12.0000001Z')\\n    datetime.datetime(2013, 12, 26, 10, 11, 12)\\n\\n    >>> iso_date_to_datetime('2013-12-26T10:11:12.000000Z')\\n    datetime.datetime(2013, 12, 26, 10, 11, 12)\\n\\n    >>> iso_date_to_datetime('2013-12-26T10:11:12Z', tz_aware=True)\\n    datetime.datetime(2013, 12, 26, 10, 11, 12, tzinfo=datetime.timezone.utc)\\n\\n    >>> iso_date_to_datetime('2013-12-26T10:11:12.000000Z', tz_aware=True)\\n    datetime.datetime(2013, 12, 26, 10, 11, 12, tzinfo=datetime.timezone.utc)\\n    \"\"\"\\n\\n    nums = DATE_TIMESPLIT.split(string)\\n    if nums[-1] == '':\\n        nums = nums[:-1]\\n\\n    if len(nums) == 7:\\n        nums[6] = nums[6][:6]\\n        nums[6] += PAD_MICRO[len(nums[6]):]\\n\\n    tzinfo = None\\n    if tz_aware:\\n        tzinfo = timezone.utc\\n\\n    the_datetime = datetime(*(int(num) for num in nums), tzinfo=tzinfo)\\n    return the_datetime",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.timeutils.iso_date_to_datetime( string, tz_aware )"
                },
                {
                    "func_id": 2229,
                    "func_name": "http_date_to_datetime",
                    "func_desc": "http_date_to_datetime",
                    "func_file": "timeutils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def http_date_to_datetime(string, tz_aware=False):\\n    \"\"\"\\n    >>> http_date_to_datetime('Thu, 26 Dec 2013 09:50:10 GMT')\\n    datetime.datetime(2013, 12, 26, 9, 50, 10)\\n\\n    >>> http_date_to_datetime('Thu, 26 Dec 2013 09:50:10 GMT', tz_aware=True)\\n    datetime.datetime(2013, 12, 26, 9, 50, 10, tzinfo=datetime.timezone.utc)\\n    \"\"\"\\n    tzinfo = None\\n    if tz_aware:\\n        tzinfo = timezone.utc\\n\\n    return datetime(*parsedate(string)[:6], tzinfo=tzinfo)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.timeutils.http_date_to_datetime( string, tz_aware )"
                },
                {
                    "func_id": 2230,
                    "func_name": "datetime_to_http_date",
                    "func_desc": "datetime_to_http_date",
                    "func_file": "timeutils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def datetime_to_http_date(the_datetime):\\n    \"\"\"\\n    >>> datetime_to_http_date(datetime(2013, 12, 26, 9, 50, 10))\\n    'Thu, 26 Dec 2013 09:50:10 GMT'\\n\\n    # Verify inverses\\n    >>> x = 'Thu, 26 Dec 2013 09:50:10 GMT'\\n    >>> datetime_to_http_date(http_date_to_datetime(x)) == x\\n    True\\n    \"\"\"\\n    timeval = calendar.timegm(the_datetime.utctimetuple())\\n    return formatdate(timeval=timeval,\\n                      localtime=False,\\n                      usegmt=True)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.timeutils.datetime_to_http_date( the_datetime )"
                },
                {
                    "func_id": 2231,
                    "func_name": "datetime_to_iso_date",
                    "func_desc": "datetime_to_iso_date",
                    "func_file": "timeutils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def datetime_to_iso_date(the_datetime, use_micros=False):\\n    \"\"\"\\n    >>> datetime_to_iso_date(datetime(2013, 12, 26, 10, 11, 12))\\n    '2013-12-26T10:11:12Z'\\n\\n    >>> datetime_to_iso_date(datetime(2013, 12, 26, 10, 11, 12, 456789))\\n    '2013-12-26T10:11:12Z'\\n\\n    >>> datetime_to_iso_date(datetime(2013, 12, 26, 10, 11, 12), use_micros=True)\\n    '2013-12-26T10:11:12Z'\\n\\n    >>> datetime_to_iso_date(datetime(2013, 12, 26, 10, 11, 12, 456789), use_micros=True)\\n    '2013-12-26T10:11:12.456789Z'\\n\\n    >>> datetime_to_iso_date(datetime(2013, 12, 26, 10, 11, 12, 1), use_micros=True)\\n    '2013-12-26T10:11:12.000001Z'\\n\\n    \"\"\"\\n\\n    if not use_micros:\\n        return the_datetime.strftime(ISO_DT)\\n    else:\\n        return the_datetime.isoformat() + 'Z'",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.timeutils.datetime_to_iso_date( the_datetime, use_micros )"
                },
                {
                    "func_id": 2232,
                    "func_name": "datetime_to_timestamp",
                    "func_desc": "datetime_to_timestamp",
                    "func_file": "timeutils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def datetime_to_timestamp(the_datetime):\\n    \"\"\"\\n    >>> datetime_to_timestamp(datetime(2013, 12, 26, 10, 11, 12))\\n    '20131226101112'\\n    \"\"\"\\n\\n    return the_datetime.strftime(TIMESTAMP_14)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.timeutils.datetime_to_timestamp( the_datetime )"
                },
                {
                    "func_id": 2233,
                    "func_name": "timestamp_now",
                    "func_desc": "timestamp_now",
                    "func_file": "timeutils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def timestamp_now():\\n    \"\"\"\\n    >>> len(timestamp_now())\\n    14\\n    \"\"\"\\n    return datetime_to_timestamp(datetime.now(timezone.utc))",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.timeutils.timestamp_now(  )"
                },
                {
                    "func_id": 2234,
                    "func_name": "timestamp20_now",
                    "func_desc": "timestamp20_now",
                    "func_file": "timeutils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def timestamp20_now():\\n    \"\"\"\\n    Create 20-digit timestamp, useful to timestamping temp files\\n\\n    >>> n = timestamp20_now()\\n    >>> timestamp20_now() >= n\\n    True\\n\\n    >>> len(n)\\n    20\\n\\n    \"\"\"\\n    now = datetime.now(timezone.utc)\\n    return now.strftime('%Y%m%d%H%M%S%f')",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.timeutils.timestamp20_now(  )"
                },
                {
                    "func_id": 2235,
                    "func_name": "iso_date_to_timestamp",
                    "func_desc": "iso_date_to_timestamp",
                    "func_file": "timeutils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def iso_date_to_timestamp(string):\\n    \"\"\"\\n    >>> iso_date_to_timestamp('2013-12-26T10:11:12Z')\\n    '20131226101112'\\n\\n    >>> iso_date_to_timestamp('2013-12-26T10:11:12')\\n    '20131226101112'\\n     \"\"\"\\n\\n    return datetime_to_timestamp(iso_date_to_datetime(string))",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.timeutils.iso_date_to_timestamp( string )"
                },
                {
                    "func_id": 2236,
                    "func_name": "timestamp_to_iso_date",
                    "func_desc": "timestamp_to_iso_date",
                    "func_file": "timeutils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def timestamp_to_iso_date(string):\\n    \"\"\"\\n    >>> timestamp_to_iso_date('20131226101112')\\n    '2013-12-26T10:11:12Z'\\n\\n    >>> timestamp_to_iso_date('20131226101112')\\n    '2013-12-26T10:11:12Z'\\n    \"\"\"\\n\\n\\n    return datetime_to_iso_date(timestamp_to_datetime(string))",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.timeutils.timestamp_to_iso_date( string )"
                },
                {
                    "func_id": 2237,
                    "func_name": "http_date_to_timestamp",
                    "func_desc": "http_date_to_timestamp",
                    "func_file": "timeutils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def http_date_to_timestamp(string):\\n    \"\"\"\\n    >>> http_date_to_timestamp('Thu, 26 Dec 2013 09:50:00 GMT')\\n    '20131226095000'\\n\\n    >>> http_date_to_timestamp('Sun, 26 Jan 2014 20:08:04 GMT')\\n    '20140126200804'\\n    \"\"\"\\n    return datetime_to_timestamp(http_date_to_datetime(string))",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.timeutils.http_date_to_timestamp( string )"
                },
                {
                    "func_id": 2238,
                    "func_name": "pad_timestamp",
                    "func_desc": "pad_timestamp",
                    "func_file": "timeutils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def pad_timestamp(string, pad_str=PAD_6_UP):\\n    \"\"\"\\n    >>> pad_timestamp('20')\\n    '209912'\\n\\n    >>> pad_timestamp('2014')\\n    '201412'\\n\\n    >>> pad_timestamp('20141011')\\n    '20141011'\\n\\n    >>> pad_timestamp('201410110010')\\n    '201410110010'\\n     \"\"\"\\n\\n    str_len = len(string)\\n    pad_len = len(pad_str)\\n\\n    if str_len < pad_len:\\n        string = string + pad_str[str_len:]\\n\\n    return string",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.timeutils.pad_timestamp( string, pad_str )"
                },
                {
                    "func_id": 2239,
                    "func_name": "timestamp_to_datetime",
                    "func_desc": "timestamp_to_datetime",
                    "func_file": "timeutils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def timestamp_to_datetime(string, tz_aware=False):\\n    \"\"\"\\n    # >14-digit -- rest ignored\\n    >>> timestamp_to_datetime('2014122609501011')\\n    datetime.datetime(2014, 12, 26, 9, 50, 10)\\n\\n    # 14-digit\\n    >>> timestamp_to_datetime('20141226095010')\\n    datetime.datetime(2014, 12, 26, 9, 50, 10)\\n\\n    # 13-digit padding\\n    >>> timestamp_to_datetime('2014122609501')\\n    datetime.datetime(2014, 12, 26, 9, 50, 59)\\n\\n    # 12-digit padding\\n    >>> timestamp_to_datetime('201412260950')\\n    datetime.datetime(2014, 12, 26, 9, 50, 59)\\n\\n    # 11-digit padding\\n    >>> timestamp_to_datetime('20141226095')\\n    datetime.datetime(2014, 12, 26, 9, 59, 59)\\n\\n    # 10-digit padding\\n    >>> timestamp_to_datetime('2014122609')\\n    datetime.datetime(2014, 12, 26, 9, 59, 59)\\n\\n    # 9-digit padding\\n    >>> timestamp_to_datetime('201412260')\\n    datetime.datetime(2014, 12, 26, 23, 59, 59)\\n\\n    # 8-digit padding\\n    >>> timestamp_to_datetime('20141226')\\n    datetime.datetime(2014, 12, 26, 23, 59, 59)\\n\\n    # 7-digit padding\\n    >>> timestamp_to_datetime('2014122')\\n    datetime.datetime(2014, 12, 31, 23, 59, 59)\\n\\n    # 6-digit padding\\n    >>> timestamp_to_datetime('201410')\\n    datetime.datetime(2014, 10, 31, 23, 59, 59)\\n\\n    # 5-digit padding\\n    >>> timestamp_to_datetime('20141')\\n    datetime.datetime(2014, 12, 31, 23, 59, 59)\\n\\n    # 4-digit padding\\n    >>> timestamp_to_datetime('2014')\\n    datetime.datetime(2014, 12, 31, 23, 59, 59)\\n\\n    # 3-digit padding\\n    >>> timestamp_to_datetime('201')\\n    datetime.datetime(2019, 12, 31, 23, 59, 59)\\n\\n    # 2-digit padding\\n    >>> timestamp_to_datetime('20')\\n    datetime.datetime(2099, 12, 31, 23, 59, 59)\\n\\n    # 1-digit padding\\n    >>> timestamp_to_datetime('2')\\n    datetime.datetime(2999, 12, 31, 23, 59, 59)\\n\\n    # 1-digit out-of-range padding\\n    >>> timestamp_to_datetime('3')\\n    datetime.datetime(2999, 12, 31, 23, 59, 59)\\n\\n    # 0-digit padding\\n    >>> timestamp_to_datetime('')\\n    datetime.datetime(2999, 12, 31, 23, 59, 59)\\n\\n    # bad month\\n    >>> timestamp_to_datetime('20131709005601')\\n    datetime.datetime(2013, 12, 9, 0, 56, 1)\\n\\n    # all out of range except minutes\\n    >>> timestamp_to_datetime('40001965252477')\\n    datetime.datetime(2999, 12, 31, 23, 24, 59)\\n\\n    # not a number!\\n    >>> timestamp_to_datetime('2010abc')\\n    datetime.datetime(2010, 12, 31, 23, 59, 59)\\n\\n    # 14-digit with tzinfo\\n    >>> timestamp_to_datetime('20141226095010', tz_aware=True)\\n    datetime.datetime(2014, 12, 26, 9, 50, 10, tzinfo=datetime.timezone.utc)\\n\\n    # 6-digit padding with tzinfo\\n    >>> timestamp_to_datetime('201410', tz_aware=True)\\n    datetime.datetime(2014, 10, 31, 23, 59, 59, tzinfo=datetime.timezone.utc)\\n\\n    # not a number! with tzinfo\\n    >>> timestamp_to_datetime('2010abc', tz_aware=True)\\n    datetime.datetime(2010, 12, 31, 23, 59, 59, tzinfo=datetime.timezone.utc)\\n\\n    \"\"\"\\n\\n    # pad to 6 digits\\n    string = pad_timestamp(string, PAD_6_UP)\\n\\n    def clamp(val, min_, max_):\\n        try:\\n            val = int(val)\\n            val = max(min_, min(val, max_))\\n            return val\\n        except:\\n            return max_\\n\\n    def extract(string, start, end, min_, max_):\\n        if len(string) >= end:\\n            return clamp(string[start:end], min_, max_)\\n        else:\\n            return max_\\n\\n    # now parse, clamp to boundary\\n    year = extract(string, 0, 4, 1900, 2999)\\n    month = extract(string, 4, 6, 1, 12)\\n    day = extract(string, 6, 8, 1, calendar.monthrange(year, month)[1])\\n    hour = extract(string, 8, 10, 0, 23)\\n    minute = extract(string, 10, 12, 0, 59)\\n    second = extract(string, 12, 14, 0, 59)\\n\\n    tzinfo = None\\n    if tz_aware:\\n        tzinfo = timezone.utc\\n\\n    return datetime(year=year,\\n                             month=month,\\n                             day=day,\\n                             hour=hour,\\n                             minute=minute,\\n                             second=second,\\n                             tzinfo=tzinfo)",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.timeutils.timestamp_to_datetime( string, tz_aware )"
                },
                {
                    "func_id": 2240,
                    "func_name": "timestamp_to_sec",
                    "func_desc": "timestamp_to_sec",
                    "func_file": "timeutils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def timestamp_to_sec(string):\\n    \"\"\"\\n    >>> timestamp_to_sec('20131226095010')\\n    1388051410\\n\\n    # rounds to end of 2014\\n    >>> timestamp_to_sec('2014')\\n    1420070399\\n    \"\"\"\\n\\n    dt = timestamp_to_datetime(string, tz_aware=True)\\n    return calendar.timegm(dt.utctimetuple())",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.timeutils.timestamp_to_sec( string )"
                },
                {
                    "func_id": 2241,
                    "func_name": "sec_to_timestamp",
                    "func_desc": "sec_to_timestamp",
                    "func_file": "timeutils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def sec_to_timestamp(secs):\\n    \"\"\"\\n    >>> sec_to_timestamp(1388051410)\\n    '20131226095010'\\n\\n    >>> sec_to_timestamp(1420070399)\\n    '20141231235959'\\n    \"\"\"\\n\\n    return datetime_to_timestamp(datetime.fromtimestamp(secs, timezone.utc))",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.timeutils.sec_to_timestamp( secs )"
                },
                {
                    "func_id": 2242,
                    "func_name": "timestamp_to_http_date",
                    "func_desc": "timestamp_to_http_date",
                    "func_file": "timeutils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def timestamp_to_http_date(string):\\n    \"\"\"\\n    >>> timestamp_to_http_date('20131226095000')\\n    'Thu, 26 Dec 2013 09:50:00 GMT'\\n\\n    >>> timestamp_to_http_date('20140126200804')\\n    'Sun, 26 Jan 2014 20:08:04 GMT'\\n    \"\"\"\\n    return datetime_to_http_date(timestamp_to_datetime(string))",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.timeutils.timestamp_to_http_date( string )"
                },
                {
                    "func_id": 2243,
                    "func_name": "clamp",
                    "func_desc": "clamp",
                    "func_file": "timeutils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def clamp(val, min_, max_):\\n        try:\\n            val = int(val)\\n            val = max(min_, min(val, max_))\\n            return val\\n        except:\\n            return max_",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.timeutils.clamp( val, min_, max_ )"
                },
                {
                    "func_id": 2244,
                    "func_name": "extract",
                    "func_desc": "extract",
                    "func_file": "timeutils",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def extract(string, start, end, min_, max_):\\n        if len(string) >= end:\\n            return clamp(string[start:end], min_, max_)\\n        else:\\n            return max_",
                    "func_fullName": "eggs.warcio-1.7.5-py3.9.egg.warcio.timeutils.extract( string, start, end, min_, max_ )"
                }
            ]
        },
        {
            "cluster_id": 43,
            "feature_id": 89,
            "feature_desc": "gamma=0.0000; k=1; a=0.25; combined=1.000; stability(ARI)=1.000; sep=1.000",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 2465,
                    "func_name": "main",
                    "func_desc": "main",
                    "func_file": "tool",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def main():\\n    if len(sys.argv) == 1:\\n        infile = sys.stdin\\n        outfile = sys.stdout\\n    elif len(sys.argv) == 2:\\n        infile = open(sys.argv[1], 'r')\\n        outfile = sys.stdout\\n    elif len(sys.argv) == 3:\\n        infile = open(sys.argv[1], 'r')\\n        outfile = open(sys.argv[2], 'w')\\n    else:\\n        raise SystemExit(sys.argv[0] + \" [infile [outfile]]\")\\n    with infile:\\n        try:\\n            obj = json.load(infile,\\n                            object_pairs_hook=json.OrderedDict,\\n                            use_decimal=True)\\n        except ValueError:\\n            raise SystemExit(sys.exc_info()[1])\\n    with outfile:\\n        json.dump(obj, outfile, sort_keys=True, indent='    ', use_decimal=True)\\n        outfile.write('\\n')",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.tool.main(  )"
                }
            ]
        },
        {
            "cluster_id": 45,
            "feature_id": 90,
            "feature_desc": "gamma=0.2241; k=2; a=0.25; combined=0.017; stability(ARI)=1.000; sep=0.724",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 2496,
                    "func_name": "_import_c_make_scanner",
                    "func_desc": "_import_c_make_scanner",
                    "func_file": "scanner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def _import_c_make_scanner():\\n    try:\\n        from ._speedups import make_scanner\\n        return make_scanner\\n    except ImportError:\\n        return None",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.scanner._import_c_make_scanner(  )"
                },
                {
                    "func_id": 2499,
                    "func_name": "scan_once",
                    "func_desc": "scan_once",
                    "func_file": "scanner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def scan_once(string, idx):\\n        if idx < 0:\\n            # Ensure the same behavior as the C speedup, otherwise\\n            # this would work for *some* negative string indices due\\n            # to the behavior of __getitem__ for strings. #98\\n            raise JSONDecodeError('Expecting value', string, idx)\\n        try:\\n            return _scan_once(string, idx)\\n        finally:\\n            memo.clear()",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.scanner.scan_once( string, idx )"
                }
            ]
        },
        {
            "cluster_id": 45,
            "feature_id": 91,
            "feature_desc": "gamma=0.2241; k=2; a=0.25; combined=0.017; stability(ARI)=1.000; sep=0.724",
            "feature_flow": "",
            "feature_notf": "",
            "feature_func_list": [
                {
                    "func_id": 2497,
                    "func_name": "py_make_scanner",
                    "func_desc": "py_make_scanner",
                    "func_file": "scanner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "def py_make_scanner(context):\\n    parse_object = context.parse_object\\n    parse_array = context.parse_array\\n    parse_string = context.parse_string\\n    match_number = NUMBER_RE.match\\n    encoding = context.encoding\\n    strict = context.strict\\n    parse_float = context.parse_float\\n    parse_int = context.parse_int\\n    parse_constant = context.parse_constant\\n    object_hook = context.object_hook\\n    object_pairs_hook = context.object_pairs_hook\\n    memo = context.memo\\n\\n    def _scan_once(string, idx):\\n        errmsg = 'Expecting value'\\n        try:\\n            nextchar = string[idx]\\n        except IndexError:\\n            raise JSONDecodeError(errmsg, string, idx)\\n\\n        if nextchar == '\"':\\n            return parse_string(string, idx + 1, encoding, strict)\\n        elif nextchar == '{':\\n            return parse_object((string, idx + 1), encoding, strict,\\n                _scan_once, object_hook, object_pairs_hook, memo)\\n        elif nextchar == '[':\\n            return parse_array((string, idx + 1), _scan_once)\\n        elif nextchar == 'n' and string[idx:idx + 4] == 'null':\\n            return None, idx + 4\\n        elif nextchar == 't' and string[idx:idx + 4] == 'true':\\n            return True, idx + 4\\n        elif nextchar == 'f' and string[idx:idx + 5] == 'false':\\n            return False, idx + 5\\n\\n        m = match_number(string, idx)\\n        if m is not None:\\n            integer, frac, exp = m.groups()\\n            if frac or exp:\\n                res = parse_float(integer + (frac or '') + (exp or ''))\\n            else:\\n                res = parse_int(integer)\\n            return res, m.end()\\n        elif parse_constant and nextchar == 'N' and string[idx:idx + 3] == 'NaN':\\n            return parse_constant('NaN'), idx + 3\\n        elif parse_constant and nextchar == 'I' and string[idx:idx + 8] == 'Infinity':\\n            return parse_constant('Infinity'), idx + 8\\n        elif parse_constant and nextchar == '-' and string[idx:idx + 9] == '-Infinity':\\n            return parse_constant('-Infinity'), idx + 9\\n        else:\\n            raise JSONDecodeError(errmsg, string, idx)\\n\\n    def scan_once(string, idx):\\n        if idx < 0:\\n            # Ensure the same behavior as the C speedup, otherwise\\n            # this would work for *some* negative string indices due\\n            # to the behavior of __getitem__ for strings. #98\\n            raise JSONDecodeError('Expecting value', string, idx)\\n        try:\\n            return _scan_once(string, idx)\\n        finally:\\n            memo.clear()\\n\\n    return scan_once",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.scanner.py_make_scanner( context )"
                },
                {
                    "func_id": 2498,
                    "func_name": "_scan_once",
                    "func_desc": "_scan_once",
                    "func_file": "scanner",
                    "func_flow": "",
                    "func_notf": "",
                    "func_code": "    def _scan_once(string, idx):\\n        errmsg = 'Expecting value'\\n        try:\\n            nextchar = string[idx]\\n        except IndexError:\\n            raise JSONDecodeError(errmsg, string, idx)\\n\\n        if nextchar == '\"':\\n            return parse_string(string, idx + 1, encoding, strict)\\n        elif nextchar == '{':\\n            return parse_object((string, idx + 1), encoding, strict,\\n                _scan_once, object_hook, object_pairs_hook, memo)\\n        elif nextchar == '[':\\n            return parse_array((string, idx + 1), _scan_once)\\n        elif nextchar == 'n' and string[idx:idx + 4] == 'null':\\n            return None, idx + 4\\n        elif nextchar == 't' and string[idx:idx + 4] == 'true':\\n            return True, idx + 4\\n        elif nextchar == 'f' and string[idx:idx + 5] == 'false':\\n            return False, idx + 5\\n\\n        m = match_number(string, idx)\\n        if m is not None:\\n            integer, frac, exp = m.groups()\\n            if frac or exp:\\n                res = parse_float(integer + (frac or '') + (exp or ''))\\n            else:\\n                res = parse_int(integer)\\n            return res, m.end()\\n        elif parse_constant and nextchar == 'N' and string[idx:idx + 3] == 'NaN':\\n            return parse_constant('NaN'), idx + 3\\n        elif parse_constant and nextchar == 'I' and string[idx:idx + 8] == 'Infinity':\\n            return parse_constant('Infinity'), idx + 8\\n        elif parse_constant and nextchar == '-' and string[idx:idx + 9] == '-Infinity':\\n            return parse_constant('-Infinity'), idx + 9\\n        else:\\n            raise JSONDecodeError(errmsg, string, idx)",
                    "func_fullName": "eggs.simplejson-3.20.1-py3.9-linux-x86_64.egg.simplejson.scanner._scan_once( string, idx )"
                }
            ]
        }
    ]
}